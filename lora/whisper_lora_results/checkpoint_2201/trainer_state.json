{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2201,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00045433893684688776,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 8.0023,
      "step": 1
    },
    {
      "epoch": 0.0009086778736937755,
      "grad_norm": NaN,
      "learning_rate": 2e-05,
      "loss": 8.3776,
      "step": 2
    },
    {
      "epoch": 0.0013630168105406633,
      "grad_norm": NaN,
      "learning_rate": 4e-05,
      "loss": 8.2424,
      "step": 3
    },
    {
      "epoch": 0.001817355747387551,
      "grad_norm": 25.824222564697266,
      "learning_rate": 6e-05,
      "loss": 8.5806,
      "step": 4
    },
    {
      "epoch": 0.002271694684234439,
      "grad_norm": 19.027311325073242,
      "learning_rate": 8e-05,
      "loss": 7.2914,
      "step": 5
    },
    {
      "epoch": 0.0027260336210813267,
      "grad_norm": 26.578123092651367,
      "learning_rate": 0.0001,
      "loss": 9.4662,
      "step": 6
    },
    {
      "epoch": 0.0031803725579282144,
      "grad_norm": 25.66640853881836,
      "learning_rate": 0.00012,
      "loss": 8.0139,
      "step": 7
    },
    {
      "epoch": 0.003634711494775102,
      "grad_norm": 16.478614807128906,
      "learning_rate": 0.00014000000000000001,
      "loss": 7.829,
      "step": 8
    },
    {
      "epoch": 0.00408905043162199,
      "grad_norm": 17.312732696533203,
      "learning_rate": 0.00016,
      "loss": 6.7323,
      "step": 9
    },
    {
      "epoch": 0.004543389368468878,
      "grad_norm": 8.53313159942627,
      "learning_rate": 0.00017999999999999998,
      "loss": 6.2305,
      "step": 10
    },
    {
      "epoch": 0.004997728305315766,
      "grad_norm": 7.02564811706543,
      "learning_rate": 0.0002,
      "loss": 6.3462,
      "step": 11
    },
    {
      "epoch": 0.005452067242162653,
      "grad_norm": 5.80635404586792,
      "learning_rate": 0.00022,
      "loss": 5.7629,
      "step": 12
    },
    {
      "epoch": 0.005906406179009541,
      "grad_norm": 3.673982858657837,
      "learning_rate": 0.00024,
      "loss": 5.7599,
      "step": 13
    },
    {
      "epoch": 0.006360745115856429,
      "grad_norm": 3.477905750274658,
      "learning_rate": 0.00026000000000000003,
      "loss": 5.1999,
      "step": 14
    },
    {
      "epoch": 0.0068150840527033164,
      "grad_norm": 3.6834943294525146,
      "learning_rate": 0.00028000000000000003,
      "loss": 5.7998,
      "step": 15
    },
    {
      "epoch": 0.007269422989550204,
      "grad_norm": 4.192676067352295,
      "learning_rate": 0.0003,
      "loss": 4.9561,
      "step": 16
    },
    {
      "epoch": 0.007723761926397092,
      "grad_norm": 4.50867223739624,
      "learning_rate": 0.00032,
      "loss": 4.863,
      "step": 17
    },
    {
      "epoch": 0.00817810086324398,
      "grad_norm": 4.193966388702393,
      "learning_rate": 0.00034,
      "loss": 4.7476,
      "step": 18
    },
    {
      "epoch": 0.008632439800090867,
      "grad_norm": 24.335716247558594,
      "learning_rate": 0.00035999999999999997,
      "loss": 5.2606,
      "step": 19
    },
    {
      "epoch": 0.009086778736937756,
      "grad_norm": 5.41240930557251,
      "learning_rate": 0.00038,
      "loss": 4.5987,
      "step": 20
    },
    {
      "epoch": 0.009541117673784643,
      "grad_norm": 25.38216781616211,
      "learning_rate": 0.0004,
      "loss": 5.1304,
      "step": 21
    },
    {
      "epoch": 0.009995456610631531,
      "grad_norm": NaN,
      "learning_rate": 0.00042,
      "loss": 6.0407,
      "step": 22
    },
    {
      "epoch": 0.010449795547478418,
      "grad_norm": 34.17319107055664,
      "learning_rate": 0.00044,
      "loss": 6.8801,
      "step": 23
    },
    {
      "epoch": 0.010904134484325307,
      "grad_norm": 16.081523895263672,
      "learning_rate": 0.00046,
      "loss": 9.3468,
      "step": 24
    },
    {
      "epoch": 0.011358473421172195,
      "grad_norm": NaN,
      "learning_rate": 0.00048,
      "loss": 13.2617,
      "step": 25
    },
    {
      "epoch": 0.011812812358019082,
      "grad_norm": 121.80638122558594,
      "learning_rate": 0.0005,
      "loss": 12.4593,
      "step": 26
    },
    {
      "epoch": 0.01226715129486597,
      "grad_norm": 118.46707153320312,
      "learning_rate": 0.0005200000000000001,
      "loss": 14.8211,
      "step": 27
    },
    {
      "epoch": 0.012721490231712857,
      "grad_norm": 38.67084503173828,
      "learning_rate": 0.00054,
      "loss": 8.8595,
      "step": 28
    },
    {
      "epoch": 0.013175829168559746,
      "grad_norm": 129.6434783935547,
      "learning_rate": 0.0005600000000000001,
      "loss": 13.0564,
      "step": 29
    },
    {
      "epoch": 0.013630168105406633,
      "grad_norm": 152.58070373535156,
      "learning_rate": 0.00058,
      "loss": 13.632,
      "step": 30
    },
    {
      "epoch": 0.014084507042253521,
      "grad_norm": Infinity,
      "learning_rate": 0.0006,
      "loss": 15.2205,
      "step": 31
    },
    {
      "epoch": 0.014538845979100408,
      "grad_norm": 128.71963500976562,
      "learning_rate": 0.00062,
      "loss": 13.2465,
      "step": 32
    },
    {
      "epoch": 0.014993184915947297,
      "grad_norm": 67.63313293457031,
      "learning_rate": 0.00064,
      "loss": 10.1143,
      "step": 33
    },
    {
      "epoch": 0.015447523852794184,
      "grad_norm": Infinity,
      "learning_rate": 0.00066,
      "loss": 12.0679,
      "step": 34
    },
    {
      "epoch": 0.01590186278964107,
      "grad_norm": 44.58478546142578,
      "learning_rate": 0.00068,
      "loss": 12.7381,
      "step": 35
    },
    {
      "epoch": 0.01635620172648796,
      "grad_norm": 129.38858032226562,
      "learning_rate": 0.0007,
      "loss": 12.162,
      "step": 36
    },
    {
      "epoch": 0.016810540663334848,
      "grad_norm": 37.70536422729492,
      "learning_rate": 0.0007199999999999999,
      "loss": 11.085,
      "step": 37
    },
    {
      "epoch": 0.017264879600181735,
      "grad_norm": 138.8385009765625,
      "learning_rate": 0.00074,
      "loss": 10.5558,
      "step": 38
    },
    {
      "epoch": 0.017719218537028625,
      "grad_norm": 526.0149536132812,
      "learning_rate": 0.00076,
      "loss": 9.0135,
      "step": 39
    },
    {
      "epoch": 0.01817355747387551,
      "grad_norm": 29.567760467529297,
      "learning_rate": 0.0007800000000000001,
      "loss": 9.3933,
      "step": 40
    },
    {
      "epoch": 0.0186278964107224,
      "grad_norm": 12.656719207763672,
      "learning_rate": 0.0008,
      "loss": 9.9319,
      "step": 41
    },
    {
      "epoch": 0.019082235347569285,
      "grad_norm": 60.86611557006836,
      "learning_rate": 0.00082,
      "loss": 8.8919,
      "step": 42
    },
    {
      "epoch": 0.019536574284416176,
      "grad_norm": 20.097326278686523,
      "learning_rate": 0.00084,
      "loss": 9.1112,
      "step": 43
    },
    {
      "epoch": 0.019990913221263062,
      "grad_norm": 250.72438049316406,
      "learning_rate": 0.00086,
      "loss": 11.8096,
      "step": 44
    },
    {
      "epoch": 0.02044525215810995,
      "grad_norm": 112.64694213867188,
      "learning_rate": 0.00088,
      "loss": 11.3893,
      "step": 45
    },
    {
      "epoch": 0.020899591094956836,
      "grad_norm": 10.068092346191406,
      "learning_rate": 0.0009000000000000001,
      "loss": 7.7605,
      "step": 46
    },
    {
      "epoch": 0.021353930031803726,
      "grad_norm": 26.70404815673828,
      "learning_rate": 0.00092,
      "loss": 7.8564,
      "step": 47
    },
    {
      "epoch": 0.021808268968650613,
      "grad_norm": 209.85520935058594,
      "learning_rate": 0.00094,
      "loss": 10.3645,
      "step": 48
    },
    {
      "epoch": 0.0222626079054975,
      "grad_norm": 4.865591526031494,
      "learning_rate": 0.00096,
      "loss": 7.9674,
      "step": 49
    },
    {
      "epoch": 0.02271694684234439,
      "grad_norm": 43.300193786621094,
      "learning_rate": 0.00098,
      "loss": 10.9355,
      "step": 50
    },
    {
      "epoch": 0.023171285779191277,
      "grad_norm": 22.19122314453125,
      "learning_rate": 0.001,
      "loss": 9.4466,
      "step": 51
    },
    {
      "epoch": 0.023625624716038164,
      "grad_norm": 22.118881225585938,
      "learning_rate": 0.0009998473981382573,
      "loss": 9.6055,
      "step": 52
    },
    {
      "epoch": 0.02407996365288505,
      "grad_norm": 9.091259956359863,
      "learning_rate": 0.0009996947962765145,
      "loss": 10.048,
      "step": 53
    },
    {
      "epoch": 0.02453430258973194,
      "grad_norm": 16.8380126953125,
      "learning_rate": 0.000999542194414772,
      "loss": 11.4062,
      "step": 54
    },
    {
      "epoch": 0.024988641526578828,
      "grad_norm": 4.073796272277832,
      "learning_rate": 0.0009993895925530293,
      "loss": 10.9784,
      "step": 55
    },
    {
      "epoch": 0.025442980463425715,
      "grad_norm": 47.10187911987305,
      "learning_rate": 0.0009992369906912865,
      "loss": 8.6613,
      "step": 56
    },
    {
      "epoch": 0.025897319400272602,
      "grad_norm": 23.968469619750977,
      "learning_rate": 0.0009990843888295438,
      "loss": 9.575,
      "step": 57
    },
    {
      "epoch": 0.026351658337119492,
      "grad_norm": 119.49044799804688,
      "learning_rate": 0.000998931786967801,
      "loss": 9.5518,
      "step": 58
    },
    {
      "epoch": 0.02680599727396638,
      "grad_norm": 7.898088455200195,
      "learning_rate": 0.0009987791851060583,
      "loss": 8.6571,
      "step": 59
    },
    {
      "epoch": 0.027260336210813266,
      "grad_norm": 38.10663986206055,
      "learning_rate": 0.0009986265832443158,
      "loss": 6.4335,
      "step": 60
    },
    {
      "epoch": 0.027714675147660156,
      "grad_norm": 10.954629898071289,
      "learning_rate": 0.0009984739813825728,
      "loss": 7.4847,
      "step": 61
    },
    {
      "epoch": 0.028169014084507043,
      "grad_norm": 5.64437198638916,
      "learning_rate": 0.00099832137952083,
      "loss": 7.3038,
      "step": 62
    },
    {
      "epoch": 0.02862335302135393,
      "grad_norm": 6.3597822189331055,
      "learning_rate": 0.0009981687776590875,
      "loss": 6.3598,
      "step": 63
    },
    {
      "epoch": 0.029077691958200817,
      "grad_norm": 4.8628644943237305,
      "learning_rate": 0.0009980161757973448,
      "loss": 5.5706,
      "step": 64
    },
    {
      "epoch": 0.029532030895047707,
      "grad_norm": 4.439299583435059,
      "learning_rate": 0.000997863573935602,
      "loss": 5.6658,
      "step": 65
    },
    {
      "epoch": 0.029986369831894594,
      "grad_norm": 5.823978424072266,
      "learning_rate": 0.0009977109720738593,
      "loss": 5.7194,
      "step": 66
    },
    {
      "epoch": 0.03044070876874148,
      "grad_norm": 4.534936904907227,
      "learning_rate": 0.0009975583702121166,
      "loss": 4.2016,
      "step": 67
    },
    {
      "epoch": 0.030895047705588367,
      "grad_norm": 28.568662643432617,
      "learning_rate": 0.0009974057683503738,
      "loss": 4.9745,
      "step": 68
    },
    {
      "epoch": 0.03134938664243526,
      "grad_norm": 124.42140197753906,
      "learning_rate": 0.0009972531664886313,
      "loss": 5.4021,
      "step": 69
    },
    {
      "epoch": 0.03180372557928214,
      "grad_norm": 4.23909854888916,
      "learning_rate": 0.0009971005646268885,
      "loss": 5.1491,
      "step": 70
    },
    {
      "epoch": 0.03225806451612903,
      "grad_norm": 4.771326541900635,
      "learning_rate": 0.0009969479627651458,
      "loss": 4.5353,
      "step": 71
    },
    {
      "epoch": 0.03271240345297592,
      "grad_norm": 4.875744342803955,
      "learning_rate": 0.000996795360903403,
      "loss": 5.2877,
      "step": 72
    },
    {
      "epoch": 0.033166742389822805,
      "grad_norm": 4.559694766998291,
      "learning_rate": 0.0009966427590416603,
      "loss": 5.2964,
      "step": 73
    },
    {
      "epoch": 0.033621081326669695,
      "grad_norm": 4.016839981079102,
      "learning_rate": 0.0009964901571799176,
      "loss": 4.2866,
      "step": 74
    },
    {
      "epoch": 0.034075420263516586,
      "grad_norm": 5.4447922706604,
      "learning_rate": 0.000996337555318175,
      "loss": 4.6195,
      "step": 75
    },
    {
      "epoch": 0.03452975920036347,
      "grad_norm": 3.736846446990967,
      "learning_rate": 0.0009961849534564323,
      "loss": 4.4598,
      "step": 76
    },
    {
      "epoch": 0.03498409813721036,
      "grad_norm": 6.394325256347656,
      "learning_rate": 0.0009960323515946896,
      "loss": 4.386,
      "step": 77
    },
    {
      "epoch": 0.03543843707405725,
      "grad_norm": 3.9506008625030518,
      "learning_rate": 0.0009958797497329468,
      "loss": 6.0491,
      "step": 78
    },
    {
      "epoch": 0.03589277601090413,
      "grad_norm": 3.4684560298919678,
      "learning_rate": 0.000995727147871204,
      "loss": 4.7314,
      "step": 79
    },
    {
      "epoch": 0.03634711494775102,
      "grad_norm": 6.110920429229736,
      "learning_rate": 0.0009955745460094613,
      "loss": 6.2104,
      "step": 80
    },
    {
      "epoch": 0.03680145388459791,
      "grad_norm": 4.684608459472656,
      "learning_rate": 0.0009954219441477186,
      "loss": 5.209,
      "step": 81
    },
    {
      "epoch": 0.0372557928214448,
      "grad_norm": 6.747181415557861,
      "learning_rate": 0.0009952693422859758,
      "loss": 4.6792,
      "step": 82
    },
    {
      "epoch": 0.03771013175829169,
      "grad_norm": 4.552679538726807,
      "learning_rate": 0.000995116740424233,
      "loss": 5.3779,
      "step": 83
    },
    {
      "epoch": 0.03816447069513857,
      "grad_norm": 4.314069747924805,
      "learning_rate": 0.0009949641385624906,
      "loss": 4.5104,
      "step": 84
    },
    {
      "epoch": 0.03861880963198546,
      "grad_norm": 6.020544528961182,
      "learning_rate": 0.0009948115367007478,
      "loss": 6.0825,
      "step": 85
    },
    {
      "epoch": 0.03907314856883235,
      "grad_norm": 4.898756980895996,
      "learning_rate": 0.000994658934839005,
      "loss": 5.3171,
      "step": 86
    },
    {
      "epoch": 0.039527487505679235,
      "grad_norm": 4.783594131469727,
      "learning_rate": 0.0009945063329772623,
      "loss": 4.2888,
      "step": 87
    },
    {
      "epoch": 0.039981826442526125,
      "grad_norm": 5.4161224365234375,
      "learning_rate": 0.0009943537311155196,
      "loss": 3.889,
      "step": 88
    },
    {
      "epoch": 0.040436165379373015,
      "grad_norm": 14.11226749420166,
      "learning_rate": 0.0009942011292537768,
      "loss": 4.3414,
      "step": 89
    },
    {
      "epoch": 0.0408905043162199,
      "grad_norm": 5.149744510650635,
      "learning_rate": 0.0009940485273920343,
      "loss": 4.0926,
      "step": 90
    },
    {
      "epoch": 0.04134484325306679,
      "grad_norm": 6.219616889953613,
      "learning_rate": 0.0009938959255302916,
      "loss": 5.0,
      "step": 91
    },
    {
      "epoch": 0.04179918218991367,
      "grad_norm": 4.310451507568359,
      "learning_rate": 0.0009937433236685488,
      "loss": 3.888,
      "step": 92
    },
    {
      "epoch": 0.04225352112676056,
      "grad_norm": 6.236973762512207,
      "learning_rate": 0.000993590721806806,
      "loss": 3.3396,
      "step": 93
    },
    {
      "epoch": 0.04270786006360745,
      "grad_norm": 4.45758581161499,
      "learning_rate": 0.0009934381199450633,
      "loss": 4.9513,
      "step": 94
    },
    {
      "epoch": 0.043162199000454336,
      "grad_norm": 4.142629146575928,
      "learning_rate": 0.0009932855180833206,
      "loss": 4.1621,
      "step": 95
    },
    {
      "epoch": 0.04361653793730123,
      "grad_norm": 4.4617919921875,
      "learning_rate": 0.000993132916221578,
      "loss": 4.0225,
      "step": 96
    },
    {
      "epoch": 0.04407087687414812,
      "grad_norm": 3.3871145248413086,
      "learning_rate": 0.0009929803143598351,
      "loss": 4.771,
      "step": 97
    },
    {
      "epoch": 0.044525215810995,
      "grad_norm": 5.589167594909668,
      "learning_rate": 0.0009928277124980924,
      "loss": 4.5124,
      "step": 98
    },
    {
      "epoch": 0.04497955474784189,
      "grad_norm": 4.178234100341797,
      "learning_rate": 0.0009926751106363498,
      "loss": 4.8029,
      "step": 99
    },
    {
      "epoch": 0.04543389368468878,
      "grad_norm": 6.137523651123047,
      "learning_rate": 0.000992522508774607,
      "loss": 4.0348,
      "step": 100
    },
    {
      "epoch": 0.045888232621535664,
      "grad_norm": 6.350948810577393,
      "learning_rate": 0.0009923699069128644,
      "loss": 5.2349,
      "step": 101
    },
    {
      "epoch": 0.046342571558382555,
      "grad_norm": 2.972569704055786,
      "learning_rate": 0.0009922173050511216,
      "loss": 4.3659,
      "step": 102
    },
    {
      "epoch": 0.04679691049522944,
      "grad_norm": 18.598766326904297,
      "learning_rate": 0.0009920647031893789,
      "loss": 5.4895,
      "step": 103
    },
    {
      "epoch": 0.04725124943207633,
      "grad_norm": 2.7093234062194824,
      "learning_rate": 0.0009919121013276361,
      "loss": 3.8883,
      "step": 104
    },
    {
      "epoch": 0.04770558836892322,
      "grad_norm": 5.899909496307373,
      "learning_rate": 0.0009917594994658936,
      "loss": 5.7114,
      "step": 105
    },
    {
      "epoch": 0.0481599273057701,
      "grad_norm": 11.964067459106445,
      "learning_rate": 0.0009916068976041509,
      "loss": 5.3596,
      "step": 106
    },
    {
      "epoch": 0.04861426624261699,
      "grad_norm": 3.6703925132751465,
      "learning_rate": 0.0009914542957424081,
      "loss": 4.5971,
      "step": 107
    },
    {
      "epoch": 0.04906860517946388,
      "grad_norm": 5.376031875610352,
      "learning_rate": 0.0009913016938806654,
      "loss": 4.906,
      "step": 108
    },
    {
      "epoch": 0.049522944116310766,
      "grad_norm": 3.6859614849090576,
      "learning_rate": 0.0009911490920189226,
      "loss": 5.1568,
      "step": 109
    },
    {
      "epoch": 0.049977283053157656,
      "grad_norm": 4.364647388458252,
      "learning_rate": 0.0009909964901571799,
      "loss": 4.7153,
      "step": 110
    },
    {
      "epoch": 0.050431621990004546,
      "grad_norm": 13.444659233093262,
      "learning_rate": 0.0009908438882954374,
      "loss": 6.3371,
      "step": 111
    },
    {
      "epoch": 0.05088596092685143,
      "grad_norm": 12.527302742004395,
      "learning_rate": 0.0009906912864336946,
      "loss": 5.2483,
      "step": 112
    },
    {
      "epoch": 0.05134029986369832,
      "grad_norm": 1.416824221611023,
      "learning_rate": 0.0009905386845719519,
      "loss": 4.328,
      "step": 113
    },
    {
      "epoch": 0.051794638800545204,
      "grad_norm": 4.218554496765137,
      "learning_rate": 0.0009903860827102091,
      "loss": 4.3904,
      "step": 114
    },
    {
      "epoch": 0.052248977737392094,
      "grad_norm": 5.373893737792969,
      "learning_rate": 0.0009902334808484664,
      "loss": 5.1213,
      "step": 115
    },
    {
      "epoch": 0.052703316674238984,
      "grad_norm": 5.558022499084473,
      "learning_rate": 0.0009900808789867236,
      "loss": 4.2808,
      "step": 116
    },
    {
      "epoch": 0.05315765561108587,
      "grad_norm": 3.5679633617401123,
      "learning_rate": 0.000989928277124981,
      "loss": 5.6393,
      "step": 117
    },
    {
      "epoch": 0.05361199454793276,
      "grad_norm": 3.5981521606445312,
      "learning_rate": 0.0009897756752632382,
      "loss": 4.6271,
      "step": 118
    },
    {
      "epoch": 0.05406633348477965,
      "grad_norm": 3.3493311405181885,
      "learning_rate": 0.0009896230734014954,
      "loss": 4.1889,
      "step": 119
    },
    {
      "epoch": 0.05452067242162653,
      "grad_norm": 4.732795238494873,
      "learning_rate": 0.0009894704715397529,
      "loss": 5.1547,
      "step": 120
    },
    {
      "epoch": 0.05497501135847342,
      "grad_norm": 5.440881252288818,
      "learning_rate": 0.0009893178696780101,
      "loss": 4.1898,
      "step": 121
    },
    {
      "epoch": 0.05542935029532031,
      "grad_norm": 6.5941901206970215,
      "learning_rate": 0.0009891652678162674,
      "loss": 3.9222,
      "step": 122
    },
    {
      "epoch": 0.055883689232167195,
      "grad_norm": 3.764658212661743,
      "learning_rate": 0.0009890126659545246,
      "loss": 5.2241,
      "step": 123
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 5.435695171356201,
      "learning_rate": 0.000988860064092782,
      "loss": 5.3936,
      "step": 124
    },
    {
      "epoch": 0.05679236710586097,
      "grad_norm": 12.230674743652344,
      "learning_rate": 0.0009887074622310392,
      "loss": 3.7325,
      "step": 125
    },
    {
      "epoch": 0.05724670604270786,
      "grad_norm": 7.476783752441406,
      "learning_rate": 0.0009885548603692966,
      "loss": 4.1259,
      "step": 126
    },
    {
      "epoch": 0.05770104497955475,
      "grad_norm": 7.368002414703369,
      "learning_rate": 0.000988402258507554,
      "loss": 5.4554,
      "step": 127
    },
    {
      "epoch": 0.05815538391640163,
      "grad_norm": 3.7337493896484375,
      "learning_rate": 0.0009882496566458111,
      "loss": 4.0602,
      "step": 128
    },
    {
      "epoch": 0.05860972285324852,
      "grad_norm": 12.5004301071167,
      "learning_rate": 0.0009880970547840684,
      "loss": 4.8755,
      "step": 129
    },
    {
      "epoch": 0.059064061790095414,
      "grad_norm": 3.43620228767395,
      "learning_rate": 0.0009879444529223257,
      "loss": 3.4166,
      "step": 130
    },
    {
      "epoch": 0.0595184007269423,
      "grad_norm": 36.46588134765625,
      "learning_rate": 0.000987791851060583,
      "loss": 4.6218,
      "step": 131
    },
    {
      "epoch": 0.05997273966378919,
      "grad_norm": 8.751091957092285,
      "learning_rate": 0.0009876392491988404,
      "loss": 5.39,
      "step": 132
    },
    {
      "epoch": 0.06042707860063608,
      "grad_norm": 8.155447959899902,
      "learning_rate": 0.0009874866473370976,
      "loss": 4.5694,
      "step": 133
    },
    {
      "epoch": 0.06088141753748296,
      "grad_norm": 8.671798706054688,
      "learning_rate": 0.0009873340454753547,
      "loss": 5.0133,
      "step": 134
    },
    {
      "epoch": 0.06133575647432985,
      "grad_norm": 8.39020824432373,
      "learning_rate": 0.0009871814436136122,
      "loss": 4.3537,
      "step": 135
    },
    {
      "epoch": 0.061790095411176735,
      "grad_norm": 6.358360767364502,
      "learning_rate": 0.0009870288417518694,
      "loss": 4.2879,
      "step": 136
    },
    {
      "epoch": 0.062244434348023625,
      "grad_norm": 6.627678871154785,
      "learning_rate": 0.0009868762398901267,
      "loss": 3.6809,
      "step": 137
    },
    {
      "epoch": 0.06269877328487052,
      "grad_norm": 3.454014778137207,
      "learning_rate": 0.000986723638028384,
      "loss": 4.529,
      "step": 138
    },
    {
      "epoch": 0.0631531122217174,
      "grad_norm": 3.807094097137451,
      "learning_rate": 0.0009865710361666412,
      "loss": 3.8101,
      "step": 139
    },
    {
      "epoch": 0.06360745115856428,
      "grad_norm": 8.410943031311035,
      "learning_rate": 0.0009864184343048984,
      "loss": 4.2768,
      "step": 140
    },
    {
      "epoch": 0.06406179009541117,
      "grad_norm": 3.9943816661834717,
      "learning_rate": 0.000986265832443156,
      "loss": 3.8826,
      "step": 141
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 4.774807929992676,
      "learning_rate": 0.0009861132305814132,
      "loss": 2.8197,
      "step": 142
    },
    {
      "epoch": 0.06497046796910495,
      "grad_norm": 3.271233558654785,
      "learning_rate": 0.0009859606287196704,
      "loss": 3.6229,
      "step": 143
    },
    {
      "epoch": 0.06542480690595184,
      "grad_norm": 2.5481090545654297,
      "learning_rate": 0.0009858080268579277,
      "loss": 3.4013,
      "step": 144
    },
    {
      "epoch": 0.06587914584279873,
      "grad_norm": 8.705131530761719,
      "learning_rate": 0.000985655424996185,
      "loss": 4.6506,
      "step": 145
    },
    {
      "epoch": 0.06633348477964561,
      "grad_norm": 4.1458916664123535,
      "learning_rate": 0.0009855028231344422,
      "loss": 4.6174,
      "step": 146
    },
    {
      "epoch": 0.0667878237164925,
      "grad_norm": 4.918854236602783,
      "learning_rate": 0.0009853502212726997,
      "loss": 3.9963,
      "step": 147
    },
    {
      "epoch": 0.06724216265333939,
      "grad_norm": 5.896379470825195,
      "learning_rate": 0.000985197619410957,
      "loss": 3.2582,
      "step": 148
    },
    {
      "epoch": 0.06769650159018628,
      "grad_norm": 4.8517022132873535,
      "learning_rate": 0.0009850450175492142,
      "loss": 4.0775,
      "step": 149
    },
    {
      "epoch": 0.06815084052703317,
      "grad_norm": 4.007078170776367,
      "learning_rate": 0.0009848924156874714,
      "loss": 4.1657,
      "step": 150
    },
    {
      "epoch": 0.06860517946388005,
      "grad_norm": 4.619518280029297,
      "learning_rate": 0.0009847398138257287,
      "loss": 3.6541,
      "step": 151
    },
    {
      "epoch": 0.06905951840072694,
      "grad_norm": 5.512119770050049,
      "learning_rate": 0.000984587211963986,
      "loss": 3.4678,
      "step": 152
    },
    {
      "epoch": 0.06951385733757383,
      "grad_norm": 4.172642230987549,
      "learning_rate": 0.0009844346101022432,
      "loss": 3.8443,
      "step": 153
    },
    {
      "epoch": 0.06996819627442072,
      "grad_norm": 7.289047718048096,
      "learning_rate": 0.0009842820082405005,
      "loss": 3.9504,
      "step": 154
    },
    {
      "epoch": 0.07042253521126761,
      "grad_norm": 17.154674530029297,
      "learning_rate": 0.0009841294063787577,
      "loss": 3.243,
      "step": 155
    },
    {
      "epoch": 0.0708768741481145,
      "grad_norm": 4.262900352478027,
      "learning_rate": 0.0009839768045170152,
      "loss": 3.3881,
      "step": 156
    },
    {
      "epoch": 0.07133121308496138,
      "grad_norm": 10.729443550109863,
      "learning_rate": 0.0009838242026552725,
      "loss": 4.8154,
      "step": 157
    },
    {
      "epoch": 0.07178555202180827,
      "grad_norm": 37.1884880065918,
      "learning_rate": 0.0009836716007935297,
      "loss": 3.9405,
      "step": 158
    },
    {
      "epoch": 0.07223989095865516,
      "grad_norm": 4.59852409362793,
      "learning_rate": 0.000983518998931787,
      "loss": 2.7521,
      "step": 159
    },
    {
      "epoch": 0.07269422989550205,
      "grad_norm": 4.466158390045166,
      "learning_rate": 0.0009833663970700442,
      "loss": 4.0475,
      "step": 160
    },
    {
      "epoch": 0.07314856883234894,
      "grad_norm": 2.4084692001342773,
      "learning_rate": 0.0009832137952083015,
      "loss": 3.2675,
      "step": 161
    },
    {
      "epoch": 0.07360290776919581,
      "grad_norm": 4.966616153717041,
      "learning_rate": 0.000983061193346559,
      "loss": 5.1032,
      "step": 162
    },
    {
      "epoch": 0.0740572467060427,
      "grad_norm": 3.4630465507507324,
      "learning_rate": 0.0009829085914848162,
      "loss": 3.749,
      "step": 163
    },
    {
      "epoch": 0.0745115856428896,
      "grad_norm": 7.548681259155273,
      "learning_rate": 0.0009827559896230735,
      "loss": 4.1643,
      "step": 164
    },
    {
      "epoch": 0.07496592457973648,
      "grad_norm": 4.637468338012695,
      "learning_rate": 0.0009826033877613307,
      "loss": 4.6352,
      "step": 165
    },
    {
      "epoch": 0.07542026351658337,
      "grad_norm": 393.98614501953125,
      "learning_rate": 0.000982450785899588,
      "loss": 3.7115,
      "step": 166
    },
    {
      "epoch": 0.07587460245343026,
      "grad_norm": 5.763677597045898,
      "learning_rate": 0.0009822981840378452,
      "loss": 3.7971,
      "step": 167
    },
    {
      "epoch": 0.07632894139027714,
      "grad_norm": 3.4287073612213135,
      "learning_rate": 0.0009821455821761027,
      "loss": 4.2323,
      "step": 168
    },
    {
      "epoch": 0.07678328032712403,
      "grad_norm": 3.8936452865600586,
      "learning_rate": 0.00098199298031436,
      "loss": 4.6735,
      "step": 169
    },
    {
      "epoch": 0.07723761926397092,
      "grad_norm": 5.318809986114502,
      "learning_rate": 0.000981840378452617,
      "loss": 4.2489,
      "step": 170
    },
    {
      "epoch": 0.07769195820081781,
      "grad_norm": 8.321727752685547,
      "learning_rate": 0.0009816877765908745,
      "loss": 3.9057,
      "step": 171
    },
    {
      "epoch": 0.0781462971376647,
      "grad_norm": Infinity,
      "learning_rate": 0.0009815351747291317,
      "loss": 3.5756,
      "step": 172
    },
    {
      "epoch": 0.07860063607451158,
      "grad_norm": 3.2687430381774902,
      "learning_rate": 0.000981382572867389,
      "loss": 4.2894,
      "step": 173
    },
    {
      "epoch": 0.07905497501135847,
      "grad_norm": 5.355385780334473,
      "learning_rate": 0.0009812299710056462,
      "loss": 3.8189,
      "step": 174
    },
    {
      "epoch": 0.07950931394820536,
      "grad_norm": 8.080278396606445,
      "learning_rate": 0.0009810773691439035,
      "loss": 4.8145,
      "step": 175
    },
    {
      "epoch": 0.07996365288505225,
      "grad_norm": 5.8763427734375,
      "learning_rate": 0.0009809247672821608,
      "loss": 4.8995,
      "step": 176
    },
    {
      "epoch": 0.08041799182189914,
      "grad_norm": 4.652189254760742,
      "learning_rate": 0.0009807721654204182,
      "loss": 4.1373,
      "step": 177
    },
    {
      "epoch": 0.08087233075874603,
      "grad_norm": 4.947461128234863,
      "learning_rate": 0.0009806195635586755,
      "loss": 3.9209,
      "step": 178
    },
    {
      "epoch": 0.0813266696955929,
      "grad_norm": 6.76459264755249,
      "learning_rate": 0.0009804669616969327,
      "loss": 3.506,
      "step": 179
    },
    {
      "epoch": 0.0817810086324398,
      "grad_norm": 4.689905166625977,
      "learning_rate": 0.00098031435983519,
      "loss": 4.7262,
      "step": 180
    },
    {
      "epoch": 0.08223534756928669,
      "grad_norm": 23.074615478515625,
      "learning_rate": 0.0009801617579734473,
      "loss": 5.1583,
      "step": 181
    },
    {
      "epoch": 0.08268968650613358,
      "grad_norm": 2.692385196685791,
      "learning_rate": 0.0009800091561117045,
      "loss": 2.9736,
      "step": 182
    },
    {
      "epoch": 0.08314402544298047,
      "grad_norm": 4.335456371307373,
      "learning_rate": 0.000979856554249962,
      "loss": 3.1265,
      "step": 183
    },
    {
      "epoch": 0.08359836437982734,
      "grad_norm": 2.8782010078430176,
      "learning_rate": 0.0009797039523882192,
      "loss": 4.1927,
      "step": 184
    },
    {
      "epoch": 0.08405270331667423,
      "grad_norm": 4.179201126098633,
      "learning_rate": 0.0009795513505264765,
      "loss": 3.8804,
      "step": 185
    },
    {
      "epoch": 0.08450704225352113,
      "grad_norm": 4.887032508850098,
      "learning_rate": 0.0009793987486647338,
      "loss": 4.7505,
      "step": 186
    },
    {
      "epoch": 0.08496138119036802,
      "grad_norm": 5.900402545928955,
      "learning_rate": 0.000979246146802991,
      "loss": 3.1536,
      "step": 187
    },
    {
      "epoch": 0.0854157201272149,
      "grad_norm": 3.810967206954956,
      "learning_rate": 0.0009790935449412483,
      "loss": 2.6752,
      "step": 188
    },
    {
      "epoch": 0.0858700590640618,
      "grad_norm": 5.397327899932861,
      "learning_rate": 0.0009789409430795055,
      "loss": 3.947,
      "step": 189
    },
    {
      "epoch": 0.08632439800090867,
      "grad_norm": 3.058849334716797,
      "learning_rate": 0.0009787883412177628,
      "loss": 3.9808,
      "step": 190
    },
    {
      "epoch": 0.08677873693775556,
      "grad_norm": 4.307729721069336,
      "learning_rate": 0.00097863573935602,
      "loss": 3.5107,
      "step": 191
    },
    {
      "epoch": 0.08723307587460245,
      "grad_norm": 6.400062084197998,
      "learning_rate": 0.0009784831374942775,
      "loss": 4.9501,
      "step": 192
    },
    {
      "epoch": 0.08768741481144934,
      "grad_norm": 3.6810007095336914,
      "learning_rate": 0.0009783305356325348,
      "loss": 4.6354,
      "step": 193
    },
    {
      "epoch": 0.08814175374829623,
      "grad_norm": 5.11810302734375,
      "learning_rate": 0.000978177933770792,
      "loss": 4.2619,
      "step": 194
    },
    {
      "epoch": 0.08859609268514311,
      "grad_norm": 6.085944175720215,
      "learning_rate": 0.0009780253319090493,
      "loss": 3.658,
      "step": 195
    },
    {
      "epoch": 0.08905043162199,
      "grad_norm": 13.353829383850098,
      "learning_rate": 0.0009778727300473065,
      "loss": 3.7545,
      "step": 196
    },
    {
      "epoch": 0.08950477055883689,
      "grad_norm": 5.361572742462158,
      "learning_rate": 0.0009777201281855638,
      "loss": 4.0232,
      "step": 197
    },
    {
      "epoch": 0.08995910949568378,
      "grad_norm": 7.019571304321289,
      "learning_rate": 0.0009775675263238213,
      "loss": 3.2647,
      "step": 198
    },
    {
      "epoch": 0.09041344843253067,
      "grad_norm": 3.7887091636657715,
      "learning_rate": 0.0009774149244620785,
      "loss": 2.7459,
      "step": 199
    },
    {
      "epoch": 0.09086778736937756,
      "grad_norm": 4.476651191711426,
      "learning_rate": 0.0009772623226003358,
      "loss": 4.352,
      "step": 200
    },
    {
      "epoch": 0.09132212630622444,
      "grad_norm": 5.926980018615723,
      "learning_rate": 0.000977109720738593,
      "loss": 3.151,
      "step": 201
    },
    {
      "epoch": 0.09177646524307133,
      "grad_norm": 5.531089782714844,
      "learning_rate": 0.0009769571188768503,
      "loss": 3.8806,
      "step": 202
    },
    {
      "epoch": 0.09223080417991822,
      "grad_norm": 4.885206699371338,
      "learning_rate": 0.0009768045170151075,
      "loss": 4.3888,
      "step": 203
    },
    {
      "epoch": 0.09268514311676511,
      "grad_norm": 5.224144458770752,
      "learning_rate": 0.000976651915153365,
      "loss": 3.1822,
      "step": 204
    },
    {
      "epoch": 0.093139482053612,
      "grad_norm": 6.5018815994262695,
      "learning_rate": 0.0009764993132916223,
      "loss": 3.73,
      "step": 205
    },
    {
      "epoch": 0.09359382099045888,
      "grad_norm": 4.337461471557617,
      "learning_rate": 0.0009763467114298795,
      "loss": 2.4075,
      "step": 206
    },
    {
      "epoch": 0.09404815992730577,
      "grad_norm": 11.796463966369629,
      "learning_rate": 0.0009761941095681367,
      "loss": 3.2493,
      "step": 207
    },
    {
      "epoch": 0.09450249886415266,
      "grad_norm": 4.554281711578369,
      "learning_rate": 0.000976041507706394,
      "loss": 3.8821,
      "step": 208
    },
    {
      "epoch": 0.09495683780099955,
      "grad_norm": 6.080577373504639,
      "learning_rate": 0.0009758889058446513,
      "loss": 4.0082,
      "step": 209
    },
    {
      "epoch": 0.09541117673784644,
      "grad_norm": 5.826722621917725,
      "learning_rate": 0.0009757363039829086,
      "loss": 3.9422,
      "step": 210
    },
    {
      "epoch": 0.09586551567469333,
      "grad_norm": 6.585216045379639,
      "learning_rate": 0.0009755837021211659,
      "loss": 3.9036,
      "step": 211
    },
    {
      "epoch": 0.0963198546115402,
      "grad_norm": 21.507322311401367,
      "learning_rate": 0.0009754311002594232,
      "loss": 4.807,
      "step": 212
    },
    {
      "epoch": 0.0967741935483871,
      "grad_norm": 4.999459266662598,
      "learning_rate": 0.0009752784983976804,
      "loss": 3.6513,
      "step": 213
    },
    {
      "epoch": 0.09722853248523398,
      "grad_norm": 3.4748528003692627,
      "learning_rate": 0.0009751258965359378,
      "loss": 4.3142,
      "step": 214
    },
    {
      "epoch": 0.09768287142208087,
      "grad_norm": 5.2093915939331055,
      "learning_rate": 0.0009749732946741951,
      "loss": 3.3314,
      "step": 215
    },
    {
      "epoch": 0.09813721035892777,
      "grad_norm": 5.477571964263916,
      "learning_rate": 0.0009748206928124523,
      "loss": 4.4049,
      "step": 216
    },
    {
      "epoch": 0.09859154929577464,
      "grad_norm": 92.94070434570312,
      "learning_rate": 0.0009746680909507097,
      "loss": 2.7441,
      "step": 217
    },
    {
      "epoch": 0.09904588823262153,
      "grad_norm": 3.993104934692383,
      "learning_rate": 0.0009745154890889669,
      "loss": 4.6918,
      "step": 218
    },
    {
      "epoch": 0.09950022716946842,
      "grad_norm": 10.868013381958008,
      "learning_rate": 0.0009743628872272242,
      "loss": 2.665,
      "step": 219
    },
    {
      "epoch": 0.09995456610631531,
      "grad_norm": 6.979304790496826,
      "learning_rate": 0.0009742102853654816,
      "loss": 4.2163,
      "step": 220
    },
    {
      "epoch": 0.1004089050431622,
      "grad_norm": 4.047073841094971,
      "learning_rate": 0.0009740576835037388,
      "loss": 3.0042,
      "step": 221
    },
    {
      "epoch": 0.10086324398000909,
      "grad_norm": 5.500774383544922,
      "learning_rate": 0.0009739050816419961,
      "loss": 4.5916,
      "step": 222
    },
    {
      "epoch": 0.10131758291685597,
      "grad_norm": 22.360429763793945,
      "learning_rate": 0.0009737524797802534,
      "loss": 2.4437,
      "step": 223
    },
    {
      "epoch": 0.10177192185370286,
      "grad_norm": 5.232440948486328,
      "learning_rate": 0.0009735998779185107,
      "loss": 4.0674,
      "step": 224
    },
    {
      "epoch": 0.10222626079054975,
      "grad_norm": 15.816807746887207,
      "learning_rate": 0.0009734472760567678,
      "loss": 2.4081,
      "step": 225
    },
    {
      "epoch": 0.10268059972739664,
      "grad_norm": 7.023444652557373,
      "learning_rate": 0.0009732946741950252,
      "loss": 3.5343,
      "step": 226
    },
    {
      "epoch": 0.10313493866424353,
      "grad_norm": 8.967727661132812,
      "learning_rate": 0.0009731420723332825,
      "loss": 2.6659,
      "step": 227
    },
    {
      "epoch": 0.10358927760109041,
      "grad_norm": 13.048861503601074,
      "learning_rate": 0.0009729894704715397,
      "loss": 3.3763,
      "step": 228
    },
    {
      "epoch": 0.1040436165379373,
      "grad_norm": 8.219457626342773,
      "learning_rate": 0.0009728368686097971,
      "loss": 2.3,
      "step": 229
    },
    {
      "epoch": 0.10449795547478419,
      "grad_norm": 8.147345542907715,
      "learning_rate": 0.0009726842667480543,
      "loss": 2.6034,
      "step": 230
    },
    {
      "epoch": 0.10495229441163108,
      "grad_norm": 11.596321105957031,
      "learning_rate": 0.0009725316648863116,
      "loss": 2.2113,
      "step": 231
    },
    {
      "epoch": 0.10540663334847797,
      "grad_norm": 5.11240816116333,
      "learning_rate": 0.000972379063024569,
      "loss": 2.5929,
      "step": 232
    },
    {
      "epoch": 0.10586097228532486,
      "grad_norm": 5.214093208312988,
      "learning_rate": 0.0009722264611628262,
      "loss": 2.3972,
      "step": 233
    },
    {
      "epoch": 0.10631531122217174,
      "grad_norm": 7.715131759643555,
      "learning_rate": 0.0009720738593010835,
      "loss": 2.6193,
      "step": 234
    },
    {
      "epoch": 0.10676965015901863,
      "grad_norm": 7.7353010177612305,
      "learning_rate": 0.0009719212574393408,
      "loss": 2.8781,
      "step": 235
    },
    {
      "epoch": 0.10722398909586552,
      "grad_norm": 17.7259464263916,
      "learning_rate": 0.0009717686555775981,
      "loss": 2.2655,
      "step": 236
    },
    {
      "epoch": 0.1076783280327124,
      "grad_norm": 9.315775871276855,
      "learning_rate": 0.0009716160537158553,
      "loss": 2.162,
      "step": 237
    },
    {
      "epoch": 0.1081326669695593,
      "grad_norm": 19.49567222595215,
      "learning_rate": 0.0009714634518541127,
      "loss": 2.1241,
      "step": 238
    },
    {
      "epoch": 0.10858700590640617,
      "grad_norm": 13.681622505187988,
      "learning_rate": 0.00097131084999237,
      "loss": 3.5965,
      "step": 239
    },
    {
      "epoch": 0.10904134484325306,
      "grad_norm": 9.754660606384277,
      "learning_rate": 0.0009711582481306272,
      "loss": 2.1734,
      "step": 240
    },
    {
      "epoch": 0.10949568378009995,
      "grad_norm": 5.145036697387695,
      "learning_rate": 0.0009710056462688846,
      "loss": 1.1966,
      "step": 241
    },
    {
      "epoch": 0.10995002271694684,
      "grad_norm": 9.710577011108398,
      "learning_rate": 0.0009708530444071418,
      "loss": 1.3841,
      "step": 242
    },
    {
      "epoch": 0.11040436165379373,
      "grad_norm": 6.591537952423096,
      "learning_rate": 0.000970700442545399,
      "loss": 1.8608,
      "step": 243
    },
    {
      "epoch": 0.11085870059064062,
      "grad_norm": 6.7716803550720215,
      "learning_rate": 0.0009705478406836564,
      "loss": 1.7105,
      "step": 244
    },
    {
      "epoch": 0.1113130395274875,
      "grad_norm": 9.092998504638672,
      "learning_rate": 0.0009703952388219136,
      "loss": 2.5472,
      "step": 245
    },
    {
      "epoch": 0.11176737846433439,
      "grad_norm": 12.923064231872559,
      "learning_rate": 0.0009702426369601709,
      "loss": 2.7263,
      "step": 246
    },
    {
      "epoch": 0.11222171740118128,
      "grad_norm": 10.285985946655273,
      "learning_rate": 0.0009700900350984282,
      "loss": 2.7098,
      "step": 247
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 4.863156318664551,
      "learning_rate": 0.0009699374332366855,
      "loss": 1.7458,
      "step": 248
    },
    {
      "epoch": 0.11313039527487506,
      "grad_norm": 6.0615410804748535,
      "learning_rate": 0.0009697848313749427,
      "loss": 1.8858,
      "step": 249
    },
    {
      "epoch": 0.11358473421172194,
      "grad_norm": 6.759240627288818,
      "learning_rate": 0.0009696322295132001,
      "loss": 2.2782,
      "step": 250
    },
    {
      "epoch": 0.11403907314856883,
      "grad_norm": 4.964827537536621,
      "learning_rate": 0.0009694796276514574,
      "loss": 1.0873,
      "step": 251
    },
    {
      "epoch": 0.11449341208541572,
      "grad_norm": 6.755768775939941,
      "learning_rate": 0.0009693270257897146,
      "loss": 1.6664,
      "step": 252
    },
    {
      "epoch": 0.11494775102226261,
      "grad_norm": 5.244935035705566,
      "learning_rate": 0.000969174423927972,
      "loss": 1.301,
      "step": 253
    },
    {
      "epoch": 0.1154020899591095,
      "grad_norm": 8.03216552734375,
      "learning_rate": 0.0009690218220662292,
      "loss": 2.082,
      "step": 254
    },
    {
      "epoch": 0.11585642889595639,
      "grad_norm": 3.4233405590057373,
      "learning_rate": 0.0009688692202044865,
      "loss": 1.2097,
      "step": 255
    },
    {
      "epoch": 0.11631076783280327,
      "grad_norm": 6.953229904174805,
      "learning_rate": 0.0009687166183427439,
      "loss": 2.3142,
      "step": 256
    },
    {
      "epoch": 0.11676510676965016,
      "grad_norm": 12.756695747375488,
      "learning_rate": 0.0009685640164810011,
      "loss": 2.375,
      "step": 257
    },
    {
      "epoch": 0.11721944570649705,
      "grad_norm": 1.88507080078125,
      "learning_rate": 0.0009684114146192584,
      "loss": 0.795,
      "step": 258
    },
    {
      "epoch": 0.11767378464334394,
      "grad_norm": 6.357827663421631,
      "learning_rate": 0.0009682588127575157,
      "loss": 1.6194,
      "step": 259
    },
    {
      "epoch": 0.11812812358019083,
      "grad_norm": 3.229525566101074,
      "learning_rate": 0.000968106210895773,
      "loss": 1.1368,
      "step": 260
    },
    {
      "epoch": 0.1185824625170377,
      "grad_norm": 7.686180114746094,
      "learning_rate": 0.0009679536090340302,
      "loss": 1.3899,
      "step": 261
    },
    {
      "epoch": 0.1190368014538846,
      "grad_norm": 6.135477542877197,
      "learning_rate": 0.0009678010071722875,
      "loss": 1.9438,
      "step": 262
    },
    {
      "epoch": 0.11949114039073148,
      "grad_norm": 5.925567150115967,
      "learning_rate": 0.0009676484053105448,
      "loss": 1.8923,
      "step": 263
    },
    {
      "epoch": 0.11994547932757837,
      "grad_norm": 7.040816307067871,
      "learning_rate": 0.000967495803448802,
      "loss": 1.4019,
      "step": 264
    },
    {
      "epoch": 0.12039981826442527,
      "grad_norm": 5.518520355224609,
      "learning_rate": 0.0009673432015870594,
      "loss": 1.7695,
      "step": 265
    },
    {
      "epoch": 0.12085415720127216,
      "grad_norm": 6.107264518737793,
      "learning_rate": 0.0009671905997253167,
      "loss": 2.3963,
      "step": 266
    },
    {
      "epoch": 0.12130849613811903,
      "grad_norm": 4.519526958465576,
      "learning_rate": 0.0009670379978635739,
      "loss": 2.1478,
      "step": 267
    },
    {
      "epoch": 0.12176283507496592,
      "grad_norm": 3.4286012649536133,
      "learning_rate": 0.0009668853960018313,
      "loss": 1.1449,
      "step": 268
    },
    {
      "epoch": 0.12221717401181281,
      "grad_norm": 4.617491245269775,
      "learning_rate": 0.0009667327941400885,
      "loss": 1.7138,
      "step": 269
    },
    {
      "epoch": 0.1226715129486597,
      "grad_norm": 7.1106648445129395,
      "learning_rate": 0.0009665801922783458,
      "loss": 1.4312,
      "step": 270
    },
    {
      "epoch": 0.1231258518855066,
      "grad_norm": 4.849409580230713,
      "learning_rate": 0.0009664275904166031,
      "loss": 1.9531,
      "step": 271
    },
    {
      "epoch": 0.12358019082235347,
      "grad_norm": 4.902888298034668,
      "learning_rate": 0.0009662749885548604,
      "loss": 1.5832,
      "step": 272
    },
    {
      "epoch": 0.12403452975920036,
      "grad_norm": 5.298555850982666,
      "learning_rate": 0.0009661223866931177,
      "loss": 1.3189,
      "step": 273
    },
    {
      "epoch": 0.12448886869604725,
      "grad_norm": 3.0021321773529053,
      "learning_rate": 0.000965969784831375,
      "loss": 0.762,
      "step": 274
    },
    {
      "epoch": 0.12494320763289414,
      "grad_norm": 10.084877967834473,
      "learning_rate": 0.0009658171829696323,
      "loss": 1.8804,
      "step": 275
    },
    {
      "epoch": 0.12539754656974103,
      "grad_norm": 3.7184712886810303,
      "learning_rate": 0.0009656645811078895,
      "loss": 1.4398,
      "step": 276
    },
    {
      "epoch": 0.1258518855065879,
      "grad_norm": 4.975409030914307,
      "learning_rate": 0.0009655119792461469,
      "loss": 1.2895,
      "step": 277
    },
    {
      "epoch": 0.1263062244434348,
      "grad_norm": 4.711068630218506,
      "learning_rate": 0.0009653593773844042,
      "loss": 1.2868,
      "step": 278
    },
    {
      "epoch": 0.1267605633802817,
      "grad_norm": 4.77157735824585,
      "learning_rate": 0.0009652067755226614,
      "loss": 1.4741,
      "step": 279
    },
    {
      "epoch": 0.12721490231712856,
      "grad_norm": 4.753172874450684,
      "learning_rate": 0.0009650541736609187,
      "loss": 1.6691,
      "step": 280
    },
    {
      "epoch": 0.12766924125397547,
      "grad_norm": 5.105186462402344,
      "learning_rate": 0.0009649015717991759,
      "loss": 2.6583,
      "step": 281
    },
    {
      "epoch": 0.12812358019082234,
      "grad_norm": 3.5031063556671143,
      "learning_rate": 0.0009647489699374332,
      "loss": 1.6488,
      "step": 282
    },
    {
      "epoch": 0.12857791912766925,
      "grad_norm": 5.482685089111328,
      "learning_rate": 0.0009645963680756906,
      "loss": 1.8768,
      "step": 283
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 4.130162239074707,
      "learning_rate": 0.0009644437662139478,
      "loss": 2.0376,
      "step": 284
    },
    {
      "epoch": 0.12948659700136303,
      "grad_norm": 4.079968452453613,
      "learning_rate": 0.0009642911643522051,
      "loss": 1.3714,
      "step": 285
    },
    {
      "epoch": 0.1299409359382099,
      "grad_norm": 5.442225933074951,
      "learning_rate": 0.0009641385624904624,
      "loss": 2.5749,
      "step": 286
    },
    {
      "epoch": 0.13039527487505678,
      "grad_norm": 3.0847272872924805,
      "learning_rate": 0.0009639859606287197,
      "loss": 1.2828,
      "step": 287
    },
    {
      "epoch": 0.1308496138119037,
      "grad_norm": 5.267121315002441,
      "learning_rate": 0.0009638333587669769,
      "loss": 2.3027,
      "step": 288
    },
    {
      "epoch": 0.13130395274875056,
      "grad_norm": 3.9282617568969727,
      "learning_rate": 0.0009636807569052343,
      "loss": 2.1601,
      "step": 289
    },
    {
      "epoch": 0.13175829168559747,
      "grad_norm": 4.273280143737793,
      "learning_rate": 0.0009635281550434916,
      "loss": 1.261,
      "step": 290
    },
    {
      "epoch": 0.13221263062244434,
      "grad_norm": 6.7581329345703125,
      "learning_rate": 0.0009633755531817488,
      "loss": 1.7971,
      "step": 291
    },
    {
      "epoch": 0.13266696955929122,
      "grad_norm": 6.447543621063232,
      "learning_rate": 0.0009632229513200062,
      "loss": 2.1721,
      "step": 292
    },
    {
      "epoch": 0.13312130849613812,
      "grad_norm": 5.7428388595581055,
      "learning_rate": 0.0009630703494582634,
      "loss": 2.0216,
      "step": 293
    },
    {
      "epoch": 0.133575647432985,
      "grad_norm": 4.247249603271484,
      "learning_rate": 0.0009629177475965207,
      "loss": 1.9773,
      "step": 294
    },
    {
      "epoch": 0.1340299863698319,
      "grad_norm": 4.143215179443359,
      "learning_rate": 0.0009627651457347781,
      "loss": 1.3673,
      "step": 295
    },
    {
      "epoch": 0.13448432530667878,
      "grad_norm": 5.418733596801758,
      "learning_rate": 0.0009626125438730353,
      "loss": 1.8236,
      "step": 296
    },
    {
      "epoch": 0.13493866424352566,
      "grad_norm": 4.523118495941162,
      "learning_rate": 0.0009624599420112926,
      "loss": 1.3294,
      "step": 297
    },
    {
      "epoch": 0.13539300318037256,
      "grad_norm": 5.696187973022461,
      "learning_rate": 0.0009623073401495498,
      "loss": 1.9017,
      "step": 298
    },
    {
      "epoch": 0.13584734211721944,
      "grad_norm": 3.8294601440429688,
      "learning_rate": 0.0009621547382878071,
      "loss": 1.2499,
      "step": 299
    },
    {
      "epoch": 0.13630168105406634,
      "grad_norm": 4.698948860168457,
      "learning_rate": 0.0009620021364260643,
      "loss": 2.1312,
      "step": 300
    },
    {
      "epoch": 0.13675601999091322,
      "grad_norm": 5.377554893493652,
      "learning_rate": 0.0009618495345643217,
      "loss": 1.8425,
      "step": 301
    },
    {
      "epoch": 0.1372103589277601,
      "grad_norm": 6.004270076751709,
      "learning_rate": 0.000961696932702579,
      "loss": 2.4913,
      "step": 302
    },
    {
      "epoch": 0.137664697864607,
      "grad_norm": 4.0011420249938965,
      "learning_rate": 0.0009615443308408362,
      "loss": 1.2079,
      "step": 303
    },
    {
      "epoch": 0.13811903680145388,
      "grad_norm": 3.1162526607513428,
      "learning_rate": 0.0009613917289790936,
      "loss": 0.8489,
      "step": 304
    },
    {
      "epoch": 0.13857337573830078,
      "grad_norm": 3.202921152114868,
      "learning_rate": 0.0009612391271173508,
      "loss": 1.4815,
      "step": 305
    },
    {
      "epoch": 0.13902771467514766,
      "grad_norm": 4.1892409324646,
      "learning_rate": 0.0009610865252556081,
      "loss": 1.8213,
      "step": 306
    },
    {
      "epoch": 0.13948205361199456,
      "grad_norm": 3.7600250244140625,
      "learning_rate": 0.0009609339233938655,
      "loss": 1.5397,
      "step": 307
    },
    {
      "epoch": 0.13993639254884144,
      "grad_norm": 10.17154312133789,
      "learning_rate": 0.0009607813215321227,
      "loss": 2.6686,
      "step": 308
    },
    {
      "epoch": 0.1403907314856883,
      "grad_norm": 6.5317792892456055,
      "learning_rate": 0.00096062871967038,
      "loss": 1.6185,
      "step": 309
    },
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 5.431933879852295,
      "learning_rate": 0.0009604761178086373,
      "loss": 1.5038,
      "step": 310
    },
    {
      "epoch": 0.1412994093593821,
      "grad_norm": 3.023481607437134,
      "learning_rate": 0.0009603235159468946,
      "loss": 1.1049,
      "step": 311
    },
    {
      "epoch": 0.141753748296229,
      "grad_norm": 2.909053087234497,
      "learning_rate": 0.0009601709140851519,
      "loss": 0.8892,
      "step": 312
    },
    {
      "epoch": 0.14220808723307587,
      "grad_norm": 5.58517599105835,
      "learning_rate": 0.0009600183122234092,
      "loss": 1.4781,
      "step": 313
    },
    {
      "epoch": 0.14266242616992275,
      "grad_norm": 4.75494909286499,
      "learning_rate": 0.0009598657103616665,
      "loss": 1.6158,
      "step": 314
    },
    {
      "epoch": 0.14311676510676966,
      "grad_norm": 4.936613082885742,
      "learning_rate": 0.0009597131084999237,
      "loss": 1.2898,
      "step": 315
    },
    {
      "epoch": 0.14357110404361653,
      "grad_norm": 6.368581295013428,
      "learning_rate": 0.000959560506638181,
      "loss": 2.2102,
      "step": 316
    },
    {
      "epoch": 0.14402544298046344,
      "grad_norm": 5.877938747406006,
      "learning_rate": 0.0009594079047764382,
      "loss": 2.1716,
      "step": 317
    },
    {
      "epoch": 0.1444797819173103,
      "grad_norm": 6.3772053718566895,
      "learning_rate": 0.0009592553029146955,
      "loss": 1.5077,
      "step": 318
    },
    {
      "epoch": 0.1449341208541572,
      "grad_norm": 3.435494899749756,
      "learning_rate": 0.0009591027010529529,
      "loss": 1.4913,
      "step": 319
    },
    {
      "epoch": 0.1453884597910041,
      "grad_norm": 3.8153128623962402,
      "learning_rate": 0.0009589500991912101,
      "loss": 1.2667,
      "step": 320
    },
    {
      "epoch": 0.14584279872785097,
      "grad_norm": 5.131373405456543,
      "learning_rate": 0.0009587974973294674,
      "loss": 1.9779,
      "step": 321
    },
    {
      "epoch": 0.14629713766469787,
      "grad_norm": 5.150716304779053,
      "learning_rate": 0.0009586448954677247,
      "loss": 1.968,
      "step": 322
    },
    {
      "epoch": 0.14675147660154475,
      "grad_norm": 6.0059494972229,
      "learning_rate": 0.000958492293605982,
      "loss": 1.8916,
      "step": 323
    },
    {
      "epoch": 0.14720581553839163,
      "grad_norm": 4.621043682098389,
      "learning_rate": 0.0009583396917442393,
      "loss": 1.5227,
      "step": 324
    },
    {
      "epoch": 0.14766015447523853,
      "grad_norm": 5.760458469390869,
      "learning_rate": 0.0009581870898824966,
      "loss": 0.9451,
      "step": 325
    },
    {
      "epoch": 0.1481144934120854,
      "grad_norm": 4.9935479164123535,
      "learning_rate": 0.0009580344880207539,
      "loss": 2.2242,
      "step": 326
    },
    {
      "epoch": 0.1485688323489323,
      "grad_norm": 4.3562331199646,
      "learning_rate": 0.0009578818861590111,
      "loss": 1.4898,
      "step": 327
    },
    {
      "epoch": 0.1490231712857792,
      "grad_norm": 3.6777570247650146,
      "learning_rate": 0.0009577292842972685,
      "loss": 0.9235,
      "step": 328
    },
    {
      "epoch": 0.1494775102226261,
      "grad_norm": 4.099028587341309,
      "learning_rate": 0.0009575766824355258,
      "loss": 1.0073,
      "step": 329
    },
    {
      "epoch": 0.14993184915947297,
      "grad_norm": 3.791944980621338,
      "learning_rate": 0.000957424080573783,
      "loss": 1.3769,
      "step": 330
    },
    {
      "epoch": 0.15038618809631984,
      "grad_norm": 5.003060817718506,
      "learning_rate": 0.0009572714787120404,
      "loss": 1.8799,
      "step": 331
    },
    {
      "epoch": 0.15084052703316675,
      "grad_norm": 6.193656921386719,
      "learning_rate": 0.0009571188768502976,
      "loss": 2.1635,
      "step": 332
    },
    {
      "epoch": 0.15129486597001363,
      "grad_norm": 6.927994728088379,
      "learning_rate": 0.0009569662749885549,
      "loss": 1.9274,
      "step": 333
    },
    {
      "epoch": 0.15174920490686053,
      "grad_norm": 4.5294013023376465,
      "learning_rate": 0.0009568136731268121,
      "loss": 1.0982,
      "step": 334
    },
    {
      "epoch": 0.1522035438437074,
      "grad_norm": 6.519195556640625,
      "learning_rate": 0.0009566610712650694,
      "loss": 1.5222,
      "step": 335
    },
    {
      "epoch": 0.15265788278055428,
      "grad_norm": 4.4936747550964355,
      "learning_rate": 0.0009565084694033267,
      "loss": 1.7168,
      "step": 336
    },
    {
      "epoch": 0.1531122217174012,
      "grad_norm": 3.4830853939056396,
      "learning_rate": 0.000956355867541584,
      "loss": 1.2097,
      "step": 337
    },
    {
      "epoch": 0.15356656065424806,
      "grad_norm": 4.848855018615723,
      "learning_rate": 0.0009562032656798413,
      "loss": 2.0716,
      "step": 338
    },
    {
      "epoch": 0.15402089959109497,
      "grad_norm": 9.708375930786133,
      "learning_rate": 0.0009560506638180985,
      "loss": 1.6628,
      "step": 339
    },
    {
      "epoch": 0.15447523852794184,
      "grad_norm": 5.300631523132324,
      "learning_rate": 0.0009558980619563559,
      "loss": 1.5746,
      "step": 340
    },
    {
      "epoch": 0.15492957746478872,
      "grad_norm": 6.133340358734131,
      "learning_rate": 0.0009557454600946132,
      "loss": 2.181,
      "step": 341
    },
    {
      "epoch": 0.15538391640163562,
      "grad_norm": 5.502054214477539,
      "learning_rate": 0.0009555928582328704,
      "loss": 2.2198,
      "step": 342
    },
    {
      "epoch": 0.1558382553384825,
      "grad_norm": 5.86580753326416,
      "learning_rate": 0.0009554402563711278,
      "loss": 1.1812,
      "step": 343
    },
    {
      "epoch": 0.1562925942753294,
      "grad_norm": 7.142019748687744,
      "learning_rate": 0.000955287654509385,
      "loss": 2.0566,
      "step": 344
    },
    {
      "epoch": 0.15674693321217628,
      "grad_norm": 4.58322286605835,
      "learning_rate": 0.0009551350526476423,
      "loss": 2.0312,
      "step": 345
    },
    {
      "epoch": 0.15720127214902316,
      "grad_norm": 4.734084129333496,
      "learning_rate": 0.0009549824507858997,
      "loss": 1.072,
      "step": 346
    },
    {
      "epoch": 0.15765561108587006,
      "grad_norm": 3.4882612228393555,
      "learning_rate": 0.0009548298489241569,
      "loss": 1.1598,
      "step": 347
    },
    {
      "epoch": 0.15810995002271694,
      "grad_norm": 5.3633198738098145,
      "learning_rate": 0.0009546772470624142,
      "loss": 1.3874,
      "step": 348
    },
    {
      "epoch": 0.15856428895956384,
      "grad_norm": 4.552365303039551,
      "learning_rate": 0.0009545246452006715,
      "loss": 0.8377,
      "step": 349
    },
    {
      "epoch": 0.15901862789641072,
      "grad_norm": 5.839667797088623,
      "learning_rate": 0.0009543720433389288,
      "loss": 2.1463,
      "step": 350
    },
    {
      "epoch": 0.15947296683325762,
      "grad_norm": 7.1999030113220215,
      "learning_rate": 0.000954219441477186,
      "loss": 1.9613,
      "step": 351
    },
    {
      "epoch": 0.1599273057701045,
      "grad_norm": 3.627861261367798,
      "learning_rate": 0.0009540668396154434,
      "loss": 1.4151,
      "step": 352
    },
    {
      "epoch": 0.16038164470695138,
      "grad_norm": 4.5662078857421875,
      "learning_rate": 0.0009539142377537006,
      "loss": 1.3432,
      "step": 353
    },
    {
      "epoch": 0.16083598364379828,
      "grad_norm": 4.794084548950195,
      "learning_rate": 0.0009537616358919578,
      "loss": 1.2252,
      "step": 354
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 6.940950870513916,
      "learning_rate": 0.0009536090340302152,
      "loss": 2.3509,
      "step": 355
    },
    {
      "epoch": 0.16174466151749206,
      "grad_norm": 4.2744140625,
      "learning_rate": 0.0009534564321684724,
      "loss": 1.3867,
      "step": 356
    },
    {
      "epoch": 0.16219900045433894,
      "grad_norm": 6.7544708251953125,
      "learning_rate": 0.0009533038303067297,
      "loss": 2.4108,
      "step": 357
    },
    {
      "epoch": 0.1626533393911858,
      "grad_norm": 5.51371955871582,
      "learning_rate": 0.0009531512284449871,
      "loss": 1.9005,
      "step": 358
    },
    {
      "epoch": 0.16310767832803272,
      "grad_norm": 7.359863758087158,
      "learning_rate": 0.0009529986265832443,
      "loss": 1.8119,
      "step": 359
    },
    {
      "epoch": 0.1635620172648796,
      "grad_norm": 6.958871841430664,
      "learning_rate": 0.0009528460247215016,
      "loss": 1.54,
      "step": 360
    },
    {
      "epoch": 0.1640163562017265,
      "grad_norm": 3.6794283390045166,
      "learning_rate": 0.0009526934228597589,
      "loss": 1.0328,
      "step": 361
    },
    {
      "epoch": 0.16447069513857338,
      "grad_norm": 3.9978232383728027,
      "learning_rate": 0.0009525408209980162,
      "loss": 1.4197,
      "step": 362
    },
    {
      "epoch": 0.16492503407542025,
      "grad_norm": 3.645087480545044,
      "learning_rate": 0.0009523882191362734,
      "loss": 1.1157,
      "step": 363
    },
    {
      "epoch": 0.16537937301226716,
      "grad_norm": 5.046257972717285,
      "learning_rate": 0.0009522356172745308,
      "loss": 1.3622,
      "step": 364
    },
    {
      "epoch": 0.16583371194911403,
      "grad_norm": 6.543088912963867,
      "learning_rate": 0.0009520830154127881,
      "loss": 1.8488,
      "step": 365
    },
    {
      "epoch": 0.16628805088596094,
      "grad_norm": 4.333006381988525,
      "learning_rate": 0.0009519304135510453,
      "loss": 1.3312,
      "step": 366
    },
    {
      "epoch": 0.1667423898228078,
      "grad_norm": 4.138701438903809,
      "learning_rate": 0.0009517778116893027,
      "loss": 1.0467,
      "step": 367
    },
    {
      "epoch": 0.1671967287596547,
      "grad_norm": 7.115666389465332,
      "learning_rate": 0.00095162520982756,
      "loss": 1.3021,
      "step": 368
    },
    {
      "epoch": 0.1676510676965016,
      "grad_norm": 4.082321643829346,
      "learning_rate": 0.0009514726079658172,
      "loss": 1.6254,
      "step": 369
    },
    {
      "epoch": 0.16810540663334847,
      "grad_norm": 5.0034685134887695,
      "learning_rate": 0.0009513200061040746,
      "loss": 1.3705,
      "step": 370
    },
    {
      "epoch": 0.16855974557019537,
      "grad_norm": 6.105103015899658,
      "learning_rate": 0.0009511674042423317,
      "loss": 1.9385,
      "step": 371
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 4.43419075012207,
      "learning_rate": 0.000951014802380589,
      "loss": 1.4608,
      "step": 372
    },
    {
      "epoch": 0.16946842344388915,
      "grad_norm": 4.995471954345703,
      "learning_rate": 0.0009508622005188463,
      "loss": 1.2076,
      "step": 373
    },
    {
      "epoch": 0.16992276238073603,
      "grad_norm": 8.26581859588623,
      "learning_rate": 0.0009507095986571036,
      "loss": 3.2799,
      "step": 374
    },
    {
      "epoch": 0.1703771013175829,
      "grad_norm": 7.827980995178223,
      "learning_rate": 0.0009505569967953608,
      "loss": 3.2027,
      "step": 375
    },
    {
      "epoch": 0.1708314402544298,
      "grad_norm": 9.408897399902344,
      "learning_rate": 0.0009504043949336182,
      "loss": 1.0714,
      "step": 376
    },
    {
      "epoch": 0.1712857791912767,
      "grad_norm": 5.053788185119629,
      "learning_rate": 0.0009502517930718755,
      "loss": 1.2166,
      "step": 377
    },
    {
      "epoch": 0.1717401181281236,
      "grad_norm": 5.507104396820068,
      "learning_rate": 0.0009500991912101327,
      "loss": 1.7135,
      "step": 378
    },
    {
      "epoch": 0.17219445706497047,
      "grad_norm": 3.9690065383911133,
      "learning_rate": 0.0009499465893483901,
      "loss": 1.7665,
      "step": 379
    },
    {
      "epoch": 0.17264879600181735,
      "grad_norm": 5.33690881729126,
      "learning_rate": 0.0009497939874866473,
      "loss": 2.0596,
      "step": 380
    },
    {
      "epoch": 0.17310313493866425,
      "grad_norm": 4.316205024719238,
      "learning_rate": 0.0009496413856249046,
      "loss": 1.522,
      "step": 381
    },
    {
      "epoch": 0.17355747387551113,
      "grad_norm": 2.7867448329925537,
      "learning_rate": 0.000949488783763162,
      "loss": 1.2558,
      "step": 382
    },
    {
      "epoch": 0.17401181281235803,
      "grad_norm": 3.978105068206787,
      "learning_rate": 0.0009493361819014192,
      "loss": 1.0527,
      "step": 383
    },
    {
      "epoch": 0.1744661517492049,
      "grad_norm": 3.791734457015991,
      "learning_rate": 0.0009491835800396765,
      "loss": 1.4442,
      "step": 384
    },
    {
      "epoch": 0.17492049068605178,
      "grad_norm": 2.4090046882629395,
      "learning_rate": 0.0009490309781779338,
      "loss": 0.8097,
      "step": 385
    },
    {
      "epoch": 0.1753748296228987,
      "grad_norm": 4.924672603607178,
      "learning_rate": 0.0009488783763161911,
      "loss": 1.5682,
      "step": 386
    },
    {
      "epoch": 0.17582916855974556,
      "grad_norm": 6.532632827758789,
      "learning_rate": 0.0009487257744544484,
      "loss": 1.7224,
      "step": 387
    },
    {
      "epoch": 0.17628350749659247,
      "grad_norm": 4.940225124359131,
      "learning_rate": 0.0009485731725927057,
      "loss": 1.1896,
      "step": 388
    },
    {
      "epoch": 0.17673784643343934,
      "grad_norm": 4.2179975509643555,
      "learning_rate": 0.0009484205707309629,
      "loss": 1.2505,
      "step": 389
    },
    {
      "epoch": 0.17719218537028622,
      "grad_norm": 2.875326156616211,
      "learning_rate": 0.0009482679688692201,
      "loss": 1.0426,
      "step": 390
    },
    {
      "epoch": 0.17764652430713312,
      "grad_norm": 6.014885425567627,
      "learning_rate": 0.0009481153670074775,
      "loss": 1.7501,
      "step": 391
    },
    {
      "epoch": 0.17810086324398,
      "grad_norm": 6.904287815093994,
      "learning_rate": 0.0009479627651457348,
      "loss": 1.3953,
      "step": 392
    },
    {
      "epoch": 0.1785552021808269,
      "grad_norm": 3.87725830078125,
      "learning_rate": 0.000947810163283992,
      "loss": 1.576,
      "step": 393
    },
    {
      "epoch": 0.17900954111767378,
      "grad_norm": 4.728071212768555,
      "learning_rate": 0.0009476575614222494,
      "loss": 2.1511,
      "step": 394
    },
    {
      "epoch": 0.17946388005452069,
      "grad_norm": 5.006023406982422,
      "learning_rate": 0.0009475049595605066,
      "loss": 1.5623,
      "step": 395
    },
    {
      "epoch": 0.17991821899136756,
      "grad_norm": 4.96576452255249,
      "learning_rate": 0.0009473523576987639,
      "loss": 2.081,
      "step": 396
    },
    {
      "epoch": 0.18037255792821444,
      "grad_norm": 7.731472015380859,
      "learning_rate": 0.0009471997558370212,
      "loss": 1.5883,
      "step": 397
    },
    {
      "epoch": 0.18082689686506134,
      "grad_norm": 5.842845916748047,
      "learning_rate": 0.0009470471539752785,
      "loss": 1.5457,
      "step": 398
    },
    {
      "epoch": 0.18128123580190822,
      "grad_norm": 8.413512229919434,
      "learning_rate": 0.0009468945521135358,
      "loss": 1.4936,
      "step": 399
    },
    {
      "epoch": 0.18173557473875512,
      "grad_norm": 7.487287998199463,
      "learning_rate": 0.0009467419502517931,
      "loss": 2.2145,
      "step": 400
    },
    {
      "epoch": 0.182189913675602,
      "grad_norm": 5.437649726867676,
      "learning_rate": 0.0009465893483900504,
      "loss": 1.7753,
      "step": 401
    },
    {
      "epoch": 0.18264425261244888,
      "grad_norm": 6.131345272064209,
      "learning_rate": 0.0009464367465283076,
      "loss": 1.4985,
      "step": 402
    },
    {
      "epoch": 0.18309859154929578,
      "grad_norm": 2.614121437072754,
      "learning_rate": 0.000946284144666565,
      "loss": 1.1369,
      "step": 403
    },
    {
      "epoch": 0.18355293048614266,
      "grad_norm": 7.6424970626831055,
      "learning_rate": 0.0009461315428048223,
      "loss": 1.9627,
      "step": 404
    },
    {
      "epoch": 0.18400726942298956,
      "grad_norm": 5.5819268226623535,
      "learning_rate": 0.0009459789409430795,
      "loss": 1.2637,
      "step": 405
    },
    {
      "epoch": 0.18446160835983644,
      "grad_norm": 4.559139728546143,
      "learning_rate": 0.0009458263390813369,
      "loss": 1.3853,
      "step": 406
    },
    {
      "epoch": 0.18491594729668331,
      "grad_norm": 3.6235055923461914,
      "learning_rate": 0.000945673737219594,
      "loss": 0.9448,
      "step": 407
    },
    {
      "epoch": 0.18537028623353022,
      "grad_norm": 5.552212715148926,
      "learning_rate": 0.0009455211353578513,
      "loss": 1.6926,
      "step": 408
    },
    {
      "epoch": 0.1858246251703771,
      "grad_norm": 4.262753486633301,
      "learning_rate": 0.0009453685334961087,
      "loss": 1.1928,
      "step": 409
    },
    {
      "epoch": 0.186278964107224,
      "grad_norm": 4.875225067138672,
      "learning_rate": 0.0009452159316343659,
      "loss": 1.331,
      "step": 410
    },
    {
      "epoch": 0.18673330304407088,
      "grad_norm": 5.551540374755859,
      "learning_rate": 0.0009450633297726232,
      "loss": 1.6461,
      "step": 411
    },
    {
      "epoch": 0.18718764198091775,
      "grad_norm": 7.446421146392822,
      "learning_rate": 0.0009449107279108805,
      "loss": 1.3242,
      "step": 412
    },
    {
      "epoch": 0.18764198091776466,
      "grad_norm": 7.013265609741211,
      "learning_rate": 0.0009447581260491378,
      "loss": 1.6484,
      "step": 413
    },
    {
      "epoch": 0.18809631985461153,
      "grad_norm": 4.385944843292236,
      "learning_rate": 0.000944605524187395,
      "loss": 1.0595,
      "step": 414
    },
    {
      "epoch": 0.18855065879145844,
      "grad_norm": 3.26851487159729,
      "learning_rate": 0.0009444529223256524,
      "loss": 1.0926,
      "step": 415
    },
    {
      "epoch": 0.1890049977283053,
      "grad_norm": 5.806207180023193,
      "learning_rate": 0.0009443003204639097,
      "loss": 2.5344,
      "step": 416
    },
    {
      "epoch": 0.18945933666515222,
      "grad_norm": 5.659852504730225,
      "learning_rate": 0.0009441477186021669,
      "loss": 1.9432,
      "step": 417
    },
    {
      "epoch": 0.1899136756019991,
      "grad_norm": 5.077337741851807,
      "learning_rate": 0.0009439951167404243,
      "loss": 1.6054,
      "step": 418
    },
    {
      "epoch": 0.19036801453884597,
      "grad_norm": 5.186604022979736,
      "learning_rate": 0.0009438425148786815,
      "loss": 2.5691,
      "step": 419
    },
    {
      "epoch": 0.19082235347569287,
      "grad_norm": 6.871816635131836,
      "learning_rate": 0.0009436899130169388,
      "loss": 1.4415,
      "step": 420
    },
    {
      "epoch": 0.19127669241253975,
      "grad_norm": 4.514774799346924,
      "learning_rate": 0.0009435373111551962,
      "loss": 1.474,
      "step": 421
    },
    {
      "epoch": 0.19173103134938665,
      "grad_norm": 2.7210142612457275,
      "learning_rate": 0.0009433847092934534,
      "loss": 1.2143,
      "step": 422
    },
    {
      "epoch": 0.19218537028623353,
      "grad_norm": 6.983016490936279,
      "learning_rate": 0.0009432321074317108,
      "loss": 1.4631,
      "step": 423
    },
    {
      "epoch": 0.1926397092230804,
      "grad_norm": 7.309233665466309,
      "learning_rate": 0.000943079505569968,
      "loss": 2.0752,
      "step": 424
    },
    {
      "epoch": 0.1930940481599273,
      "grad_norm": 2.8916566371917725,
      "learning_rate": 0.0009429269037082253,
      "loss": 0.6985,
      "step": 425
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 5.820022106170654,
      "learning_rate": 0.0009427743018464824,
      "loss": 2.4693,
      "step": 426
    },
    {
      "epoch": 0.1940027260336211,
      "grad_norm": 7.536548137664795,
      "learning_rate": 0.0009426216999847398,
      "loss": 2.2119,
      "step": 427
    },
    {
      "epoch": 0.19445706497046797,
      "grad_norm": 4.727935791015625,
      "learning_rate": 0.0009424690981229971,
      "loss": 1.3794,
      "step": 428
    },
    {
      "epoch": 0.19491140390731485,
      "grad_norm": 6.465019702911377,
      "learning_rate": 0.0009423164962612543,
      "loss": 1.9791,
      "step": 429
    },
    {
      "epoch": 0.19536574284416175,
      "grad_norm": 5.451267242431641,
      "learning_rate": 0.0009421638943995117,
      "loss": 1.7393,
      "step": 430
    },
    {
      "epoch": 0.19582008178100863,
      "grad_norm": 7.749899387359619,
      "learning_rate": 0.0009420112925377689,
      "loss": 2.6558,
      "step": 431
    },
    {
      "epoch": 0.19627442071785553,
      "grad_norm": 4.837016582489014,
      "learning_rate": 0.0009418586906760263,
      "loss": 1.2864,
      "step": 432
    },
    {
      "epoch": 0.1967287596547024,
      "grad_norm": 4.787230968475342,
      "learning_rate": 0.0009417060888142836,
      "loss": 1.4421,
      "step": 433
    },
    {
      "epoch": 0.19718309859154928,
      "grad_norm": 4.947290420532227,
      "learning_rate": 0.0009415534869525408,
      "loss": 1.1981,
      "step": 434
    },
    {
      "epoch": 0.1976374375283962,
      "grad_norm": 5.157255172729492,
      "learning_rate": 0.0009414008850907982,
      "loss": 1.2748,
      "step": 435
    },
    {
      "epoch": 0.19809177646524306,
      "grad_norm": 8.211413383483887,
      "learning_rate": 0.0009412482832290554,
      "loss": 1.7366,
      "step": 436
    },
    {
      "epoch": 0.19854611540208997,
      "grad_norm": 4.752699851989746,
      "learning_rate": 0.0009410956813673127,
      "loss": 1.5452,
      "step": 437
    },
    {
      "epoch": 0.19900045433893684,
      "grad_norm": 4.8603596687316895,
      "learning_rate": 0.0009409430795055701,
      "loss": 1.8112,
      "step": 438
    },
    {
      "epoch": 0.19945479327578375,
      "grad_norm": 6.218367099761963,
      "learning_rate": 0.0009407904776438273,
      "loss": 2.2075,
      "step": 439
    },
    {
      "epoch": 0.19990913221263062,
      "grad_norm": 2.5570054054260254,
      "learning_rate": 0.0009406378757820846,
      "loss": 0.7072,
      "step": 440
    },
    {
      "epoch": 0.2003634711494775,
      "grad_norm": 4.988033294677734,
      "learning_rate": 0.0009404852739203419,
      "loss": 2.3541,
      "step": 441
    },
    {
      "epoch": 0.2008178100863244,
      "grad_norm": 4.045705795288086,
      "learning_rate": 0.0009403326720585992,
      "loss": 0.9018,
      "step": 442
    },
    {
      "epoch": 0.20127214902317128,
      "grad_norm": 7.6420159339904785,
      "learning_rate": 0.0009401800701968565,
      "loss": 1.9201,
      "step": 443
    },
    {
      "epoch": 0.20172648796001819,
      "grad_norm": 9.172426223754883,
      "learning_rate": 0.0009400274683351137,
      "loss": 0.9007,
      "step": 444
    },
    {
      "epoch": 0.20218082689686506,
      "grad_norm": 3.9377553462982178,
      "learning_rate": 0.000939874866473371,
      "loss": 1.2728,
      "step": 445
    },
    {
      "epoch": 0.20263516583371194,
      "grad_norm": 5.961678981781006,
      "learning_rate": 0.0009397222646116282,
      "loss": 1.3516,
      "step": 446
    },
    {
      "epoch": 0.20308950477055884,
      "grad_norm": 7.464356422424316,
      "learning_rate": 0.0009395696627498856,
      "loss": 1.9587,
      "step": 447
    },
    {
      "epoch": 0.20354384370740572,
      "grad_norm": 5.066741466522217,
      "learning_rate": 0.0009394170608881428,
      "loss": 1.8978,
      "step": 448
    },
    {
      "epoch": 0.20399818264425262,
      "grad_norm": 8.800253868103027,
      "learning_rate": 0.0009392644590264001,
      "loss": 2.306,
      "step": 449
    },
    {
      "epoch": 0.2044525215810995,
      "grad_norm": 4.4850640296936035,
      "learning_rate": 0.0009391118571646575,
      "loss": 1.2291,
      "step": 450
    },
    {
      "epoch": 0.20490686051794638,
      "grad_norm": 7.148253440856934,
      "learning_rate": 0.0009389592553029147,
      "loss": 1.8733,
      "step": 451
    },
    {
      "epoch": 0.20536119945479328,
      "grad_norm": 3.8067727088928223,
      "learning_rate": 0.000938806653441172,
      "loss": 1.5931,
      "step": 452
    },
    {
      "epoch": 0.20581553839164016,
      "grad_norm": 2.8213534355163574,
      "learning_rate": 0.0009386540515794293,
      "loss": 0.7292,
      "step": 453
    },
    {
      "epoch": 0.20626987732848706,
      "grad_norm": 5.033578395843506,
      "learning_rate": 0.0009385014497176866,
      "loss": 1.351,
      "step": 454
    },
    {
      "epoch": 0.20672421626533394,
      "grad_norm": 7.203210353851318,
      "learning_rate": 0.0009383488478559439,
      "loss": 1.9794,
      "step": 455
    },
    {
      "epoch": 0.20717855520218081,
      "grad_norm": 6.388524532318115,
      "learning_rate": 0.0009381962459942012,
      "loss": 2.3093,
      "step": 456
    },
    {
      "epoch": 0.20763289413902772,
      "grad_norm": 8.489537239074707,
      "learning_rate": 0.0009380436441324585,
      "loss": 1.4224,
      "step": 457
    },
    {
      "epoch": 0.2080872330758746,
      "grad_norm": 6.995753765106201,
      "learning_rate": 0.0009378910422707157,
      "loss": 1.8721,
      "step": 458
    },
    {
      "epoch": 0.2085415720127215,
      "grad_norm": 6.279966831207275,
      "learning_rate": 0.0009377384404089731,
      "loss": 1.3,
      "step": 459
    },
    {
      "epoch": 0.20899591094956838,
      "grad_norm": 5.739799499511719,
      "learning_rate": 0.0009375858385472304,
      "loss": 1.5835,
      "step": 460
    },
    {
      "epoch": 0.20945024988641528,
      "grad_norm": 6.339118480682373,
      "learning_rate": 0.0009374332366854876,
      "loss": 1.6173,
      "step": 461
    },
    {
      "epoch": 0.20990458882326216,
      "grad_norm": 7.6310200691223145,
      "learning_rate": 0.0009372806348237449,
      "loss": 2.0345,
      "step": 462
    },
    {
      "epoch": 0.21035892776010903,
      "grad_norm": 5.157326698303223,
      "learning_rate": 0.0009371280329620021,
      "loss": 2.051,
      "step": 463
    },
    {
      "epoch": 0.21081326669695594,
      "grad_norm": 3.588669538497925,
      "learning_rate": 0.0009369754311002594,
      "loss": 1.1911,
      "step": 464
    },
    {
      "epoch": 0.2112676056338028,
      "grad_norm": 4.343991279602051,
      "learning_rate": 0.0009368228292385167,
      "loss": 1.7756,
      "step": 465
    },
    {
      "epoch": 0.21172194457064972,
      "grad_norm": 5.403534889221191,
      "learning_rate": 0.000936670227376774,
      "loss": 1.8344,
      "step": 466
    },
    {
      "epoch": 0.2121762835074966,
      "grad_norm": 4.733722686767578,
      "learning_rate": 0.0009365176255150313,
      "loss": 1.6148,
      "step": 467
    },
    {
      "epoch": 0.21263062244434347,
      "grad_norm": 4.588109493255615,
      "learning_rate": 0.0009363650236532886,
      "loss": 1.3201,
      "step": 468
    },
    {
      "epoch": 0.21308496138119037,
      "grad_norm": 5.961502552032471,
      "learning_rate": 0.0009362124217915459,
      "loss": 1.4417,
      "step": 469
    },
    {
      "epoch": 0.21353930031803725,
      "grad_norm": 6.888246536254883,
      "learning_rate": 0.0009360598199298031,
      "loss": 1.8016,
      "step": 470
    },
    {
      "epoch": 0.21399363925488415,
      "grad_norm": 6.24324369430542,
      "learning_rate": 0.0009359072180680605,
      "loss": 2.0089,
      "step": 471
    },
    {
      "epoch": 0.21444797819173103,
      "grad_norm": 4.939812183380127,
      "learning_rate": 0.0009357546162063178,
      "loss": 1.6091,
      "step": 472
    },
    {
      "epoch": 0.2149023171285779,
      "grad_norm": 4.994645595550537,
      "learning_rate": 0.000935602014344575,
      "loss": 1.1565,
      "step": 473
    },
    {
      "epoch": 0.2153566560654248,
      "grad_norm": 4.653181552886963,
      "learning_rate": 0.0009354494124828324,
      "loss": 1.1146,
      "step": 474
    },
    {
      "epoch": 0.2158109950022717,
      "grad_norm": 4.534937381744385,
      "learning_rate": 0.0009352968106210896,
      "loss": 1.5195,
      "step": 475
    },
    {
      "epoch": 0.2162653339391186,
      "grad_norm": 6.994931221008301,
      "learning_rate": 0.0009351442087593469,
      "loss": 1.5121,
      "step": 476
    },
    {
      "epoch": 0.21671967287596547,
      "grad_norm": 7.108386039733887,
      "learning_rate": 0.0009349916068976043,
      "loss": 2.3431,
      "step": 477
    },
    {
      "epoch": 0.21717401181281235,
      "grad_norm": 5.635793685913086,
      "learning_rate": 0.0009348390050358615,
      "loss": 1.5281,
      "step": 478
    },
    {
      "epoch": 0.21762835074965925,
      "grad_norm": 3.227243185043335,
      "learning_rate": 0.0009346864031741188,
      "loss": 1.0244,
      "step": 479
    },
    {
      "epoch": 0.21808268968650613,
      "grad_norm": 5.443169593811035,
      "learning_rate": 0.000934533801312376,
      "loss": 2.0097,
      "step": 480
    },
    {
      "epoch": 0.21853702862335303,
      "grad_norm": 4.196178913116455,
      "learning_rate": 0.0009343811994506333,
      "loss": 0.8677,
      "step": 481
    },
    {
      "epoch": 0.2189913675601999,
      "grad_norm": 5.010131359100342,
      "learning_rate": 0.0009342285975888905,
      "loss": 1.1281,
      "step": 482
    },
    {
      "epoch": 0.2194457064970468,
      "grad_norm": 4.863934516906738,
      "learning_rate": 0.0009340759957271479,
      "loss": 1.1286,
      "step": 483
    },
    {
      "epoch": 0.2199000454338937,
      "grad_norm": 6.517072677612305,
      "learning_rate": 0.0009339233938654052,
      "loss": 1.7019,
      "step": 484
    },
    {
      "epoch": 0.22035438437074056,
      "grad_norm": 3.062812089920044,
      "learning_rate": 0.0009337707920036624,
      "loss": 0.884,
      "step": 485
    },
    {
      "epoch": 0.22080872330758747,
      "grad_norm": 5.0589728355407715,
      "learning_rate": 0.0009336181901419198,
      "loss": 2.0554,
      "step": 486
    },
    {
      "epoch": 0.22126306224443434,
      "grad_norm": 5.991934776306152,
      "learning_rate": 0.000933465588280177,
      "loss": 2.6971,
      "step": 487
    },
    {
      "epoch": 0.22171740118128125,
      "grad_norm": 4.487901210784912,
      "learning_rate": 0.0009333129864184343,
      "loss": 1.1536,
      "step": 488
    },
    {
      "epoch": 0.22217174011812812,
      "grad_norm": 4.74009895324707,
      "learning_rate": 0.0009331603845566917,
      "loss": 1.5551,
      "step": 489
    },
    {
      "epoch": 0.222626079054975,
      "grad_norm": 5.753303050994873,
      "learning_rate": 0.0009330077826949489,
      "loss": 2.2021,
      "step": 490
    },
    {
      "epoch": 0.2230804179918219,
      "grad_norm": 4.603237628936768,
      "learning_rate": 0.0009328551808332062,
      "loss": 0.8779,
      "step": 491
    },
    {
      "epoch": 0.22353475692866878,
      "grad_norm": 3.4563276767730713,
      "learning_rate": 0.0009327025789714635,
      "loss": 1.3759,
      "step": 492
    },
    {
      "epoch": 0.22398909586551569,
      "grad_norm": 4.847369194030762,
      "learning_rate": 0.0009325499771097208,
      "loss": 1.4832,
      "step": 493
    },
    {
      "epoch": 0.22444343480236256,
      "grad_norm": 7.879354000091553,
      "learning_rate": 0.000932397375247978,
      "loss": 1.6465,
      "step": 494
    },
    {
      "epoch": 0.22489777373920944,
      "grad_norm": 2.858377695083618,
      "learning_rate": 0.0009322447733862354,
      "loss": 0.8818,
      "step": 495
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 4.103867053985596,
      "learning_rate": 0.0009320921715244927,
      "loss": 1.1715,
      "step": 496
    },
    {
      "epoch": 0.22580645161290322,
      "grad_norm": 4.983833312988281,
      "learning_rate": 0.0009319395696627499,
      "loss": 1.978,
      "step": 497
    },
    {
      "epoch": 0.22626079054975012,
      "grad_norm": 5.14265251159668,
      "learning_rate": 0.0009317869678010073,
      "loss": 1.3398,
      "step": 498
    },
    {
      "epoch": 0.226715129486597,
      "grad_norm": 4.244718074798584,
      "learning_rate": 0.0009316343659392644,
      "loss": 0.7031,
      "step": 499
    },
    {
      "epoch": 0.22716946842344388,
      "grad_norm": 4.052267551422119,
      "learning_rate": 0.0009314817640775217,
      "loss": 1.3007,
      "step": 500
    },
    {
      "epoch": 0.22762380736029078,
      "grad_norm": 3.4163036346435547,
      "learning_rate": 0.0009313291622157791,
      "loss": 0.7556,
      "step": 501
    },
    {
      "epoch": 0.22807814629713766,
      "grad_norm": 4.7087860107421875,
      "learning_rate": 0.0009311765603540363,
      "loss": 1.235,
      "step": 502
    },
    {
      "epoch": 0.22853248523398456,
      "grad_norm": 5.557741165161133,
      "learning_rate": 0.0009310239584922936,
      "loss": 2.3006,
      "step": 503
    },
    {
      "epoch": 0.22898682417083144,
      "grad_norm": 6.312621593475342,
      "learning_rate": 0.0009308713566305509,
      "loss": 1.7725,
      "step": 504
    },
    {
      "epoch": 0.22944116310767831,
      "grad_norm": 7.39268684387207,
      "learning_rate": 0.0009307187547688082,
      "loss": 1.9056,
      "step": 505
    },
    {
      "epoch": 0.22989550204452522,
      "grad_norm": 3.0886428356170654,
      "learning_rate": 0.0009305661529070654,
      "loss": 0.9491,
      "step": 506
    },
    {
      "epoch": 0.2303498409813721,
      "grad_norm": 4.250034809112549,
      "learning_rate": 0.0009304135510453228,
      "loss": 0.9298,
      "step": 507
    },
    {
      "epoch": 0.230804179918219,
      "grad_norm": 3.524555206298828,
      "learning_rate": 0.0009302609491835801,
      "loss": 1.5024,
      "step": 508
    },
    {
      "epoch": 0.23125851885506588,
      "grad_norm": 4.884903907775879,
      "learning_rate": 0.0009301083473218373,
      "loss": 1.6223,
      "step": 509
    },
    {
      "epoch": 0.23171285779191278,
      "grad_norm": 6.04000186920166,
      "learning_rate": 0.0009299557454600947,
      "loss": 1.4292,
      "step": 510
    },
    {
      "epoch": 0.23216719672875966,
      "grad_norm": 4.823760986328125,
      "learning_rate": 0.000929803143598352,
      "loss": 1.2106,
      "step": 511
    },
    {
      "epoch": 0.23262153566560653,
      "grad_norm": 3.3610918521881104,
      "learning_rate": 0.0009296505417366092,
      "loss": 1.5498,
      "step": 512
    },
    {
      "epoch": 0.23307587460245344,
      "grad_norm": 9.057700157165527,
      "learning_rate": 0.0009294979398748666,
      "loss": 1.916,
      "step": 513
    },
    {
      "epoch": 0.2335302135393003,
      "grad_norm": 3.9425299167633057,
      "learning_rate": 0.0009293453380131238,
      "loss": 1.3938,
      "step": 514
    },
    {
      "epoch": 0.23398455247614722,
      "grad_norm": 5.285974025726318,
      "learning_rate": 0.0009291927361513811,
      "loss": 1.9874,
      "step": 515
    },
    {
      "epoch": 0.2344388914129941,
      "grad_norm": 4.023134231567383,
      "learning_rate": 0.0009290401342896384,
      "loss": 0.8728,
      "step": 516
    },
    {
      "epoch": 0.23489323034984097,
      "grad_norm": 7.8974928855896,
      "learning_rate": 0.0009288875324278956,
      "loss": 1.6332,
      "step": 517
    },
    {
      "epoch": 0.23534756928668787,
      "grad_norm": 7.250007152557373,
      "learning_rate": 0.0009287349305661529,
      "loss": 2.14,
      "step": 518
    },
    {
      "epoch": 0.23580190822353475,
      "grad_norm": 5.450918674468994,
      "learning_rate": 0.0009285823287044102,
      "loss": 1.3023,
      "step": 519
    },
    {
      "epoch": 0.23625624716038165,
      "grad_norm": 6.659681797027588,
      "learning_rate": 0.0009284297268426675,
      "loss": 1.7903,
      "step": 520
    },
    {
      "epoch": 0.23671058609722853,
      "grad_norm": 9.149643898010254,
      "learning_rate": 0.0009282771249809247,
      "loss": 2.3153,
      "step": 521
    },
    {
      "epoch": 0.2371649250340754,
      "grad_norm": 4.200259685516357,
      "learning_rate": 0.0009281245231191821,
      "loss": 1.4539,
      "step": 522
    },
    {
      "epoch": 0.2376192639709223,
      "grad_norm": 6.091450214385986,
      "learning_rate": 0.0009279719212574393,
      "loss": 2.5699,
      "step": 523
    },
    {
      "epoch": 0.2380736029077692,
      "grad_norm": 3.177809000015259,
      "learning_rate": 0.0009278193193956966,
      "loss": 0.8812,
      "step": 524
    },
    {
      "epoch": 0.2385279418446161,
      "grad_norm": 4.273008346557617,
      "learning_rate": 0.000927666717533954,
      "loss": 1.3761,
      "step": 525
    },
    {
      "epoch": 0.23898228078146297,
      "grad_norm": 2.9500999450683594,
      "learning_rate": 0.0009275141156722112,
      "loss": 0.7171,
      "step": 526
    },
    {
      "epoch": 0.23943661971830985,
      "grad_norm": 4.709161281585693,
      "learning_rate": 0.0009273615138104685,
      "loss": 1.1487,
      "step": 527
    },
    {
      "epoch": 0.23989095865515675,
      "grad_norm": 6.275514125823975,
      "learning_rate": 0.0009272089119487258,
      "loss": 2.0007,
      "step": 528
    },
    {
      "epoch": 0.24034529759200363,
      "grad_norm": 3.549431800842285,
      "learning_rate": 0.0009270563100869831,
      "loss": 1.5391,
      "step": 529
    },
    {
      "epoch": 0.24079963652885053,
      "grad_norm": 3.827634334564209,
      "learning_rate": 0.0009269037082252404,
      "loss": 2.0854,
      "step": 530
    },
    {
      "epoch": 0.2412539754656974,
      "grad_norm": 3.044224262237549,
      "learning_rate": 0.0009267511063634977,
      "loss": 0.8641,
      "step": 531
    },
    {
      "epoch": 0.2417083144025443,
      "grad_norm": 4.237382888793945,
      "learning_rate": 0.000926598504501755,
      "loss": 0.8488,
      "step": 532
    },
    {
      "epoch": 0.2421626533393912,
      "grad_norm": 6.293590545654297,
      "learning_rate": 0.0009264459026400122,
      "loss": 1.7358,
      "step": 533
    },
    {
      "epoch": 0.24261699227623806,
      "grad_norm": 4.809879302978516,
      "learning_rate": 0.0009262933007782696,
      "loss": 1.1807,
      "step": 534
    },
    {
      "epoch": 0.24307133121308497,
      "grad_norm": 4.56773567199707,
      "learning_rate": 0.0009261406989165268,
      "loss": 1.4164,
      "step": 535
    },
    {
      "epoch": 0.24352567014993184,
      "grad_norm": 4.453222274780273,
      "learning_rate": 0.000925988097054784,
      "loss": 1.759,
      "step": 536
    },
    {
      "epoch": 0.24398000908677875,
      "grad_norm": 4.046647548675537,
      "learning_rate": 0.0009258354951930414,
      "loss": 0.8062,
      "step": 537
    },
    {
      "epoch": 0.24443434802362562,
      "grad_norm": 5.971732139587402,
      "learning_rate": 0.0009256828933312986,
      "loss": 1.7187,
      "step": 538
    },
    {
      "epoch": 0.2448886869604725,
      "grad_norm": 4.943645000457764,
      "learning_rate": 0.0009255302914695559,
      "loss": 0.962,
      "step": 539
    },
    {
      "epoch": 0.2453430258973194,
      "grad_norm": 2.518033504486084,
      "learning_rate": 0.0009253776896078133,
      "loss": 1.064,
      "step": 540
    },
    {
      "epoch": 0.24579736483416628,
      "grad_norm": 8.027234077453613,
      "learning_rate": 0.0009252250877460705,
      "loss": 2.5086,
      "step": 541
    },
    {
      "epoch": 0.2462517037710132,
      "grad_norm": 5.246140003204346,
      "learning_rate": 0.0009250724858843278,
      "loss": 1.5558,
      "step": 542
    },
    {
      "epoch": 0.24670604270786006,
      "grad_norm": 5.483332633972168,
      "learning_rate": 0.0009249198840225851,
      "loss": 2.0179,
      "step": 543
    },
    {
      "epoch": 0.24716038164470694,
      "grad_norm": 3.19100022315979,
      "learning_rate": 0.0009247672821608424,
      "loss": 0.7591,
      "step": 544
    },
    {
      "epoch": 0.24761472058155384,
      "grad_norm": 6.662031650543213,
      "learning_rate": 0.0009246146802990996,
      "loss": 1.5568,
      "step": 545
    },
    {
      "epoch": 0.24806905951840072,
      "grad_norm": 5.204570293426514,
      "learning_rate": 0.000924462078437357,
      "loss": 0.8228,
      "step": 546
    },
    {
      "epoch": 0.24852339845524762,
      "grad_norm": 5.730715274810791,
      "learning_rate": 0.0009243094765756143,
      "loss": 1.5326,
      "step": 547
    },
    {
      "epoch": 0.2489777373920945,
      "grad_norm": 6.679883003234863,
      "learning_rate": 0.0009241568747138715,
      "loss": 1.529,
      "step": 548
    },
    {
      "epoch": 0.24943207632894138,
      "grad_norm": 10.029292106628418,
      "learning_rate": 0.0009240042728521289,
      "loss": 1.7187,
      "step": 549
    },
    {
      "epoch": 0.24988641526578828,
      "grad_norm": 5.180072784423828,
      "learning_rate": 0.0009238516709903861,
      "loss": 1.4721,
      "step": 550
    },
    {
      "epoch": 0.2503407542026352,
      "grad_norm": 3.6068618297576904,
      "learning_rate": 0.0009236990691286434,
      "loss": 0.528,
      "step": 551
    },
    {
      "epoch": 0.25079509313948206,
      "grad_norm": 4.126284599304199,
      "learning_rate": 0.0009235464672669008,
      "loss": 1.4289,
      "step": 552
    },
    {
      "epoch": 0.25124943207632894,
      "grad_norm": 8.755053520202637,
      "learning_rate": 0.0009233938654051579,
      "loss": 1.3203,
      "step": 553
    },
    {
      "epoch": 0.2517037710131758,
      "grad_norm": 5.032867431640625,
      "learning_rate": 0.0009232412635434152,
      "loss": 1.4822,
      "step": 554
    },
    {
      "epoch": 0.2521581099500227,
      "grad_norm": 6.3786396980285645,
      "learning_rate": 0.0009230886616816725,
      "loss": 0.8053,
      "step": 555
    },
    {
      "epoch": 0.2526124488868696,
      "grad_norm": 5.1681671142578125,
      "learning_rate": 0.0009229360598199298,
      "loss": 2.4012,
      "step": 556
    },
    {
      "epoch": 0.2530667878237165,
      "grad_norm": 4.100263595581055,
      "learning_rate": 0.000922783457958187,
      "loss": 0.8291,
      "step": 557
    },
    {
      "epoch": 0.2535211267605634,
      "grad_norm": 3.3364055156707764,
      "learning_rate": 0.0009226308560964444,
      "loss": 0.5106,
      "step": 558
    },
    {
      "epoch": 0.25397546569741025,
      "grad_norm": 4.887160301208496,
      "learning_rate": 0.0009224782542347017,
      "loss": 1.0372,
      "step": 559
    },
    {
      "epoch": 0.25442980463425713,
      "grad_norm": 4.523820877075195,
      "learning_rate": 0.0009223256523729589,
      "loss": 1.1551,
      "step": 560
    },
    {
      "epoch": 0.25488414357110406,
      "grad_norm": 5.33235502243042,
      "learning_rate": 0.0009221730505112163,
      "loss": 1.3016,
      "step": 561
    },
    {
      "epoch": 0.25533848250795094,
      "grad_norm": 6.062936305999756,
      "learning_rate": 0.0009220204486494735,
      "loss": 1.3854,
      "step": 562
    },
    {
      "epoch": 0.2557928214447978,
      "grad_norm": 6.7851433753967285,
      "learning_rate": 0.0009218678467877308,
      "loss": 2.0811,
      "step": 563
    },
    {
      "epoch": 0.2562471603816447,
      "grad_norm": 5.023318290710449,
      "learning_rate": 0.0009217152449259882,
      "loss": 1.1233,
      "step": 564
    },
    {
      "epoch": 0.2567014993184916,
      "grad_norm": 5.412562370300293,
      "learning_rate": 0.0009215626430642454,
      "loss": 1.226,
      "step": 565
    },
    {
      "epoch": 0.2571558382553385,
      "grad_norm": 5.4040913581848145,
      "learning_rate": 0.0009214100412025027,
      "loss": 1.1275,
      "step": 566
    },
    {
      "epoch": 0.2576101771921854,
      "grad_norm": 4.76725959777832,
      "learning_rate": 0.00092125743934076,
      "loss": 1.6946,
      "step": 567
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 4.946346759796143,
      "learning_rate": 0.0009211048374790173,
      "loss": 1.3524,
      "step": 568
    },
    {
      "epoch": 0.2585188550658791,
      "grad_norm": 5.389051914215088,
      "learning_rate": 0.0009209522356172746,
      "loss": 1.7749,
      "step": 569
    },
    {
      "epoch": 0.25897319400272606,
      "grad_norm": 5.0096755027771,
      "learning_rate": 0.0009207996337555319,
      "loss": 1.3946,
      "step": 570
    },
    {
      "epoch": 0.25942753293957294,
      "grad_norm": 5.072806358337402,
      "learning_rate": 0.0009206470318937892,
      "loss": 2.0729,
      "step": 571
    },
    {
      "epoch": 0.2598818718764198,
      "grad_norm": 4.723791122436523,
      "learning_rate": 0.0009204944300320463,
      "loss": 1.3054,
      "step": 572
    },
    {
      "epoch": 0.2603362108132667,
      "grad_norm": 5.575528144836426,
      "learning_rate": 0.0009203418281703037,
      "loss": 1.5758,
      "step": 573
    },
    {
      "epoch": 0.26079054975011356,
      "grad_norm": 6.84596061706543,
      "learning_rate": 0.0009201892263085609,
      "loss": 1.7747,
      "step": 574
    },
    {
      "epoch": 0.2612448886869605,
      "grad_norm": 8.100935935974121,
      "learning_rate": 0.0009200366244468182,
      "loss": 0.7659,
      "step": 575
    },
    {
      "epoch": 0.2616992276238074,
      "grad_norm": 5.519986629486084,
      "learning_rate": 0.0009198840225850756,
      "loss": 2.2464,
      "step": 576
    },
    {
      "epoch": 0.26215356656065425,
      "grad_norm": 4.773331642150879,
      "learning_rate": 0.0009197314207233328,
      "loss": 1.33,
      "step": 577
    },
    {
      "epoch": 0.2626079054975011,
      "grad_norm": 5.48815393447876,
      "learning_rate": 0.0009195788188615901,
      "loss": 1.2504,
      "step": 578
    },
    {
      "epoch": 0.263062244434348,
      "grad_norm": 4.384014129638672,
      "learning_rate": 0.0009194262169998474,
      "loss": 0.918,
      "step": 579
    },
    {
      "epoch": 0.26351658337119493,
      "grad_norm": 4.061789512634277,
      "learning_rate": 0.0009192736151381047,
      "loss": 1.2569,
      "step": 580
    },
    {
      "epoch": 0.2639709223080418,
      "grad_norm": 7.078236103057861,
      "learning_rate": 0.000919121013276362,
      "loss": 1.0958,
      "step": 581
    },
    {
      "epoch": 0.2644252612448887,
      "grad_norm": 4.4202117919921875,
      "learning_rate": 0.0009189684114146193,
      "loss": 1.1517,
      "step": 582
    },
    {
      "epoch": 0.26487960018173556,
      "grad_norm": 4.816877841949463,
      "learning_rate": 0.0009188158095528766,
      "loss": 1.5249,
      "step": 583
    },
    {
      "epoch": 0.26533393911858244,
      "grad_norm": 5.253818035125732,
      "learning_rate": 0.0009186632076911338,
      "loss": 1.8154,
      "step": 584
    },
    {
      "epoch": 0.26578827805542937,
      "grad_norm": 7.396517276763916,
      "learning_rate": 0.0009185106058293912,
      "loss": 2.1731,
      "step": 585
    },
    {
      "epoch": 0.26624261699227625,
      "grad_norm": 4.9590301513671875,
      "learning_rate": 0.0009183580039676485,
      "loss": 0.8487,
      "step": 586
    },
    {
      "epoch": 0.2666969559291231,
      "grad_norm": 6.355540752410889,
      "learning_rate": 0.0009182054021059057,
      "loss": 2.0731,
      "step": 587
    },
    {
      "epoch": 0.26715129486597,
      "grad_norm": 4.560359954833984,
      "learning_rate": 0.0009180528002441631,
      "loss": 1.2604,
      "step": 588
    },
    {
      "epoch": 0.2676056338028169,
      "grad_norm": 7.685898303985596,
      "learning_rate": 0.0009179001983824203,
      "loss": 2.0147,
      "step": 589
    },
    {
      "epoch": 0.2680599727396638,
      "grad_norm": 4.235246181488037,
      "learning_rate": 0.0009177475965206775,
      "loss": 0.9788,
      "step": 590
    },
    {
      "epoch": 0.2685143116765107,
      "grad_norm": 4.622025489807129,
      "learning_rate": 0.0009175949946589348,
      "loss": 1.0884,
      "step": 591
    },
    {
      "epoch": 0.26896865061335756,
      "grad_norm": 7.128083229064941,
      "learning_rate": 0.0009174423927971921,
      "loss": 1.599,
      "step": 592
    },
    {
      "epoch": 0.26942298955020444,
      "grad_norm": 3.528709888458252,
      "learning_rate": 0.0009172897909354494,
      "loss": 0.8349,
      "step": 593
    },
    {
      "epoch": 0.2698773284870513,
      "grad_norm": 3.4778661727905273,
      "learning_rate": 0.0009171371890737067,
      "loss": 0.8394,
      "step": 594
    },
    {
      "epoch": 0.27033166742389825,
      "grad_norm": 3.74234676361084,
      "learning_rate": 0.000916984587211964,
      "loss": 1.3801,
      "step": 595
    },
    {
      "epoch": 0.2707860063607451,
      "grad_norm": 5.004946231842041,
      "learning_rate": 0.0009168319853502212,
      "loss": 1.0736,
      "step": 596
    },
    {
      "epoch": 0.271240345297592,
      "grad_norm": 6.021921634674072,
      "learning_rate": 0.0009166793834884786,
      "loss": 2.0274,
      "step": 597
    },
    {
      "epoch": 0.2716946842344389,
      "grad_norm": 4.134975910186768,
      "learning_rate": 0.0009165267816267359,
      "loss": 1.258,
      "step": 598
    },
    {
      "epoch": 0.27214902317128575,
      "grad_norm": 8.249998092651367,
      "learning_rate": 0.0009163741797649931,
      "loss": 1.2967,
      "step": 599
    },
    {
      "epoch": 0.2726033621081327,
      "grad_norm": 5.583903789520264,
      "learning_rate": 0.0009162215779032505,
      "loss": 1.0022,
      "step": 600
    },
    {
      "epoch": 0.27305770104497956,
      "grad_norm": 5.472000598907471,
      "learning_rate": 0.0009160689760415077,
      "loss": 1.8403,
      "step": 601
    },
    {
      "epoch": 0.27351203998182644,
      "grad_norm": 7.100162029266357,
      "learning_rate": 0.000915916374179765,
      "loss": 2.2491,
      "step": 602
    },
    {
      "epoch": 0.2739663789186733,
      "grad_norm": 6.804991722106934,
      "learning_rate": 0.0009157637723180224,
      "loss": 1.4767,
      "step": 603
    },
    {
      "epoch": 0.2744207178555202,
      "grad_norm": 3.161573886871338,
      "learning_rate": 0.0009156111704562796,
      "loss": 0.3372,
      "step": 604
    },
    {
      "epoch": 0.2748750567923671,
      "grad_norm": 4.5229973793029785,
      "learning_rate": 0.0009154585685945369,
      "loss": 1.0128,
      "step": 605
    },
    {
      "epoch": 0.275329395729214,
      "grad_norm": 5.653120040893555,
      "learning_rate": 0.0009153059667327942,
      "loss": 1.336,
      "step": 606
    },
    {
      "epoch": 0.2757837346660609,
      "grad_norm": 2.4268925189971924,
      "learning_rate": 0.0009151533648710515,
      "loss": 0.6291,
      "step": 607
    },
    {
      "epoch": 0.27623807360290775,
      "grad_norm": 1.5728938579559326,
      "learning_rate": 0.0009150007630093086,
      "loss": 0.2365,
      "step": 608
    },
    {
      "epoch": 0.2766924125397547,
      "grad_norm": 5.880955696105957,
      "learning_rate": 0.000914848161147566,
      "loss": 1.3506,
      "step": 609
    },
    {
      "epoch": 0.27714675147660156,
      "grad_norm": 3.3033854961395264,
      "learning_rate": 0.0009146955592858233,
      "loss": 0.7932,
      "step": 610
    },
    {
      "epoch": 0.27760109041344844,
      "grad_norm": 8.630804061889648,
      "learning_rate": 0.0009145429574240805,
      "loss": 2.1612,
      "step": 611
    },
    {
      "epoch": 0.2780554293502953,
      "grad_norm": 7.046489238739014,
      "learning_rate": 0.0009143903555623379,
      "loss": 1.8017,
      "step": 612
    },
    {
      "epoch": 0.2785097682871422,
      "grad_norm": 4.609836578369141,
      "learning_rate": 0.0009142377537005951,
      "loss": 1.0406,
      "step": 613
    },
    {
      "epoch": 0.2789641072239891,
      "grad_norm": 6.363504409790039,
      "learning_rate": 0.0009140851518388524,
      "loss": 1.6503,
      "step": 614
    },
    {
      "epoch": 0.279418446160836,
      "grad_norm": 5.37036657333374,
      "learning_rate": 0.0009139325499771098,
      "loss": 0.8727,
      "step": 615
    },
    {
      "epoch": 0.2798727850976829,
      "grad_norm": 6.577094554901123,
      "learning_rate": 0.000913779948115367,
      "loss": 1.4743,
      "step": 616
    },
    {
      "epoch": 0.28032712403452975,
      "grad_norm": 5.413032531738281,
      "learning_rate": 0.0009136273462536243,
      "loss": 0.7801,
      "step": 617
    },
    {
      "epoch": 0.2807814629713766,
      "grad_norm": 4.95162296295166,
      "learning_rate": 0.0009134747443918816,
      "loss": 1.1429,
      "step": 618
    },
    {
      "epoch": 0.28123580190822356,
      "grad_norm": 5.99307918548584,
      "learning_rate": 0.0009133221425301389,
      "loss": 1.0337,
      "step": 619
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 7.192525386810303,
      "learning_rate": 0.0009131695406683961,
      "loss": 1.4794,
      "step": 620
    },
    {
      "epoch": 0.2821444797819173,
      "grad_norm": 5.389986991882324,
      "learning_rate": 0.0009130169388066535,
      "loss": 1.5985,
      "step": 621
    },
    {
      "epoch": 0.2825988187187642,
      "grad_norm": 6.388218879699707,
      "learning_rate": 0.0009128643369449108,
      "loss": 1.39,
      "step": 622
    },
    {
      "epoch": 0.28305315765561107,
      "grad_norm": 7.198439598083496,
      "learning_rate": 0.000912711735083168,
      "loss": 2.0477,
      "step": 623
    },
    {
      "epoch": 0.283507496592458,
      "grad_norm": 6.589672565460205,
      "learning_rate": 0.0009125591332214254,
      "loss": 1.0592,
      "step": 624
    },
    {
      "epoch": 0.2839618355293049,
      "grad_norm": 7.428500175476074,
      "learning_rate": 0.0009124065313596826,
      "loss": 1.7249,
      "step": 625
    },
    {
      "epoch": 0.28441617446615175,
      "grad_norm": 2.6205573081970215,
      "learning_rate": 0.0009122539294979398,
      "loss": 0.709,
      "step": 626
    },
    {
      "epoch": 0.2848705134029986,
      "grad_norm": 8.424569129943848,
      "learning_rate": 0.0009121013276361972,
      "loss": 2.031,
      "step": 627
    },
    {
      "epoch": 0.2853248523398455,
      "grad_norm": 6.911533355712891,
      "learning_rate": 0.0009119487257744544,
      "loss": 1.0313,
      "step": 628
    },
    {
      "epoch": 0.28577919127669243,
      "grad_norm": 7.072881698608398,
      "learning_rate": 0.0009117961239127117,
      "loss": 2.3812,
      "step": 629
    },
    {
      "epoch": 0.2862335302135393,
      "grad_norm": 5.830142974853516,
      "learning_rate": 0.000911643522050969,
      "loss": 1.522,
      "step": 630
    },
    {
      "epoch": 0.2866878691503862,
      "grad_norm": 5.957951545715332,
      "learning_rate": 0.0009114909201892263,
      "loss": 1.3728,
      "step": 631
    },
    {
      "epoch": 0.28714220808723306,
      "grad_norm": 4.633702754974365,
      "learning_rate": 0.0009113383183274835,
      "loss": 1.134,
      "step": 632
    },
    {
      "epoch": 0.28759654702407994,
      "grad_norm": 4.490708351135254,
      "learning_rate": 0.0009111857164657409,
      "loss": 1.1693,
      "step": 633
    },
    {
      "epoch": 0.2880508859609269,
      "grad_norm": 5.500217437744141,
      "learning_rate": 0.0009110331146039982,
      "loss": 1.699,
      "step": 634
    },
    {
      "epoch": 0.28850522489777375,
      "grad_norm": 5.468127250671387,
      "learning_rate": 0.0009108805127422554,
      "loss": 1.4934,
      "step": 635
    },
    {
      "epoch": 0.2889595638346206,
      "grad_norm": 4.226212978363037,
      "learning_rate": 0.0009107279108805128,
      "loss": 0.9285,
      "step": 636
    },
    {
      "epoch": 0.2894139027714675,
      "grad_norm": 6.202568531036377,
      "learning_rate": 0.00091057530901877,
      "loss": 1.3562,
      "step": 637
    },
    {
      "epoch": 0.2898682417083144,
      "grad_norm": 4.931166648864746,
      "learning_rate": 0.0009104227071570273,
      "loss": 1.7361,
      "step": 638
    },
    {
      "epoch": 0.2903225806451613,
      "grad_norm": 5.546168327331543,
      "learning_rate": 0.0009102701052952847,
      "loss": 1.4409,
      "step": 639
    },
    {
      "epoch": 0.2907769195820082,
      "grad_norm": 4.295429706573486,
      "learning_rate": 0.0009101175034335419,
      "loss": 1.3949,
      "step": 640
    },
    {
      "epoch": 0.29123125851885506,
      "grad_norm": 5.603832244873047,
      "learning_rate": 0.0009099649015717992,
      "loss": 1.2488,
      "step": 641
    },
    {
      "epoch": 0.29168559745570194,
      "grad_norm": 3.2638766765594482,
      "learning_rate": 0.0009098122997100565,
      "loss": 0.7774,
      "step": 642
    },
    {
      "epoch": 0.2921399363925488,
      "grad_norm": 4.885572910308838,
      "learning_rate": 0.0009096596978483138,
      "loss": 1.5704,
      "step": 643
    },
    {
      "epoch": 0.29259427532939575,
      "grad_norm": 7.38744592666626,
      "learning_rate": 0.0009095070959865711,
      "loss": 2.2956,
      "step": 644
    },
    {
      "epoch": 0.2930486142662426,
      "grad_norm": 2.9849119186401367,
      "learning_rate": 0.0009093544941248283,
      "loss": 0.6937,
      "step": 645
    },
    {
      "epoch": 0.2935029532030895,
      "grad_norm": 4.190621376037598,
      "learning_rate": 0.0009092018922630856,
      "loss": 1.3973,
      "step": 646
    },
    {
      "epoch": 0.2939572921399364,
      "grad_norm": 6.036888599395752,
      "learning_rate": 0.0009090492904013428,
      "loss": 1.2425,
      "step": 647
    },
    {
      "epoch": 0.29441163107678325,
      "grad_norm": 5.346428871154785,
      "learning_rate": 0.0009088966885396002,
      "loss": 1.2555,
      "step": 648
    },
    {
      "epoch": 0.2948659700136302,
      "grad_norm": 5.6636576652526855,
      "learning_rate": 0.0009087440866778574,
      "loss": 1.3117,
      "step": 649
    },
    {
      "epoch": 0.29532030895047706,
      "grad_norm": 4.384967803955078,
      "learning_rate": 0.0009085914848161147,
      "loss": 0.9854,
      "step": 650
    },
    {
      "epoch": 0.29577464788732394,
      "grad_norm": 6.367961883544922,
      "learning_rate": 0.0009084388829543721,
      "loss": 1.6923,
      "step": 651
    },
    {
      "epoch": 0.2962289868241708,
      "grad_norm": 6.390758514404297,
      "learning_rate": 0.0009082862810926293,
      "loss": 1.4467,
      "step": 652
    },
    {
      "epoch": 0.29668332576101775,
      "grad_norm": 5.711019515991211,
      "learning_rate": 0.0009081336792308866,
      "loss": 1.4172,
      "step": 653
    },
    {
      "epoch": 0.2971376646978646,
      "grad_norm": 7.737837791442871,
      "learning_rate": 0.000907981077369144,
      "loss": 1.3,
      "step": 654
    },
    {
      "epoch": 0.2975920036347115,
      "grad_norm": 7.054724216461182,
      "learning_rate": 0.0009078284755074012,
      "loss": 1.6923,
      "step": 655
    },
    {
      "epoch": 0.2980463425715584,
      "grad_norm": 10.139739990234375,
      "learning_rate": 0.0009076758736456585,
      "loss": 1.1614,
      "step": 656
    },
    {
      "epoch": 0.29850068150840525,
      "grad_norm": 5.842681884765625,
      "learning_rate": 0.0009075232717839158,
      "loss": 0.8983,
      "step": 657
    },
    {
      "epoch": 0.2989550204452522,
      "grad_norm": 4.3310394287109375,
      "learning_rate": 0.0009073706699221731,
      "loss": 1.4263,
      "step": 658
    },
    {
      "epoch": 0.29940935938209906,
      "grad_norm": 4.853456497192383,
      "learning_rate": 0.0009072180680604303,
      "loss": 1.3812,
      "step": 659
    },
    {
      "epoch": 0.29986369831894594,
      "grad_norm": 7.142138481140137,
      "learning_rate": 0.0009070654661986877,
      "loss": 1.5735,
      "step": 660
    },
    {
      "epoch": 0.3003180372557928,
      "grad_norm": 6.551620960235596,
      "learning_rate": 0.000906912864336945,
      "loss": 1.7851,
      "step": 661
    },
    {
      "epoch": 0.3007723761926397,
      "grad_norm": 4.533819198608398,
      "learning_rate": 0.0009067602624752022,
      "loss": 1.9254,
      "step": 662
    },
    {
      "epoch": 0.3012267151294866,
      "grad_norm": 4.494215488433838,
      "learning_rate": 0.0009066076606134595,
      "loss": 1.3916,
      "step": 663
    },
    {
      "epoch": 0.3016810540663335,
      "grad_norm": 3.2439515590667725,
      "learning_rate": 0.0009064550587517167,
      "loss": 0.7816,
      "step": 664
    },
    {
      "epoch": 0.3021353930031804,
      "grad_norm": 5.469624996185303,
      "learning_rate": 0.000906302456889974,
      "loss": 1.414,
      "step": 665
    },
    {
      "epoch": 0.30258973194002725,
      "grad_norm": 6.507264614105225,
      "learning_rate": 0.0009061498550282314,
      "loss": 1.4308,
      "step": 666
    },
    {
      "epoch": 0.3030440708768741,
      "grad_norm": 7.361557483673096,
      "learning_rate": 0.0009059972531664886,
      "loss": 1.8834,
      "step": 667
    },
    {
      "epoch": 0.30349840981372106,
      "grad_norm": 5.164240837097168,
      "learning_rate": 0.0009058446513047459,
      "loss": 0.7788,
      "step": 668
    },
    {
      "epoch": 0.30395274875056794,
      "grad_norm": 8.768113136291504,
      "learning_rate": 0.0009056920494430032,
      "loss": 1.8272,
      "step": 669
    },
    {
      "epoch": 0.3044070876874148,
      "grad_norm": 4.383128643035889,
      "learning_rate": 0.0009055394475812605,
      "loss": 1.4038,
      "step": 670
    },
    {
      "epoch": 0.3048614266242617,
      "grad_norm": 5.78069543838501,
      "learning_rate": 0.0009053868457195177,
      "loss": 1.7492,
      "step": 671
    },
    {
      "epoch": 0.30531576556110857,
      "grad_norm": 5.917608261108398,
      "learning_rate": 0.0009052342438577751,
      "loss": 1.7623,
      "step": 672
    },
    {
      "epoch": 0.3057701044979555,
      "grad_norm": 6.333732604980469,
      "learning_rate": 0.0009050816419960324,
      "loss": 2.1098,
      "step": 673
    },
    {
      "epoch": 0.3062244434348024,
      "grad_norm": 4.973388671875,
      "learning_rate": 0.0009049290401342896,
      "loss": 1.1607,
      "step": 674
    },
    {
      "epoch": 0.30667878237164925,
      "grad_norm": 2.311116933822632,
      "learning_rate": 0.000904776438272547,
      "loss": 0.5885,
      "step": 675
    },
    {
      "epoch": 0.3071331213084961,
      "grad_norm": 5.2455525398254395,
      "learning_rate": 0.0009046238364108042,
      "loss": 1.0814,
      "step": 676
    },
    {
      "epoch": 0.307587460245343,
      "grad_norm": 4.60811185836792,
      "learning_rate": 0.0009044712345490616,
      "loss": 1.0617,
      "step": 677
    },
    {
      "epoch": 0.30804179918218993,
      "grad_norm": 5.379466533660889,
      "learning_rate": 0.0009043186326873189,
      "loss": 1.5779,
      "step": 678
    },
    {
      "epoch": 0.3084961381190368,
      "grad_norm": 4.111013889312744,
      "learning_rate": 0.0009041660308255761,
      "loss": 1.3002,
      "step": 679
    },
    {
      "epoch": 0.3089504770558837,
      "grad_norm": 5.266077995300293,
      "learning_rate": 0.0009040134289638335,
      "loss": 1.4072,
      "step": 680
    },
    {
      "epoch": 0.30940481599273056,
      "grad_norm": 4.921057224273682,
      "learning_rate": 0.0009038608271020906,
      "loss": 1.3558,
      "step": 681
    },
    {
      "epoch": 0.30985915492957744,
      "grad_norm": 4.8222880363464355,
      "learning_rate": 0.0009037082252403479,
      "loss": 1.4584,
      "step": 682
    },
    {
      "epoch": 0.3103134938664244,
      "grad_norm": 3.070258378982544,
      "learning_rate": 0.0009035556233786051,
      "loss": 0.455,
      "step": 683
    },
    {
      "epoch": 0.31076783280327125,
      "grad_norm": 5.135226249694824,
      "learning_rate": 0.0009034030215168625,
      "loss": 1.1599,
      "step": 684
    },
    {
      "epoch": 0.3112221717401181,
      "grad_norm": 6.606522560119629,
      "learning_rate": 0.0009032504196551198,
      "loss": 1.5742,
      "step": 685
    },
    {
      "epoch": 0.311676510676965,
      "grad_norm": 4.983480930328369,
      "learning_rate": 0.0009030978177933771,
      "loss": 1.3242,
      "step": 686
    },
    {
      "epoch": 0.3121308496138119,
      "grad_norm": 5.344476699829102,
      "learning_rate": 0.0009029452159316344,
      "loss": 1.0436,
      "step": 687
    },
    {
      "epoch": 0.3125851885506588,
      "grad_norm": 5.968101501464844,
      "learning_rate": 0.0009027926140698916,
      "loss": 1.3507,
      "step": 688
    },
    {
      "epoch": 0.3130395274875057,
      "grad_norm": 5.695165634155273,
      "learning_rate": 0.000902640012208149,
      "loss": 1.3303,
      "step": 689
    },
    {
      "epoch": 0.31349386642435256,
      "grad_norm": 5.3923797607421875,
      "learning_rate": 0.0009024874103464063,
      "loss": 1.8582,
      "step": 690
    },
    {
      "epoch": 0.31394820536119944,
      "grad_norm": 8.225834846496582,
      "learning_rate": 0.0009023348084846635,
      "loss": 1.2481,
      "step": 691
    },
    {
      "epoch": 0.3144025442980463,
      "grad_norm": 4.872406005859375,
      "learning_rate": 0.0009021822066229209,
      "loss": 1.0433,
      "step": 692
    },
    {
      "epoch": 0.31485688323489325,
      "grad_norm": 5.271617889404297,
      "learning_rate": 0.0009020296047611781,
      "loss": 1.1346,
      "step": 693
    },
    {
      "epoch": 0.3153112221717401,
      "grad_norm": 3.7310118675231934,
      "learning_rate": 0.0009018770028994354,
      "loss": 0.6126,
      "step": 694
    },
    {
      "epoch": 0.315765561108587,
      "grad_norm": 4.534595966339111,
      "learning_rate": 0.0009017244010376928,
      "loss": 1.8715,
      "step": 695
    },
    {
      "epoch": 0.3162199000454339,
      "grad_norm": 7.9659624099731445,
      "learning_rate": 0.00090157179917595,
      "loss": 1.2456,
      "step": 696
    },
    {
      "epoch": 0.3166742389822808,
      "grad_norm": 2.746971607208252,
      "learning_rate": 0.0009014191973142073,
      "loss": 0.1998,
      "step": 697
    },
    {
      "epoch": 0.3171285779191277,
      "grad_norm": 4.802375793457031,
      "learning_rate": 0.0009012665954524646,
      "loss": 1.5132,
      "step": 698
    },
    {
      "epoch": 0.31758291685597456,
      "grad_norm": 4.404576778411865,
      "learning_rate": 0.0009011139935907218,
      "loss": 0.9696,
      "step": 699
    },
    {
      "epoch": 0.31803725579282144,
      "grad_norm": 4.736330509185791,
      "learning_rate": 0.000900961391728979,
      "loss": 1.0385,
      "step": 700
    },
    {
      "epoch": 0.3184915947296683,
      "grad_norm": 4.363767623901367,
      "learning_rate": 0.0009008087898672364,
      "loss": 1.2364,
      "step": 701
    },
    {
      "epoch": 0.31894593366651525,
      "grad_norm": 3.9340288639068604,
      "learning_rate": 0.0009006561880054937,
      "loss": 0.6827,
      "step": 702
    },
    {
      "epoch": 0.3194002726033621,
      "grad_norm": 6.679687976837158,
      "learning_rate": 0.0009005035861437509,
      "loss": 1.5536,
      "step": 703
    },
    {
      "epoch": 0.319854611540209,
      "grad_norm": 7.124037265777588,
      "learning_rate": 0.0009003509842820083,
      "loss": 1.5573,
      "step": 704
    },
    {
      "epoch": 0.3203089504770559,
      "grad_norm": 10.348806381225586,
      "learning_rate": 0.0009001983824202655,
      "loss": 1.1839,
      "step": 705
    },
    {
      "epoch": 0.32076328941390275,
      "grad_norm": 7.161068439483643,
      "learning_rate": 0.0009000457805585228,
      "loss": 0.9171,
      "step": 706
    },
    {
      "epoch": 0.3212176283507497,
      "grad_norm": 5.507075309753418,
      "learning_rate": 0.0008998931786967802,
      "loss": 1.051,
      "step": 707
    },
    {
      "epoch": 0.32167196728759656,
      "grad_norm": 5.178337097167969,
      "learning_rate": 0.0008997405768350374,
      "loss": 1.146,
      "step": 708
    },
    {
      "epoch": 0.32212630622444344,
      "grad_norm": 7.401108741760254,
      "learning_rate": 0.0008995879749732947,
      "loss": 1.0331,
      "step": 709
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 8.693160057067871,
      "learning_rate": 0.000899435373111552,
      "loss": 1.9827,
      "step": 710
    },
    {
      "epoch": 0.3230349840981372,
      "grad_norm": 4.160111427307129,
      "learning_rate": 0.0008992827712498093,
      "loss": 1.0471,
      "step": 711
    },
    {
      "epoch": 0.3234893230349841,
      "grad_norm": 5.818142890930176,
      "learning_rate": 0.0008991301693880666,
      "loss": 1.8707,
      "step": 712
    },
    {
      "epoch": 0.323943661971831,
      "grad_norm": 7.5347065925598145,
      "learning_rate": 0.0008989775675263239,
      "loss": 1.625,
      "step": 713
    },
    {
      "epoch": 0.3243980009086779,
      "grad_norm": 7.068375110626221,
      "learning_rate": 0.0008988249656645812,
      "loss": 2.4389,
      "step": 714
    },
    {
      "epoch": 0.32485233984552475,
      "grad_norm": 4.19640588760376,
      "learning_rate": 0.0008986723638028384,
      "loss": 1.0454,
      "step": 715
    },
    {
      "epoch": 0.3253066787823716,
      "grad_norm": 5.324029922485352,
      "learning_rate": 0.0008985197619410958,
      "loss": 1.2791,
      "step": 716
    },
    {
      "epoch": 0.32576101771921856,
      "grad_norm": 5.373900890350342,
      "learning_rate": 0.000898367160079353,
      "loss": 0.9585,
      "step": 717
    },
    {
      "epoch": 0.32621535665606544,
      "grad_norm": 6.265317916870117,
      "learning_rate": 0.0008982145582176102,
      "loss": 1.0317,
      "step": 718
    },
    {
      "epoch": 0.3266696955929123,
      "grad_norm": 4.960346221923828,
      "learning_rate": 0.0008980619563558676,
      "loss": 1.25,
      "step": 719
    },
    {
      "epoch": 0.3271240345297592,
      "grad_norm": 5.012333393096924,
      "learning_rate": 0.0008979093544941248,
      "loss": 1.0481,
      "step": 720
    },
    {
      "epoch": 0.32757837346660607,
      "grad_norm": 4.924685955047607,
      "learning_rate": 0.0008977567526323821,
      "loss": 0.9619,
      "step": 721
    },
    {
      "epoch": 0.328032712403453,
      "grad_norm": 6.789018154144287,
      "learning_rate": 0.0008976041507706394,
      "loss": 1.07,
      "step": 722
    },
    {
      "epoch": 0.3284870513402999,
      "grad_norm": 4.378010272979736,
      "learning_rate": 0.0008974515489088967,
      "loss": 1.3325,
      "step": 723
    },
    {
      "epoch": 0.32894139027714675,
      "grad_norm": 5.990737438201904,
      "learning_rate": 0.000897298947047154,
      "loss": 0.9738,
      "step": 724
    },
    {
      "epoch": 0.3293957292139936,
      "grad_norm": 6.337378978729248,
      "learning_rate": 0.0008971463451854113,
      "loss": 1.2698,
      "step": 725
    },
    {
      "epoch": 0.3298500681508405,
      "grad_norm": 7.552202224731445,
      "learning_rate": 0.0008969937433236686,
      "loss": 1.6657,
      "step": 726
    },
    {
      "epoch": 0.33030440708768743,
      "grad_norm": 5.4488115310668945,
      "learning_rate": 0.0008968411414619258,
      "loss": 1.807,
      "step": 727
    },
    {
      "epoch": 0.3307587460245343,
      "grad_norm": 3.7958643436431885,
      "learning_rate": 0.0008966885396001832,
      "loss": 0.8535,
      "step": 728
    },
    {
      "epoch": 0.3312130849613812,
      "grad_norm": 5.843702793121338,
      "learning_rate": 0.0008965359377384405,
      "loss": 0.9989,
      "step": 729
    },
    {
      "epoch": 0.33166742389822806,
      "grad_norm": 6.102712631225586,
      "learning_rate": 0.0008963833358766977,
      "loss": 1.6119,
      "step": 730
    },
    {
      "epoch": 0.33212176283507494,
      "grad_norm": 2.939974546432495,
      "learning_rate": 0.0008962307340149551,
      "loss": 0.7871,
      "step": 731
    },
    {
      "epoch": 0.3325761017719219,
      "grad_norm": 4.828990459442139,
      "learning_rate": 0.0008960781321532123,
      "loss": 0.7795,
      "step": 732
    },
    {
      "epoch": 0.33303044070876875,
      "grad_norm": 6.127473831176758,
      "learning_rate": 0.0008959255302914696,
      "loss": 1.1245,
      "step": 733
    },
    {
      "epoch": 0.3334847796456156,
      "grad_norm": 5.2644734382629395,
      "learning_rate": 0.000895772928429727,
      "loss": 1.8222,
      "step": 734
    },
    {
      "epoch": 0.3339391185824625,
      "grad_norm": 4.509850025177002,
      "learning_rate": 0.0008956203265679842,
      "loss": 2.5741,
      "step": 735
    },
    {
      "epoch": 0.3343934575193094,
      "grad_norm": 4.659401893615723,
      "learning_rate": 0.0008954677247062414,
      "loss": 0.7392,
      "step": 736
    },
    {
      "epoch": 0.3348477964561563,
      "grad_norm": 7.231908321380615,
      "learning_rate": 0.0008953151228444987,
      "loss": 1.4515,
      "step": 737
    },
    {
      "epoch": 0.3353021353930032,
      "grad_norm": 5.600584030151367,
      "learning_rate": 0.000895162520982756,
      "loss": 1.5619,
      "step": 738
    },
    {
      "epoch": 0.33575647432985006,
      "grad_norm": 6.941690921783447,
      "learning_rate": 0.0008950099191210132,
      "loss": 2.0901,
      "step": 739
    },
    {
      "epoch": 0.33621081326669694,
      "grad_norm": 7.821681022644043,
      "learning_rate": 0.0008948573172592706,
      "loss": 1.814,
      "step": 740
    },
    {
      "epoch": 0.3366651522035438,
      "grad_norm": 4.955844879150391,
      "learning_rate": 0.0008947047153975279,
      "loss": 1.5052,
      "step": 741
    },
    {
      "epoch": 0.33711949114039075,
      "grad_norm": 5.747659206390381,
      "learning_rate": 0.0008945521135357851,
      "loss": 2.0925,
      "step": 742
    },
    {
      "epoch": 0.3375738300772376,
      "grad_norm": 5.617347717285156,
      "learning_rate": 0.0008943995116740425,
      "loss": 1.4257,
      "step": 743
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 3.721226215362549,
      "learning_rate": 0.0008942469098122997,
      "loss": 1.0654,
      "step": 744
    },
    {
      "epoch": 0.3384825079509314,
      "grad_norm": 5.194368839263916,
      "learning_rate": 0.000894094307950557,
      "loss": 1.3531,
      "step": 745
    },
    {
      "epoch": 0.3389368468877783,
      "grad_norm": 3.6242027282714844,
      "learning_rate": 0.0008939417060888144,
      "loss": 1.0402,
      "step": 746
    },
    {
      "epoch": 0.3393911858246252,
      "grad_norm": 6.254701137542725,
      "learning_rate": 0.0008937891042270716,
      "loss": 1.5445,
      "step": 747
    },
    {
      "epoch": 0.33984552476147206,
      "grad_norm": 7.0447869300842285,
      "learning_rate": 0.0008936365023653289,
      "loss": 0.9114,
      "step": 748
    },
    {
      "epoch": 0.34029986369831894,
      "grad_norm": 6.379082202911377,
      "learning_rate": 0.0008934839005035862,
      "loss": 1.7807,
      "step": 749
    },
    {
      "epoch": 0.3407542026351658,
      "grad_norm": 5.904658317565918,
      "learning_rate": 0.0008933312986418435,
      "loss": 1.3471,
      "step": 750
    },
    {
      "epoch": 0.34120854157201275,
      "grad_norm": 5.320494174957275,
      "learning_rate": 0.0008931786967801007,
      "loss": 0.8985,
      "step": 751
    },
    {
      "epoch": 0.3416628805088596,
      "grad_norm": 3.576894760131836,
      "learning_rate": 0.0008930260949183581,
      "loss": 0.7627,
      "step": 752
    },
    {
      "epoch": 0.3421172194457065,
      "grad_norm": 7.7538323402404785,
      "learning_rate": 0.0008928734930566154,
      "loss": 0.5025,
      "step": 753
    },
    {
      "epoch": 0.3425715583825534,
      "grad_norm": 11.220712661743164,
      "learning_rate": 0.0008927208911948725,
      "loss": 3.6933,
      "step": 754
    },
    {
      "epoch": 0.34302589731940025,
      "grad_norm": 3.9716765880584717,
      "learning_rate": 0.0008925682893331299,
      "loss": 0.5121,
      "step": 755
    },
    {
      "epoch": 0.3434802362562472,
      "grad_norm": 7.0838775634765625,
      "learning_rate": 0.0008924156874713871,
      "loss": 1.2683,
      "step": 756
    },
    {
      "epoch": 0.34393457519309406,
      "grad_norm": 8.348575592041016,
      "learning_rate": 0.0008922630856096444,
      "loss": 2.5024,
      "step": 757
    },
    {
      "epoch": 0.34438891412994094,
      "grad_norm": 7.98122501373291,
      "learning_rate": 0.0008921104837479018,
      "loss": 1.9194,
      "step": 758
    },
    {
      "epoch": 0.3448432530667878,
      "grad_norm": 4.48157262802124,
      "learning_rate": 0.000891957881886159,
      "loss": 1.4625,
      "step": 759
    },
    {
      "epoch": 0.3452975920036347,
      "grad_norm": 4.723053932189941,
      "learning_rate": 0.0008918052800244163,
      "loss": 0.4183,
      "step": 760
    },
    {
      "epoch": 0.3457519309404816,
      "grad_norm": 6.398916721343994,
      "learning_rate": 0.0008916526781626736,
      "loss": 1.2318,
      "step": 761
    },
    {
      "epoch": 0.3462062698773285,
      "grad_norm": 7.294674396514893,
      "learning_rate": 0.0008915000763009309,
      "loss": 1.7608,
      "step": 762
    },
    {
      "epoch": 0.3466606088141754,
      "grad_norm": 5.144762992858887,
      "learning_rate": 0.0008913474744391881,
      "loss": 1.8781,
      "step": 763
    },
    {
      "epoch": 0.34711494775102225,
      "grad_norm": 2.862628698348999,
      "learning_rate": 0.0008911948725774455,
      "loss": 0.8013,
      "step": 764
    },
    {
      "epoch": 0.34756928668786913,
      "grad_norm": 6.13777494430542,
      "learning_rate": 0.0008910422707157028,
      "loss": 0.9416,
      "step": 765
    },
    {
      "epoch": 0.34802362562471606,
      "grad_norm": 5.450795650482178,
      "learning_rate": 0.00089088966885396,
      "loss": 1.7573,
      "step": 766
    },
    {
      "epoch": 0.34847796456156294,
      "grad_norm": 7.270195007324219,
      "learning_rate": 0.0008907370669922174,
      "loss": 1.5138,
      "step": 767
    },
    {
      "epoch": 0.3489323034984098,
      "grad_norm": 4.835293292999268,
      "learning_rate": 0.0008905844651304746,
      "loss": 0.9317,
      "step": 768
    },
    {
      "epoch": 0.3493866424352567,
      "grad_norm": 4.970389366149902,
      "learning_rate": 0.0008904318632687319,
      "loss": 0.4813,
      "step": 769
    },
    {
      "epoch": 0.34984098137210357,
      "grad_norm": 5.852909564971924,
      "learning_rate": 0.0008902792614069893,
      "loss": 1.3894,
      "step": 770
    },
    {
      "epoch": 0.3502953203089505,
      "grad_norm": 4.468610763549805,
      "learning_rate": 0.0008901266595452465,
      "loss": 1.1838,
      "step": 771
    },
    {
      "epoch": 0.3507496592457974,
      "grad_norm": 4.038933277130127,
      "learning_rate": 0.0008899740576835038,
      "loss": 1.2799,
      "step": 772
    },
    {
      "epoch": 0.35120399818264425,
      "grad_norm": 4.139795303344727,
      "learning_rate": 0.000889821455821761,
      "loss": 0.9444,
      "step": 773
    },
    {
      "epoch": 0.3516583371194911,
      "grad_norm": 7.196143627166748,
      "learning_rate": 0.0008896688539600183,
      "loss": 1.0944,
      "step": 774
    },
    {
      "epoch": 0.352112676056338,
      "grad_norm": 7.289319038391113,
      "learning_rate": 0.0008895162520982755,
      "loss": 1.5625,
      "step": 775
    },
    {
      "epoch": 0.35256701499318494,
      "grad_norm": 5.9315409660339355,
      "learning_rate": 0.0008893636502365329,
      "loss": 1.0123,
      "step": 776
    },
    {
      "epoch": 0.3530213539300318,
      "grad_norm": 4.678885459899902,
      "learning_rate": 0.0008892110483747902,
      "loss": 0.8268,
      "step": 777
    },
    {
      "epoch": 0.3534756928668787,
      "grad_norm": 5.393078804016113,
      "learning_rate": 0.0008890584465130474,
      "loss": 1.6318,
      "step": 778
    },
    {
      "epoch": 0.35393003180372556,
      "grad_norm": 6.044681549072266,
      "learning_rate": 0.0008889058446513048,
      "loss": 1.6828,
      "step": 779
    },
    {
      "epoch": 0.35438437074057244,
      "grad_norm": 4.718715190887451,
      "learning_rate": 0.000888753242789562,
      "loss": 0.8647,
      "step": 780
    },
    {
      "epoch": 0.3548387096774194,
      "grad_norm": 4.99652624130249,
      "learning_rate": 0.0008886006409278193,
      "loss": 1.3498,
      "step": 781
    },
    {
      "epoch": 0.35529304861426625,
      "grad_norm": 15.826578140258789,
      "learning_rate": 0.0008884480390660767,
      "loss": 1.0379,
      "step": 782
    },
    {
      "epoch": 0.3557473875511131,
      "grad_norm": 6.590808868408203,
      "learning_rate": 0.0008882954372043339,
      "loss": 0.7933,
      "step": 783
    },
    {
      "epoch": 0.35620172648796,
      "grad_norm": 6.815788745880127,
      "learning_rate": 0.0008881428353425912,
      "loss": 1.8551,
      "step": 784
    },
    {
      "epoch": 0.3566560654248069,
      "grad_norm": 8.120882034301758,
      "learning_rate": 0.0008879902334808485,
      "loss": 1.666,
      "step": 785
    },
    {
      "epoch": 0.3571104043616538,
      "grad_norm": 6.315911769866943,
      "learning_rate": 0.0008878376316191058,
      "loss": 1.5484,
      "step": 786
    },
    {
      "epoch": 0.3575647432985007,
      "grad_norm": 6.714621067047119,
      "learning_rate": 0.0008876850297573631,
      "loss": 1.8189,
      "step": 787
    },
    {
      "epoch": 0.35801908223534756,
      "grad_norm": 4.559648513793945,
      "learning_rate": 0.0008875324278956204,
      "loss": 1.3818,
      "step": 788
    },
    {
      "epoch": 0.35847342117219444,
      "grad_norm": 5.749180793762207,
      "learning_rate": 0.0008873798260338777,
      "loss": 1.7525,
      "step": 789
    },
    {
      "epoch": 0.35892776010904137,
      "grad_norm": 6.948873043060303,
      "learning_rate": 0.0008872272241721349,
      "loss": 1.1463,
      "step": 790
    },
    {
      "epoch": 0.35938209904588825,
      "grad_norm": 5.125828742980957,
      "learning_rate": 0.0008870746223103922,
      "loss": 1.5909,
      "step": 791
    },
    {
      "epoch": 0.3598364379827351,
      "grad_norm": 2.661564350128174,
      "learning_rate": 0.0008869220204486495,
      "loss": 0.5803,
      "step": 792
    },
    {
      "epoch": 0.360290776919582,
      "grad_norm": 4.652291297912598,
      "learning_rate": 0.0008867694185869067,
      "loss": 1.1044,
      "step": 793
    },
    {
      "epoch": 0.3607451158564289,
      "grad_norm": 4.028030872344971,
      "learning_rate": 0.0008866168167251641,
      "loss": 0.8222,
      "step": 794
    },
    {
      "epoch": 0.3611994547932758,
      "grad_norm": 3.693725824356079,
      "learning_rate": 0.0008864642148634213,
      "loss": 1.1521,
      "step": 795
    },
    {
      "epoch": 0.3616537937301227,
      "grad_norm": 6.290371417999268,
      "learning_rate": 0.0008863116130016786,
      "loss": 1.7265,
      "step": 796
    },
    {
      "epoch": 0.36210813266696956,
      "grad_norm": 4.38830041885376,
      "learning_rate": 0.000886159011139936,
      "loss": 1.1135,
      "step": 797
    },
    {
      "epoch": 0.36256247160381644,
      "grad_norm": 6.2588210105896,
      "learning_rate": 0.0008860064092781932,
      "loss": 2.1466,
      "step": 798
    },
    {
      "epoch": 0.3630168105406633,
      "grad_norm": 4.5905632972717285,
      "learning_rate": 0.0008858538074164505,
      "loss": 1.1098,
      "step": 799
    },
    {
      "epoch": 0.36347114947751025,
      "grad_norm": 8.847102165222168,
      "learning_rate": 0.0008857012055547078,
      "loss": 1.9507,
      "step": 800
    },
    {
      "epoch": 0.3639254884143571,
      "grad_norm": 4.761922359466553,
      "learning_rate": 0.0008855486036929651,
      "loss": 1.698,
      "step": 801
    },
    {
      "epoch": 0.364379827351204,
      "grad_norm": 4.210709095001221,
      "learning_rate": 0.0008853960018312223,
      "loss": 1.3085,
      "step": 802
    },
    {
      "epoch": 0.3648341662880509,
      "grad_norm": 4.917298316955566,
      "learning_rate": 0.0008852433999694797,
      "loss": 0.8398,
      "step": 803
    },
    {
      "epoch": 0.36528850522489775,
      "grad_norm": 4.388576984405518,
      "learning_rate": 0.000885090798107737,
      "loss": 1.3174,
      "step": 804
    },
    {
      "epoch": 0.3657428441617447,
      "grad_norm": 5.961156368255615,
      "learning_rate": 0.0008849381962459942,
      "loss": 1.8372,
      "step": 805
    },
    {
      "epoch": 0.36619718309859156,
      "grad_norm": 4.9643096923828125,
      "learning_rate": 0.0008847855943842516,
      "loss": 0.8173,
      "step": 806
    },
    {
      "epoch": 0.36665152203543844,
      "grad_norm": 5.263026237487793,
      "learning_rate": 0.0008846329925225088,
      "loss": 1.2083,
      "step": 807
    },
    {
      "epoch": 0.3671058609722853,
      "grad_norm": 5.512087345123291,
      "learning_rate": 0.0008844803906607661,
      "loss": 0.9233,
      "step": 808
    },
    {
      "epoch": 0.3675601999091322,
      "grad_norm": 4.0441975593566895,
      "learning_rate": 0.0008843277887990234,
      "loss": 0.4826,
      "step": 809
    },
    {
      "epoch": 0.3680145388459791,
      "grad_norm": 7.15964412689209,
      "learning_rate": 0.0008841751869372806,
      "loss": 1.4408,
      "step": 810
    },
    {
      "epoch": 0.368468877782826,
      "grad_norm": 5.7863450050354,
      "learning_rate": 0.0008840225850755379,
      "loss": 0.8129,
      "step": 811
    },
    {
      "epoch": 0.3689232167196729,
      "grad_norm": 8.042669296264648,
      "learning_rate": 0.0008838699832137952,
      "loss": 1.0594,
      "step": 812
    },
    {
      "epoch": 0.36937755565651975,
      "grad_norm": 8.159673690795898,
      "learning_rate": 0.0008837173813520525,
      "loss": 1.1793,
      "step": 813
    },
    {
      "epoch": 0.36983189459336663,
      "grad_norm": 4.126338005065918,
      "learning_rate": 0.0008835647794903097,
      "loss": 0.6974,
      "step": 814
    },
    {
      "epoch": 0.37028623353021356,
      "grad_norm": 5.75996732711792,
      "learning_rate": 0.0008834121776285671,
      "loss": 0.7462,
      "step": 815
    },
    {
      "epoch": 0.37074057246706044,
      "grad_norm": 5.039966106414795,
      "learning_rate": 0.0008832595757668244,
      "loss": 0.8713,
      "step": 816
    },
    {
      "epoch": 0.3711949114039073,
      "grad_norm": 8.12533950805664,
      "learning_rate": 0.0008831069739050816,
      "loss": 0.9585,
      "step": 817
    },
    {
      "epoch": 0.3716492503407542,
      "grad_norm": 5.671520709991455,
      "learning_rate": 0.000882954372043339,
      "loss": 1.0822,
      "step": 818
    },
    {
      "epoch": 0.37210358927760107,
      "grad_norm": 6.228645324707031,
      "learning_rate": 0.0008828017701815962,
      "loss": 0.7568,
      "step": 819
    },
    {
      "epoch": 0.372557928214448,
      "grad_norm": 5.231075286865234,
      "learning_rate": 0.0008826491683198535,
      "loss": 1.2278,
      "step": 820
    },
    {
      "epoch": 0.3730122671512949,
      "grad_norm": 6.376522064208984,
      "learning_rate": 0.0008824965664581109,
      "loss": 1.3174,
      "step": 821
    },
    {
      "epoch": 0.37346660608814175,
      "grad_norm": 4.5015130043029785,
      "learning_rate": 0.0008823439645963681,
      "loss": 1.1981,
      "step": 822
    },
    {
      "epoch": 0.3739209450249886,
      "grad_norm": 3.693192481994629,
      "learning_rate": 0.0008821913627346254,
      "loss": 0.4418,
      "step": 823
    },
    {
      "epoch": 0.3743752839618355,
      "grad_norm": 7.570511341094971,
      "learning_rate": 0.0008820387608728827,
      "loss": 1.6689,
      "step": 824
    },
    {
      "epoch": 0.37482962289868244,
      "grad_norm": 4.831517696380615,
      "learning_rate": 0.00088188615901114,
      "loss": 0.7746,
      "step": 825
    },
    {
      "epoch": 0.3752839618355293,
      "grad_norm": 6.0755133628845215,
      "learning_rate": 0.0008817335571493973,
      "loss": 1.2679,
      "step": 826
    },
    {
      "epoch": 0.3757383007723762,
      "grad_norm": 3.312351942062378,
      "learning_rate": 0.0008815809552876545,
      "loss": 0.6349,
      "step": 827
    },
    {
      "epoch": 0.37619263970922306,
      "grad_norm": 5.031361103057861,
      "learning_rate": 0.0008814283534259118,
      "loss": 1.0149,
      "step": 828
    },
    {
      "epoch": 0.37664697864606994,
      "grad_norm": 4.737961292266846,
      "learning_rate": 0.000881275751564169,
      "loss": 0.8932,
      "step": 829
    },
    {
      "epoch": 0.3771013175829169,
      "grad_norm": 5.504453182220459,
      "learning_rate": 0.0008811231497024264,
      "loss": 0.7911,
      "step": 830
    },
    {
      "epoch": 0.37755565651976375,
      "grad_norm": 6.514984607696533,
      "learning_rate": 0.0008809705478406836,
      "loss": 1.8092,
      "step": 831
    },
    {
      "epoch": 0.3780099954566106,
      "grad_norm": 6.701390266418457,
      "learning_rate": 0.0008808179459789409,
      "loss": 2.2639,
      "step": 832
    },
    {
      "epoch": 0.3784643343934575,
      "grad_norm": 7.493356227874756,
      "learning_rate": 0.0008806653441171983,
      "loss": 1.7451,
      "step": 833
    },
    {
      "epoch": 0.37891867333030443,
      "grad_norm": 5.3426079750061035,
      "learning_rate": 0.0008805127422554555,
      "loss": 1.4081,
      "step": 834
    },
    {
      "epoch": 0.3793730122671513,
      "grad_norm": 5.853097438812256,
      "learning_rate": 0.0008803601403937128,
      "loss": 1.2593,
      "step": 835
    },
    {
      "epoch": 0.3798273512039982,
      "grad_norm": 5.506092071533203,
      "learning_rate": 0.0008802075385319701,
      "loss": 1.0256,
      "step": 836
    },
    {
      "epoch": 0.38028169014084506,
      "grad_norm": 5.42415189743042,
      "learning_rate": 0.0008800549366702274,
      "loss": 1.5649,
      "step": 837
    },
    {
      "epoch": 0.38073602907769194,
      "grad_norm": 6.9796247482299805,
      "learning_rate": 0.0008799023348084847,
      "loss": 1.4461,
      "step": 838
    },
    {
      "epoch": 0.38119036801453887,
      "grad_norm": 6.075656890869141,
      "learning_rate": 0.000879749732946742,
      "loss": 1.5696,
      "step": 839
    },
    {
      "epoch": 0.38164470695138575,
      "grad_norm": 5.317514896392822,
      "learning_rate": 0.0008795971310849993,
      "loss": 1.1687,
      "step": 840
    },
    {
      "epoch": 0.3820990458882326,
      "grad_norm": 3.9273173809051514,
      "learning_rate": 0.0008794445292232565,
      "loss": 0.4521,
      "step": 841
    },
    {
      "epoch": 0.3825533848250795,
      "grad_norm": 5.507394313812256,
      "learning_rate": 0.0008792919273615139,
      "loss": 1.3065,
      "step": 842
    },
    {
      "epoch": 0.3830077237619264,
      "grad_norm": 3.2257485389709473,
      "learning_rate": 0.0008791393254997712,
      "loss": 0.7686,
      "step": 843
    },
    {
      "epoch": 0.3834620626987733,
      "grad_norm": 7.638891220092773,
      "learning_rate": 0.0008789867236380284,
      "loss": 2.0235,
      "step": 844
    },
    {
      "epoch": 0.3839164016356202,
      "grad_norm": 5.754235744476318,
      "learning_rate": 0.0008788341217762858,
      "loss": 0.9875,
      "step": 845
    },
    {
      "epoch": 0.38437074057246706,
      "grad_norm": 8.527482986450195,
      "learning_rate": 0.0008786815199145429,
      "loss": 1.2834,
      "step": 846
    },
    {
      "epoch": 0.38482507950931394,
      "grad_norm": 7.179009914398193,
      "learning_rate": 0.0008785289180528002,
      "loss": 1.9082,
      "step": 847
    },
    {
      "epoch": 0.3852794184461608,
      "grad_norm": 8.557568550109863,
      "learning_rate": 0.0008783763161910575,
      "loss": 1.0206,
      "step": 848
    },
    {
      "epoch": 0.38573375738300775,
      "grad_norm": 5.695643901824951,
      "learning_rate": 0.0008782237143293148,
      "loss": 1.2159,
      "step": 849
    },
    {
      "epoch": 0.3861880963198546,
      "grad_norm": 4.623987197875977,
      "learning_rate": 0.0008780711124675721,
      "loss": 1.2844,
      "step": 850
    },
    {
      "epoch": 0.3866424352567015,
      "grad_norm": 5.601416110992432,
      "learning_rate": 0.0008779185106058294,
      "loss": 1.8982,
      "step": 851
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 7.359823226928711,
      "learning_rate": 0.0008777659087440867,
      "loss": 1.6388,
      "step": 852
    },
    {
      "epoch": 0.38755111313039525,
      "grad_norm": 4.786874771118164,
      "learning_rate": 0.0008776133068823439,
      "loss": 0.8866,
      "step": 853
    },
    {
      "epoch": 0.3880054520672422,
      "grad_norm": 5.887673854827881,
      "learning_rate": 0.0008774607050206013,
      "loss": 0.774,
      "step": 854
    },
    {
      "epoch": 0.38845979100408906,
      "grad_norm": 6.920986175537109,
      "learning_rate": 0.0008773081031588586,
      "loss": 2.1393,
      "step": 855
    },
    {
      "epoch": 0.38891412994093594,
      "grad_norm": 4.936712265014648,
      "learning_rate": 0.0008771555012971158,
      "loss": 1.0528,
      "step": 856
    },
    {
      "epoch": 0.3893684688777828,
      "grad_norm": 5.552990913391113,
      "learning_rate": 0.0008770028994353732,
      "loss": 1.588,
      "step": 857
    },
    {
      "epoch": 0.3898228078146297,
      "grad_norm": 7.129572868347168,
      "learning_rate": 0.0008768502975736304,
      "loss": 1.02,
      "step": 858
    },
    {
      "epoch": 0.3902771467514766,
      "grad_norm": 5.0391316413879395,
      "learning_rate": 0.0008766976957118877,
      "loss": 1.2694,
      "step": 859
    },
    {
      "epoch": 0.3907314856883235,
      "grad_norm": 8.063055992126465,
      "learning_rate": 0.000876545093850145,
      "loss": 1.7669,
      "step": 860
    },
    {
      "epoch": 0.3911858246251704,
      "grad_norm": 7.054976463317871,
      "learning_rate": 0.0008763924919884023,
      "loss": 1.2926,
      "step": 861
    },
    {
      "epoch": 0.39164016356201725,
      "grad_norm": 5.328644752502441,
      "learning_rate": 0.0008762398901266596,
      "loss": 1.2329,
      "step": 862
    },
    {
      "epoch": 0.39209450249886413,
      "grad_norm": 3.397826910018921,
      "learning_rate": 0.0008760872882649169,
      "loss": 0.7181,
      "step": 863
    },
    {
      "epoch": 0.39254884143571106,
      "grad_norm": 3.349475860595703,
      "learning_rate": 0.0008759346864031741,
      "loss": 0.7142,
      "step": 864
    },
    {
      "epoch": 0.39300318037255794,
      "grad_norm": 7.951776027679443,
      "learning_rate": 0.0008757820845414313,
      "loss": 1.4526,
      "step": 865
    },
    {
      "epoch": 0.3934575193094048,
      "grad_norm": 5.146064281463623,
      "learning_rate": 0.0008756294826796887,
      "loss": 0.5813,
      "step": 866
    },
    {
      "epoch": 0.3939118582462517,
      "grad_norm": 4.871695041656494,
      "learning_rate": 0.000875476880817946,
      "loss": 0.9001,
      "step": 867
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 4.13064432144165,
      "learning_rate": 0.0008753242789562032,
      "loss": 0.8851,
      "step": 868
    },
    {
      "epoch": 0.3948205361199455,
      "grad_norm": 8.221821784973145,
      "learning_rate": 0.0008751716770944606,
      "loss": 2.8987,
      "step": 869
    },
    {
      "epoch": 0.3952748750567924,
      "grad_norm": 6.750838756561279,
      "learning_rate": 0.0008750190752327178,
      "loss": 1.2941,
      "step": 870
    },
    {
      "epoch": 0.39572921399363925,
      "grad_norm": 5.362662315368652,
      "learning_rate": 0.0008748664733709751,
      "loss": 1.6922,
      "step": 871
    },
    {
      "epoch": 0.3961835529304861,
      "grad_norm": 4.805442810058594,
      "learning_rate": 0.0008747138715092325,
      "loss": 1.0973,
      "step": 872
    },
    {
      "epoch": 0.396637891867333,
      "grad_norm": 5.9169535636901855,
      "learning_rate": 0.0008745612696474897,
      "loss": 1.4078,
      "step": 873
    },
    {
      "epoch": 0.39709223080417994,
      "grad_norm": 3.709094524383545,
      "learning_rate": 0.000874408667785747,
      "loss": 1.0237,
      "step": 874
    },
    {
      "epoch": 0.3975465697410268,
      "grad_norm": 6.004878520965576,
      "learning_rate": 0.0008742560659240043,
      "loss": 2.2223,
      "step": 875
    },
    {
      "epoch": 0.3980009086778737,
      "grad_norm": 2.8631155490875244,
      "learning_rate": 0.0008741034640622616,
      "loss": 0.4194,
      "step": 876
    },
    {
      "epoch": 0.39845524761472056,
      "grad_norm": 4.2946271896362305,
      "learning_rate": 0.0008739508622005188,
      "loss": 1.3251,
      "step": 877
    },
    {
      "epoch": 0.3989095865515675,
      "grad_norm": 5.739575386047363,
      "learning_rate": 0.0008737982603387762,
      "loss": 1.5933,
      "step": 878
    },
    {
      "epoch": 0.3993639254884144,
      "grad_norm": 7.450702667236328,
      "learning_rate": 0.0008736456584770335,
      "loss": 2.1154,
      "step": 879
    },
    {
      "epoch": 0.39981826442526125,
      "grad_norm": 3.881716251373291,
      "learning_rate": 0.0008734930566152907,
      "loss": 0.8806,
      "step": 880
    },
    {
      "epoch": 0.4002726033621081,
      "grad_norm": 7.835771083831787,
      "learning_rate": 0.0008733404547535481,
      "loss": 1.6434,
      "step": 881
    },
    {
      "epoch": 0.400726942298955,
      "grad_norm": 5.30209493637085,
      "learning_rate": 0.0008731878528918052,
      "loss": 1.2209,
      "step": 882
    },
    {
      "epoch": 0.40118128123580193,
      "grad_norm": 4.560446739196777,
      "learning_rate": 0.0008730352510300625,
      "loss": 1.0081,
      "step": 883
    },
    {
      "epoch": 0.4016356201726488,
      "grad_norm": 5.753317356109619,
      "learning_rate": 0.0008728826491683199,
      "loss": 1.7058,
      "step": 884
    },
    {
      "epoch": 0.4020899591094957,
      "grad_norm": 3.2562057971954346,
      "learning_rate": 0.0008727300473065771,
      "loss": 0.5335,
      "step": 885
    },
    {
      "epoch": 0.40254429804634256,
      "grad_norm": 7.66619348526001,
      "learning_rate": 0.0008725774454448344,
      "loss": 1.7394,
      "step": 886
    },
    {
      "epoch": 0.40299863698318944,
      "grad_norm": 5.992953777313232,
      "learning_rate": 0.0008724248435830917,
      "loss": 1.6394,
      "step": 887
    },
    {
      "epoch": 0.40345297592003637,
      "grad_norm": 3.056389093399048,
      "learning_rate": 0.000872272241721349,
      "loss": 0.3788,
      "step": 888
    },
    {
      "epoch": 0.40390731485688325,
      "grad_norm": 3.3203887939453125,
      "learning_rate": 0.0008721196398596062,
      "loss": 0.4388,
      "step": 889
    },
    {
      "epoch": 0.4043616537937301,
      "grad_norm": 3.465618371963501,
      "learning_rate": 0.0008719670379978636,
      "loss": 0.9635,
      "step": 890
    },
    {
      "epoch": 0.404815992730577,
      "grad_norm": 4.590958118438721,
      "learning_rate": 0.0008718144361361209,
      "loss": 1.3972,
      "step": 891
    },
    {
      "epoch": 0.4052703316674239,
      "grad_norm": 6.1119184494018555,
      "learning_rate": 0.0008716618342743781,
      "loss": 1.0774,
      "step": 892
    },
    {
      "epoch": 0.4057246706042708,
      "grad_norm": 2.7479076385498047,
      "learning_rate": 0.0008715092324126355,
      "loss": 0.6592,
      "step": 893
    },
    {
      "epoch": 0.4061790095411177,
      "grad_norm": 5.125417709350586,
      "learning_rate": 0.0008713566305508927,
      "loss": 1.2492,
      "step": 894
    },
    {
      "epoch": 0.40663334847796456,
      "grad_norm": 5.337588787078857,
      "learning_rate": 0.00087120402868915,
      "loss": 1.9068,
      "step": 895
    },
    {
      "epoch": 0.40708768741481144,
      "grad_norm": 7.3836669921875,
      "learning_rate": 0.0008710514268274074,
      "loss": 1.8366,
      "step": 896
    },
    {
      "epoch": 0.4075420263516583,
      "grad_norm": 8.530547142028809,
      "learning_rate": 0.0008708988249656646,
      "loss": 2.0357,
      "step": 897
    },
    {
      "epoch": 0.40799636528850525,
      "grad_norm": 7.696071147918701,
      "learning_rate": 0.0008707462231039219,
      "loss": 1.7091,
      "step": 898
    },
    {
      "epoch": 0.4084507042253521,
      "grad_norm": 5.895552158355713,
      "learning_rate": 0.0008705936212421792,
      "loss": 1.5901,
      "step": 899
    },
    {
      "epoch": 0.408905043162199,
      "grad_norm": 6.105013847351074,
      "learning_rate": 0.0008704410193804364,
      "loss": 1.9,
      "step": 900
    },
    {
      "epoch": 0.4093593820990459,
      "grad_norm": 6.181286811828613,
      "learning_rate": 0.0008702884175186936,
      "loss": 2.2476,
      "step": 901
    },
    {
      "epoch": 0.40981372103589275,
      "grad_norm": 4.8901591300964355,
      "learning_rate": 0.000870135815656951,
      "loss": 1.2356,
      "step": 902
    },
    {
      "epoch": 0.4102680599727397,
      "grad_norm": 5.564911365509033,
      "learning_rate": 0.0008699832137952083,
      "loss": 1.7435,
      "step": 903
    },
    {
      "epoch": 0.41072239890958656,
      "grad_norm": 4.7510247230529785,
      "learning_rate": 0.0008698306119334655,
      "loss": 1.3415,
      "step": 904
    },
    {
      "epoch": 0.41117673784643344,
      "grad_norm": 8.729755401611328,
      "learning_rate": 0.0008696780100717229,
      "loss": 1.8198,
      "step": 905
    },
    {
      "epoch": 0.4116310767832803,
      "grad_norm": 5.696547508239746,
      "learning_rate": 0.0008695254082099801,
      "loss": 0.7062,
      "step": 906
    },
    {
      "epoch": 0.4120854157201272,
      "grad_norm": 6.156518459320068,
      "learning_rate": 0.0008693728063482374,
      "loss": 1.2757,
      "step": 907
    },
    {
      "epoch": 0.4125397546569741,
      "grad_norm": 6.9368815422058105,
      "learning_rate": 0.0008692202044864948,
      "loss": 1.4089,
      "step": 908
    },
    {
      "epoch": 0.412994093593821,
      "grad_norm": 8.933419227600098,
      "learning_rate": 0.000869067602624752,
      "loss": 1.2313,
      "step": 909
    },
    {
      "epoch": 0.4134484325306679,
      "grad_norm": 4.753856658935547,
      "learning_rate": 0.0008689150007630093,
      "loss": 0.8183,
      "step": 910
    },
    {
      "epoch": 0.41390277146751475,
      "grad_norm": 7.750436782836914,
      "learning_rate": 0.0008687623989012666,
      "loss": 1.529,
      "step": 911
    },
    {
      "epoch": 0.41435711040436163,
      "grad_norm": 6.572569370269775,
      "learning_rate": 0.0008686097970395239,
      "loss": 1.346,
      "step": 912
    },
    {
      "epoch": 0.41481144934120856,
      "grad_norm": 5.664178848266602,
      "learning_rate": 0.0008684571951777812,
      "loss": 1.2673,
      "step": 913
    },
    {
      "epoch": 0.41526578827805544,
      "grad_norm": 6.847504615783691,
      "learning_rate": 0.0008683045933160385,
      "loss": 2.7972,
      "step": 914
    },
    {
      "epoch": 0.4157201272149023,
      "grad_norm": 4.16949462890625,
      "learning_rate": 0.0008681519914542958,
      "loss": 0.5847,
      "step": 915
    },
    {
      "epoch": 0.4161744661517492,
      "grad_norm": 5.101690769195557,
      "learning_rate": 0.000867999389592553,
      "loss": 1.5828,
      "step": 916
    },
    {
      "epoch": 0.41662880508859607,
      "grad_norm": 3.6873440742492676,
      "learning_rate": 0.0008678467877308104,
      "loss": 0.811,
      "step": 917
    },
    {
      "epoch": 0.417083144025443,
      "grad_norm": 4.957457542419434,
      "learning_rate": 0.0008676941858690677,
      "loss": 1.2381,
      "step": 918
    },
    {
      "epoch": 0.4175374829622899,
      "grad_norm": 7.953632354736328,
      "learning_rate": 0.0008675415840073248,
      "loss": 1.8537,
      "step": 919
    },
    {
      "epoch": 0.41799182189913675,
      "grad_norm": 5.226185321807861,
      "learning_rate": 0.0008673889821455822,
      "loss": 0.8752,
      "step": 920
    },
    {
      "epoch": 0.4184461608359836,
      "grad_norm": 4.199342250823975,
      "learning_rate": 0.0008672363802838394,
      "loss": 1.0391,
      "step": 921
    },
    {
      "epoch": 0.41890049977283056,
      "grad_norm": 5.26807975769043,
      "learning_rate": 0.0008670837784220967,
      "loss": 1.1868,
      "step": 922
    },
    {
      "epoch": 0.41935483870967744,
      "grad_norm": 5.914754390716553,
      "learning_rate": 0.000866931176560354,
      "loss": 0.9474,
      "step": 923
    },
    {
      "epoch": 0.4198091776465243,
      "grad_norm": 5.258687973022461,
      "learning_rate": 0.0008667785746986113,
      "loss": 1.2182,
      "step": 924
    },
    {
      "epoch": 0.4202635165833712,
      "grad_norm": 7.529735565185547,
      "learning_rate": 0.0008666259728368686,
      "loss": 1.7517,
      "step": 925
    },
    {
      "epoch": 0.42071785552021806,
      "grad_norm": 2.809624195098877,
      "learning_rate": 0.0008664733709751259,
      "loss": 1.205,
      "step": 926
    },
    {
      "epoch": 0.421172194457065,
      "grad_norm": 6.192967414855957,
      "learning_rate": 0.0008663207691133832,
      "loss": 1.5507,
      "step": 927
    },
    {
      "epoch": 0.4216265333939119,
      "grad_norm": 6.6328959465026855,
      "learning_rate": 0.0008661681672516404,
      "loss": 0.8576,
      "step": 928
    },
    {
      "epoch": 0.42208087233075875,
      "grad_norm": 5.689620018005371,
      "learning_rate": 0.0008660155653898978,
      "loss": 1.043,
      "step": 929
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 5.4058003425598145,
      "learning_rate": 0.0008658629635281551,
      "loss": 1.1403,
      "step": 930
    },
    {
      "epoch": 0.4229895502044525,
      "grad_norm": 5.170819282531738,
      "learning_rate": 0.0008657103616664124,
      "loss": 1.4704,
      "step": 931
    },
    {
      "epoch": 0.42344388914129943,
      "grad_norm": 5.411779880523682,
      "learning_rate": 0.0008655577598046697,
      "loss": 1.2149,
      "step": 932
    },
    {
      "epoch": 0.4238982280781463,
      "grad_norm": 6.52142858505249,
      "learning_rate": 0.0008654051579429269,
      "loss": 1.3392,
      "step": 933
    },
    {
      "epoch": 0.4243525670149932,
      "grad_norm": 4.843770503997803,
      "learning_rate": 0.0008652525560811843,
      "loss": 0.9096,
      "step": 934
    },
    {
      "epoch": 0.42480690595184006,
      "grad_norm": 6.689940929412842,
      "learning_rate": 0.0008650999542194416,
      "loss": 1.6074,
      "step": 935
    },
    {
      "epoch": 0.42526124488868694,
      "grad_norm": 4.926410675048828,
      "learning_rate": 0.0008649473523576988,
      "loss": 0.9179,
      "step": 936
    },
    {
      "epoch": 0.42571558382553387,
      "grad_norm": 5.093119144439697,
      "learning_rate": 0.000864794750495956,
      "loss": 1.662,
      "step": 937
    },
    {
      "epoch": 0.42616992276238075,
      "grad_norm": 7.606739521026611,
      "learning_rate": 0.0008646421486342133,
      "loss": 2.168,
      "step": 938
    },
    {
      "epoch": 0.4266242616992276,
      "grad_norm": 6.797503471374512,
      "learning_rate": 0.0008644895467724706,
      "loss": 1.9714,
      "step": 939
    },
    {
      "epoch": 0.4270786006360745,
      "grad_norm": 6.741391181945801,
      "learning_rate": 0.000864336944910728,
      "loss": 1.1171,
      "step": 940
    },
    {
      "epoch": 0.4275329395729214,
      "grad_norm": 9.400270462036133,
      "learning_rate": 0.0008641843430489852,
      "loss": 1.03,
      "step": 941
    },
    {
      "epoch": 0.4279872785097683,
      "grad_norm": 4.27142333984375,
      "learning_rate": 0.0008640317411872425,
      "loss": 0.6237,
      "step": 942
    },
    {
      "epoch": 0.4284416174466152,
      "grad_norm": 6.776236057281494,
      "learning_rate": 0.0008638791393254998,
      "loss": 1.5474,
      "step": 943
    },
    {
      "epoch": 0.42889595638346206,
      "grad_norm": 6.66179084777832,
      "learning_rate": 0.0008637265374637571,
      "loss": 0.9864,
      "step": 944
    },
    {
      "epoch": 0.42935029532030894,
      "grad_norm": 5.889345645904541,
      "learning_rate": 0.0008635739356020143,
      "loss": 0.869,
      "step": 945
    },
    {
      "epoch": 0.4298046342571558,
      "grad_norm": 7.195096969604492,
      "learning_rate": 0.0008634213337402717,
      "loss": 0.7958,
      "step": 946
    },
    {
      "epoch": 0.43025897319400275,
      "grad_norm": 3.841414213180542,
      "learning_rate": 0.000863268731878529,
      "loss": 0.4608,
      "step": 947
    },
    {
      "epoch": 0.4307133121308496,
      "grad_norm": 4.008823871612549,
      "learning_rate": 0.0008631161300167862,
      "loss": 0.4375,
      "step": 948
    },
    {
      "epoch": 0.4311676510676965,
      "grad_norm": 6.528337001800537,
      "learning_rate": 0.0008629635281550436,
      "loss": 1.5188,
      "step": 949
    },
    {
      "epoch": 0.4316219900045434,
      "grad_norm": 5.091749668121338,
      "learning_rate": 0.0008628109262933008,
      "loss": 1.5312,
      "step": 950
    },
    {
      "epoch": 0.43207632894139025,
      "grad_norm": 5.9130425453186035,
      "learning_rate": 0.0008626583244315581,
      "loss": 1.0513,
      "step": 951
    },
    {
      "epoch": 0.4325306678782372,
      "grad_norm": 7.981223106384277,
      "learning_rate": 0.0008625057225698155,
      "loss": 1.5899,
      "step": 952
    },
    {
      "epoch": 0.43298500681508406,
      "grad_norm": 4.713680744171143,
      "learning_rate": 0.0008623531207080727,
      "loss": 1.0954,
      "step": 953
    },
    {
      "epoch": 0.43343934575193094,
      "grad_norm": 3.2080931663513184,
      "learning_rate": 0.00086220051884633,
      "loss": 0.2,
      "step": 954
    },
    {
      "epoch": 0.4338936846887778,
      "grad_norm": 4.3739166259765625,
      "learning_rate": 0.0008620479169845872,
      "loss": 0.8257,
      "step": 955
    },
    {
      "epoch": 0.4343480236256247,
      "grad_norm": 6.5156121253967285,
      "learning_rate": 0.0008618953151228445,
      "loss": 2.4876,
      "step": 956
    },
    {
      "epoch": 0.4348023625624716,
      "grad_norm": 7.063384056091309,
      "learning_rate": 0.0008617427132611017,
      "loss": 1.7366,
      "step": 957
    },
    {
      "epoch": 0.4352567014993185,
      "grad_norm": 4.522094249725342,
      "learning_rate": 0.0008615901113993591,
      "loss": 1.1107,
      "step": 958
    },
    {
      "epoch": 0.4357110404361654,
      "grad_norm": 4.6341776847839355,
      "learning_rate": 0.0008614375095376164,
      "loss": 1.1924,
      "step": 959
    },
    {
      "epoch": 0.43616537937301225,
      "grad_norm": 6.497812271118164,
      "learning_rate": 0.0008612849076758736,
      "loss": 0.8585,
      "step": 960
    },
    {
      "epoch": 0.43661971830985913,
      "grad_norm": 6.07902193069458,
      "learning_rate": 0.000861132305814131,
      "loss": 1.19,
      "step": 961
    },
    {
      "epoch": 0.43707405724670606,
      "grad_norm": 3.8967456817626953,
      "learning_rate": 0.0008609797039523882,
      "loss": 0.7496,
      "step": 962
    },
    {
      "epoch": 0.43752839618355294,
      "grad_norm": 7.391471862792969,
      "learning_rate": 0.0008608271020906455,
      "loss": 1.0022,
      "step": 963
    },
    {
      "epoch": 0.4379827351203998,
      "grad_norm": 6.980207443237305,
      "learning_rate": 0.0008606745002289029,
      "loss": 0.8355,
      "step": 964
    },
    {
      "epoch": 0.4384370740572467,
      "grad_norm": 4.290483474731445,
      "learning_rate": 0.0008605218983671601,
      "loss": 0.8613,
      "step": 965
    },
    {
      "epoch": 0.4388914129940936,
      "grad_norm": 5.353556156158447,
      "learning_rate": 0.0008603692965054174,
      "loss": 0.8788,
      "step": 966
    },
    {
      "epoch": 0.4393457519309405,
      "grad_norm": 5.208590030670166,
      "learning_rate": 0.0008602166946436747,
      "loss": 1.5826,
      "step": 967
    },
    {
      "epoch": 0.4398000908677874,
      "grad_norm": 6.183323383331299,
      "learning_rate": 0.000860064092781932,
      "loss": 1.175,
      "step": 968
    },
    {
      "epoch": 0.44025442980463425,
      "grad_norm": 5.985810279846191,
      "learning_rate": 0.0008599114909201893,
      "loss": 0.8497,
      "step": 969
    },
    {
      "epoch": 0.4407087687414811,
      "grad_norm": 5.852529525756836,
      "learning_rate": 0.0008597588890584466,
      "loss": 0.8327,
      "step": 970
    },
    {
      "epoch": 0.44116310767832806,
      "grad_norm": 5.094454288482666,
      "learning_rate": 0.0008596062871967039,
      "loss": 0.6737,
      "step": 971
    },
    {
      "epoch": 0.44161744661517494,
      "grad_norm": 7.0584797859191895,
      "learning_rate": 0.0008594536853349611,
      "loss": 1.3817,
      "step": 972
    },
    {
      "epoch": 0.4420717855520218,
      "grad_norm": 5.17043924331665,
      "learning_rate": 0.0008593010834732184,
      "loss": 0.7932,
      "step": 973
    },
    {
      "epoch": 0.4425261244888687,
      "grad_norm": 4.956395149230957,
      "learning_rate": 0.0008591484816114756,
      "loss": 0.919,
      "step": 974
    },
    {
      "epoch": 0.44298046342571556,
      "grad_norm": 5.982621669769287,
      "learning_rate": 0.0008589958797497329,
      "loss": 1.1108,
      "step": 975
    },
    {
      "epoch": 0.4434348023625625,
      "grad_norm": 6.408397674560547,
      "learning_rate": 0.0008588432778879903,
      "loss": 1.2061,
      "step": 976
    },
    {
      "epoch": 0.4438891412994094,
      "grad_norm": 7.211100101470947,
      "learning_rate": 0.0008586906760262475,
      "loss": 1.1162,
      "step": 977
    },
    {
      "epoch": 0.44434348023625625,
      "grad_norm": 6.028185844421387,
      "learning_rate": 0.0008585380741645048,
      "loss": 1.9802,
      "step": 978
    },
    {
      "epoch": 0.4447978191731031,
      "grad_norm": 6.788867950439453,
      "learning_rate": 0.0008583854723027621,
      "loss": 1.5684,
      "step": 979
    },
    {
      "epoch": 0.44525215810995,
      "grad_norm": 7.551860332489014,
      "learning_rate": 0.0008582328704410194,
      "loss": 1.8892,
      "step": 980
    },
    {
      "epoch": 0.44570649704679693,
      "grad_norm": 8.397481918334961,
      "learning_rate": 0.0008580802685792767,
      "loss": 1.4161,
      "step": 981
    },
    {
      "epoch": 0.4461608359836438,
      "grad_norm": 5.002186298370361,
      "learning_rate": 0.000857927666717534,
      "loss": 0.9486,
      "step": 982
    },
    {
      "epoch": 0.4466151749204907,
      "grad_norm": 9.530433654785156,
      "learning_rate": 0.0008577750648557913,
      "loss": 1.7995,
      "step": 983
    },
    {
      "epoch": 0.44706951385733756,
      "grad_norm": 7.6902055740356445,
      "learning_rate": 0.0008576224629940485,
      "loss": 1.1942,
      "step": 984
    },
    {
      "epoch": 0.44752385279418444,
      "grad_norm": 4.214831829071045,
      "learning_rate": 0.0008574698611323059,
      "loss": 1.5227,
      "step": 985
    },
    {
      "epoch": 0.44797819173103137,
      "grad_norm": 6.611765384674072,
      "learning_rate": 0.0008573172592705632,
      "loss": 1.4416,
      "step": 986
    },
    {
      "epoch": 0.44843253066787825,
      "grad_norm": 6.718103408813477,
      "learning_rate": 0.0008571646574088204,
      "loss": 1.2857,
      "step": 987
    },
    {
      "epoch": 0.4488868696047251,
      "grad_norm": 6.35604190826416,
      "learning_rate": 0.0008570120555470778,
      "loss": 1.4592,
      "step": 988
    },
    {
      "epoch": 0.449341208541572,
      "grad_norm": 4.740074157714844,
      "learning_rate": 0.000856859453685335,
      "loss": 0.4984,
      "step": 989
    },
    {
      "epoch": 0.4497955474784189,
      "grad_norm": 2.1703381538391113,
      "learning_rate": 0.0008567068518235923,
      "loss": 0.2979,
      "step": 990
    },
    {
      "epoch": 0.4502498864152658,
      "grad_norm": 11.879271507263184,
      "learning_rate": 0.0008565542499618497,
      "loss": 1.7121,
      "step": 991
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 6.456932067871094,
      "learning_rate": 0.0008564016481001068,
      "loss": 1.4912,
      "step": 992
    },
    {
      "epoch": 0.45115856428895956,
      "grad_norm": 5.193727493286133,
      "learning_rate": 0.0008562490462383641,
      "loss": 1.2286,
      "step": 993
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 4.299095630645752,
      "learning_rate": 0.0008560964443766214,
      "loss": 0.4361,
      "step": 994
    },
    {
      "epoch": 0.4520672421626533,
      "grad_norm": 4.750974178314209,
      "learning_rate": 0.0008559438425148787,
      "loss": 0.9665,
      "step": 995
    },
    {
      "epoch": 0.45252158109950025,
      "grad_norm": 6.530337810516357,
      "learning_rate": 0.0008557912406531359,
      "loss": 2.0856,
      "step": 996
    },
    {
      "epoch": 0.4529759200363471,
      "grad_norm": 4.6142072677612305,
      "learning_rate": 0.0008556386387913933,
      "loss": 1.1444,
      "step": 997
    },
    {
      "epoch": 0.453430258973194,
      "grad_norm": 4.032329082489014,
      "learning_rate": 0.0008554860369296506,
      "loss": 0.883,
      "step": 998
    },
    {
      "epoch": 0.4538845979100409,
      "grad_norm": 6.458658218383789,
      "learning_rate": 0.0008553334350679078,
      "loss": 1.5612,
      "step": 999
    },
    {
      "epoch": 0.45433893684688775,
      "grad_norm": 6.138057708740234,
      "learning_rate": 0.0008551808332061652,
      "loss": 1.2868,
      "step": 1000
    },
    {
      "epoch": 0.4547932757837347,
      "grad_norm": 6.380071640014648,
      "learning_rate": 0.0008550282313444224,
      "loss": 0.8025,
      "step": 1001
    },
    {
      "epoch": 0.45524761472058156,
      "grad_norm": 4.58574914932251,
      "learning_rate": 0.0008548756294826797,
      "loss": 1.3185,
      "step": 1002
    },
    {
      "epoch": 0.45570195365742844,
      "grad_norm": 5.173280715942383,
      "learning_rate": 0.000854723027620937,
      "loss": 0.5902,
      "step": 1003
    },
    {
      "epoch": 0.4561562925942753,
      "grad_norm": 5.617477893829346,
      "learning_rate": 0.0008545704257591943,
      "loss": 1.4722,
      "step": 1004
    },
    {
      "epoch": 0.4566106315311222,
      "grad_norm": 9.842035293579102,
      "learning_rate": 0.0008544178238974516,
      "loss": 1.6373,
      "step": 1005
    },
    {
      "epoch": 0.4570649704679691,
      "grad_norm": 7.32267951965332,
      "learning_rate": 0.0008542652220357089,
      "loss": 1.781,
      "step": 1006
    },
    {
      "epoch": 0.457519309404816,
      "grad_norm": 5.314257621765137,
      "learning_rate": 0.0008541126201739662,
      "loss": 0.9008,
      "step": 1007
    },
    {
      "epoch": 0.4579736483416629,
      "grad_norm": 5.942941665649414,
      "learning_rate": 0.0008539600183122234,
      "loss": 1.5513,
      "step": 1008
    },
    {
      "epoch": 0.45842798727850975,
      "grad_norm": 5.07900857925415,
      "learning_rate": 0.0008538074164504808,
      "loss": 1.5369,
      "step": 1009
    },
    {
      "epoch": 0.45888232621535663,
      "grad_norm": 6.6903076171875,
      "learning_rate": 0.000853654814588738,
      "loss": 1.8422,
      "step": 1010
    },
    {
      "epoch": 0.45933666515220356,
      "grad_norm": 2.4380643367767334,
      "learning_rate": 0.0008535022127269952,
      "loss": 0.4334,
      "step": 1011
    },
    {
      "epoch": 0.45979100408905044,
      "grad_norm": 3.6741573810577393,
      "learning_rate": 0.0008533496108652526,
      "loss": 0.4538,
      "step": 1012
    },
    {
      "epoch": 0.4602453430258973,
      "grad_norm": 7.181878089904785,
      "learning_rate": 0.0008531970090035098,
      "loss": 1.63,
      "step": 1013
    },
    {
      "epoch": 0.4606996819627442,
      "grad_norm": 7.914843559265137,
      "learning_rate": 0.0008530444071417671,
      "loss": 1.5862,
      "step": 1014
    },
    {
      "epoch": 0.4611540208995911,
      "grad_norm": 5.013667583465576,
      "learning_rate": 0.0008528918052800245,
      "loss": 1.1057,
      "step": 1015
    },
    {
      "epoch": 0.461608359836438,
      "grad_norm": 4.393171787261963,
      "learning_rate": 0.0008527392034182817,
      "loss": 0.8305,
      "step": 1016
    },
    {
      "epoch": 0.4620626987732849,
      "grad_norm": 3.380945920944214,
      "learning_rate": 0.000852586601556539,
      "loss": 0.4266,
      "step": 1017
    },
    {
      "epoch": 0.46251703771013175,
      "grad_norm": 6.450294017791748,
      "learning_rate": 0.0008524339996947963,
      "loss": 1.1641,
      "step": 1018
    },
    {
      "epoch": 0.4629713766469786,
      "grad_norm": 6.865687370300293,
      "learning_rate": 0.0008522813978330536,
      "loss": 1.0847,
      "step": 1019
    },
    {
      "epoch": 0.46342571558382556,
      "grad_norm": 4.857980728149414,
      "learning_rate": 0.0008521287959713108,
      "loss": 0.8739,
      "step": 1020
    },
    {
      "epoch": 0.46388005452067244,
      "grad_norm": 5.361921310424805,
      "learning_rate": 0.0008519761941095682,
      "loss": 0.7581,
      "step": 1021
    },
    {
      "epoch": 0.4643343934575193,
      "grad_norm": 5.7912163734436035,
      "learning_rate": 0.0008518235922478255,
      "loss": 1.2129,
      "step": 1022
    },
    {
      "epoch": 0.4647887323943662,
      "grad_norm": 9.393000602722168,
      "learning_rate": 0.0008516709903860827,
      "loss": 2.8299,
      "step": 1023
    },
    {
      "epoch": 0.46524307133121307,
      "grad_norm": 6.408736705780029,
      "learning_rate": 0.0008515183885243401,
      "loss": 1.2587,
      "step": 1024
    },
    {
      "epoch": 0.46569741026806,
      "grad_norm": 3.2343766689300537,
      "learning_rate": 0.0008513657866625973,
      "loss": 0.4129,
      "step": 1025
    },
    {
      "epoch": 0.4661517492049069,
      "grad_norm": 5.080480575561523,
      "learning_rate": 0.0008512131848008546,
      "loss": 1.2855,
      "step": 1026
    },
    {
      "epoch": 0.46660608814175375,
      "grad_norm": 5.9554972648620605,
      "learning_rate": 0.000851060582939112,
      "loss": 0.7575,
      "step": 1027
    },
    {
      "epoch": 0.4670604270786006,
      "grad_norm": 5.896932125091553,
      "learning_rate": 0.0008509079810773691,
      "loss": 1.4098,
      "step": 1028
    },
    {
      "epoch": 0.4675147660154475,
      "grad_norm": 6.0760016441345215,
      "learning_rate": 0.0008507553792156264,
      "loss": 0.8272,
      "step": 1029
    },
    {
      "epoch": 0.46796910495229443,
      "grad_norm": 6.415463924407959,
      "learning_rate": 0.0008506027773538837,
      "loss": 1.2483,
      "step": 1030
    },
    {
      "epoch": 0.4684234438891413,
      "grad_norm": 4.7757697105407715,
      "learning_rate": 0.000850450175492141,
      "loss": 0.8672,
      "step": 1031
    },
    {
      "epoch": 0.4688777828259882,
      "grad_norm": 7.364598274230957,
      "learning_rate": 0.0008502975736303982,
      "loss": 1.6919,
      "step": 1032
    },
    {
      "epoch": 0.46933212176283506,
      "grad_norm": 6.956244468688965,
      "learning_rate": 0.0008501449717686556,
      "loss": 2.1183,
      "step": 1033
    },
    {
      "epoch": 0.46978646069968194,
      "grad_norm": 4.836253643035889,
      "learning_rate": 0.0008499923699069129,
      "loss": 0.8949,
      "step": 1034
    },
    {
      "epoch": 0.4702407996365289,
      "grad_norm": 6.674531936645508,
      "learning_rate": 0.0008498397680451701,
      "loss": 1.9401,
      "step": 1035
    },
    {
      "epoch": 0.47069513857337575,
      "grad_norm": 4.968378067016602,
      "learning_rate": 0.0008496871661834275,
      "loss": 1.3732,
      "step": 1036
    },
    {
      "epoch": 0.4711494775102226,
      "grad_norm": 9.626267433166504,
      "learning_rate": 0.0008495345643216847,
      "loss": 1.0569,
      "step": 1037
    },
    {
      "epoch": 0.4716038164470695,
      "grad_norm": 5.505126476287842,
      "learning_rate": 0.000849381962459942,
      "loss": 1.2286,
      "step": 1038
    },
    {
      "epoch": 0.4720581553839164,
      "grad_norm": 9.345490455627441,
      "learning_rate": 0.0008492293605981994,
      "loss": 1.6806,
      "step": 1039
    },
    {
      "epoch": 0.4725124943207633,
      "grad_norm": 3.725044012069702,
      "learning_rate": 0.0008490767587364566,
      "loss": 1.2942,
      "step": 1040
    },
    {
      "epoch": 0.4729668332576102,
      "grad_norm": 7.118791103363037,
      "learning_rate": 0.0008489241568747139,
      "loss": 1.2514,
      "step": 1041
    },
    {
      "epoch": 0.47342117219445706,
      "grad_norm": 6.948042869567871,
      "learning_rate": 0.0008487715550129712,
      "loss": 1.6835,
      "step": 1042
    },
    {
      "epoch": 0.47387551113130394,
      "grad_norm": 5.219738483428955,
      "learning_rate": 0.0008486189531512285,
      "loss": 1.0965,
      "step": 1043
    },
    {
      "epoch": 0.4743298500681508,
      "grad_norm": 5.083114147186279,
      "learning_rate": 0.0008484663512894858,
      "loss": 0.8009,
      "step": 1044
    },
    {
      "epoch": 0.47478418900499775,
      "grad_norm": 7.639681816101074,
      "learning_rate": 0.0008483137494277431,
      "loss": 0.897,
      "step": 1045
    },
    {
      "epoch": 0.4752385279418446,
      "grad_norm": 3.6835720539093018,
      "learning_rate": 0.0008481611475660003,
      "loss": 0.617,
      "step": 1046
    },
    {
      "epoch": 0.4756928668786915,
      "grad_norm": 7.283473491668701,
      "learning_rate": 0.0008480085457042575,
      "loss": 1.6139,
      "step": 1047
    },
    {
      "epoch": 0.4761472058155384,
      "grad_norm": 9.359955787658691,
      "learning_rate": 0.0008478559438425149,
      "loss": 1.3503,
      "step": 1048
    },
    {
      "epoch": 0.47660154475238525,
      "grad_norm": 3.748080015182495,
      "learning_rate": 0.0008477033419807721,
      "loss": 0.5581,
      "step": 1049
    },
    {
      "epoch": 0.4770558836892322,
      "grad_norm": 4.528615474700928,
      "learning_rate": 0.0008475507401190294,
      "loss": 0.6533,
      "step": 1050
    },
    {
      "epoch": 0.47751022262607906,
      "grad_norm": 3.3676633834838867,
      "learning_rate": 0.0008473981382572868,
      "loss": 0.747,
      "step": 1051
    },
    {
      "epoch": 0.47796456156292594,
      "grad_norm": 9.729265213012695,
      "learning_rate": 0.000847245536395544,
      "loss": 1.6583,
      "step": 1052
    },
    {
      "epoch": 0.4784189004997728,
      "grad_norm": 7.407498836517334,
      "learning_rate": 0.0008470929345338013,
      "loss": 1.0277,
      "step": 1053
    },
    {
      "epoch": 0.4788732394366197,
      "grad_norm": 5.873044967651367,
      "learning_rate": 0.0008469403326720586,
      "loss": 1.3591,
      "step": 1054
    },
    {
      "epoch": 0.4793275783734666,
      "grad_norm": 9.989470481872559,
      "learning_rate": 0.0008467877308103159,
      "loss": 1.1562,
      "step": 1055
    },
    {
      "epoch": 0.4797819173103135,
      "grad_norm": 6.393779277801514,
      "learning_rate": 0.0008466351289485732,
      "loss": 1.4065,
      "step": 1056
    },
    {
      "epoch": 0.4802362562471604,
      "grad_norm": 5.903590202331543,
      "learning_rate": 0.0008464825270868305,
      "loss": 0.8985,
      "step": 1057
    },
    {
      "epoch": 0.48069059518400725,
      "grad_norm": 4.940035820007324,
      "learning_rate": 0.0008463299252250878,
      "loss": 1.2563,
      "step": 1058
    },
    {
      "epoch": 0.4811449341208542,
      "grad_norm": 6.836970806121826,
      "learning_rate": 0.000846177323363345,
      "loss": 1.6439,
      "step": 1059
    },
    {
      "epoch": 0.48159927305770106,
      "grad_norm": 5.041175365447998,
      "learning_rate": 0.0008460247215016024,
      "loss": 1.1691,
      "step": 1060
    },
    {
      "epoch": 0.48205361199454794,
      "grad_norm": 6.179610252380371,
      "learning_rate": 0.0008458721196398597,
      "loss": 0.9738,
      "step": 1061
    },
    {
      "epoch": 0.4825079509313948,
      "grad_norm": 6.3361053466796875,
      "learning_rate": 0.0008457195177781169,
      "loss": 1.6368,
      "step": 1062
    },
    {
      "epoch": 0.4829622898682417,
      "grad_norm": 8.360262870788574,
      "learning_rate": 0.0008455669159163743,
      "loss": 1.4182,
      "step": 1063
    },
    {
      "epoch": 0.4834166288050886,
      "grad_norm": 5.877464771270752,
      "learning_rate": 0.0008454143140546315,
      "loss": 1.5698,
      "step": 1064
    },
    {
      "epoch": 0.4838709677419355,
      "grad_norm": 3.9838290214538574,
      "learning_rate": 0.0008452617121928887,
      "loss": 0.6296,
      "step": 1065
    },
    {
      "epoch": 0.4843253066787824,
      "grad_norm": 6.497221946716309,
      "learning_rate": 0.000845109110331146,
      "loss": 1.5373,
      "step": 1066
    },
    {
      "epoch": 0.48477964561562925,
      "grad_norm": 4.204268455505371,
      "learning_rate": 0.0008449565084694033,
      "loss": 0.5592,
      "step": 1067
    },
    {
      "epoch": 0.4852339845524761,
      "grad_norm": 7.243320465087891,
      "learning_rate": 0.0008448039066076606,
      "loss": 1.1569,
      "step": 1068
    },
    {
      "epoch": 0.48568832348932306,
      "grad_norm": 7.655782222747803,
      "learning_rate": 0.0008446513047459179,
      "loss": 1.6234,
      "step": 1069
    },
    {
      "epoch": 0.48614266242616994,
      "grad_norm": 3.4744296073913574,
      "learning_rate": 0.0008444987028841752,
      "loss": 0.7883,
      "step": 1070
    },
    {
      "epoch": 0.4865970013630168,
      "grad_norm": 5.540771007537842,
      "learning_rate": 0.0008443461010224324,
      "loss": 0.9551,
      "step": 1071
    },
    {
      "epoch": 0.4870513402998637,
      "grad_norm": 6.8821001052856445,
      "learning_rate": 0.0008441934991606898,
      "loss": 1.5747,
      "step": 1072
    },
    {
      "epoch": 0.48750567923671057,
      "grad_norm": 7.079697608947754,
      "learning_rate": 0.0008440408972989471,
      "loss": 1.4531,
      "step": 1073
    },
    {
      "epoch": 0.4879600181735575,
      "grad_norm": 3.9076895713806152,
      "learning_rate": 0.0008438882954372043,
      "loss": 0.5766,
      "step": 1074
    },
    {
      "epoch": 0.4884143571104044,
      "grad_norm": 7.287618637084961,
      "learning_rate": 0.0008437356935754617,
      "loss": 0.7804,
      "step": 1075
    },
    {
      "epoch": 0.48886869604725125,
      "grad_norm": 6.263627052307129,
      "learning_rate": 0.0008435830917137189,
      "loss": 0.8192,
      "step": 1076
    },
    {
      "epoch": 0.4893230349840981,
      "grad_norm": 8.775127410888672,
      "learning_rate": 0.0008434304898519762,
      "loss": 1.4447,
      "step": 1077
    },
    {
      "epoch": 0.489777373920945,
      "grad_norm": 6.353113174438477,
      "learning_rate": 0.0008432778879902336,
      "loss": 1.1747,
      "step": 1078
    },
    {
      "epoch": 0.49023171285779193,
      "grad_norm": 6.539178371429443,
      "learning_rate": 0.0008431252861284908,
      "loss": 2.0047,
      "step": 1079
    },
    {
      "epoch": 0.4906860517946388,
      "grad_norm": 4.7845139503479,
      "learning_rate": 0.0008429726842667481,
      "loss": 0.6835,
      "step": 1080
    },
    {
      "epoch": 0.4911403907314857,
      "grad_norm": 3.2114803791046143,
      "learning_rate": 0.0008428200824050054,
      "loss": 0.7961,
      "step": 1081
    },
    {
      "epoch": 0.49159472966833256,
      "grad_norm": 8.696600914001465,
      "learning_rate": 0.0008426674805432627,
      "loss": 1.8851,
      "step": 1082
    },
    {
      "epoch": 0.49204906860517944,
      "grad_norm": 7.021031856536865,
      "learning_rate": 0.0008425148786815198,
      "loss": 0.9314,
      "step": 1083
    },
    {
      "epoch": 0.4925034075420264,
      "grad_norm": 17.19550132751465,
      "learning_rate": 0.0008423622768197772,
      "loss": 1.7705,
      "step": 1084
    },
    {
      "epoch": 0.49295774647887325,
      "grad_norm": 4.5938920974731445,
      "learning_rate": 0.0008422096749580345,
      "loss": 0.645,
      "step": 1085
    },
    {
      "epoch": 0.4934120854157201,
      "grad_norm": 5.021948337554932,
      "learning_rate": 0.0008420570730962917,
      "loss": 1.7244,
      "step": 1086
    },
    {
      "epoch": 0.493866424352567,
      "grad_norm": 6.16664981842041,
      "learning_rate": 0.0008419044712345491,
      "loss": 0.713,
      "step": 1087
    },
    {
      "epoch": 0.4943207632894139,
      "grad_norm": 6.615296363830566,
      "learning_rate": 0.0008417518693728063,
      "loss": 1.769,
      "step": 1088
    },
    {
      "epoch": 0.4947751022262608,
      "grad_norm": 7.072011947631836,
      "learning_rate": 0.0008415992675110636,
      "loss": 1.3384,
      "step": 1089
    },
    {
      "epoch": 0.4952294411631077,
      "grad_norm": 8.322399139404297,
      "learning_rate": 0.000841446665649321,
      "loss": 2.1046,
      "step": 1090
    },
    {
      "epoch": 0.49568378009995456,
      "grad_norm": 7.297142028808594,
      "learning_rate": 0.0008412940637875782,
      "loss": 1.1217,
      "step": 1091
    },
    {
      "epoch": 0.49613811903680144,
      "grad_norm": 5.8210673332214355,
      "learning_rate": 0.0008411414619258355,
      "loss": 1.0339,
      "step": 1092
    },
    {
      "epoch": 0.4965924579736483,
      "grad_norm": 5.07891845703125,
      "learning_rate": 0.0008409888600640928,
      "loss": 1.0438,
      "step": 1093
    },
    {
      "epoch": 0.49704679691049525,
      "grad_norm": 5.2758636474609375,
      "learning_rate": 0.0008408362582023501,
      "loss": 0.5346,
      "step": 1094
    },
    {
      "epoch": 0.4975011358473421,
      "grad_norm": 7.319534778594971,
      "learning_rate": 0.0008406836563406074,
      "loss": 1.594,
      "step": 1095
    },
    {
      "epoch": 0.497955474784189,
      "grad_norm": 5.124091148376465,
      "learning_rate": 0.0008405310544788647,
      "loss": 0.6298,
      "step": 1096
    },
    {
      "epoch": 0.4984098137210359,
      "grad_norm": 5.642143726348877,
      "learning_rate": 0.000840378452617122,
      "loss": 1.2714,
      "step": 1097
    },
    {
      "epoch": 0.49886415265788275,
      "grad_norm": 6.534734725952148,
      "learning_rate": 0.0008402258507553792,
      "loss": 0.9762,
      "step": 1098
    },
    {
      "epoch": 0.4993184915947297,
      "grad_norm": 6.664302349090576,
      "learning_rate": 0.0008400732488936366,
      "loss": 2.434,
      "step": 1099
    },
    {
      "epoch": 0.49977283053157656,
      "grad_norm": 7.810424327850342,
      "learning_rate": 0.0008399206470318939,
      "loss": 1.1881,
      "step": 1100
    },
    {
      "epoch": 0.5002271694684235,
      "grad_norm": 6.587212085723877,
      "learning_rate": 0.000839768045170151,
      "loss": 1.2737,
      "step": 1101
    },
    {
      "epoch": 0.5006815084052704,
      "grad_norm": 5.747220993041992,
      "learning_rate": 0.0008396154433084084,
      "loss": 1.4719,
      "step": 1102
    },
    {
      "epoch": 0.5011358473421172,
      "grad_norm": 6.571447372436523,
      "learning_rate": 0.0008394628414466656,
      "loss": 1.0354,
      "step": 1103
    },
    {
      "epoch": 0.5015901862789641,
      "grad_norm": 7.127504825592041,
      "learning_rate": 0.0008393102395849229,
      "loss": 1.1939,
      "step": 1104
    },
    {
      "epoch": 0.502044525215811,
      "grad_norm": 7.037901878356934,
      "learning_rate": 0.0008391576377231802,
      "loss": 1.576,
      "step": 1105
    },
    {
      "epoch": 0.5024988641526579,
      "grad_norm": 4.822382926940918,
      "learning_rate": 0.0008390050358614375,
      "loss": 1.3234,
      "step": 1106
    },
    {
      "epoch": 0.5029532030895048,
      "grad_norm": 5.6152262687683105,
      "learning_rate": 0.0008388524339996948,
      "loss": 0.9346,
      "step": 1107
    },
    {
      "epoch": 0.5034075420263516,
      "grad_norm": 7.399785995483398,
      "learning_rate": 0.0008386998321379521,
      "loss": 1.6886,
      "step": 1108
    },
    {
      "epoch": 0.5038618809631985,
      "grad_norm": 8.636186599731445,
      "learning_rate": 0.0008385472302762094,
      "loss": 1.324,
      "step": 1109
    },
    {
      "epoch": 0.5043162199000454,
      "grad_norm": 6.949794769287109,
      "learning_rate": 0.0008383946284144666,
      "loss": 2.395,
      "step": 1110
    },
    {
      "epoch": 0.5047705588368924,
      "grad_norm": 5.682286739349365,
      "learning_rate": 0.000838242026552724,
      "loss": 1.0192,
      "step": 1111
    },
    {
      "epoch": 0.5052248977737392,
      "grad_norm": 8.152521133422852,
      "learning_rate": 0.0008380894246909813,
      "loss": 1.4793,
      "step": 1112
    },
    {
      "epoch": 0.5056792367105861,
      "grad_norm": 5.671889781951904,
      "learning_rate": 0.0008379368228292385,
      "loss": 1.525,
      "step": 1113
    },
    {
      "epoch": 0.506133575647433,
      "grad_norm": 3.8371431827545166,
      "learning_rate": 0.0008377842209674959,
      "loss": 1.1724,
      "step": 1114
    },
    {
      "epoch": 0.5065879145842799,
      "grad_norm": 5.2708420753479,
      "learning_rate": 0.0008376316191057531,
      "loss": 1.6699,
      "step": 1115
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 3.933154821395874,
      "learning_rate": 0.0008374790172440104,
      "loss": 0.5225,
      "step": 1116
    },
    {
      "epoch": 0.5074965924579736,
      "grad_norm": 9.281172752380371,
      "learning_rate": 0.0008373264153822678,
      "loss": 1.5916,
      "step": 1117
    },
    {
      "epoch": 0.5079509313948205,
      "grad_norm": 3.2661590576171875,
      "learning_rate": 0.000837173813520525,
      "loss": 0.3318,
      "step": 1118
    },
    {
      "epoch": 0.5084052703316674,
      "grad_norm": 5.06362771987915,
      "learning_rate": 0.0008370212116587822,
      "loss": 1.1835,
      "step": 1119
    },
    {
      "epoch": 0.5088596092685143,
      "grad_norm": 7.5969953536987305,
      "learning_rate": 0.0008368686097970395,
      "loss": 1.5246,
      "step": 1120
    },
    {
      "epoch": 0.5093139482053612,
      "grad_norm": 5.2594895362854,
      "learning_rate": 0.0008367160079352968,
      "loss": 1.2345,
      "step": 1121
    },
    {
      "epoch": 0.5097682871422081,
      "grad_norm": 5.665925979614258,
      "learning_rate": 0.000836563406073554,
      "loss": 1.1251,
      "step": 1122
    },
    {
      "epoch": 0.510222626079055,
      "grad_norm": 7.767950534820557,
      "learning_rate": 0.0008364108042118114,
      "loss": 0.8699,
      "step": 1123
    },
    {
      "epoch": 0.5106769650159019,
      "grad_norm": 4.963507175445557,
      "learning_rate": 0.0008362582023500687,
      "loss": 1.0921,
      "step": 1124
    },
    {
      "epoch": 0.5111313039527488,
      "grad_norm": 6.4657063484191895,
      "learning_rate": 0.0008361056004883259,
      "loss": 1.4953,
      "step": 1125
    },
    {
      "epoch": 0.5115856428895956,
      "grad_norm": 3.8104875087738037,
      "learning_rate": 0.0008359529986265833,
      "loss": 0.657,
      "step": 1126
    },
    {
      "epoch": 0.5120399818264425,
      "grad_norm": 5.053524017333984,
      "learning_rate": 0.0008358003967648405,
      "loss": 1.0779,
      "step": 1127
    },
    {
      "epoch": 0.5124943207632894,
      "grad_norm": 7.919530868530273,
      "learning_rate": 0.0008356477949030978,
      "loss": 1.1535,
      "step": 1128
    },
    {
      "epoch": 0.5129486597001363,
      "grad_norm": 6.943070888519287,
      "learning_rate": 0.0008354951930413552,
      "loss": 1.2327,
      "step": 1129
    },
    {
      "epoch": 0.5134029986369832,
      "grad_norm": 4.677811145782471,
      "learning_rate": 0.0008353425911796124,
      "loss": 0.5897,
      "step": 1130
    },
    {
      "epoch": 0.5138573375738301,
      "grad_norm": 3.222764015197754,
      "learning_rate": 0.0008351899893178697,
      "loss": 0.5926,
      "step": 1131
    },
    {
      "epoch": 0.514311676510677,
      "grad_norm": 8.361616134643555,
      "learning_rate": 0.000835037387456127,
      "loss": 1.0879,
      "step": 1132
    },
    {
      "epoch": 0.5147660154475239,
      "grad_norm": 5.022936820983887,
      "learning_rate": 0.0008348847855943843,
      "loss": 0.8763,
      "step": 1133
    },
    {
      "epoch": 0.5152203543843707,
      "grad_norm": 4.736138343811035,
      "learning_rate": 0.0008347321837326415,
      "loss": 1.1375,
      "step": 1134
    },
    {
      "epoch": 0.5156746933212176,
      "grad_norm": 6.446150302886963,
      "learning_rate": 0.0008345795818708989,
      "loss": 0.9612,
      "step": 1135
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 5.588617324829102,
      "learning_rate": 0.0008344269800091562,
      "loss": 0.7953,
      "step": 1136
    },
    {
      "epoch": 0.5165833711949114,
      "grad_norm": 4.436561107635498,
      "learning_rate": 0.0008342743781474134,
      "loss": 1.5106,
      "step": 1137
    },
    {
      "epoch": 0.5170377101317583,
      "grad_norm": 6.238348007202148,
      "learning_rate": 0.0008341217762856707,
      "loss": 1.6455,
      "step": 1138
    },
    {
      "epoch": 0.5174920490686051,
      "grad_norm": 3.2097113132476807,
      "learning_rate": 0.0008339691744239279,
      "loss": 0.7943,
      "step": 1139
    },
    {
      "epoch": 0.5179463880054521,
      "grad_norm": 5.05265998840332,
      "learning_rate": 0.0008338165725621852,
      "loss": 1.8043,
      "step": 1140
    },
    {
      "epoch": 0.518400726942299,
      "grad_norm": 6.3679656982421875,
      "learning_rate": 0.0008336639707004426,
      "loss": 2.1458,
      "step": 1141
    },
    {
      "epoch": 0.5188550658791459,
      "grad_norm": 7.319841384887695,
      "learning_rate": 0.0008335113688386998,
      "loss": 1.3739,
      "step": 1142
    },
    {
      "epoch": 0.5193094048159927,
      "grad_norm": 3.1988539695739746,
      "learning_rate": 0.0008333587669769571,
      "loss": 0.3195,
      "step": 1143
    },
    {
      "epoch": 0.5197637437528396,
      "grad_norm": 6.674330234527588,
      "learning_rate": 0.0008332061651152144,
      "loss": 0.7525,
      "step": 1144
    },
    {
      "epoch": 0.5202180826896865,
      "grad_norm": 6.5078630447387695,
      "learning_rate": 0.0008330535632534717,
      "loss": 1.3959,
      "step": 1145
    },
    {
      "epoch": 0.5206724216265334,
      "grad_norm": 5.118145942687988,
      "learning_rate": 0.000832900961391729,
      "loss": 0.8671,
      "step": 1146
    },
    {
      "epoch": 0.5211267605633803,
      "grad_norm": 5.7324090003967285,
      "learning_rate": 0.0008327483595299863,
      "loss": 2.0736,
      "step": 1147
    },
    {
      "epoch": 0.5215810995002271,
      "grad_norm": 6.452836990356445,
      "learning_rate": 0.0008325957576682436,
      "loss": 1.4646,
      "step": 1148
    },
    {
      "epoch": 0.522035438437074,
      "grad_norm": 6.395796298980713,
      "learning_rate": 0.0008324431558065008,
      "loss": 0.639,
      "step": 1149
    },
    {
      "epoch": 0.522489777373921,
      "grad_norm": 7.015491485595703,
      "learning_rate": 0.0008322905539447582,
      "loss": 1.8082,
      "step": 1150
    },
    {
      "epoch": 0.5229441163107679,
      "grad_norm": 5.786884307861328,
      "learning_rate": 0.0008321379520830154,
      "loss": 1.9942,
      "step": 1151
    },
    {
      "epoch": 0.5233984552476147,
      "grad_norm": 6.377841472625732,
      "learning_rate": 0.0008319853502212727,
      "loss": 1.7593,
      "step": 1152
    },
    {
      "epoch": 0.5238527941844616,
      "grad_norm": 3.702681064605713,
      "learning_rate": 0.0008318327483595301,
      "loss": 0.6748,
      "step": 1153
    },
    {
      "epoch": 0.5243071331213085,
      "grad_norm": 6.831049919128418,
      "learning_rate": 0.0008316801464977873,
      "loss": 0.9887,
      "step": 1154
    },
    {
      "epoch": 0.5247614720581554,
      "grad_norm": 6.707858085632324,
      "learning_rate": 0.0008315275446360446,
      "loss": 2.1175,
      "step": 1155
    },
    {
      "epoch": 0.5252158109950023,
      "grad_norm": 8.513182640075684,
      "learning_rate": 0.0008313749427743018,
      "loss": 0.6443,
      "step": 1156
    },
    {
      "epoch": 0.5256701499318491,
      "grad_norm": 7.185368061065674,
      "learning_rate": 0.0008312223409125591,
      "loss": 1.3889,
      "step": 1157
    },
    {
      "epoch": 0.526124488868696,
      "grad_norm": 5.176680088043213,
      "learning_rate": 0.0008310697390508163,
      "loss": 1.0179,
      "step": 1158
    },
    {
      "epoch": 0.5265788278055429,
      "grad_norm": 4.861725807189941,
      "learning_rate": 0.0008309171371890737,
      "loss": 0.4887,
      "step": 1159
    },
    {
      "epoch": 0.5270331667423899,
      "grad_norm": 7.278319835662842,
      "learning_rate": 0.000830764535327331,
      "loss": 1.2909,
      "step": 1160
    },
    {
      "epoch": 0.5274875056792367,
      "grad_norm": 17.27720832824707,
      "learning_rate": 0.0008306119334655882,
      "loss": 1.8176,
      "step": 1161
    },
    {
      "epoch": 0.5279418446160836,
      "grad_norm": 5.87204122543335,
      "learning_rate": 0.0008304593316038456,
      "loss": 1.4236,
      "step": 1162
    },
    {
      "epoch": 0.5283961835529305,
      "grad_norm": 6.304206848144531,
      "learning_rate": 0.0008303067297421028,
      "loss": 1.7068,
      "step": 1163
    },
    {
      "epoch": 0.5288505224897774,
      "grad_norm": 4.693657875061035,
      "learning_rate": 0.0008301541278803601,
      "loss": 0.6465,
      "step": 1164
    },
    {
      "epoch": 0.5293048614266243,
      "grad_norm": 6.963006496429443,
      "learning_rate": 0.0008300015260186175,
      "loss": 2.3457,
      "step": 1165
    },
    {
      "epoch": 0.5297592003634711,
      "grad_norm": 6.1390461921691895,
      "learning_rate": 0.0008298489241568747,
      "loss": 1.7583,
      "step": 1166
    },
    {
      "epoch": 0.530213539300318,
      "grad_norm": 5.272864818572998,
      "learning_rate": 0.000829696322295132,
      "loss": 0.7739,
      "step": 1167
    },
    {
      "epoch": 0.5306678782371649,
      "grad_norm": 4.5414347648620605,
      "learning_rate": 0.0008295437204333893,
      "loss": 1.3345,
      "step": 1168
    },
    {
      "epoch": 0.5311222171740119,
      "grad_norm": 9.698951721191406,
      "learning_rate": 0.0008293911185716466,
      "loss": 1.9161,
      "step": 1169
    },
    {
      "epoch": 0.5315765561108587,
      "grad_norm": 7.420163631439209,
      "learning_rate": 0.0008292385167099039,
      "loss": 1.7181,
      "step": 1170
    },
    {
      "epoch": 0.5320308950477056,
      "grad_norm": 9.188798904418945,
      "learning_rate": 0.0008290859148481612,
      "loss": 0.7597,
      "step": 1171
    },
    {
      "epoch": 0.5324852339845525,
      "grad_norm": 4.536269664764404,
      "learning_rate": 0.0008289333129864185,
      "loss": 0.4638,
      "step": 1172
    },
    {
      "epoch": 0.5329395729213994,
      "grad_norm": 4.662639617919922,
      "learning_rate": 0.0008287807111246757,
      "loss": 1.0449,
      "step": 1173
    },
    {
      "epoch": 0.5333939118582463,
      "grad_norm": 6.209084510803223,
      "learning_rate": 0.000828628109262933,
      "loss": 0.8974,
      "step": 1174
    },
    {
      "epoch": 0.5338482507950931,
      "grad_norm": 4.942394733428955,
      "learning_rate": 0.0008284755074011902,
      "loss": 1.1607,
      "step": 1175
    },
    {
      "epoch": 0.53430258973194,
      "grad_norm": 6.583189487457275,
      "learning_rate": 0.0008283229055394475,
      "loss": 1.5189,
      "step": 1176
    },
    {
      "epoch": 0.5347569286687869,
      "grad_norm": 4.831203937530518,
      "learning_rate": 0.0008281703036777049,
      "loss": 1.679,
      "step": 1177
    },
    {
      "epoch": 0.5352112676056338,
      "grad_norm": 13.487860679626465,
      "learning_rate": 0.0008280177018159621,
      "loss": 1.8475,
      "step": 1178
    },
    {
      "epoch": 0.5356656065424807,
      "grad_norm": 6.313546180725098,
      "learning_rate": 0.0008278650999542194,
      "loss": 1.3374,
      "step": 1179
    },
    {
      "epoch": 0.5361199454793276,
      "grad_norm": 4.745636940002441,
      "learning_rate": 0.0008277124980924767,
      "loss": 1.2737,
      "step": 1180
    },
    {
      "epoch": 0.5365742844161745,
      "grad_norm": 6.67850399017334,
      "learning_rate": 0.000827559896230734,
      "loss": 0.9239,
      "step": 1181
    },
    {
      "epoch": 0.5370286233530214,
      "grad_norm": 6.6718034744262695,
      "learning_rate": 0.0008274072943689913,
      "loss": 1.4754,
      "step": 1182
    },
    {
      "epoch": 0.5374829622898682,
      "grad_norm": 6.956282138824463,
      "learning_rate": 0.0008272546925072486,
      "loss": 2.2666,
      "step": 1183
    },
    {
      "epoch": 0.5379373012267151,
      "grad_norm": 8.297225952148438,
      "learning_rate": 0.0008271020906455059,
      "loss": 1.5886,
      "step": 1184
    },
    {
      "epoch": 0.538391640163562,
      "grad_norm": 9.301739692687988,
      "learning_rate": 0.0008269494887837632,
      "loss": 0.9983,
      "step": 1185
    },
    {
      "epoch": 0.5388459791004089,
      "grad_norm": 6.073221683502197,
      "learning_rate": 0.0008267968869220205,
      "loss": 1.1862,
      "step": 1186
    },
    {
      "epoch": 0.5393003180372558,
      "grad_norm": 2.7531702518463135,
      "learning_rate": 0.0008266442850602778,
      "loss": 0.3537,
      "step": 1187
    },
    {
      "epoch": 0.5397546569741026,
      "grad_norm": 4.274366855621338,
      "learning_rate": 0.0008264916831985351,
      "loss": 0.7858,
      "step": 1188
    },
    {
      "epoch": 0.5402089959109496,
      "grad_norm": 6.951312065124512,
      "learning_rate": 0.0008263390813367924,
      "loss": 1.4619,
      "step": 1189
    },
    {
      "epoch": 0.5406633348477965,
      "grad_norm": 6.464057922363281,
      "learning_rate": 0.0008261864794750496,
      "loss": 0.9225,
      "step": 1190
    },
    {
      "epoch": 0.5411176737846434,
      "grad_norm": 6.679666996002197,
      "learning_rate": 0.000826033877613307,
      "loss": 0.9605,
      "step": 1191
    },
    {
      "epoch": 0.5415720127214902,
      "grad_norm": 4.815444469451904,
      "learning_rate": 0.0008258812757515642,
      "loss": 0.8086,
      "step": 1192
    },
    {
      "epoch": 0.5420263516583371,
      "grad_norm": 6.160940170288086,
      "learning_rate": 0.0008257286738898214,
      "loss": 2.3253,
      "step": 1193
    },
    {
      "epoch": 0.542480690595184,
      "grad_norm": 3.3507702350616455,
      "learning_rate": 0.0008255760720280788,
      "loss": 0.2128,
      "step": 1194
    },
    {
      "epoch": 0.5429350295320309,
      "grad_norm": 3.254721164703369,
      "learning_rate": 0.000825423470166336,
      "loss": 0.5995,
      "step": 1195
    },
    {
      "epoch": 0.5433893684688778,
      "grad_norm": 6.513477802276611,
      "learning_rate": 0.0008252708683045933,
      "loss": 1.3237,
      "step": 1196
    },
    {
      "epoch": 0.5438437074057246,
      "grad_norm": 3.6162710189819336,
      "learning_rate": 0.0008251182664428506,
      "loss": 0.7148,
      "step": 1197
    },
    {
      "epoch": 0.5442980463425715,
      "grad_norm": 5.494419097900391,
      "learning_rate": 0.0008249656645811079,
      "loss": 0.5587,
      "step": 1198
    },
    {
      "epoch": 0.5447523852794185,
      "grad_norm": 3.4626121520996094,
      "learning_rate": 0.0008248130627193652,
      "loss": 0.2397,
      "step": 1199
    },
    {
      "epoch": 0.5452067242162654,
      "grad_norm": 5.082940101623535,
      "learning_rate": 0.0008246604608576225,
      "loss": 0.6891,
      "step": 1200
    },
    {
      "epoch": 0.5456610631531122,
      "grad_norm": 3.9760026931762695,
      "learning_rate": 0.0008245078589958798,
      "loss": 0.7846,
      "step": 1201
    },
    {
      "epoch": 0.5461154020899591,
      "grad_norm": 5.9122490882873535,
      "learning_rate": 0.000824355257134137,
      "loss": 0.5325,
      "step": 1202
    },
    {
      "epoch": 0.546569741026806,
      "grad_norm": 5.3019208908081055,
      "learning_rate": 0.0008242026552723944,
      "loss": 1.1994,
      "step": 1203
    },
    {
      "epoch": 0.5470240799636529,
      "grad_norm": 5.0390944480896,
      "learning_rate": 0.0008240500534106517,
      "loss": 0.8099,
      "step": 1204
    },
    {
      "epoch": 0.5474784189004998,
      "grad_norm": 5.425317287445068,
      "learning_rate": 0.0008238974515489089,
      "loss": 1.157,
      "step": 1205
    },
    {
      "epoch": 0.5479327578373466,
      "grad_norm": 6.418102264404297,
      "learning_rate": 0.0008237448496871663,
      "loss": 1.3961,
      "step": 1206
    },
    {
      "epoch": 0.5483870967741935,
      "grad_norm": 9.10991096496582,
      "learning_rate": 0.0008235922478254235,
      "loss": 1.5522,
      "step": 1207
    },
    {
      "epoch": 0.5488414357110404,
      "grad_norm": 5.911365032196045,
      "learning_rate": 0.0008234396459636808,
      "loss": 1.1203,
      "step": 1208
    },
    {
      "epoch": 0.5492957746478874,
      "grad_norm": 8.328518867492676,
      "learning_rate": 0.0008232870441019382,
      "loss": 1.8095,
      "step": 1209
    },
    {
      "epoch": 0.5497501135847342,
      "grad_norm": 7.175042152404785,
      "learning_rate": 0.0008231344422401954,
      "loss": 1.5219,
      "step": 1210
    },
    {
      "epoch": 0.5502044525215811,
      "grad_norm": 6.133046627044678,
      "learning_rate": 0.0008229818403784526,
      "loss": 0.9384,
      "step": 1211
    },
    {
      "epoch": 0.550658791458428,
      "grad_norm": 5.517056941986084,
      "learning_rate": 0.0008228292385167099,
      "loss": 0.8382,
      "step": 1212
    },
    {
      "epoch": 0.5511131303952749,
      "grad_norm": 7.617383003234863,
      "learning_rate": 0.0008226766366549672,
      "loss": 1.714,
      "step": 1213
    },
    {
      "epoch": 0.5515674693321218,
      "grad_norm": 9.26024055480957,
      "learning_rate": 0.0008225240347932244,
      "loss": 2.0166,
      "step": 1214
    },
    {
      "epoch": 0.5520218082689686,
      "grad_norm": 6.745128631591797,
      "learning_rate": 0.0008223714329314818,
      "loss": 1.2403,
      "step": 1215
    },
    {
      "epoch": 0.5524761472058155,
      "grad_norm": 4.949755668640137,
      "learning_rate": 0.0008222188310697391,
      "loss": 0.8338,
      "step": 1216
    },
    {
      "epoch": 0.5529304861426624,
      "grad_norm": 3.5189788341522217,
      "learning_rate": 0.0008220662292079963,
      "loss": 0.5103,
      "step": 1217
    },
    {
      "epoch": 0.5533848250795094,
      "grad_norm": 5.342153549194336,
      "learning_rate": 0.0008219136273462537,
      "loss": 0.9218,
      "step": 1218
    },
    {
      "epoch": 0.5538391640163562,
      "grad_norm": 13.7302827835083,
      "learning_rate": 0.0008217610254845109,
      "loss": 1.039,
      "step": 1219
    },
    {
      "epoch": 0.5542935029532031,
      "grad_norm": 4.3553996086120605,
      "learning_rate": 0.0008216084236227682,
      "loss": 0.5273,
      "step": 1220
    },
    {
      "epoch": 0.55474784189005,
      "grad_norm": 7.788926124572754,
      "learning_rate": 0.0008214558217610256,
      "loss": 1.5554,
      "step": 1221
    },
    {
      "epoch": 0.5552021808268969,
      "grad_norm": 5.65952730178833,
      "learning_rate": 0.0008213032198992828,
      "loss": 1.0313,
      "step": 1222
    },
    {
      "epoch": 0.5556565197637438,
      "grad_norm": 6.534519672393799,
      "learning_rate": 0.0008211506180375401,
      "loss": 2.0169,
      "step": 1223
    },
    {
      "epoch": 0.5561108587005906,
      "grad_norm": 4.024695873260498,
      "learning_rate": 0.0008209980161757974,
      "loss": 0.7382,
      "step": 1224
    },
    {
      "epoch": 0.5565651976374375,
      "grad_norm": 3.806488513946533,
      "learning_rate": 0.0008208454143140547,
      "loss": 0.8968,
      "step": 1225
    },
    {
      "epoch": 0.5570195365742844,
      "grad_norm": 6.14376974105835,
      "learning_rate": 0.000820692812452312,
      "loss": 0.9687,
      "step": 1226
    },
    {
      "epoch": 0.5574738755111313,
      "grad_norm": 7.916499614715576,
      "learning_rate": 0.0008205402105905693,
      "loss": 1.6188,
      "step": 1227
    },
    {
      "epoch": 0.5579282144479782,
      "grad_norm": 7.786999702453613,
      "learning_rate": 0.0008203876087288266,
      "loss": 0.6235,
      "step": 1228
    },
    {
      "epoch": 0.5583825533848251,
      "grad_norm": 5.394393444061279,
      "learning_rate": 0.0008202350068670837,
      "loss": 0.9468,
      "step": 1229
    },
    {
      "epoch": 0.558836892321672,
      "grad_norm": 3.7229905128479004,
      "learning_rate": 0.0008200824050053411,
      "loss": 0.7669,
      "step": 1230
    },
    {
      "epoch": 0.5592912312585189,
      "grad_norm": 7.466763973236084,
      "learning_rate": 0.0008199298031435983,
      "loss": 1.4316,
      "step": 1231
    },
    {
      "epoch": 0.5597455701953657,
      "grad_norm": 6.7799601554870605,
      "learning_rate": 0.0008197772012818556,
      "loss": 1.9075,
      "step": 1232
    },
    {
      "epoch": 0.5601999091322126,
      "grad_norm": 4.002566337585449,
      "learning_rate": 0.000819624599420113,
      "loss": 0.9072,
      "step": 1233
    },
    {
      "epoch": 0.5606542480690595,
      "grad_norm": 5.971920967102051,
      "learning_rate": 0.0008194719975583702,
      "loss": 0.5071,
      "step": 1234
    },
    {
      "epoch": 0.5611085870059064,
      "grad_norm": 6.923343181610107,
      "learning_rate": 0.0008193193956966275,
      "loss": 0.8392,
      "step": 1235
    },
    {
      "epoch": 0.5615629259427533,
      "grad_norm": 7.6955885887146,
      "learning_rate": 0.0008191667938348848,
      "loss": 1.3326,
      "step": 1236
    },
    {
      "epoch": 0.5620172648796001,
      "grad_norm": 4.405864715576172,
      "learning_rate": 0.0008190141919731421,
      "loss": 1.1797,
      "step": 1237
    },
    {
      "epoch": 0.5624716038164471,
      "grad_norm": 5.175873756408691,
      "learning_rate": 0.0008188615901113994,
      "loss": 0.875,
      "step": 1238
    },
    {
      "epoch": 0.562925942753294,
      "grad_norm": 6.434118747711182,
      "learning_rate": 0.0008187089882496567,
      "loss": 0.9034,
      "step": 1239
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 3.1608283519744873,
      "learning_rate": 0.000818556386387914,
      "loss": 0.4508,
      "step": 1240
    },
    {
      "epoch": 0.5638346206269877,
      "grad_norm": 6.271414756774902,
      "learning_rate": 0.0008184037845261712,
      "loss": 1.4124,
      "step": 1241
    },
    {
      "epoch": 0.5642889595638346,
      "grad_norm": 5.498674392700195,
      "learning_rate": 0.0008182511826644286,
      "loss": 1.3382,
      "step": 1242
    },
    {
      "epoch": 0.5647432985006815,
      "grad_norm": 6.5768351554870605,
      "learning_rate": 0.0008180985808026859,
      "loss": 1.266,
      "step": 1243
    },
    {
      "epoch": 0.5651976374375284,
      "grad_norm": 6.520320415496826,
      "learning_rate": 0.0008179459789409431,
      "loss": 0.9217,
      "step": 1244
    },
    {
      "epoch": 0.5656519763743753,
      "grad_norm": 4.976265907287598,
      "learning_rate": 0.0008177933770792005,
      "loss": 1.1161,
      "step": 1245
    },
    {
      "epoch": 0.5661063153112221,
      "grad_norm": 6.112398147583008,
      "learning_rate": 0.0008176407752174577,
      "loss": 1.4038,
      "step": 1246
    },
    {
      "epoch": 0.566560654248069,
      "grad_norm": 8.129387855529785,
      "learning_rate": 0.0008174881733557149,
      "loss": 1.5316,
      "step": 1247
    },
    {
      "epoch": 0.567014993184916,
      "grad_norm": 5.897031784057617,
      "learning_rate": 0.0008173355714939722,
      "loss": 1.1407,
      "step": 1248
    },
    {
      "epoch": 0.5674693321217629,
      "grad_norm": 5.369862079620361,
      "learning_rate": 0.0008171829696322295,
      "loss": 0.9827,
      "step": 1249
    },
    {
      "epoch": 0.5679236710586097,
      "grad_norm": 6.0362958908081055,
      "learning_rate": 0.0008170303677704868,
      "loss": 0.3878,
      "step": 1250
    },
    {
      "epoch": 0.5683780099954566,
      "grad_norm": 3.661496877670288,
      "learning_rate": 0.0008168777659087441,
      "loss": 0.9978,
      "step": 1251
    },
    {
      "epoch": 0.5688323489323035,
      "grad_norm": 5.139003276824951,
      "learning_rate": 0.0008167251640470014,
      "loss": 0.6848,
      "step": 1252
    },
    {
      "epoch": 0.5692866878691504,
      "grad_norm": 7.862900733947754,
      "learning_rate": 0.0008165725621852586,
      "loss": 1.0405,
      "step": 1253
    },
    {
      "epoch": 0.5697410268059973,
      "grad_norm": 16.890684127807617,
      "learning_rate": 0.000816419960323516,
      "loss": 1.089,
      "step": 1254
    },
    {
      "epoch": 0.5701953657428441,
      "grad_norm": 7.233978271484375,
      "learning_rate": 0.0008162673584617733,
      "loss": 1.4882,
      "step": 1255
    },
    {
      "epoch": 0.570649704679691,
      "grad_norm": 6.825947284698486,
      "learning_rate": 0.0008161147566000305,
      "loss": 1.0065,
      "step": 1256
    },
    {
      "epoch": 0.5711040436165379,
      "grad_norm": 6.668953895568848,
      "learning_rate": 0.0008159621547382879,
      "loss": 1.0159,
      "step": 1257
    },
    {
      "epoch": 0.5715583825533849,
      "grad_norm": 7.447165012359619,
      "learning_rate": 0.0008158095528765451,
      "loss": 1.0214,
      "step": 1258
    },
    {
      "epoch": 0.5720127214902317,
      "grad_norm": 7.2910990715026855,
      "learning_rate": 0.0008156569510148024,
      "loss": 0.8842,
      "step": 1259
    },
    {
      "epoch": 0.5724670604270786,
      "grad_norm": 5.445218086242676,
      "learning_rate": 0.0008155043491530598,
      "loss": 0.7926,
      "step": 1260
    },
    {
      "epoch": 0.5729213993639255,
      "grad_norm": 7.814641952514648,
      "learning_rate": 0.000815351747291317,
      "loss": 1.1274,
      "step": 1261
    },
    {
      "epoch": 0.5733757383007724,
      "grad_norm": 7.4753875732421875,
      "learning_rate": 0.0008151991454295743,
      "loss": 1.5286,
      "step": 1262
    },
    {
      "epoch": 0.5738300772376193,
      "grad_norm": 5.316451549530029,
      "learning_rate": 0.0008150465435678316,
      "loss": 1.1217,
      "step": 1263
    },
    {
      "epoch": 0.5742844161744661,
      "grad_norm": 5.877849578857422,
      "learning_rate": 0.0008148939417060889,
      "loss": 1.0975,
      "step": 1264
    },
    {
      "epoch": 0.574738755111313,
      "grad_norm": 7.696946144104004,
      "learning_rate": 0.000814741339844346,
      "loss": 1.7816,
      "step": 1265
    },
    {
      "epoch": 0.5751930940481599,
      "grad_norm": 2.7154650688171387,
      "learning_rate": 0.0008145887379826034,
      "loss": 0.3643,
      "step": 1266
    },
    {
      "epoch": 0.5756474329850069,
      "grad_norm": 5.1808977127075195,
      "learning_rate": 0.0008144361361208607,
      "loss": 1.1688,
      "step": 1267
    },
    {
      "epoch": 0.5761017719218537,
      "grad_norm": 5.9901957511901855,
      "learning_rate": 0.0008142835342591179,
      "loss": 1.2481,
      "step": 1268
    },
    {
      "epoch": 0.5765561108587006,
      "grad_norm": 5.425498008728027,
      "learning_rate": 0.0008141309323973753,
      "loss": 1.3147,
      "step": 1269
    },
    {
      "epoch": 0.5770104497955475,
      "grad_norm": 6.687105178833008,
      "learning_rate": 0.0008139783305356325,
      "loss": 1.4879,
      "step": 1270
    },
    {
      "epoch": 0.5774647887323944,
      "grad_norm": 4.831216335296631,
      "learning_rate": 0.0008138257286738898,
      "loss": 0.8243,
      "step": 1271
    },
    {
      "epoch": 0.5779191276692413,
      "grad_norm": 6.691722869873047,
      "learning_rate": 0.0008136731268121472,
      "loss": 1.2698,
      "step": 1272
    },
    {
      "epoch": 0.5783734666060881,
      "grad_norm": 135.3623809814453,
      "learning_rate": 0.0008135205249504044,
      "loss": 1.2937,
      "step": 1273
    },
    {
      "epoch": 0.578827805542935,
      "grad_norm": 7.011422634124756,
      "learning_rate": 0.0008133679230886617,
      "loss": 1.8289,
      "step": 1274
    },
    {
      "epoch": 0.5792821444797819,
      "grad_norm": 9.466841697692871,
      "learning_rate": 0.000813215321226919,
      "loss": 0.6793,
      "step": 1275
    },
    {
      "epoch": 0.5797364834166288,
      "grad_norm": 8.610464096069336,
      "learning_rate": 0.0008130627193651763,
      "loss": 1.5084,
      "step": 1276
    },
    {
      "epoch": 0.5801908223534757,
      "grad_norm": 5.0471625328063965,
      "learning_rate": 0.0008129101175034335,
      "loss": 0.5818,
      "step": 1277
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 4.7672271728515625,
      "learning_rate": 0.0008127575156416909,
      "loss": 0.9364,
      "step": 1278
    },
    {
      "epoch": 0.5810995002271695,
      "grad_norm": 3.6336934566497803,
      "learning_rate": 0.0008126049137799482,
      "loss": 0.556,
      "step": 1279
    },
    {
      "epoch": 0.5815538391640164,
      "grad_norm": 5.421381950378418,
      "learning_rate": 0.0008124523119182054,
      "loss": 1.2465,
      "step": 1280
    },
    {
      "epoch": 0.5820081781008632,
      "grad_norm": 7.01259183883667,
      "learning_rate": 0.0008122997100564628,
      "loss": 1.1732,
      "step": 1281
    },
    {
      "epoch": 0.5824625170377101,
      "grad_norm": 6.30565071105957,
      "learning_rate": 0.00081214710819472,
      "loss": 0.8785,
      "step": 1282
    },
    {
      "epoch": 0.582916855974557,
      "grad_norm": 6.582678318023682,
      "learning_rate": 0.0008119945063329773,
      "loss": 1.1427,
      "step": 1283
    },
    {
      "epoch": 0.5833711949114039,
      "grad_norm": 6.8486175537109375,
      "learning_rate": 0.0008118419044712346,
      "loss": 1.0795,
      "step": 1284
    },
    {
      "epoch": 0.5838255338482508,
      "grad_norm": 6.883326530456543,
      "learning_rate": 0.0008116893026094918,
      "loss": 1.7116,
      "step": 1285
    },
    {
      "epoch": 0.5842798727850976,
      "grad_norm": 7.0595622062683105,
      "learning_rate": 0.0008115367007477491,
      "loss": 1.7128,
      "step": 1286
    },
    {
      "epoch": 0.5847342117219446,
      "grad_norm": 6.186928749084473,
      "learning_rate": 0.0008113840988860064,
      "loss": 1.3797,
      "step": 1287
    },
    {
      "epoch": 0.5851885506587915,
      "grad_norm": 6.214039325714111,
      "learning_rate": 0.0008112314970242637,
      "loss": 1.3521,
      "step": 1288
    },
    {
      "epoch": 0.5856428895956384,
      "grad_norm": 5.564186096191406,
      "learning_rate": 0.000811078895162521,
      "loss": 0.8776,
      "step": 1289
    },
    {
      "epoch": 0.5860972285324852,
      "grad_norm": 7.293272495269775,
      "learning_rate": 0.0008109262933007783,
      "loss": 1.5496,
      "step": 1290
    },
    {
      "epoch": 0.5865515674693321,
      "grad_norm": 5.1530070304870605,
      "learning_rate": 0.0008107736914390356,
      "loss": 0.6217,
      "step": 1291
    },
    {
      "epoch": 0.587005906406179,
      "grad_norm": 6.410775184631348,
      "learning_rate": 0.0008106210895772928,
      "loss": 1.0615,
      "step": 1292
    },
    {
      "epoch": 0.5874602453430259,
      "grad_norm": 6.223714351654053,
      "learning_rate": 0.0008104684877155502,
      "loss": 1.1997,
      "step": 1293
    },
    {
      "epoch": 0.5879145842798728,
      "grad_norm": 6.520752906799316,
      "learning_rate": 0.0008103158858538074,
      "loss": 1.5266,
      "step": 1294
    },
    {
      "epoch": 0.5883689232167196,
      "grad_norm": 6.416818141937256,
      "learning_rate": 0.0008101632839920647,
      "loss": 1.4006,
      "step": 1295
    },
    {
      "epoch": 0.5888232621535665,
      "grad_norm": 3.686058282852173,
      "learning_rate": 0.0008100106821303221,
      "loss": 0.6443,
      "step": 1296
    },
    {
      "epoch": 0.5892776010904135,
      "grad_norm": 5.8341875076293945,
      "learning_rate": 0.0008098580802685793,
      "loss": 1.3254,
      "step": 1297
    },
    {
      "epoch": 0.5897319400272604,
      "grad_norm": 5.7881364822387695,
      "learning_rate": 0.0008097054784068366,
      "loss": 0.9228,
      "step": 1298
    },
    {
      "epoch": 0.5901862789641072,
      "grad_norm": 4.518800258636475,
      "learning_rate": 0.0008095528765450939,
      "loss": 0.5747,
      "step": 1299
    },
    {
      "epoch": 0.5906406179009541,
      "grad_norm": 6.2555012702941895,
      "learning_rate": 0.0008094002746833512,
      "loss": 1.6169,
      "step": 1300
    },
    {
      "epoch": 0.591094956837801,
      "grad_norm": 2.8178484439849854,
      "learning_rate": 0.0008092476728216085,
      "loss": 0.5721,
      "step": 1301
    },
    {
      "epoch": 0.5915492957746479,
      "grad_norm": 7.749699115753174,
      "learning_rate": 0.0008090950709598657,
      "loss": 2.427,
      "step": 1302
    },
    {
      "epoch": 0.5920036347114948,
      "grad_norm": 9.012787818908691,
      "learning_rate": 0.000808942469098123,
      "loss": 1.473,
      "step": 1303
    },
    {
      "epoch": 0.5924579736483416,
      "grad_norm": 5.610328674316406,
      "learning_rate": 0.0008087898672363802,
      "loss": 1.1548,
      "step": 1304
    },
    {
      "epoch": 0.5929123125851885,
      "grad_norm": 6.9034504890441895,
      "learning_rate": 0.0008086372653746376,
      "loss": 1.299,
      "step": 1305
    },
    {
      "epoch": 0.5933666515220355,
      "grad_norm": 5.955702781677246,
      "learning_rate": 0.0008084846635128948,
      "loss": 1.6329,
      "step": 1306
    },
    {
      "epoch": 0.5938209904588824,
      "grad_norm": 5.2477192878723145,
      "learning_rate": 0.0008083320616511521,
      "loss": 0.6427,
      "step": 1307
    },
    {
      "epoch": 0.5942753293957292,
      "grad_norm": 5.981815814971924,
      "learning_rate": 0.0008081794597894095,
      "loss": 1.4448,
      "step": 1308
    },
    {
      "epoch": 0.5947296683325761,
      "grad_norm": 4.528540134429932,
      "learning_rate": 0.0008080268579276667,
      "loss": 0.8158,
      "step": 1309
    },
    {
      "epoch": 0.595184007269423,
      "grad_norm": 5.4870076179504395,
      "learning_rate": 0.000807874256065924,
      "loss": 1.2084,
      "step": 1310
    },
    {
      "epoch": 0.5956383462062699,
      "grad_norm": 9.952052116394043,
      "learning_rate": 0.0008077216542041813,
      "loss": 1.7544,
      "step": 1311
    },
    {
      "epoch": 0.5960926851431168,
      "grad_norm": 7.670504570007324,
      "learning_rate": 0.0008075690523424386,
      "loss": 1.6137,
      "step": 1312
    },
    {
      "epoch": 0.5965470240799636,
      "grad_norm": 5.205831527709961,
      "learning_rate": 0.0008074164504806959,
      "loss": 1.0316,
      "step": 1313
    },
    {
      "epoch": 0.5970013630168105,
      "grad_norm": 8.450263977050781,
      "learning_rate": 0.0008072638486189532,
      "loss": 1.7017,
      "step": 1314
    },
    {
      "epoch": 0.5974557019536574,
      "grad_norm": 4.336009979248047,
      "learning_rate": 0.0008071112467572105,
      "loss": 0.8639,
      "step": 1315
    },
    {
      "epoch": 0.5979100408905044,
      "grad_norm": 7.516353607177734,
      "learning_rate": 0.0008069586448954677,
      "loss": 1.5313,
      "step": 1316
    },
    {
      "epoch": 0.5983643798273512,
      "grad_norm": 5.664214134216309,
      "learning_rate": 0.0008068060430337251,
      "loss": 1.4991,
      "step": 1317
    },
    {
      "epoch": 0.5988187187641981,
      "grad_norm": 7.5650811195373535,
      "learning_rate": 0.0008066534411719824,
      "loss": 0.856,
      "step": 1318
    },
    {
      "epoch": 0.599273057701045,
      "grad_norm": 4.182690143585205,
      "learning_rate": 0.0008065008393102396,
      "loss": 0.6543,
      "step": 1319
    },
    {
      "epoch": 0.5997273966378919,
      "grad_norm": 5.345774173736572,
      "learning_rate": 0.0008063482374484969,
      "loss": 1.8945,
      "step": 1320
    },
    {
      "epoch": 0.6001817355747388,
      "grad_norm": 4.078822612762451,
      "learning_rate": 0.0008061956355867541,
      "loss": 1.0504,
      "step": 1321
    },
    {
      "epoch": 0.6006360745115856,
      "grad_norm": 4.63715124130249,
      "learning_rate": 0.0008060430337250114,
      "loss": 1.111,
      "step": 1322
    },
    {
      "epoch": 0.6010904134484325,
      "grad_norm": 6.130924224853516,
      "learning_rate": 0.0008058904318632687,
      "loss": 1.5079,
      "step": 1323
    },
    {
      "epoch": 0.6015447523852794,
      "grad_norm": 6.413631916046143,
      "learning_rate": 0.000805737830001526,
      "loss": 2.005,
      "step": 1324
    },
    {
      "epoch": 0.6019990913221263,
      "grad_norm": 3.857496500015259,
      "learning_rate": 0.0008055852281397833,
      "loss": 0.6019,
      "step": 1325
    },
    {
      "epoch": 0.6024534302589732,
      "grad_norm": 6.786165714263916,
      "learning_rate": 0.0008054326262780406,
      "loss": 0.6747,
      "step": 1326
    },
    {
      "epoch": 0.6029077691958201,
      "grad_norm": 6.860152244567871,
      "learning_rate": 0.0008052800244162979,
      "loss": 1.1176,
      "step": 1327
    },
    {
      "epoch": 0.603362108132667,
      "grad_norm": 7.444455623626709,
      "learning_rate": 0.0008051274225545551,
      "loss": 1.9436,
      "step": 1328
    },
    {
      "epoch": 0.6038164470695139,
      "grad_norm": 7.655731678009033,
      "learning_rate": 0.0008049748206928125,
      "loss": 1.4623,
      "step": 1329
    },
    {
      "epoch": 0.6042707860063607,
      "grad_norm": 2.7612476348876953,
      "learning_rate": 0.0008048222188310698,
      "loss": 0.427,
      "step": 1330
    },
    {
      "epoch": 0.6047251249432076,
      "grad_norm": 6.44251012802124,
      "learning_rate": 0.000804669616969327,
      "loss": 1.3309,
      "step": 1331
    },
    {
      "epoch": 0.6051794638800545,
      "grad_norm": 5.426219940185547,
      "learning_rate": 0.0008045170151075844,
      "loss": 0.7048,
      "step": 1332
    },
    {
      "epoch": 0.6056338028169014,
      "grad_norm": 5.859844207763672,
      "learning_rate": 0.0008043644132458416,
      "loss": 1.1779,
      "step": 1333
    },
    {
      "epoch": 0.6060881417537483,
      "grad_norm": 4.717188835144043,
      "learning_rate": 0.0008042118113840989,
      "loss": 0.9108,
      "step": 1334
    },
    {
      "epoch": 0.6065424806905951,
      "grad_norm": 6.960063457489014,
      "learning_rate": 0.0008040592095223563,
      "loss": 0.4815,
      "step": 1335
    },
    {
      "epoch": 0.6069968196274421,
      "grad_norm": 5.074649333953857,
      "learning_rate": 0.0008039066076606135,
      "loss": 1.0845,
      "step": 1336
    },
    {
      "epoch": 0.607451158564289,
      "grad_norm": 8.223470687866211,
      "learning_rate": 0.0008037540057988708,
      "loss": 2.088,
      "step": 1337
    },
    {
      "epoch": 0.6079054975011359,
      "grad_norm": 5.972082138061523,
      "learning_rate": 0.000803601403937128,
      "loss": 1.0045,
      "step": 1338
    },
    {
      "epoch": 0.6083598364379827,
      "grad_norm": 5.990791320800781,
      "learning_rate": 0.0008034488020753853,
      "loss": 1.4569,
      "step": 1339
    },
    {
      "epoch": 0.6088141753748296,
      "grad_norm": 7.417426586151123,
      "learning_rate": 0.0008032962002136425,
      "loss": 0.794,
      "step": 1340
    },
    {
      "epoch": 0.6092685143116765,
      "grad_norm": 5.831076622009277,
      "learning_rate": 0.0008031435983518999,
      "loss": 1.2372,
      "step": 1341
    },
    {
      "epoch": 0.6097228532485234,
      "grad_norm": 5.57807731628418,
      "learning_rate": 0.0008029909964901572,
      "loss": 0.9567,
      "step": 1342
    },
    {
      "epoch": 0.6101771921853703,
      "grad_norm": 3.2722156047821045,
      "learning_rate": 0.0008028383946284144,
      "loss": 0.2963,
      "step": 1343
    },
    {
      "epoch": 0.6106315311222171,
      "grad_norm": 7.051586627960205,
      "learning_rate": 0.0008026857927666718,
      "loss": 1.602,
      "step": 1344
    },
    {
      "epoch": 0.611085870059064,
      "grad_norm": 6.264026641845703,
      "learning_rate": 0.000802533190904929,
      "loss": 1.048,
      "step": 1345
    },
    {
      "epoch": 0.611540208995911,
      "grad_norm": 5.663254737854004,
      "learning_rate": 0.0008023805890431863,
      "loss": 0.8167,
      "step": 1346
    },
    {
      "epoch": 0.6119945479327579,
      "grad_norm": 5.417767524719238,
      "learning_rate": 0.0008022279871814437,
      "loss": 1.258,
      "step": 1347
    },
    {
      "epoch": 0.6124488868696047,
      "grad_norm": 5.060525417327881,
      "learning_rate": 0.0008020753853197009,
      "loss": 0.7135,
      "step": 1348
    },
    {
      "epoch": 0.6129032258064516,
      "grad_norm": 9.488626480102539,
      "learning_rate": 0.0008019227834579582,
      "loss": 1.5387,
      "step": 1349
    },
    {
      "epoch": 0.6133575647432985,
      "grad_norm": 5.014365196228027,
      "learning_rate": 0.0008017701815962155,
      "loss": 0.8847,
      "step": 1350
    },
    {
      "epoch": 0.6138119036801454,
      "grad_norm": 5.703094005584717,
      "learning_rate": 0.0008016175797344728,
      "loss": 0.8571,
      "step": 1351
    },
    {
      "epoch": 0.6142662426169923,
      "grad_norm": 6.202407360076904,
      "learning_rate": 0.00080146497787273,
      "loss": 1.0533,
      "step": 1352
    },
    {
      "epoch": 0.6147205815538391,
      "grad_norm": 3.1154391765594482,
      "learning_rate": 0.0008013123760109874,
      "loss": 0.3302,
      "step": 1353
    },
    {
      "epoch": 0.615174920490686,
      "grad_norm": 9.458215713500977,
      "learning_rate": 0.0008011597741492447,
      "loss": 0.9946,
      "step": 1354
    },
    {
      "epoch": 0.615629259427533,
      "grad_norm": 5.511579513549805,
      "learning_rate": 0.0008010071722875019,
      "loss": 0.5526,
      "step": 1355
    },
    {
      "epoch": 0.6160835983643799,
      "grad_norm": 6.057048320770264,
      "learning_rate": 0.0008008545704257593,
      "loss": 0.4071,
      "step": 1356
    },
    {
      "epoch": 0.6165379373012267,
      "grad_norm": 4.5907769203186035,
      "learning_rate": 0.0008007019685640164,
      "loss": 1.2564,
      "step": 1357
    },
    {
      "epoch": 0.6169922762380736,
      "grad_norm": 5.483401775360107,
      "learning_rate": 0.0008005493667022737,
      "loss": 1.6652,
      "step": 1358
    },
    {
      "epoch": 0.6174466151749205,
      "grad_norm": 6.2452392578125,
      "learning_rate": 0.0008003967648405311,
      "loss": 0.9617,
      "step": 1359
    },
    {
      "epoch": 0.6179009541117674,
      "grad_norm": 4.959096431732178,
      "learning_rate": 0.0008002441629787883,
      "loss": 0.8546,
      "step": 1360
    },
    {
      "epoch": 0.6183552930486143,
      "grad_norm": 5.823307991027832,
      "learning_rate": 0.0008000915611170456,
      "loss": 1.0947,
      "step": 1361
    },
    {
      "epoch": 0.6188096319854611,
      "grad_norm": 6.209987163543701,
      "learning_rate": 0.0007999389592553029,
      "loss": 0.8287,
      "step": 1362
    },
    {
      "epoch": 0.619263970922308,
      "grad_norm": 5.136107444763184,
      "learning_rate": 0.0007997863573935602,
      "loss": 1.1176,
      "step": 1363
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 6.723048210144043,
      "learning_rate": 0.0007996337555318175,
      "loss": 1.6752,
      "step": 1364
    },
    {
      "epoch": 0.6201726487960019,
      "grad_norm": 5.681275844573975,
      "learning_rate": 0.0007994811536700748,
      "loss": 1.1357,
      "step": 1365
    },
    {
      "epoch": 0.6206269877328487,
      "grad_norm": 6.596471309661865,
      "learning_rate": 0.0007993285518083321,
      "loss": 1.057,
      "step": 1366
    },
    {
      "epoch": 0.6210813266696956,
      "grad_norm": 3.7734224796295166,
      "learning_rate": 0.0007991759499465893,
      "loss": 0.5056,
      "step": 1367
    },
    {
      "epoch": 0.6215356656065425,
      "grad_norm": 7.0453104972839355,
      "learning_rate": 0.0007990233480848467,
      "loss": 1.1784,
      "step": 1368
    },
    {
      "epoch": 0.6219900045433894,
      "grad_norm": 4.920443058013916,
      "learning_rate": 0.000798870746223104,
      "loss": 1.3006,
      "step": 1369
    },
    {
      "epoch": 0.6224443434802363,
      "grad_norm": 6.466322898864746,
      "learning_rate": 0.0007987181443613612,
      "loss": 1.2372,
      "step": 1370
    },
    {
      "epoch": 0.6228986824170831,
      "grad_norm": 7.534252643585205,
      "learning_rate": 0.0007985655424996186,
      "loss": 1.8052,
      "step": 1371
    },
    {
      "epoch": 0.62335302135393,
      "grad_norm": 7.078782081604004,
      "learning_rate": 0.0007984129406378758,
      "loss": 1.6658,
      "step": 1372
    },
    {
      "epoch": 0.6238073602907769,
      "grad_norm": 9.594943046569824,
      "learning_rate": 0.0007982603387761331,
      "loss": 1.2632,
      "step": 1373
    },
    {
      "epoch": 0.6242616992276238,
      "grad_norm": 7.196069717407227,
      "learning_rate": 0.0007981077369143905,
      "loss": 0.8855,
      "step": 1374
    },
    {
      "epoch": 0.6247160381644707,
      "grad_norm": 5.710907459259033,
      "learning_rate": 0.0007979551350526476,
      "loss": 0.594,
      "step": 1375
    },
    {
      "epoch": 0.6251703771013176,
      "grad_norm": 4.289903163909912,
      "learning_rate": 0.0007978025331909049,
      "loss": 1.0955,
      "step": 1376
    },
    {
      "epoch": 0.6256247160381645,
      "grad_norm": 5.825680732727051,
      "learning_rate": 0.0007976499313291622,
      "loss": 0.5891,
      "step": 1377
    },
    {
      "epoch": 0.6260790549750114,
      "grad_norm": 3.9199907779693604,
      "learning_rate": 0.0007974973294674195,
      "loss": 0.7235,
      "step": 1378
    },
    {
      "epoch": 0.6265333939118582,
      "grad_norm": 3.767669916152954,
      "learning_rate": 0.0007973447276056767,
      "loss": 0.3258,
      "step": 1379
    },
    {
      "epoch": 0.6269877328487051,
      "grad_norm": 4.285425186157227,
      "learning_rate": 0.0007971921257439341,
      "loss": 0.548,
      "step": 1380
    },
    {
      "epoch": 0.627442071785552,
      "grad_norm": 3.5331737995147705,
      "learning_rate": 0.0007970395238821914,
      "loss": 0.4491,
      "step": 1381
    },
    {
      "epoch": 0.6278964107223989,
      "grad_norm": 6.446519374847412,
      "learning_rate": 0.0007968869220204486,
      "loss": 1.3006,
      "step": 1382
    },
    {
      "epoch": 0.6283507496592458,
      "grad_norm": 8.172139167785645,
      "learning_rate": 0.000796734320158706,
      "loss": 1.771,
      "step": 1383
    },
    {
      "epoch": 0.6288050885960926,
      "grad_norm": 4.228353023529053,
      "learning_rate": 0.0007965817182969632,
      "loss": 0.6265,
      "step": 1384
    },
    {
      "epoch": 0.6292594275329396,
      "grad_norm": 7.055757999420166,
      "learning_rate": 0.0007964291164352205,
      "loss": 1.4252,
      "step": 1385
    },
    {
      "epoch": 0.6297137664697865,
      "grad_norm": 5.32549524307251,
      "learning_rate": 0.0007962765145734779,
      "loss": 1.22,
      "step": 1386
    },
    {
      "epoch": 0.6301681054066334,
      "grad_norm": 5.628271102905273,
      "learning_rate": 0.0007961239127117351,
      "loss": 1.0192,
      "step": 1387
    },
    {
      "epoch": 0.6306224443434802,
      "grad_norm": 5.718201637268066,
      "learning_rate": 0.0007959713108499924,
      "loss": 1.6616,
      "step": 1388
    },
    {
      "epoch": 0.6310767832803271,
      "grad_norm": 3.283595561981201,
      "learning_rate": 0.0007958187089882497,
      "loss": 0.3505,
      "step": 1389
    },
    {
      "epoch": 0.631531122217174,
      "grad_norm": 6.083111763000488,
      "learning_rate": 0.000795666107126507,
      "loss": 1.1207,
      "step": 1390
    },
    {
      "epoch": 0.6319854611540209,
      "grad_norm": 5.485536098480225,
      "learning_rate": 0.0007955135052647642,
      "loss": 1.0057,
      "step": 1391
    },
    {
      "epoch": 0.6324398000908678,
      "grad_norm": 4.74648380279541,
      "learning_rate": 0.0007953609034030216,
      "loss": 0.8247,
      "step": 1392
    },
    {
      "epoch": 0.6328941390277146,
      "grad_norm": 8.810240745544434,
      "learning_rate": 0.0007952083015412788,
      "loss": 1.7624,
      "step": 1393
    },
    {
      "epoch": 0.6333484779645616,
      "grad_norm": 6.062629699707031,
      "learning_rate": 0.000795055699679536,
      "loss": 2.0413,
      "step": 1394
    },
    {
      "epoch": 0.6338028169014085,
      "grad_norm": 5.3618855476379395,
      "learning_rate": 0.0007949030978177934,
      "loss": 1.0277,
      "step": 1395
    },
    {
      "epoch": 0.6342571558382554,
      "grad_norm": 5.8260393142700195,
      "learning_rate": 0.0007947504959560506,
      "loss": 1.1445,
      "step": 1396
    },
    {
      "epoch": 0.6347114947751022,
      "grad_norm": 6.546052932739258,
      "learning_rate": 0.0007945978940943079,
      "loss": 1.2671,
      "step": 1397
    },
    {
      "epoch": 0.6351658337119491,
      "grad_norm": 3.6654515266418457,
      "learning_rate": 0.0007944452922325653,
      "loss": 0.4309,
      "step": 1398
    },
    {
      "epoch": 0.635620172648796,
      "grad_norm": 9.943631172180176,
      "learning_rate": 0.0007942926903708225,
      "loss": 0.7706,
      "step": 1399
    },
    {
      "epoch": 0.6360745115856429,
      "grad_norm": 3.4915106296539307,
      "learning_rate": 0.0007941400885090798,
      "loss": 0.5856,
      "step": 1400
    },
    {
      "epoch": 0.6365288505224898,
      "grad_norm": 5.764359951019287,
      "learning_rate": 0.0007939874866473371,
      "loss": 1.2559,
      "step": 1401
    },
    {
      "epoch": 0.6369831894593366,
      "grad_norm": 5.755843639373779,
      "learning_rate": 0.0007938348847855944,
      "loss": 1.1239,
      "step": 1402
    },
    {
      "epoch": 0.6374375283961835,
      "grad_norm": 1.3969237804412842,
      "learning_rate": 0.0007936822829238516,
      "loss": 0.0715,
      "step": 1403
    },
    {
      "epoch": 0.6378918673330305,
      "grad_norm": 8.127410888671875,
      "learning_rate": 0.000793529681062109,
      "loss": 0.6025,
      "step": 1404
    },
    {
      "epoch": 0.6383462062698774,
      "grad_norm": 5.827754020690918,
      "learning_rate": 0.0007933770792003663,
      "loss": 1.6998,
      "step": 1405
    },
    {
      "epoch": 0.6388005452067242,
      "grad_norm": 4.133856773376465,
      "learning_rate": 0.0007932244773386235,
      "loss": 0.5807,
      "step": 1406
    },
    {
      "epoch": 0.6392548841435711,
      "grad_norm": 4.753881931304932,
      "learning_rate": 0.0007930718754768809,
      "loss": 0.8609,
      "step": 1407
    },
    {
      "epoch": 0.639709223080418,
      "grad_norm": 4.866656303405762,
      "learning_rate": 0.0007929192736151381,
      "loss": 0.6177,
      "step": 1408
    },
    {
      "epoch": 0.6401635620172649,
      "grad_norm": 8.758550643920898,
      "learning_rate": 0.0007927666717533954,
      "loss": 1.5523,
      "step": 1409
    },
    {
      "epoch": 0.6406179009541118,
      "grad_norm": 2.951582193374634,
      "learning_rate": 0.0007926140698916528,
      "loss": 0.4994,
      "step": 1410
    },
    {
      "epoch": 0.6410722398909586,
      "grad_norm": 6.293062686920166,
      "learning_rate": 0.0007924614680299099,
      "loss": 1.7727,
      "step": 1411
    },
    {
      "epoch": 0.6415265788278055,
      "grad_norm": 4.464162826538086,
      "learning_rate": 0.0007923088661681672,
      "loss": 1.2461,
      "step": 1412
    },
    {
      "epoch": 0.6419809177646524,
      "grad_norm": 5.647894382476807,
      "learning_rate": 0.0007921562643064245,
      "loss": 1.7729,
      "step": 1413
    },
    {
      "epoch": 0.6424352567014994,
      "grad_norm": 3.2501158714294434,
      "learning_rate": 0.0007920036624446818,
      "loss": 0.8582,
      "step": 1414
    },
    {
      "epoch": 0.6428895956383462,
      "grad_norm": 9.438764572143555,
      "learning_rate": 0.000791851060582939,
      "loss": 1.0518,
      "step": 1415
    },
    {
      "epoch": 0.6433439345751931,
      "grad_norm": 4.167375087738037,
      "learning_rate": 0.0007916984587211964,
      "loss": 0.5139,
      "step": 1416
    },
    {
      "epoch": 0.64379827351204,
      "grad_norm": 4.920766830444336,
      "learning_rate": 0.0007915458568594537,
      "loss": 0.6679,
      "step": 1417
    },
    {
      "epoch": 0.6442526124488869,
      "grad_norm": 5.576142311096191,
      "learning_rate": 0.0007913932549977109,
      "loss": 1.1112,
      "step": 1418
    },
    {
      "epoch": 0.6447069513857338,
      "grad_norm": 5.950145244598389,
      "learning_rate": 0.0007912406531359683,
      "loss": 0.5672,
      "step": 1419
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 7.0068678855896,
      "learning_rate": 0.0007910880512742255,
      "loss": 0.9249,
      "step": 1420
    },
    {
      "epoch": 0.6456156292594275,
      "grad_norm": 6.743326187133789,
      "learning_rate": 0.0007909354494124828,
      "loss": 0.9449,
      "step": 1421
    },
    {
      "epoch": 0.6460699681962744,
      "grad_norm": 6.350657939910889,
      "learning_rate": 0.0007907828475507402,
      "loss": 1.7957,
      "step": 1422
    },
    {
      "epoch": 0.6465243071331213,
      "grad_norm": 9.829829216003418,
      "learning_rate": 0.0007906302456889974,
      "loss": 1.4843,
      "step": 1423
    },
    {
      "epoch": 0.6469786460699682,
      "grad_norm": 5.616198539733887,
      "learning_rate": 0.0007904776438272547,
      "loss": 1.296,
      "step": 1424
    },
    {
      "epoch": 0.6474329850068151,
      "grad_norm": 5.9122090339660645,
      "learning_rate": 0.000790325041965512,
      "loss": 1.0046,
      "step": 1425
    },
    {
      "epoch": 0.647887323943662,
      "grad_norm": 6.836793422698975,
      "learning_rate": 0.0007901724401037693,
      "loss": 1.4868,
      "step": 1426
    },
    {
      "epoch": 0.6483416628805089,
      "grad_norm": 5.500484943389893,
      "learning_rate": 0.0007900198382420266,
      "loss": 1.3434,
      "step": 1427
    },
    {
      "epoch": 0.6487960018173557,
      "grad_norm": 5.411383152008057,
      "learning_rate": 0.0007898672363802839,
      "loss": 1.1458,
      "step": 1428
    },
    {
      "epoch": 0.6492503407542026,
      "grad_norm": 7.5787835121154785,
      "learning_rate": 0.0007897146345185412,
      "loss": 2.1217,
      "step": 1429
    },
    {
      "epoch": 0.6497046796910495,
      "grad_norm": 4.120079517364502,
      "learning_rate": 0.0007895620326567983,
      "loss": 0.3922,
      "step": 1430
    },
    {
      "epoch": 0.6501590186278964,
      "grad_norm": 6.109663963317871,
      "learning_rate": 0.0007894094307950557,
      "loss": 1.2149,
      "step": 1431
    },
    {
      "epoch": 0.6506133575647433,
      "grad_norm": 4.440192699432373,
      "learning_rate": 0.000789256828933313,
      "loss": 0.5231,
      "step": 1432
    },
    {
      "epoch": 0.6510676965015901,
      "grad_norm": 8.766824722290039,
      "learning_rate": 0.0007891042270715702,
      "loss": 1.4329,
      "step": 1433
    },
    {
      "epoch": 0.6515220354384371,
      "grad_norm": 5.94691801071167,
      "learning_rate": 0.0007889516252098276,
      "loss": 1.1446,
      "step": 1434
    },
    {
      "epoch": 0.651976374375284,
      "grad_norm": 4.759953498840332,
      "learning_rate": 0.0007887990233480848,
      "loss": 1.2445,
      "step": 1435
    },
    {
      "epoch": 0.6524307133121309,
      "grad_norm": 6.440614223480225,
      "learning_rate": 0.0007886464214863421,
      "loss": 1.9847,
      "step": 1436
    },
    {
      "epoch": 0.6528850522489777,
      "grad_norm": 5.059123992919922,
      "learning_rate": 0.0007884938196245994,
      "loss": 0.738,
      "step": 1437
    },
    {
      "epoch": 0.6533393911858246,
      "grad_norm": 10.837761878967285,
      "learning_rate": 0.0007883412177628567,
      "loss": 2.0845,
      "step": 1438
    },
    {
      "epoch": 0.6537937301226715,
      "grad_norm": 7.2627644538879395,
      "learning_rate": 0.0007881886159011141,
      "loss": 1.2267,
      "step": 1439
    },
    {
      "epoch": 0.6542480690595184,
      "grad_norm": 4.1303019523620605,
      "learning_rate": 0.0007880360140393713,
      "loss": 0.9608,
      "step": 1440
    },
    {
      "epoch": 0.6547024079963653,
      "grad_norm": 3.623751163482666,
      "learning_rate": 0.0007878834121776286,
      "loss": 0.4298,
      "step": 1441
    },
    {
      "epoch": 0.6551567469332121,
      "grad_norm": 9.190818786621094,
      "learning_rate": 0.000787730810315886,
      "loss": 1.2648,
      "step": 1442
    },
    {
      "epoch": 0.6556110858700591,
      "grad_norm": 5.1518778800964355,
      "learning_rate": 0.0007875782084541432,
      "loss": 1.3192,
      "step": 1443
    },
    {
      "epoch": 0.656065424806906,
      "grad_norm": 3.5116569995880127,
      "learning_rate": 0.0007874256065924005,
      "loss": 0.5517,
      "step": 1444
    },
    {
      "epoch": 0.6565197637437529,
      "grad_norm": 9.495993614196777,
      "learning_rate": 0.0007872730047306578,
      "loss": 1.6873,
      "step": 1445
    },
    {
      "epoch": 0.6569741026805997,
      "grad_norm": 3.444287061691284,
      "learning_rate": 0.0007871204028689151,
      "loss": 0.4371,
      "step": 1446
    },
    {
      "epoch": 0.6574284416174466,
      "grad_norm": 4.256266117095947,
      "learning_rate": 0.0007869678010071723,
      "loss": 0.7089,
      "step": 1447
    },
    {
      "epoch": 0.6578827805542935,
      "grad_norm": 6.797482967376709,
      "learning_rate": 0.0007868151991454296,
      "loss": 1.4317,
      "step": 1448
    },
    {
      "epoch": 0.6583371194911404,
      "grad_norm": 5.82199764251709,
      "learning_rate": 0.0007866625972836868,
      "loss": 1.6526,
      "step": 1449
    },
    {
      "epoch": 0.6587914584279873,
      "grad_norm": 5.155043601989746,
      "learning_rate": 0.0007865099954219441,
      "loss": 0.6916,
      "step": 1450
    },
    {
      "epoch": 0.6592457973648341,
      "grad_norm": 5.637386322021484,
      "learning_rate": 0.0007863573935602015,
      "loss": 0.7454,
      "step": 1451
    },
    {
      "epoch": 0.659700136301681,
      "grad_norm": 5.094969272613525,
      "learning_rate": 0.0007862047916984587,
      "loss": 0.5969,
      "step": 1452
    },
    {
      "epoch": 0.660154475238528,
      "grad_norm": 7.282687187194824,
      "learning_rate": 0.000786052189836716,
      "loss": 1.0657,
      "step": 1453
    },
    {
      "epoch": 0.6606088141753749,
      "grad_norm": 5.214996814727783,
      "learning_rate": 0.0007858995879749733,
      "loss": 0.9292,
      "step": 1454
    },
    {
      "epoch": 0.6610631531122217,
      "grad_norm": 5.580793380737305,
      "learning_rate": 0.0007857469861132306,
      "loss": 0.5234,
      "step": 1455
    },
    {
      "epoch": 0.6615174920490686,
      "grad_norm": 6.12857723236084,
      "learning_rate": 0.0007855943842514879,
      "loss": 1.0119,
      "step": 1456
    },
    {
      "epoch": 0.6619718309859155,
      "grad_norm": 6.981881618499756,
      "learning_rate": 0.0007854417823897452,
      "loss": 0.9887,
      "step": 1457
    },
    {
      "epoch": 0.6624261699227624,
      "grad_norm": 9.628764152526855,
      "learning_rate": 0.0007852891805280025,
      "loss": 1.2945,
      "step": 1458
    },
    {
      "epoch": 0.6628805088596093,
      "grad_norm": 8.11777114868164,
      "learning_rate": 0.0007851365786662597,
      "loss": 1.4057,
      "step": 1459
    },
    {
      "epoch": 0.6633348477964561,
      "grad_norm": 7.322922229766846,
      "learning_rate": 0.0007849839768045171,
      "loss": 1.0113,
      "step": 1460
    },
    {
      "epoch": 0.663789186733303,
      "grad_norm": 5.016434192657471,
      "learning_rate": 0.0007848313749427744,
      "loss": 1.0668,
      "step": 1461
    },
    {
      "epoch": 0.6642435256701499,
      "grad_norm": 3.883370876312256,
      "learning_rate": 0.0007846787730810316,
      "loss": 0.4982,
      "step": 1462
    },
    {
      "epoch": 0.6646978646069969,
      "grad_norm": 5.175708770751953,
      "learning_rate": 0.000784526171219289,
      "loss": 0.8169,
      "step": 1463
    },
    {
      "epoch": 0.6651522035438437,
      "grad_norm": 7.206801414489746,
      "learning_rate": 0.0007843735693575462,
      "loss": 1.5054,
      "step": 1464
    },
    {
      "epoch": 0.6656065424806906,
      "grad_norm": 4.503102779388428,
      "learning_rate": 0.0007842209674958035,
      "loss": 0.4972,
      "step": 1465
    },
    {
      "epoch": 0.6660608814175375,
      "grad_norm": 5.49851131439209,
      "learning_rate": 0.0007840683656340607,
      "loss": 1.5189,
      "step": 1466
    },
    {
      "epoch": 0.6665152203543844,
      "grad_norm": 5.620057582855225,
      "learning_rate": 0.000783915763772318,
      "loss": 0.9887,
      "step": 1467
    },
    {
      "epoch": 0.6669695592912313,
      "grad_norm": 4.410693645477295,
      "learning_rate": 0.0007837631619105753,
      "loss": 0.5424,
      "step": 1468
    },
    {
      "epoch": 0.6674238982280781,
      "grad_norm": 4.28333044052124,
      "learning_rate": 0.0007836105600488326,
      "loss": 0.7419,
      "step": 1469
    },
    {
      "epoch": 0.667878237164925,
      "grad_norm": 9.179056167602539,
      "learning_rate": 0.0007834579581870899,
      "loss": 0.8312,
      "step": 1470
    },
    {
      "epoch": 0.6683325761017719,
      "grad_norm": 6.452798843383789,
      "learning_rate": 0.0007833053563253471,
      "loss": 0.9159,
      "step": 1471
    },
    {
      "epoch": 0.6687869150386188,
      "grad_norm": 5.789968490600586,
      "learning_rate": 0.0007831527544636045,
      "loss": 0.7285,
      "step": 1472
    },
    {
      "epoch": 0.6692412539754657,
      "grad_norm": 7.148740291595459,
      "learning_rate": 0.0007830001526018618,
      "loss": 1.1279,
      "step": 1473
    },
    {
      "epoch": 0.6696955929123126,
      "grad_norm": 6.354269027709961,
      "learning_rate": 0.000782847550740119,
      "loss": 1.5954,
      "step": 1474
    },
    {
      "epoch": 0.6701499318491595,
      "grad_norm": 7.1886186599731445,
      "learning_rate": 0.0007826949488783764,
      "loss": 0.6978,
      "step": 1475
    },
    {
      "epoch": 0.6706042707860064,
      "grad_norm": 9.729846000671387,
      "learning_rate": 0.0007825423470166336,
      "loss": 1.1452,
      "step": 1476
    },
    {
      "epoch": 0.6710586097228532,
      "grad_norm": 6.579062461853027,
      "learning_rate": 0.0007823897451548909,
      "loss": 0.9163,
      "step": 1477
    },
    {
      "epoch": 0.6715129486597001,
      "grad_norm": 6.972972869873047,
      "learning_rate": 0.0007822371432931483,
      "loss": 1.5189,
      "step": 1478
    },
    {
      "epoch": 0.671967287596547,
      "grad_norm": 8.208090782165527,
      "learning_rate": 0.0007820845414314055,
      "loss": 1.654,
      "step": 1479
    },
    {
      "epoch": 0.6724216265333939,
      "grad_norm": 10.716583251953125,
      "learning_rate": 0.0007819319395696628,
      "loss": 0.701,
      "step": 1480
    },
    {
      "epoch": 0.6728759654702408,
      "grad_norm": 3.6777091026306152,
      "learning_rate": 0.0007817793377079201,
      "loss": 0.7026,
      "step": 1481
    },
    {
      "epoch": 0.6733303044070876,
      "grad_norm": 5.356451034545898,
      "learning_rate": 0.0007816267358461774,
      "loss": 0.9798,
      "step": 1482
    },
    {
      "epoch": 0.6737846433439346,
      "grad_norm": 6.674968719482422,
      "learning_rate": 0.0007814741339844347,
      "loss": 0.9207,
      "step": 1483
    },
    {
      "epoch": 0.6742389822807815,
      "grad_norm": 6.04261589050293,
      "learning_rate": 0.0007813215321226919,
      "loss": 1.1735,
      "step": 1484
    },
    {
      "epoch": 0.6746933212176284,
      "grad_norm": 9.377655029296875,
      "learning_rate": 0.0007811689302609492,
      "loss": 1.288,
      "step": 1485
    },
    {
      "epoch": 0.6751476601544752,
      "grad_norm": 7.524163722991943,
      "learning_rate": 0.0007810163283992064,
      "loss": 2.0197,
      "step": 1486
    },
    {
      "epoch": 0.6756019990913221,
      "grad_norm": 5.156660556793213,
      "learning_rate": 0.0007808637265374638,
      "loss": 0.7673,
      "step": 1487
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 6.030290603637695,
      "learning_rate": 0.000780711124675721,
      "loss": 0.8745,
      "step": 1488
    },
    {
      "epoch": 0.6765106769650159,
      "grad_norm": 6.245311737060547,
      "learning_rate": 0.0007805585228139783,
      "loss": 1.3397,
      "step": 1489
    },
    {
      "epoch": 0.6769650159018628,
      "grad_norm": 4.7669572830200195,
      "learning_rate": 0.0007804059209522357,
      "loss": 0.4862,
      "step": 1490
    },
    {
      "epoch": 0.6774193548387096,
      "grad_norm": 6.157237529754639,
      "learning_rate": 0.0007802533190904929,
      "loss": 1.0786,
      "step": 1491
    },
    {
      "epoch": 0.6778736937755566,
      "grad_norm": 7.469592094421387,
      "learning_rate": 0.0007801007172287502,
      "loss": 1.4169,
      "step": 1492
    },
    {
      "epoch": 0.6783280327124035,
      "grad_norm": 6.10805082321167,
      "learning_rate": 0.0007799481153670075,
      "loss": 0.9854,
      "step": 1493
    },
    {
      "epoch": 0.6787823716492504,
      "grad_norm": 7.521082878112793,
      "learning_rate": 0.0007797955135052648,
      "loss": 1.8691,
      "step": 1494
    },
    {
      "epoch": 0.6792367105860972,
      "grad_norm": 8.136005401611328,
      "learning_rate": 0.000779642911643522,
      "loss": 1.7169,
      "step": 1495
    },
    {
      "epoch": 0.6796910495229441,
      "grad_norm": 6.511387348175049,
      "learning_rate": 0.0007794903097817794,
      "loss": 0.6014,
      "step": 1496
    },
    {
      "epoch": 0.680145388459791,
      "grad_norm": 9.368496894836426,
      "learning_rate": 0.0007793377079200367,
      "loss": 0.8594,
      "step": 1497
    },
    {
      "epoch": 0.6805997273966379,
      "grad_norm": 4.296799182891846,
      "learning_rate": 0.0007791851060582939,
      "loss": 0.5524,
      "step": 1498
    },
    {
      "epoch": 0.6810540663334848,
      "grad_norm": 14.663606643676758,
      "learning_rate": 0.0007790325041965513,
      "loss": 1.2876,
      "step": 1499
    },
    {
      "epoch": 0.6815084052703316,
      "grad_norm": 6.059589862823486,
      "learning_rate": 0.0007788799023348086,
      "loss": 1.0711,
      "step": 1500
    },
    {
      "epoch": 0.6819627442071785,
      "grad_norm": 9.492984771728516,
      "learning_rate": 0.0007787273004730658,
      "loss": 0.848,
      "step": 1501
    },
    {
      "epoch": 0.6824170831440255,
      "grad_norm": 7.135290145874023,
      "learning_rate": 0.0007785746986113232,
      "loss": 1.5994,
      "step": 1502
    },
    {
      "epoch": 0.6828714220808724,
      "grad_norm": 6.599969387054443,
      "learning_rate": 0.0007784220967495803,
      "loss": 0.8878,
      "step": 1503
    },
    {
      "epoch": 0.6833257610177192,
      "grad_norm": 4.767058372497559,
      "learning_rate": 0.0007782694948878376,
      "loss": 0.7764,
      "step": 1504
    },
    {
      "epoch": 0.6837800999545661,
      "grad_norm": 9.062597274780273,
      "learning_rate": 0.0007781168930260949,
      "loss": 2.1517,
      "step": 1505
    },
    {
      "epoch": 0.684234438891413,
      "grad_norm": 5.226751327514648,
      "learning_rate": 0.0007779642911643522,
      "loss": 0.7559,
      "step": 1506
    },
    {
      "epoch": 0.6846887778282599,
      "grad_norm": 9.25583553314209,
      "learning_rate": 0.0007778116893026095,
      "loss": 1.0533,
      "step": 1507
    },
    {
      "epoch": 0.6851431167651068,
      "grad_norm": 3.965822458267212,
      "learning_rate": 0.0007776590874408668,
      "loss": 0.7256,
      "step": 1508
    },
    {
      "epoch": 0.6855974557019536,
      "grad_norm": 5.797781467437744,
      "learning_rate": 0.0007775064855791241,
      "loss": 1.2128,
      "step": 1509
    },
    {
      "epoch": 0.6860517946388005,
      "grad_norm": 4.342081546783447,
      "learning_rate": 0.0007773538837173813,
      "loss": 0.7311,
      "step": 1510
    },
    {
      "epoch": 0.6865061335756474,
      "grad_norm": 4.3423848152160645,
      "learning_rate": 0.0007772012818556387,
      "loss": 0.7665,
      "step": 1511
    },
    {
      "epoch": 0.6869604725124944,
      "grad_norm": 6.515761852264404,
      "learning_rate": 0.000777048679993896,
      "loss": 1.1413,
      "step": 1512
    },
    {
      "epoch": 0.6874148114493412,
      "grad_norm": 5.433284759521484,
      "learning_rate": 0.0007768960781321532,
      "loss": 0.7827,
      "step": 1513
    },
    {
      "epoch": 0.6878691503861881,
      "grad_norm": 7.485538482666016,
      "learning_rate": 0.0007767434762704106,
      "loss": 1.1826,
      "step": 1514
    },
    {
      "epoch": 0.688323489323035,
      "grad_norm": 5.362311363220215,
      "learning_rate": 0.0007765908744086678,
      "loss": 1.4854,
      "step": 1515
    },
    {
      "epoch": 0.6887778282598819,
      "grad_norm": 8.92164134979248,
      "learning_rate": 0.0007764382725469251,
      "loss": 1.5214,
      "step": 1516
    },
    {
      "epoch": 0.6892321671967288,
      "grad_norm": 6.739617824554443,
      "learning_rate": 0.0007762856706851825,
      "loss": 0.9598,
      "step": 1517
    },
    {
      "epoch": 0.6896865061335756,
      "grad_norm": 4.788415431976318,
      "learning_rate": 0.0007761330688234397,
      "loss": 0.9576,
      "step": 1518
    },
    {
      "epoch": 0.6901408450704225,
      "grad_norm": 6.1607985496521,
      "learning_rate": 0.000775980466961697,
      "loss": 1.6307,
      "step": 1519
    },
    {
      "epoch": 0.6905951840072694,
      "grad_norm": 7.770327568054199,
      "learning_rate": 0.0007758278650999543,
      "loss": 2.059,
      "step": 1520
    },
    {
      "epoch": 0.6910495229441163,
      "grad_norm": 4.683786392211914,
      "learning_rate": 0.0007756752632382115,
      "loss": 1.0407,
      "step": 1521
    },
    {
      "epoch": 0.6915038618809632,
      "grad_norm": 4.092618942260742,
      "learning_rate": 0.0007755226613764687,
      "loss": 0.7087,
      "step": 1522
    },
    {
      "epoch": 0.6919582008178101,
      "grad_norm": 5.251509666442871,
      "learning_rate": 0.0007753700595147261,
      "loss": 1.6426,
      "step": 1523
    },
    {
      "epoch": 0.692412539754657,
      "grad_norm": 5.172855377197266,
      "learning_rate": 0.0007752174576529834,
      "loss": 1.5765,
      "step": 1524
    },
    {
      "epoch": 0.6928668786915039,
      "grad_norm": 7.6597490310668945,
      "learning_rate": 0.0007750648557912406,
      "loss": 1.4523,
      "step": 1525
    },
    {
      "epoch": 0.6933212176283507,
      "grad_norm": 4.652515888214111,
      "learning_rate": 0.000774912253929498,
      "loss": 1.4269,
      "step": 1526
    },
    {
      "epoch": 0.6937755565651976,
      "grad_norm": 7.290930271148682,
      "learning_rate": 0.0007747596520677552,
      "loss": 1.113,
      "step": 1527
    },
    {
      "epoch": 0.6942298955020445,
      "grad_norm": 5.931819915771484,
      "learning_rate": 0.0007746070502060125,
      "loss": 0.9715,
      "step": 1528
    },
    {
      "epoch": 0.6946842344388914,
      "grad_norm": 6.531838893890381,
      "learning_rate": 0.0007744544483442699,
      "loss": 0.9047,
      "step": 1529
    },
    {
      "epoch": 0.6951385733757383,
      "grad_norm": 6.730010986328125,
      "learning_rate": 0.0007743018464825271,
      "loss": 1.1914,
      "step": 1530
    },
    {
      "epoch": 0.6955929123125852,
      "grad_norm": 7.018351078033447,
      "learning_rate": 0.0007741492446207844,
      "loss": 0.9429,
      "step": 1531
    },
    {
      "epoch": 0.6960472512494321,
      "grad_norm": 7.287688732147217,
      "learning_rate": 0.0007739966427590417,
      "loss": 0.7757,
      "step": 1532
    },
    {
      "epoch": 0.696501590186279,
      "grad_norm": 9.200973510742188,
      "learning_rate": 0.000773844040897299,
      "loss": 1.7328,
      "step": 1533
    },
    {
      "epoch": 0.6969559291231259,
      "grad_norm": 4.618945121765137,
      "learning_rate": 0.0007736914390355562,
      "loss": 0.7445,
      "step": 1534
    },
    {
      "epoch": 0.6974102680599727,
      "grad_norm": 5.345422267913818,
      "learning_rate": 0.0007735388371738136,
      "loss": 1.2869,
      "step": 1535
    },
    {
      "epoch": 0.6978646069968196,
      "grad_norm": 4.401554107666016,
      "learning_rate": 0.0007733862353120709,
      "loss": 1.3004,
      "step": 1536
    },
    {
      "epoch": 0.6983189459336665,
      "grad_norm": 7.883388519287109,
      "learning_rate": 0.0007732336334503281,
      "loss": 1.2593,
      "step": 1537
    },
    {
      "epoch": 0.6987732848705134,
      "grad_norm": 3.0849945545196533,
      "learning_rate": 0.0007730810315885855,
      "loss": 0.4453,
      "step": 1538
    },
    {
      "epoch": 0.6992276238073603,
      "grad_norm": 4.450264930725098,
      "learning_rate": 0.0007729284297268426,
      "loss": 0.8562,
      "step": 1539
    },
    {
      "epoch": 0.6996819627442071,
      "grad_norm": 3.762633800506592,
      "learning_rate": 0.0007727758278650999,
      "loss": 0.5024,
      "step": 1540
    },
    {
      "epoch": 0.7001363016810541,
      "grad_norm": 6.823173522949219,
      "learning_rate": 0.0007726232260033573,
      "loss": 1.8143,
      "step": 1541
    },
    {
      "epoch": 0.700590640617901,
      "grad_norm": 2.8456997871398926,
      "learning_rate": 0.0007724706241416145,
      "loss": 0.4872,
      "step": 1542
    },
    {
      "epoch": 0.7010449795547479,
      "grad_norm": 6.853029251098633,
      "learning_rate": 0.0007723180222798718,
      "loss": 1.0588,
      "step": 1543
    },
    {
      "epoch": 0.7014993184915947,
      "grad_norm": 4.821033477783203,
      "learning_rate": 0.0007721654204181291,
      "loss": 1.1164,
      "step": 1544
    },
    {
      "epoch": 0.7019536574284416,
      "grad_norm": 8.11875057220459,
      "learning_rate": 0.0007720128185563864,
      "loss": 1.3042,
      "step": 1545
    },
    {
      "epoch": 0.7024079963652885,
      "grad_norm": 6.086859226226807,
      "learning_rate": 0.0007718602166946436,
      "loss": 1.0809,
      "step": 1546
    },
    {
      "epoch": 0.7028623353021354,
      "grad_norm": 5.140821933746338,
      "learning_rate": 0.000771707614832901,
      "loss": 0.7947,
      "step": 1547
    },
    {
      "epoch": 0.7033166742389823,
      "grad_norm": 9.196961402893066,
      "learning_rate": 0.0007715550129711583,
      "loss": 2.5656,
      "step": 1548
    },
    {
      "epoch": 0.7037710131758291,
      "grad_norm": 6.393149375915527,
      "learning_rate": 0.0007714024111094155,
      "loss": 1.0148,
      "step": 1549
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 7.553123474121094,
      "learning_rate": 0.0007712498092476729,
      "loss": 1.7205,
      "step": 1550
    },
    {
      "epoch": 0.704679691049523,
      "grad_norm": 7.0878190994262695,
      "learning_rate": 0.0007710972073859301,
      "loss": 1.2792,
      "step": 1551
    },
    {
      "epoch": 0.7051340299863699,
      "grad_norm": 8.402579307556152,
      "learning_rate": 0.0007709446055241874,
      "loss": 1.5116,
      "step": 1552
    },
    {
      "epoch": 0.7055883689232167,
      "grad_norm": 4.504055023193359,
      "learning_rate": 0.0007707920036624448,
      "loss": 0.5619,
      "step": 1553
    },
    {
      "epoch": 0.7060427078600636,
      "grad_norm": 4.677282810211182,
      "learning_rate": 0.000770639401800702,
      "loss": 0.6349,
      "step": 1554
    },
    {
      "epoch": 0.7064970467969105,
      "grad_norm": 8.145313262939453,
      "learning_rate": 0.0007704867999389593,
      "loss": 0.5449,
      "step": 1555
    },
    {
      "epoch": 0.7069513857337574,
      "grad_norm": 4.115911483764648,
      "learning_rate": 0.0007703341980772166,
      "loss": 0.8061,
      "step": 1556
    },
    {
      "epoch": 0.7074057246706043,
      "grad_norm": 8.55687141418457,
      "learning_rate": 0.0007701815962154738,
      "loss": 1.7611,
      "step": 1557
    },
    {
      "epoch": 0.7078600636074511,
      "grad_norm": 6.850009918212891,
      "learning_rate": 0.000770028994353731,
      "loss": 1.5924,
      "step": 1558
    },
    {
      "epoch": 0.708314402544298,
      "grad_norm": 6.343586444854736,
      "learning_rate": 0.0007698763924919884,
      "loss": 0.8532,
      "step": 1559
    },
    {
      "epoch": 0.7087687414811449,
      "grad_norm": 6.01372766494751,
      "learning_rate": 0.0007697237906302457,
      "loss": 0.8644,
      "step": 1560
    },
    {
      "epoch": 0.7092230804179919,
      "grad_norm": 4.733336448669434,
      "learning_rate": 0.0007695711887685029,
      "loss": 0.8007,
      "step": 1561
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 6.03196907043457,
      "learning_rate": 0.0007694185869067603,
      "loss": 1.1461,
      "step": 1562
    },
    {
      "epoch": 0.7101317582916856,
      "grad_norm": 4.771724700927734,
      "learning_rate": 0.0007692659850450175,
      "loss": 0.7205,
      "step": 1563
    },
    {
      "epoch": 0.7105860972285325,
      "grad_norm": 6.774246692657471,
      "learning_rate": 0.0007691133831832748,
      "loss": 1.4306,
      "step": 1564
    },
    {
      "epoch": 0.7110404361653794,
      "grad_norm": 5.846048831939697,
      "learning_rate": 0.0007689607813215322,
      "loss": 0.9516,
      "step": 1565
    },
    {
      "epoch": 0.7114947751022263,
      "grad_norm": 5.837342262268066,
      "learning_rate": 0.0007688081794597894,
      "loss": 1.1761,
      "step": 1566
    },
    {
      "epoch": 0.7119491140390731,
      "grad_norm": 4.686714172363281,
      "learning_rate": 0.0007686555775980467,
      "loss": 0.5,
      "step": 1567
    },
    {
      "epoch": 0.71240345297592,
      "grad_norm": 4.85838508605957,
      "learning_rate": 0.000768502975736304,
      "loss": 0.8804,
      "step": 1568
    },
    {
      "epoch": 0.7128577919127669,
      "grad_norm": 4.354681968688965,
      "learning_rate": 0.0007683503738745613,
      "loss": 0.3498,
      "step": 1569
    },
    {
      "epoch": 0.7133121308496138,
      "grad_norm": 4.168893814086914,
      "learning_rate": 0.0007681977720128186,
      "loss": 0.5305,
      "step": 1570
    },
    {
      "epoch": 0.7137664697864607,
      "grad_norm": 5.924803733825684,
      "learning_rate": 0.0007680451701510759,
      "loss": 1.2703,
      "step": 1571
    },
    {
      "epoch": 0.7142208087233076,
      "grad_norm": 6.737338066101074,
      "learning_rate": 0.0007678925682893332,
      "loss": 0.9048,
      "step": 1572
    },
    {
      "epoch": 0.7146751476601545,
      "grad_norm": 6.255313396453857,
      "learning_rate": 0.0007677399664275904,
      "loss": 0.7105,
      "step": 1573
    },
    {
      "epoch": 0.7151294865970014,
      "grad_norm": 6.282099723815918,
      "learning_rate": 0.0007675873645658478,
      "loss": 0.9504,
      "step": 1574
    },
    {
      "epoch": 0.7155838255338483,
      "grad_norm": 7.160847187042236,
      "learning_rate": 0.0007674347627041051,
      "loss": 1.345,
      "step": 1575
    },
    {
      "epoch": 0.7160381644706951,
      "grad_norm": 4.949448108673096,
      "learning_rate": 0.0007672821608423622,
      "loss": 0.8307,
      "step": 1576
    },
    {
      "epoch": 0.716492503407542,
      "grad_norm": 7.166744232177734,
      "learning_rate": 0.0007671295589806196,
      "loss": 0.9721,
      "step": 1577
    },
    {
      "epoch": 0.7169468423443889,
      "grad_norm": 6.6720709800720215,
      "learning_rate": 0.0007669769571188768,
      "loss": 1.0444,
      "step": 1578
    },
    {
      "epoch": 0.7174011812812358,
      "grad_norm": 4.1474151611328125,
      "learning_rate": 0.0007668243552571341,
      "loss": 0.5663,
      "step": 1579
    },
    {
      "epoch": 0.7178555202180827,
      "grad_norm": 5.961559295654297,
      "learning_rate": 0.0007666717533953914,
      "loss": 1.3907,
      "step": 1580
    },
    {
      "epoch": 0.7183098591549296,
      "grad_norm": 3.3881473541259766,
      "learning_rate": 0.0007665191515336487,
      "loss": 0.282,
      "step": 1581
    },
    {
      "epoch": 0.7187641980917765,
      "grad_norm": 5.4837164878845215,
      "learning_rate": 0.000766366549671906,
      "loss": 0.4787,
      "step": 1582
    },
    {
      "epoch": 0.7192185370286234,
      "grad_norm": 3.0646378993988037,
      "learning_rate": 0.0007662139478101633,
      "loss": 0.2425,
      "step": 1583
    },
    {
      "epoch": 0.7196728759654702,
      "grad_norm": 5.724050045013428,
      "learning_rate": 0.0007660613459484206,
      "loss": 1.0488,
      "step": 1584
    },
    {
      "epoch": 0.7201272149023171,
      "grad_norm": 3.9141883850097656,
      "learning_rate": 0.0007659087440866778,
      "loss": 0.5948,
      "step": 1585
    },
    {
      "epoch": 0.720581553839164,
      "grad_norm": 6.572970867156982,
      "learning_rate": 0.0007657561422249352,
      "loss": 0.8082,
      "step": 1586
    },
    {
      "epoch": 0.7210358927760109,
      "grad_norm": 7.3082146644592285,
      "learning_rate": 0.0007656035403631925,
      "loss": 1.5664,
      "step": 1587
    },
    {
      "epoch": 0.7214902317128578,
      "grad_norm": 9.14210319519043,
      "learning_rate": 0.0007654509385014497,
      "loss": 1.0212,
      "step": 1588
    },
    {
      "epoch": 0.7219445706497046,
      "grad_norm": 5.716622829437256,
      "learning_rate": 0.0007652983366397071,
      "loss": 1.2407,
      "step": 1589
    },
    {
      "epoch": 0.7223989095865516,
      "grad_norm": 5.185332775115967,
      "learning_rate": 0.0007651457347779643,
      "loss": 1.1018,
      "step": 1590
    },
    {
      "epoch": 0.7228532485233985,
      "grad_norm": 6.5982770919799805,
      "learning_rate": 0.0007649931329162216,
      "loss": 0.6491,
      "step": 1591
    },
    {
      "epoch": 0.7233075874602454,
      "grad_norm": 7.9192585945129395,
      "learning_rate": 0.000764840531054479,
      "loss": 0.4611,
      "step": 1592
    },
    {
      "epoch": 0.7237619263970922,
      "grad_norm": 4.177964210510254,
      "learning_rate": 0.0007646879291927362,
      "loss": 0.5758,
      "step": 1593
    },
    {
      "epoch": 0.7242162653339391,
      "grad_norm": 7.277731418609619,
      "learning_rate": 0.0007645353273309934,
      "loss": 1.4928,
      "step": 1594
    },
    {
      "epoch": 0.724670604270786,
      "grad_norm": 8.003886222839355,
      "learning_rate": 0.0007643827254692507,
      "loss": 1.749,
      "step": 1595
    },
    {
      "epoch": 0.7251249432076329,
      "grad_norm": 5.956092834472656,
      "learning_rate": 0.000764230123607508,
      "loss": 0.6622,
      "step": 1596
    },
    {
      "epoch": 0.7255792821444798,
      "grad_norm": 5.664785385131836,
      "learning_rate": 0.0007640775217457652,
      "loss": 1.0564,
      "step": 1597
    },
    {
      "epoch": 0.7260336210813266,
      "grad_norm": 4.484902381896973,
      "learning_rate": 0.0007639249198840226,
      "loss": 0.7678,
      "step": 1598
    },
    {
      "epoch": 0.7264879600181735,
      "grad_norm": 5.584417819976807,
      "learning_rate": 0.0007637723180222799,
      "loss": 0.4762,
      "step": 1599
    },
    {
      "epoch": 0.7269422989550205,
      "grad_norm": 8.49598217010498,
      "learning_rate": 0.0007636197161605371,
      "loss": 1.9802,
      "step": 1600
    },
    {
      "epoch": 0.7273966378918674,
      "grad_norm": 6.948309421539307,
      "learning_rate": 0.0007634671142987945,
      "loss": 0.695,
      "step": 1601
    },
    {
      "epoch": 0.7278509768287142,
      "grad_norm": 5.079593181610107,
      "learning_rate": 0.0007633145124370517,
      "loss": 0.8935,
      "step": 1602
    },
    {
      "epoch": 0.7283053157655611,
      "grad_norm": 6.517108917236328,
      "learning_rate": 0.000763161910575309,
      "loss": 0.8205,
      "step": 1603
    },
    {
      "epoch": 0.728759654702408,
      "grad_norm": 4.810179710388184,
      "learning_rate": 0.0007630093087135664,
      "loss": 1.0391,
      "step": 1604
    },
    {
      "epoch": 0.7292139936392549,
      "grad_norm": 12.650346755981445,
      "learning_rate": 0.0007628567068518236,
      "loss": 2.2693,
      "step": 1605
    },
    {
      "epoch": 0.7296683325761018,
      "grad_norm": 3.1088368892669678,
      "learning_rate": 0.0007627041049900809,
      "loss": 0.3982,
      "step": 1606
    },
    {
      "epoch": 0.7301226715129486,
      "grad_norm": 8.742657661437988,
      "learning_rate": 0.0007625515031283382,
      "loss": 1.9332,
      "step": 1607
    },
    {
      "epoch": 0.7305770104497955,
      "grad_norm": 5.368611812591553,
      "learning_rate": 0.0007623989012665955,
      "loss": 0.7944,
      "step": 1608
    },
    {
      "epoch": 0.7310313493866424,
      "grad_norm": 6.625491142272949,
      "learning_rate": 0.0007622462994048528,
      "loss": 1.5114,
      "step": 1609
    },
    {
      "epoch": 0.7314856883234894,
      "grad_norm": 7.580885410308838,
      "learning_rate": 0.0007620936975431101,
      "loss": 1.7313,
      "step": 1610
    },
    {
      "epoch": 0.7319400272603362,
      "grad_norm": 4.708335876464844,
      "learning_rate": 0.0007619410956813674,
      "loss": 0.7979,
      "step": 1611
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 4.57747745513916,
      "learning_rate": 0.0007617884938196245,
      "loss": 1.1964,
      "step": 1612
    },
    {
      "epoch": 0.73284870513403,
      "grad_norm": 6.878145217895508,
      "learning_rate": 0.0007616358919578819,
      "loss": 1.2714,
      "step": 1613
    },
    {
      "epoch": 0.7333030440708769,
      "grad_norm": 10.470074653625488,
      "learning_rate": 0.0007614832900961391,
      "loss": 1.3518,
      "step": 1614
    },
    {
      "epoch": 0.7337573830077238,
      "grad_norm": 3.1122324466705322,
      "learning_rate": 0.0007613306882343964,
      "loss": 0.4551,
      "step": 1615
    },
    {
      "epoch": 0.7342117219445706,
      "grad_norm": 8.36365032196045,
      "learning_rate": 0.0007611780863726538,
      "loss": 1.4812,
      "step": 1616
    },
    {
      "epoch": 0.7346660608814175,
      "grad_norm": 8.632975578308105,
      "learning_rate": 0.000761025484510911,
      "loss": 2.356,
      "step": 1617
    },
    {
      "epoch": 0.7351203998182644,
      "grad_norm": 4.512323379516602,
      "learning_rate": 0.0007608728826491683,
      "loss": 0.6632,
      "step": 1618
    },
    {
      "epoch": 0.7355747387551114,
      "grad_norm": 7.969712734222412,
      "learning_rate": 0.0007607202807874256,
      "loss": 0.8963,
      "step": 1619
    },
    {
      "epoch": 0.7360290776919582,
      "grad_norm": 5.192809581756592,
      "learning_rate": 0.0007605676789256829,
      "loss": 0.8381,
      "step": 1620
    },
    {
      "epoch": 0.7364834166288051,
      "grad_norm": 7.5919880867004395,
      "learning_rate": 0.0007604150770639402,
      "loss": 1.0346,
      "step": 1621
    },
    {
      "epoch": 0.736937755565652,
      "grad_norm": 3.9427573680877686,
      "learning_rate": 0.0007602624752021975,
      "loss": 0.6769,
      "step": 1622
    },
    {
      "epoch": 0.7373920945024989,
      "grad_norm": 7.154022216796875,
      "learning_rate": 0.0007601098733404548,
      "loss": 1.393,
      "step": 1623
    },
    {
      "epoch": 0.7378464334393458,
      "grad_norm": 8.014837265014648,
      "learning_rate": 0.000759957271478712,
      "loss": 1.6902,
      "step": 1624
    },
    {
      "epoch": 0.7383007723761926,
      "grad_norm": 4.697291374206543,
      "learning_rate": 0.0007598046696169694,
      "loss": 0.6971,
      "step": 1625
    },
    {
      "epoch": 0.7387551113130395,
      "grad_norm": 7.6764655113220215,
      "learning_rate": 0.0007596520677552267,
      "loss": 0.6538,
      "step": 1626
    },
    {
      "epoch": 0.7392094502498864,
      "grad_norm": 5.032561302185059,
      "learning_rate": 0.0007594994658934839,
      "loss": 0.9029,
      "step": 1627
    },
    {
      "epoch": 0.7396637891867333,
      "grad_norm": 7.035595893859863,
      "learning_rate": 0.0007593468640317413,
      "loss": 1.4714,
      "step": 1628
    },
    {
      "epoch": 0.7401181281235802,
      "grad_norm": 5.268275737762451,
      "learning_rate": 0.0007591942621699985,
      "loss": 0.8728,
      "step": 1629
    },
    {
      "epoch": 0.7405724670604271,
      "grad_norm": 4.525938510894775,
      "learning_rate": 0.0007590416603082557,
      "loss": 0.9361,
      "step": 1630
    },
    {
      "epoch": 0.741026805997274,
      "grad_norm": 2.658475637435913,
      "learning_rate": 0.000758889058446513,
      "loss": 0.45,
      "step": 1631
    },
    {
      "epoch": 0.7414811449341209,
      "grad_norm": 4.318480014801025,
      "learning_rate": 0.0007587364565847703,
      "loss": 0.7521,
      "step": 1632
    },
    {
      "epoch": 0.7419354838709677,
      "grad_norm": 4.106709957122803,
      "learning_rate": 0.0007585838547230276,
      "loss": 0.8179,
      "step": 1633
    },
    {
      "epoch": 0.7423898228078146,
      "grad_norm": 6.026836395263672,
      "learning_rate": 0.0007584312528612849,
      "loss": 1.13,
      "step": 1634
    },
    {
      "epoch": 0.7428441617446615,
      "grad_norm": 12.278581619262695,
      "learning_rate": 0.0007582786509995422,
      "loss": 1.0593,
      "step": 1635
    },
    {
      "epoch": 0.7432985006815084,
      "grad_norm": 4.075274467468262,
      "learning_rate": 0.0007581260491377994,
      "loss": 0.7264,
      "step": 1636
    },
    {
      "epoch": 0.7437528396183553,
      "grad_norm": 5.087875843048096,
      "learning_rate": 0.0007579734472760568,
      "loss": 0.721,
      "step": 1637
    },
    {
      "epoch": 0.7442071785552021,
      "grad_norm": 7.425424575805664,
      "learning_rate": 0.000757820845414314,
      "loss": 2.0446,
      "step": 1638
    },
    {
      "epoch": 0.7446615174920491,
      "grad_norm": 7.854901313781738,
      "learning_rate": 0.0007576682435525713,
      "loss": 1.6911,
      "step": 1639
    },
    {
      "epoch": 0.745115856428896,
      "grad_norm": 7.711874485015869,
      "learning_rate": 0.0007575156416908287,
      "loss": 1.6452,
      "step": 1640
    },
    {
      "epoch": 0.7455701953657429,
      "grad_norm": 4.428966999053955,
      "learning_rate": 0.0007573630398290859,
      "loss": 0.781,
      "step": 1641
    },
    {
      "epoch": 0.7460245343025897,
      "grad_norm": 8.361799240112305,
      "learning_rate": 0.0007572104379673432,
      "loss": 2.1412,
      "step": 1642
    },
    {
      "epoch": 0.7464788732394366,
      "grad_norm": 14.019743919372559,
      "learning_rate": 0.0007570578361056006,
      "loss": 0.8274,
      "step": 1643
    },
    {
      "epoch": 0.7469332121762835,
      "grad_norm": 5.561796188354492,
      "learning_rate": 0.0007569052342438578,
      "loss": 1.0718,
      "step": 1644
    },
    {
      "epoch": 0.7473875511131304,
      "grad_norm": 6.128159046173096,
      "learning_rate": 0.0007567526323821151,
      "loss": 0.8158,
      "step": 1645
    },
    {
      "epoch": 0.7478418900499773,
      "grad_norm": 6.306799411773682,
      "learning_rate": 0.0007566000305203724,
      "loss": 0.6875,
      "step": 1646
    },
    {
      "epoch": 0.7482962289868241,
      "grad_norm": 5.912095546722412,
      "learning_rate": 0.0007564474286586297,
      "loss": 1.2025,
      "step": 1647
    },
    {
      "epoch": 0.748750567923671,
      "grad_norm": 4.168496131896973,
      "learning_rate": 0.0007562948267968869,
      "loss": 0.7048,
      "step": 1648
    },
    {
      "epoch": 0.749204906860518,
      "grad_norm": 12.14144229888916,
      "learning_rate": 0.0007561422249351442,
      "loss": 0.9869,
      "step": 1649
    },
    {
      "epoch": 0.7496592457973649,
      "grad_norm": 4.290177345275879,
      "learning_rate": 0.0007559896230734015,
      "loss": 0.7869,
      "step": 1650
    },
    {
      "epoch": 0.7501135847342117,
      "grad_norm": 6.370873928070068,
      "learning_rate": 0.0007558370212116587,
      "loss": 1.8465,
      "step": 1651
    },
    {
      "epoch": 0.7505679236710586,
      "grad_norm": 5.69942569732666,
      "learning_rate": 0.0007556844193499161,
      "loss": 0.7578,
      "step": 1652
    },
    {
      "epoch": 0.7510222626079055,
      "grad_norm": 5.903049468994141,
      "learning_rate": 0.0007555318174881733,
      "loss": 1.08,
      "step": 1653
    },
    {
      "epoch": 0.7514766015447524,
      "grad_norm": 4.0709381103515625,
      "learning_rate": 0.0007553792156264306,
      "loss": 0.8388,
      "step": 1654
    },
    {
      "epoch": 0.7519309404815993,
      "grad_norm": 5.611852645874023,
      "learning_rate": 0.000755226613764688,
      "loss": 1.1289,
      "step": 1655
    },
    {
      "epoch": 0.7523852794184461,
      "grad_norm": 6.473081588745117,
      "learning_rate": 0.0007550740119029452,
      "loss": 0.7621,
      "step": 1656
    },
    {
      "epoch": 0.752839618355293,
      "grad_norm": 4.991074562072754,
      "learning_rate": 0.0007549214100412025,
      "loss": 1.3535,
      "step": 1657
    },
    {
      "epoch": 0.7532939572921399,
      "grad_norm": 8.110067367553711,
      "learning_rate": 0.0007547688081794598,
      "loss": 1.4643,
      "step": 1658
    },
    {
      "epoch": 0.7537482962289869,
      "grad_norm": 5.342687129974365,
      "learning_rate": 0.0007546162063177171,
      "loss": 1.1467,
      "step": 1659
    },
    {
      "epoch": 0.7542026351658337,
      "grad_norm": 6.783014297485352,
      "learning_rate": 0.0007544636044559743,
      "loss": 1.3423,
      "step": 1660
    },
    {
      "epoch": 0.7546569741026806,
      "grad_norm": 8.455617904663086,
      "learning_rate": 0.0007543110025942317,
      "loss": 1.6355,
      "step": 1661
    },
    {
      "epoch": 0.7551113130395275,
      "grad_norm": 11.67534065246582,
      "learning_rate": 0.000754158400732489,
      "loss": 1.6126,
      "step": 1662
    },
    {
      "epoch": 0.7555656519763744,
      "grad_norm": 8.222787857055664,
      "learning_rate": 0.0007540057988707462,
      "loss": 1.2738,
      "step": 1663
    },
    {
      "epoch": 0.7560199909132213,
      "grad_norm": 4.123501300811768,
      "learning_rate": 0.0007538531970090036,
      "loss": 0.2718,
      "step": 1664
    },
    {
      "epoch": 0.7564743298500681,
      "grad_norm": 6.324760437011719,
      "learning_rate": 0.0007537005951472608,
      "loss": 0.7034,
      "step": 1665
    },
    {
      "epoch": 0.756928668786915,
      "grad_norm": 8.047626495361328,
      "learning_rate": 0.0007535479932855181,
      "loss": 1.6115,
      "step": 1666
    },
    {
      "epoch": 0.7573830077237619,
      "grad_norm": 6.493831157684326,
      "learning_rate": 0.0007533953914237754,
      "loss": 1.2344,
      "step": 1667
    },
    {
      "epoch": 0.7578373466606089,
      "grad_norm": 6.425709247589111,
      "learning_rate": 0.0007532427895620326,
      "loss": 0.7106,
      "step": 1668
    },
    {
      "epoch": 0.7582916855974557,
      "grad_norm": 4.499257564544678,
      "learning_rate": 0.0007530901877002899,
      "loss": 1.3235,
      "step": 1669
    },
    {
      "epoch": 0.7587460245343026,
      "grad_norm": 4.733073711395264,
      "learning_rate": 0.0007529375858385472,
      "loss": 0.9366,
      "step": 1670
    },
    {
      "epoch": 0.7592003634711495,
      "grad_norm": 4.393289089202881,
      "learning_rate": 0.0007527849839768045,
      "loss": 0.5276,
      "step": 1671
    },
    {
      "epoch": 0.7596547024079964,
      "grad_norm": 8.228900909423828,
      "learning_rate": 0.0007526323821150617,
      "loss": 0.9671,
      "step": 1672
    },
    {
      "epoch": 0.7601090413448433,
      "grad_norm": 5.771513938903809,
      "learning_rate": 0.0007524797802533191,
      "loss": 1.1978,
      "step": 1673
    },
    {
      "epoch": 0.7605633802816901,
      "grad_norm": 7.778597831726074,
      "learning_rate": 0.0007523271783915764,
      "loss": 1.2571,
      "step": 1674
    },
    {
      "epoch": 0.761017719218537,
      "grad_norm": 5.2204742431640625,
      "learning_rate": 0.0007521745765298336,
      "loss": 1.2329,
      "step": 1675
    },
    {
      "epoch": 0.7614720581553839,
      "grad_norm": 6.2904791831970215,
      "learning_rate": 0.000752021974668091,
      "loss": 1.0975,
      "step": 1676
    },
    {
      "epoch": 0.7619263970922308,
      "grad_norm": 8.519696235656738,
      "learning_rate": 0.0007518693728063482,
      "loss": 1.8318,
      "step": 1677
    },
    {
      "epoch": 0.7623807360290777,
      "grad_norm": 5.8324971199035645,
      "learning_rate": 0.0007517167709446055,
      "loss": 0.927,
      "step": 1678
    },
    {
      "epoch": 0.7628350749659246,
      "grad_norm": 9.13117504119873,
      "learning_rate": 0.0007515641690828629,
      "loss": 1.827,
      "step": 1679
    },
    {
      "epoch": 0.7632894139027715,
      "grad_norm": 6.279883861541748,
      "learning_rate": 0.0007514115672211201,
      "loss": 1.2573,
      "step": 1680
    },
    {
      "epoch": 0.7637437528396184,
      "grad_norm": 6.865844249725342,
      "learning_rate": 0.0007512589653593774,
      "loss": 1.1013,
      "step": 1681
    },
    {
      "epoch": 0.7641980917764652,
      "grad_norm": 8.680283546447754,
      "learning_rate": 0.0007511063634976347,
      "loss": 1.2843,
      "step": 1682
    },
    {
      "epoch": 0.7646524307133121,
      "grad_norm": 8.263264656066895,
      "learning_rate": 0.000750953761635892,
      "loss": 2.6908,
      "step": 1683
    },
    {
      "epoch": 0.765106769650159,
      "grad_norm": 6.060764789581299,
      "learning_rate": 0.0007508011597741494,
      "loss": 0.951,
      "step": 1684
    },
    {
      "epoch": 0.7655611085870059,
      "grad_norm": 4.649331092834473,
      "learning_rate": 0.0007506485579124065,
      "loss": 1.1269,
      "step": 1685
    },
    {
      "epoch": 0.7660154475238528,
      "grad_norm": 3.099290370941162,
      "learning_rate": 0.0007504959560506638,
      "loss": 0.3723,
      "step": 1686
    },
    {
      "epoch": 0.7664697864606996,
      "grad_norm": 5.840421676635742,
      "learning_rate": 0.000750343354188921,
      "loss": 1.6247,
      "step": 1687
    },
    {
      "epoch": 0.7669241253975466,
      "grad_norm": 9.141487121582031,
      "learning_rate": 0.0007501907523271784,
      "loss": 1.6078,
      "step": 1688
    },
    {
      "epoch": 0.7673784643343935,
      "grad_norm": 7.4466447830200195,
      "learning_rate": 0.0007500381504654356,
      "loss": 1.647,
      "step": 1689
    },
    {
      "epoch": 0.7678328032712404,
      "grad_norm": 4.828832626342773,
      "learning_rate": 0.0007498855486036929,
      "loss": 1.0999,
      "step": 1690
    },
    {
      "epoch": 0.7682871422080872,
      "grad_norm": 5.943685054779053,
      "learning_rate": 0.0007497329467419503,
      "loss": 1.1121,
      "step": 1691
    },
    {
      "epoch": 0.7687414811449341,
      "grad_norm": 6.2399067878723145,
      "learning_rate": 0.0007495803448802075,
      "loss": 0.8645,
      "step": 1692
    },
    {
      "epoch": 0.769195820081781,
      "grad_norm": 10.21334457397461,
      "learning_rate": 0.0007494277430184649,
      "loss": 1.2468,
      "step": 1693
    },
    {
      "epoch": 0.7696501590186279,
      "grad_norm": 8.743246078491211,
      "learning_rate": 0.0007492751411567221,
      "loss": 1.6583,
      "step": 1694
    },
    {
      "epoch": 0.7701044979554748,
      "grad_norm": 6.2244439125061035,
      "learning_rate": 0.0007491225392949794,
      "loss": 0.9671,
      "step": 1695
    },
    {
      "epoch": 0.7705588368923216,
      "grad_norm": 3.7252533435821533,
      "learning_rate": 0.0007489699374332368,
      "loss": 0.5197,
      "step": 1696
    },
    {
      "epoch": 0.7710131758291685,
      "grad_norm": 7.393462181091309,
      "learning_rate": 0.000748817335571494,
      "loss": 1.8443,
      "step": 1697
    },
    {
      "epoch": 0.7714675147660155,
      "grad_norm": 6.198462009429932,
      "learning_rate": 0.0007486647337097513,
      "loss": 0.8716,
      "step": 1698
    },
    {
      "epoch": 0.7719218537028624,
      "grad_norm": 4.162138938903809,
      "learning_rate": 0.0007485121318480086,
      "loss": 0.4211,
      "step": 1699
    },
    {
      "epoch": 0.7723761926397092,
      "grad_norm": 4.988912105560303,
      "learning_rate": 0.0007483595299862659,
      "loss": 1.4142,
      "step": 1700
    },
    {
      "epoch": 0.7728305315765561,
      "grad_norm": 4.599401473999023,
      "learning_rate": 0.0007482069281245232,
      "loss": 1.0,
      "step": 1701
    },
    {
      "epoch": 0.773284870513403,
      "grad_norm": 4.370307445526123,
      "learning_rate": 0.0007480543262627805,
      "loss": 0.6151,
      "step": 1702
    },
    {
      "epoch": 0.7737392094502499,
      "grad_norm": 6.181445121765137,
      "learning_rate": 0.0007479017244010377,
      "loss": 0.5831,
      "step": 1703
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 4.886289119720459,
      "learning_rate": 0.0007477491225392949,
      "loss": 0.9631,
      "step": 1704
    },
    {
      "epoch": 0.7746478873239436,
      "grad_norm": 6.923476219177246,
      "learning_rate": 0.0007475965206775523,
      "loss": 0.7589,
      "step": 1705
    },
    {
      "epoch": 0.7751022262607905,
      "grad_norm": 6.887092590332031,
      "learning_rate": 0.0007474439188158095,
      "loss": 1.5093,
      "step": 1706
    },
    {
      "epoch": 0.7755565651976375,
      "grad_norm": 5.323845863342285,
      "learning_rate": 0.0007472913169540668,
      "loss": 0.9897,
      "step": 1707
    },
    {
      "epoch": 0.7760109041344844,
      "grad_norm": 5.029244899749756,
      "learning_rate": 0.0007471387150923242,
      "loss": 0.5427,
      "step": 1708
    },
    {
      "epoch": 0.7764652430713312,
      "grad_norm": 5.758382797241211,
      "learning_rate": 0.0007469861132305814,
      "loss": 0.7495,
      "step": 1709
    },
    {
      "epoch": 0.7769195820081781,
      "grad_norm": 5.757443904876709,
      "learning_rate": 0.0007468335113688387,
      "loss": 1.0312,
      "step": 1710
    },
    {
      "epoch": 0.777373920945025,
      "grad_norm": 17.89948272705078,
      "learning_rate": 0.000746680909507096,
      "loss": 2.0773,
      "step": 1711
    },
    {
      "epoch": 0.7778282598818719,
      "grad_norm": 6.717979431152344,
      "learning_rate": 0.0007465283076453533,
      "loss": 1.4686,
      "step": 1712
    },
    {
      "epoch": 0.7782825988187188,
      "grad_norm": 4.78776741027832,
      "learning_rate": 0.0007463757057836106,
      "loss": 0.8591,
      "step": 1713
    },
    {
      "epoch": 0.7787369377555656,
      "grad_norm": 4.390621662139893,
      "learning_rate": 0.0007462231039218679,
      "loss": 0.7471,
      "step": 1714
    },
    {
      "epoch": 0.7791912766924125,
      "grad_norm": 7.507774829864502,
      "learning_rate": 0.0007460705020601252,
      "loss": 1.2579,
      "step": 1715
    },
    {
      "epoch": 0.7796456156292594,
      "grad_norm": 6.770198345184326,
      "learning_rate": 0.0007459179001983824,
      "loss": 0.8653,
      "step": 1716
    },
    {
      "epoch": 0.7800999545661064,
      "grad_norm": 8.512845993041992,
      "learning_rate": 0.0007457652983366398,
      "loss": 0.9269,
      "step": 1717
    },
    {
      "epoch": 0.7805542935029532,
      "grad_norm": 6.145389556884766,
      "learning_rate": 0.0007456126964748971,
      "loss": 1.2347,
      "step": 1718
    },
    {
      "epoch": 0.7810086324398001,
      "grad_norm": 5.091829299926758,
      "learning_rate": 0.0007454600946131543,
      "loss": 0.7127,
      "step": 1719
    },
    {
      "epoch": 0.781462971376647,
      "grad_norm": 5.0236406326293945,
      "learning_rate": 0.0007453074927514117,
      "loss": 0.7501,
      "step": 1720
    },
    {
      "epoch": 0.7819173103134939,
      "grad_norm": 6.669018268585205,
      "learning_rate": 0.0007451548908896689,
      "loss": 1.4914,
      "step": 1721
    },
    {
      "epoch": 0.7823716492503408,
      "grad_norm": 3.4523611068725586,
      "learning_rate": 0.0007450022890279261,
      "loss": 0.3122,
      "step": 1722
    },
    {
      "epoch": 0.7828259881871876,
      "grad_norm": 10.143718719482422,
      "learning_rate": 0.0007448496871661834,
      "loss": 1.0083,
      "step": 1723
    },
    {
      "epoch": 0.7832803271240345,
      "grad_norm": 2.423574447631836,
      "learning_rate": 0.0007446970853044407,
      "loss": 0.3154,
      "step": 1724
    },
    {
      "epoch": 0.7837346660608814,
      "grad_norm": 4.852202415466309,
      "learning_rate": 0.000744544483442698,
      "loss": 0.9623,
      "step": 1725
    },
    {
      "epoch": 0.7841890049977283,
      "grad_norm": 4.977994918823242,
      "learning_rate": 0.0007443918815809553,
      "loss": 1.1576,
      "step": 1726
    },
    {
      "epoch": 0.7846433439345752,
      "grad_norm": 6.673286437988281,
      "learning_rate": 0.0007442392797192126,
      "loss": 0.9625,
      "step": 1727
    },
    {
      "epoch": 0.7850976828714221,
      "grad_norm": 4.40566873550415,
      "learning_rate": 0.0007440866778574698,
      "loss": 0.5228,
      "step": 1728
    },
    {
      "epoch": 0.785552021808269,
      "grad_norm": 8.336620330810547,
      "learning_rate": 0.0007439340759957272,
      "loss": 1.759,
      "step": 1729
    },
    {
      "epoch": 0.7860063607451159,
      "grad_norm": 6.658580303192139,
      "learning_rate": 0.0007437814741339845,
      "loss": 0.919,
      "step": 1730
    },
    {
      "epoch": 0.7864606996819627,
      "grad_norm": 3.516814708709717,
      "learning_rate": 0.0007436288722722417,
      "loss": 0.5198,
      "step": 1731
    },
    {
      "epoch": 0.7869150386188096,
      "grad_norm": 7.029669761657715,
      "learning_rate": 0.0007434762704104991,
      "loss": 0.7626,
      "step": 1732
    },
    {
      "epoch": 0.7873693775556565,
      "grad_norm": 6.825895309448242,
      "learning_rate": 0.0007433236685487563,
      "loss": 0.9518,
      "step": 1733
    },
    {
      "epoch": 0.7878237164925034,
      "grad_norm": 6.938842296600342,
      "learning_rate": 0.0007431710666870136,
      "loss": 1.0041,
      "step": 1734
    },
    {
      "epoch": 0.7882780554293503,
      "grad_norm": 8.446057319641113,
      "learning_rate": 0.000743018464825271,
      "loss": 1.4962,
      "step": 1735
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 3.173511028289795,
      "learning_rate": 0.0007428658629635282,
      "loss": 0.3971,
      "step": 1736
    },
    {
      "epoch": 0.7891867333030441,
      "grad_norm": 7.380854606628418,
      "learning_rate": 0.0007427132611017855,
      "loss": 1.7504,
      "step": 1737
    },
    {
      "epoch": 0.789641072239891,
      "grad_norm": 4.066669464111328,
      "learning_rate": 0.0007425606592400428,
      "loss": 0.4162,
      "step": 1738
    },
    {
      "epoch": 0.7900954111767379,
      "grad_norm": 4.5273756980896,
      "learning_rate": 0.0007424080573783001,
      "loss": 0.6166,
      "step": 1739
    },
    {
      "epoch": 0.7905497501135847,
      "grad_norm": 3.9709744453430176,
      "learning_rate": 0.0007422554555165572,
      "loss": 0.3963,
      "step": 1740
    },
    {
      "epoch": 0.7910040890504316,
      "grad_norm": 4.944493293762207,
      "learning_rate": 0.0007421028536548146,
      "loss": 0.959,
      "step": 1741
    },
    {
      "epoch": 0.7914584279872785,
      "grad_norm": 5.835123538970947,
      "learning_rate": 0.0007419502517930719,
      "loss": 0.9646,
      "step": 1742
    },
    {
      "epoch": 0.7919127669241254,
      "grad_norm": 4.559453010559082,
      "learning_rate": 0.0007417976499313291,
      "loss": 1.0965,
      "step": 1743
    },
    {
      "epoch": 0.7923671058609723,
      "grad_norm": 7.348373889923096,
      "learning_rate": 0.0007416450480695865,
      "loss": 1.2508,
      "step": 1744
    },
    {
      "epoch": 0.7928214447978191,
      "grad_norm": 7.297049045562744,
      "learning_rate": 0.0007414924462078437,
      "loss": 0.7523,
      "step": 1745
    },
    {
      "epoch": 0.793275783734666,
      "grad_norm": 3.630253791809082,
      "learning_rate": 0.000741339844346101,
      "loss": 0.7229,
      "step": 1746
    },
    {
      "epoch": 0.793730122671513,
      "grad_norm": 4.479067325592041,
      "learning_rate": 0.0007411872424843584,
      "loss": 0.5932,
      "step": 1747
    },
    {
      "epoch": 0.7941844616083599,
      "grad_norm": 7.232802867889404,
      "learning_rate": 0.0007410346406226156,
      "loss": 1.5923,
      "step": 1748
    },
    {
      "epoch": 0.7946388005452067,
      "grad_norm": 5.384496212005615,
      "learning_rate": 0.0007408820387608729,
      "loss": 1.1077,
      "step": 1749
    },
    {
      "epoch": 0.7950931394820536,
      "grad_norm": 3.11967134475708,
      "learning_rate": 0.0007407294368991302,
      "loss": 0.3546,
      "step": 1750
    },
    {
      "epoch": 0.7955474784189005,
      "grad_norm": 5.280231475830078,
      "learning_rate": 0.0007405768350373875,
      "loss": 1.1249,
      "step": 1751
    },
    {
      "epoch": 0.7960018173557474,
      "grad_norm": 4.573071479797363,
      "learning_rate": 0.0007404242331756448,
      "loss": 0.6613,
      "step": 1752
    },
    {
      "epoch": 0.7964561562925943,
      "grad_norm": 7.660194396972656,
      "learning_rate": 0.0007402716313139021,
      "loss": 1.4079,
      "step": 1753
    },
    {
      "epoch": 0.7969104952294411,
      "grad_norm": 7.905807018280029,
      "learning_rate": 0.0007401190294521594,
      "loss": 1.2013,
      "step": 1754
    },
    {
      "epoch": 0.797364834166288,
      "grad_norm": 2.716914653778076,
      "learning_rate": 0.0007399664275904166,
      "loss": 0.2603,
      "step": 1755
    },
    {
      "epoch": 0.797819173103135,
      "grad_norm": 5.549619674682617,
      "learning_rate": 0.000739813825728674,
      "loss": 1.0474,
      "step": 1756
    },
    {
      "epoch": 0.7982735120399819,
      "grad_norm": 3.3877310752868652,
      "learning_rate": 0.0007396612238669313,
      "loss": 0.8083,
      "step": 1757
    },
    {
      "epoch": 0.7987278509768287,
      "grad_norm": 4.99025297164917,
      "learning_rate": 0.0007395086220051884,
      "loss": 1.1539,
      "step": 1758
    },
    {
      "epoch": 0.7991821899136756,
      "grad_norm": 6.229437828063965,
      "learning_rate": 0.0007393560201434458,
      "loss": 1.2304,
      "step": 1759
    },
    {
      "epoch": 0.7996365288505225,
      "grad_norm": 4.519434928894043,
      "learning_rate": 0.000739203418281703,
      "loss": 0.5344,
      "step": 1760
    },
    {
      "epoch": 0.8000908677873694,
      "grad_norm": 3.0162479877471924,
      "learning_rate": 0.0007390508164199603,
      "loss": 0.41,
      "step": 1761
    },
    {
      "epoch": 0.8005452067242163,
      "grad_norm": 6.627788543701172,
      "learning_rate": 0.0007388982145582176,
      "loss": 1.7461,
      "step": 1762
    },
    {
      "epoch": 0.8009995456610631,
      "grad_norm": 4.341945171356201,
      "learning_rate": 0.0007387456126964749,
      "loss": 0.7238,
      "step": 1763
    },
    {
      "epoch": 0.80145388459791,
      "grad_norm": 4.2279558181762695,
      "learning_rate": 0.0007385930108347322,
      "loss": 0.4246,
      "step": 1764
    },
    {
      "epoch": 0.8019082235347569,
      "grad_norm": 5.881608009338379,
      "learning_rate": 0.0007384404089729895,
      "loss": 1.291,
      "step": 1765
    },
    {
      "epoch": 0.8023625624716039,
      "grad_norm": 4.099097728729248,
      "learning_rate": 0.0007382878071112468,
      "loss": 0.4485,
      "step": 1766
    },
    {
      "epoch": 0.8028169014084507,
      "grad_norm": 4.7362871170043945,
      "learning_rate": 0.000738135205249504,
      "loss": 0.9016,
      "step": 1767
    },
    {
      "epoch": 0.8032712403452976,
      "grad_norm": 5.212220191955566,
      "learning_rate": 0.0007379826033877614,
      "loss": 0.8551,
      "step": 1768
    },
    {
      "epoch": 0.8037255792821445,
      "grad_norm": 5.721065521240234,
      "learning_rate": 0.0007378300015260187,
      "loss": 0.882,
      "step": 1769
    },
    {
      "epoch": 0.8041799182189914,
      "grad_norm": 3.469090700149536,
      "learning_rate": 0.0007376773996642759,
      "loss": 0.2671,
      "step": 1770
    },
    {
      "epoch": 0.8046342571558383,
      "grad_norm": 10.323538780212402,
      "learning_rate": 0.0007375247978025333,
      "loss": 0.6668,
      "step": 1771
    },
    {
      "epoch": 0.8050885960926851,
      "grad_norm": 3.2319443225860596,
      "learning_rate": 0.0007373721959407905,
      "loss": 0.3787,
      "step": 1772
    },
    {
      "epoch": 0.805542935029532,
      "grad_norm": 6.987828254699707,
      "learning_rate": 0.0007372195940790478,
      "loss": 0.7465,
      "step": 1773
    },
    {
      "epoch": 0.8059972739663789,
      "grad_norm": 7.655574798583984,
      "learning_rate": 0.0007370669922173052,
      "loss": 1.8912,
      "step": 1774
    },
    {
      "epoch": 0.8064516129032258,
      "grad_norm": 10.028095245361328,
      "learning_rate": 0.0007369143903555624,
      "loss": 2.1226,
      "step": 1775
    },
    {
      "epoch": 0.8069059518400727,
      "grad_norm": 5.089249134063721,
      "learning_rate": 0.0007367617884938196,
      "loss": 0.763,
      "step": 1776
    },
    {
      "epoch": 0.8073602907769196,
      "grad_norm": 6.949735641479492,
      "learning_rate": 0.0007366091866320769,
      "loss": 0.9887,
      "step": 1777
    },
    {
      "epoch": 0.8078146297137665,
      "grad_norm": 6.018179893493652,
      "learning_rate": 0.0007364565847703342,
      "loss": 1.4703,
      "step": 1778
    },
    {
      "epoch": 0.8082689686506134,
      "grad_norm": 3.6515347957611084,
      "learning_rate": 0.0007363039829085914,
      "loss": 0.3964,
      "step": 1779
    },
    {
      "epoch": 0.8087233075874602,
      "grad_norm": 6.471455097198486,
      "learning_rate": 0.0007361513810468488,
      "loss": 1.2641,
      "step": 1780
    },
    {
      "epoch": 0.8091776465243071,
      "grad_norm": 4.729971408843994,
      "learning_rate": 0.0007359987791851061,
      "loss": 0.5695,
      "step": 1781
    },
    {
      "epoch": 0.809631985461154,
      "grad_norm": 7.841596603393555,
      "learning_rate": 0.0007358461773233633,
      "loss": 1.1882,
      "step": 1782
    },
    {
      "epoch": 0.8100863243980009,
      "grad_norm": 5.986236095428467,
      "learning_rate": 0.0007356935754616207,
      "loss": 0.6714,
      "step": 1783
    },
    {
      "epoch": 0.8105406633348478,
      "grad_norm": 6.701154708862305,
      "learning_rate": 0.0007355409735998779,
      "loss": 0.6167,
      "step": 1784
    },
    {
      "epoch": 0.8109950022716946,
      "grad_norm": 8.643529891967773,
      "learning_rate": 0.0007353883717381352,
      "loss": 1.2355,
      "step": 1785
    },
    {
      "epoch": 0.8114493412085416,
      "grad_norm": 6.616333961486816,
      "learning_rate": 0.0007352357698763926,
      "loss": 1.3891,
      "step": 1786
    },
    {
      "epoch": 0.8119036801453885,
      "grad_norm": 7.736835956573486,
      "learning_rate": 0.0007350831680146498,
      "loss": 1.2452,
      "step": 1787
    },
    {
      "epoch": 0.8123580190822354,
      "grad_norm": 7.108563423156738,
      "learning_rate": 0.0007349305661529071,
      "loss": 0.6932,
      "step": 1788
    },
    {
      "epoch": 0.8128123580190822,
      "grad_norm": 6.60685396194458,
      "learning_rate": 0.0007347779642911644,
      "loss": 1.3226,
      "step": 1789
    },
    {
      "epoch": 0.8132666969559291,
      "grad_norm": 6.902732849121094,
      "learning_rate": 0.0007346253624294217,
      "loss": 0.8349,
      "step": 1790
    },
    {
      "epoch": 0.813721035892776,
      "grad_norm": 4.7068657875061035,
      "learning_rate": 0.0007344727605676789,
      "loss": 0.8939,
      "step": 1791
    },
    {
      "epoch": 0.8141753748296229,
      "grad_norm": 5.200576305389404,
      "learning_rate": 0.0007343201587059363,
      "loss": 1.0515,
      "step": 1792
    },
    {
      "epoch": 0.8146297137664698,
      "grad_norm": 5.960424900054932,
      "learning_rate": 0.0007341675568441936,
      "loss": 1.0383,
      "step": 1793
    },
    {
      "epoch": 0.8150840527033166,
      "grad_norm": 5.234079360961914,
      "learning_rate": 0.0007340149549824508,
      "loss": 0.9445,
      "step": 1794
    },
    {
      "epoch": 0.8155383916401635,
      "grad_norm": 3.396192789077759,
      "learning_rate": 0.0007338623531207081,
      "loss": 0.6497,
      "step": 1795
    },
    {
      "epoch": 0.8159927305770105,
      "grad_norm": 2.605476140975952,
      "learning_rate": 0.0007337097512589653,
      "loss": 0.2615,
      "step": 1796
    },
    {
      "epoch": 0.8164470695138574,
      "grad_norm": 5.556195259094238,
      "learning_rate": 0.0007335571493972226,
      "loss": 0.8271,
      "step": 1797
    },
    {
      "epoch": 0.8169014084507042,
      "grad_norm": 6.7656025886535645,
      "learning_rate": 0.00073340454753548,
      "loss": 1.3223,
      "step": 1798
    },
    {
      "epoch": 0.8173557473875511,
      "grad_norm": 7.054252624511719,
      "learning_rate": 0.0007332519456737372,
      "loss": 0.7383,
      "step": 1799
    },
    {
      "epoch": 0.817810086324398,
      "grad_norm": 5.50374174118042,
      "learning_rate": 0.0007330993438119945,
      "loss": 1.1818,
      "step": 1800
    },
    {
      "epoch": 0.8182644252612449,
      "grad_norm": 6.3486175537109375,
      "learning_rate": 0.0007329467419502518,
      "loss": 0.8965,
      "step": 1801
    },
    {
      "epoch": 0.8187187641980918,
      "grad_norm": 4.901193141937256,
      "learning_rate": 0.0007327941400885091,
      "loss": 0.8484,
      "step": 1802
    },
    {
      "epoch": 0.8191731031349386,
      "grad_norm": 4.3269853591918945,
      "learning_rate": 0.0007326415382267663,
      "loss": 0.3125,
      "step": 1803
    },
    {
      "epoch": 0.8196274420717855,
      "grad_norm": 7.678954601287842,
      "learning_rate": 0.0007324889363650237,
      "loss": 1.6111,
      "step": 1804
    },
    {
      "epoch": 0.8200817810086325,
      "grad_norm": 5.734117031097412,
      "learning_rate": 0.000732336334503281,
      "loss": 1.022,
      "step": 1805
    },
    {
      "epoch": 0.8205361199454794,
      "grad_norm": 10.40030288696289,
      "learning_rate": 0.0007321837326415382,
      "loss": 0.9908,
      "step": 1806
    },
    {
      "epoch": 0.8209904588823262,
      "grad_norm": 10.432741165161133,
      "learning_rate": 0.0007320311307797956,
      "loss": 2.4123,
      "step": 1807
    },
    {
      "epoch": 0.8214447978191731,
      "grad_norm": 7.810321807861328,
      "learning_rate": 0.0007318785289180528,
      "loss": 2.1518,
      "step": 1808
    },
    {
      "epoch": 0.82189913675602,
      "grad_norm": 4.6946282386779785,
      "learning_rate": 0.0007317259270563101,
      "loss": 0.7509,
      "step": 1809
    },
    {
      "epoch": 0.8223534756928669,
      "grad_norm": 5.418595790863037,
      "learning_rate": 0.0007315733251945675,
      "loss": 0.7142,
      "step": 1810
    },
    {
      "epoch": 0.8228078146297138,
      "grad_norm": 6.26311731338501,
      "learning_rate": 0.0007314207233328247,
      "loss": 0.7122,
      "step": 1811
    },
    {
      "epoch": 0.8232621535665606,
      "grad_norm": 6.581384658813477,
      "learning_rate": 0.000731268121471082,
      "loss": 1.4393,
      "step": 1812
    },
    {
      "epoch": 0.8237164925034075,
      "grad_norm": 2.869912624359131,
      "learning_rate": 0.0007311155196093392,
      "loss": 0.4808,
      "step": 1813
    },
    {
      "epoch": 0.8241708314402544,
      "grad_norm": 3.541003704071045,
      "learning_rate": 0.0007309629177475965,
      "loss": 0.504,
      "step": 1814
    },
    {
      "epoch": 0.8246251703771014,
      "grad_norm": 6.688127040863037,
      "learning_rate": 0.0007308103158858537,
      "loss": 1.4102,
      "step": 1815
    },
    {
      "epoch": 0.8250795093139482,
      "grad_norm": 6.942176342010498,
      "learning_rate": 0.0007306577140241111,
      "loss": 1.4597,
      "step": 1816
    },
    {
      "epoch": 0.8255338482507951,
      "grad_norm": 5.932709217071533,
      "learning_rate": 0.0007305051121623684,
      "loss": 1.0327,
      "step": 1817
    },
    {
      "epoch": 0.825988187187642,
      "grad_norm": 6.758030891418457,
      "learning_rate": 0.0007303525103006256,
      "loss": 0.8473,
      "step": 1818
    },
    {
      "epoch": 0.8264425261244889,
      "grad_norm": 7.3684234619140625,
      "learning_rate": 0.000730199908438883,
      "loss": 1.858,
      "step": 1819
    },
    {
      "epoch": 0.8268968650613358,
      "grad_norm": 8.905669212341309,
      "learning_rate": 0.0007300473065771402,
      "loss": 1.5766,
      "step": 1820
    },
    {
      "epoch": 0.8273512039981826,
      "grad_norm": 6.20892858505249,
      "learning_rate": 0.0007298947047153975,
      "loss": 1.3151,
      "step": 1821
    },
    {
      "epoch": 0.8278055429350295,
      "grad_norm": 8.539573669433594,
      "learning_rate": 0.0007297421028536549,
      "loss": 1.7091,
      "step": 1822
    },
    {
      "epoch": 0.8282598818718764,
      "grad_norm": 4.137585163116455,
      "learning_rate": 0.0007295895009919121,
      "loss": 1.2406,
      "step": 1823
    },
    {
      "epoch": 0.8287142208087233,
      "grad_norm": 3.8927860260009766,
      "learning_rate": 0.0007294368991301694,
      "loss": 0.6067,
      "step": 1824
    },
    {
      "epoch": 0.8291685597455702,
      "grad_norm": 5.993590354919434,
      "learning_rate": 0.0007292842972684267,
      "loss": 0.5672,
      "step": 1825
    },
    {
      "epoch": 0.8296228986824171,
      "grad_norm": 4.685159206390381,
      "learning_rate": 0.000729131695406684,
      "loss": 0.5205,
      "step": 1826
    },
    {
      "epoch": 0.830077237619264,
      "grad_norm": 6.563284873962402,
      "learning_rate": 0.0007289790935449413,
      "loss": 1.1197,
      "step": 1827
    },
    {
      "epoch": 0.8305315765561109,
      "grad_norm": 6.0055832862854,
      "learning_rate": 0.0007288264916831986,
      "loss": 1.0767,
      "step": 1828
    },
    {
      "epoch": 0.8309859154929577,
      "grad_norm": 6.123500823974609,
      "learning_rate": 0.0007286738898214559,
      "loss": 0.9821,
      "step": 1829
    },
    {
      "epoch": 0.8314402544298046,
      "grad_norm": 7.038214206695557,
      "learning_rate": 0.0007285212879597131,
      "loss": 0.4442,
      "step": 1830
    },
    {
      "epoch": 0.8318945933666515,
      "grad_norm": 4.880025386810303,
      "learning_rate": 0.0007283686860979704,
      "loss": 0.5378,
      "step": 1831
    },
    {
      "epoch": 0.8323489323034984,
      "grad_norm": 3.7434487342834473,
      "learning_rate": 0.0007282160842362276,
      "loss": 0.5596,
      "step": 1832
    },
    {
      "epoch": 0.8328032712403453,
      "grad_norm": 8.969287872314453,
      "learning_rate": 0.0007280634823744849,
      "loss": 1.6091,
      "step": 1833
    },
    {
      "epoch": 0.8332576101771921,
      "grad_norm": 6.876681804656982,
      "learning_rate": 0.0007279108805127423,
      "loss": 1.2113,
      "step": 1834
    },
    {
      "epoch": 0.8337119491140391,
      "grad_norm": 5.8361592292785645,
      "learning_rate": 0.0007277582786509995,
      "loss": 1.017,
      "step": 1835
    },
    {
      "epoch": 0.834166288050886,
      "grad_norm": 9.343118667602539,
      "learning_rate": 0.0007276056767892568,
      "loss": 0.9007,
      "step": 1836
    },
    {
      "epoch": 0.8346206269877329,
      "grad_norm": 14.064599990844727,
      "learning_rate": 0.0007274530749275141,
      "loss": 1.701,
      "step": 1837
    },
    {
      "epoch": 0.8350749659245797,
      "grad_norm": 5.453215599060059,
      "learning_rate": 0.0007273004730657714,
      "loss": 0.9605,
      "step": 1838
    },
    {
      "epoch": 0.8355293048614266,
      "grad_norm": 8.192866325378418,
      "learning_rate": 0.0007271478712040287,
      "loss": 2.493,
      "step": 1839
    },
    {
      "epoch": 0.8359836437982735,
      "grad_norm": 6.940364837646484,
      "learning_rate": 0.000726995269342286,
      "loss": 1.8251,
      "step": 1840
    },
    {
      "epoch": 0.8364379827351204,
      "grad_norm": 6.204977512359619,
      "learning_rate": 0.0007268426674805433,
      "loss": 0.9884,
      "step": 1841
    },
    {
      "epoch": 0.8368923216719673,
      "grad_norm": 6.997471332550049,
      "learning_rate": 0.0007266900656188005,
      "loss": 1.0378,
      "step": 1842
    },
    {
      "epoch": 0.8373466606088141,
      "grad_norm": 5.668605804443359,
      "learning_rate": 0.0007265374637570579,
      "loss": 0.6817,
      "step": 1843
    },
    {
      "epoch": 0.8378009995456611,
      "grad_norm": 7.069530487060547,
      "learning_rate": 0.0007263848618953152,
      "loss": 0.8249,
      "step": 1844
    },
    {
      "epoch": 0.838255338482508,
      "grad_norm": 7.501122951507568,
      "learning_rate": 0.0007262322600335724,
      "loss": 1.2926,
      "step": 1845
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 7.877874851226807,
      "learning_rate": 0.0007260796581718298,
      "loss": 1.742,
      "step": 1846
    },
    {
      "epoch": 0.8391640163562017,
      "grad_norm": 6.3793206214904785,
      "learning_rate": 0.000725927056310087,
      "loss": 0.8588,
      "step": 1847
    },
    {
      "epoch": 0.8396183552930486,
      "grad_norm": 8.79432487487793,
      "learning_rate": 0.0007257744544483443,
      "loss": 1.1639,
      "step": 1848
    },
    {
      "epoch": 0.8400726942298955,
      "grad_norm": 4.240730285644531,
      "learning_rate": 0.0007256218525866015,
      "loss": 0.8709,
      "step": 1849
    },
    {
      "epoch": 0.8405270331667424,
      "grad_norm": 4.2784624099731445,
      "learning_rate": 0.0007254692507248588,
      "loss": 0.5233,
      "step": 1850
    },
    {
      "epoch": 0.8409813721035893,
      "grad_norm": 2.969538450241089,
      "learning_rate": 0.0007253166488631161,
      "loss": 0.5829,
      "step": 1851
    },
    {
      "epoch": 0.8414357110404361,
      "grad_norm": 5.4641852378845215,
      "learning_rate": 0.0007251640470013734,
      "loss": 0.7332,
      "step": 1852
    },
    {
      "epoch": 0.841890049977283,
      "grad_norm": 5.894360065460205,
      "learning_rate": 0.0007250114451396307,
      "loss": 0.7876,
      "step": 1853
    },
    {
      "epoch": 0.84234438891413,
      "grad_norm": 5.752452373504639,
      "learning_rate": 0.0007248588432778879,
      "loss": 1.2132,
      "step": 1854
    },
    {
      "epoch": 0.8427987278509769,
      "grad_norm": 3.9444353580474854,
      "learning_rate": 0.0007247062414161453,
      "loss": 0.7134,
      "step": 1855
    },
    {
      "epoch": 0.8432530667878237,
      "grad_norm": 4.319251537322998,
      "learning_rate": 0.0007245536395544026,
      "loss": 0.4658,
      "step": 1856
    },
    {
      "epoch": 0.8437074057246706,
      "grad_norm": 5.848561763763428,
      "learning_rate": 0.0007244010376926598,
      "loss": 0.9482,
      "step": 1857
    },
    {
      "epoch": 0.8441617446615175,
      "grad_norm": 7.805546283721924,
      "learning_rate": 0.0007242484358309172,
      "loss": 0.9119,
      "step": 1858
    },
    {
      "epoch": 0.8446160835983644,
      "grad_norm": 6.325373649597168,
      "learning_rate": 0.0007240958339691744,
      "loss": 0.4798,
      "step": 1859
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 8.164684295654297,
      "learning_rate": 0.0007239432321074317,
      "loss": 1.2866,
      "step": 1860
    },
    {
      "epoch": 0.8455247614720581,
      "grad_norm": 7.494873523712158,
      "learning_rate": 0.0007237906302456891,
      "loss": 1.4496,
      "step": 1861
    },
    {
      "epoch": 0.845979100408905,
      "grad_norm": 4.397274971008301,
      "learning_rate": 0.0007236380283839463,
      "loss": 0.9527,
      "step": 1862
    },
    {
      "epoch": 0.8464334393457519,
      "grad_norm": 4.942344665527344,
      "learning_rate": 0.0007234854265222036,
      "loss": 0.9766,
      "step": 1863
    },
    {
      "epoch": 0.8468877782825989,
      "grad_norm": 5.646752834320068,
      "learning_rate": 0.0007233328246604609,
      "loss": 1.7562,
      "step": 1864
    },
    {
      "epoch": 0.8473421172194457,
      "grad_norm": 8.126665115356445,
      "learning_rate": 0.0007231802227987182,
      "loss": 1.4772,
      "step": 1865
    },
    {
      "epoch": 0.8477964561562926,
      "grad_norm": 6.436826705932617,
      "learning_rate": 0.0007230276209369754,
      "loss": 1.8961,
      "step": 1866
    },
    {
      "epoch": 0.8482507950931395,
      "grad_norm": 8.196511268615723,
      "learning_rate": 0.0007228750190752328,
      "loss": 1.011,
      "step": 1867
    },
    {
      "epoch": 0.8487051340299864,
      "grad_norm": 6.160547733306885,
      "learning_rate": 0.00072272241721349,
      "loss": 0.9505,
      "step": 1868
    },
    {
      "epoch": 0.8491594729668333,
      "grad_norm": 6.139994144439697,
      "learning_rate": 0.0007225698153517472,
      "loss": 0.918,
      "step": 1869
    },
    {
      "epoch": 0.8496138119036801,
      "grad_norm": 8.811053276062012,
      "learning_rate": 0.0007224172134900046,
      "loss": 1.1765,
      "step": 1870
    },
    {
      "epoch": 0.850068150840527,
      "grad_norm": 7.752175331115723,
      "learning_rate": 0.0007222646116282618,
      "loss": 0.946,
      "step": 1871
    },
    {
      "epoch": 0.8505224897773739,
      "grad_norm": 4.393950462341309,
      "learning_rate": 0.0007221120097665191,
      "loss": 0.49,
      "step": 1872
    },
    {
      "epoch": 0.8509768287142208,
      "grad_norm": 4.8644208908081055,
      "learning_rate": 0.0007219594079047765,
      "loss": 0.8355,
      "step": 1873
    },
    {
      "epoch": 0.8514311676510677,
      "grad_norm": 6.503478527069092,
      "learning_rate": 0.0007218068060430337,
      "loss": 1.7693,
      "step": 1874
    },
    {
      "epoch": 0.8518855065879146,
      "grad_norm": 5.191594123840332,
      "learning_rate": 0.000721654204181291,
      "loss": 0.8513,
      "step": 1875
    },
    {
      "epoch": 0.8523398455247615,
      "grad_norm": 6.490695953369141,
      "learning_rate": 0.0007215016023195483,
      "loss": 1.4365,
      "step": 1876
    },
    {
      "epoch": 0.8527941844616084,
      "grad_norm": 5.439698219299316,
      "learning_rate": 0.0007213490004578056,
      "loss": 0.6865,
      "step": 1877
    },
    {
      "epoch": 0.8532485233984552,
      "grad_norm": 6.476342678070068,
      "learning_rate": 0.0007211963985960629,
      "loss": 1.0204,
      "step": 1878
    },
    {
      "epoch": 0.8537028623353021,
      "grad_norm": 3.7910218238830566,
      "learning_rate": 0.0007210437967343202,
      "loss": 0.4433,
      "step": 1879
    },
    {
      "epoch": 0.854157201272149,
      "grad_norm": 6.71093225479126,
      "learning_rate": 0.0007208911948725775,
      "loss": 1.3994,
      "step": 1880
    },
    {
      "epoch": 0.8546115402089959,
      "grad_norm": 5.0969390869140625,
      "learning_rate": 0.0007207385930108347,
      "loss": 1.0751,
      "step": 1881
    },
    {
      "epoch": 0.8550658791458428,
      "grad_norm": 3.127092123031616,
      "learning_rate": 0.0007205859911490921,
      "loss": 0.2548,
      "step": 1882
    },
    {
      "epoch": 0.8555202180826896,
      "grad_norm": 7.107275485992432,
      "learning_rate": 0.0007204333892873494,
      "loss": 1.2255,
      "step": 1883
    },
    {
      "epoch": 0.8559745570195366,
      "grad_norm": 3.266273021697998,
      "learning_rate": 0.0007202807874256066,
      "loss": 0.4288,
      "step": 1884
    },
    {
      "epoch": 0.8564288959563835,
      "grad_norm": 5.210782527923584,
      "learning_rate": 0.000720128185563864,
      "loss": 0.6763,
      "step": 1885
    },
    {
      "epoch": 0.8568832348932304,
      "grad_norm": 7.342110633850098,
      "learning_rate": 0.0007199755837021211,
      "loss": 1.4129,
      "step": 1886
    },
    {
      "epoch": 0.8573375738300772,
      "grad_norm": 9.53869915008545,
      "learning_rate": 0.0007198229818403784,
      "loss": 0.5519,
      "step": 1887
    },
    {
      "epoch": 0.8577919127669241,
      "grad_norm": 6.55891227722168,
      "learning_rate": 0.0007196703799786357,
      "loss": 1.3736,
      "step": 1888
    },
    {
      "epoch": 0.858246251703771,
      "grad_norm": 3.9626853466033936,
      "learning_rate": 0.000719517778116893,
      "loss": 0.7924,
      "step": 1889
    },
    {
      "epoch": 0.8587005906406179,
      "grad_norm": 6.990512847900391,
      "learning_rate": 0.0007193651762551503,
      "loss": 1.1103,
      "step": 1890
    },
    {
      "epoch": 0.8591549295774648,
      "grad_norm": 8.044767379760742,
      "learning_rate": 0.0007192125743934076,
      "loss": 0.8168,
      "step": 1891
    },
    {
      "epoch": 0.8596092685143116,
      "grad_norm": 5.258362770080566,
      "learning_rate": 0.0007190599725316649,
      "loss": 0.5206,
      "step": 1892
    },
    {
      "epoch": 0.8600636074511586,
      "grad_norm": 6.528013229370117,
      "learning_rate": 0.0007189073706699221,
      "loss": 0.7153,
      "step": 1893
    },
    {
      "epoch": 0.8605179463880055,
      "grad_norm": 7.497963905334473,
      "learning_rate": 0.0007187547688081795,
      "loss": 1.1803,
      "step": 1894
    },
    {
      "epoch": 0.8609722853248524,
      "grad_norm": 5.800473690032959,
      "learning_rate": 0.0007186021669464368,
      "loss": 0.7142,
      "step": 1895
    },
    {
      "epoch": 0.8614266242616992,
      "grad_norm": 5.07067346572876,
      "learning_rate": 0.000718449565084694,
      "loss": 0.6818,
      "step": 1896
    },
    {
      "epoch": 0.8618809631985461,
      "grad_norm": 4.473248481750488,
      "learning_rate": 0.0007182969632229514,
      "loss": 0.7373,
      "step": 1897
    },
    {
      "epoch": 0.862335302135393,
      "grad_norm": 11.67205810546875,
      "learning_rate": 0.0007181443613612086,
      "loss": 2.0626,
      "step": 1898
    },
    {
      "epoch": 0.8627896410722399,
      "grad_norm": 6.872565746307373,
      "learning_rate": 0.0007179917594994659,
      "loss": 0.8128,
      "step": 1899
    },
    {
      "epoch": 0.8632439800090868,
      "grad_norm": 4.947861671447754,
      "learning_rate": 0.0007178391576377233,
      "loss": 0.6253,
      "step": 1900
    },
    {
      "epoch": 0.8636983189459336,
      "grad_norm": 5.619885444641113,
      "learning_rate": 0.0007176865557759805,
      "loss": 1.3029,
      "step": 1901
    },
    {
      "epoch": 0.8641526578827805,
      "grad_norm": 5.444053649902344,
      "learning_rate": 0.0007175339539142378,
      "loss": 1.1244,
      "step": 1902
    },
    {
      "epoch": 0.8646069968196275,
      "grad_norm": 5.304084777832031,
      "learning_rate": 0.0007173813520524951,
      "loss": 1.151,
      "step": 1903
    },
    {
      "epoch": 0.8650613357564744,
      "grad_norm": 6.978363037109375,
      "learning_rate": 0.0007172287501907523,
      "loss": 1.0198,
      "step": 1904
    },
    {
      "epoch": 0.8655156746933212,
      "grad_norm": 5.182434558868408,
      "learning_rate": 0.0007170761483290095,
      "loss": 1.2855,
      "step": 1905
    },
    {
      "epoch": 0.8659700136301681,
      "grad_norm": 7.5368876457214355,
      "learning_rate": 0.0007169235464672669,
      "loss": 0.8473,
      "step": 1906
    },
    {
      "epoch": 0.866424352567015,
      "grad_norm": 5.3685302734375,
      "learning_rate": 0.0007167709446055242,
      "loss": 0.825,
      "step": 1907
    },
    {
      "epoch": 0.8668786915038619,
      "grad_norm": 7.244801044464111,
      "learning_rate": 0.0007166183427437814,
      "loss": 1.0562,
      "step": 1908
    },
    {
      "epoch": 0.8673330304407088,
      "grad_norm": 4.107983589172363,
      "learning_rate": 0.0007164657408820388,
      "loss": 0.3071,
      "step": 1909
    },
    {
      "epoch": 0.8677873693775556,
      "grad_norm": 6.746215343475342,
      "learning_rate": 0.000716313139020296,
      "loss": 1.9459,
      "step": 1910
    },
    {
      "epoch": 0.8682417083144025,
      "grad_norm": 5.089417457580566,
      "learning_rate": 0.0007161605371585533,
      "loss": 1.5397,
      "step": 1911
    },
    {
      "epoch": 0.8686960472512494,
      "grad_norm": 4.712769031524658,
      "learning_rate": 0.0007160079352968107,
      "loss": 0.8534,
      "step": 1912
    },
    {
      "epoch": 0.8691503861880964,
      "grad_norm": 4.7029876708984375,
      "learning_rate": 0.0007158553334350679,
      "loss": 0.6969,
      "step": 1913
    },
    {
      "epoch": 0.8696047251249432,
      "grad_norm": 6.375208377838135,
      "learning_rate": 0.0007157027315733252,
      "loss": 1.0629,
      "step": 1914
    },
    {
      "epoch": 0.8700590640617901,
      "grad_norm": 4.202013969421387,
      "learning_rate": 0.0007155501297115825,
      "loss": 0.6608,
      "step": 1915
    },
    {
      "epoch": 0.870513402998637,
      "grad_norm": 4.874462127685547,
      "learning_rate": 0.0007153975278498398,
      "loss": 0.6259,
      "step": 1916
    },
    {
      "epoch": 0.8709677419354839,
      "grad_norm": 2.966601848602295,
      "learning_rate": 0.000715244925988097,
      "loss": 0.385,
      "step": 1917
    },
    {
      "epoch": 0.8714220808723308,
      "grad_norm": 5.664721965789795,
      "learning_rate": 0.0007150923241263544,
      "loss": 0.7865,
      "step": 1918
    },
    {
      "epoch": 0.8718764198091776,
      "grad_norm": 4.8374810218811035,
      "learning_rate": 0.0007149397222646117,
      "loss": 0.6713,
      "step": 1919
    },
    {
      "epoch": 0.8723307587460245,
      "grad_norm": 6.022850036621094,
      "learning_rate": 0.0007147871204028689,
      "loss": 1.3627,
      "step": 1920
    },
    {
      "epoch": 0.8727850976828714,
      "grad_norm": 6.103410720825195,
      "learning_rate": 0.0007146345185411263,
      "loss": 0.5571,
      "step": 1921
    },
    {
      "epoch": 0.8732394366197183,
      "grad_norm": 5.345830917358398,
      "learning_rate": 0.0007144819166793834,
      "loss": 0.2844,
      "step": 1922
    },
    {
      "epoch": 0.8736937755565652,
      "grad_norm": 6.236686706542969,
      "learning_rate": 0.0007143293148176407,
      "loss": 1.3729,
      "step": 1923
    },
    {
      "epoch": 0.8741481144934121,
      "grad_norm": 6.340873718261719,
      "learning_rate": 0.0007141767129558981,
      "loss": 0.45,
      "step": 1924
    },
    {
      "epoch": 0.874602453430259,
      "grad_norm": 5.220257759094238,
      "learning_rate": 0.0007140241110941553,
      "loss": 0.8306,
      "step": 1925
    },
    {
      "epoch": 0.8750567923671059,
      "grad_norm": 4.683383941650391,
      "learning_rate": 0.0007138715092324126,
      "loss": 0.4811,
      "step": 1926
    },
    {
      "epoch": 0.8755111313039527,
      "grad_norm": 4.469421863555908,
      "learning_rate": 0.0007137189073706699,
      "loss": 1.1149,
      "step": 1927
    },
    {
      "epoch": 0.8759654702407996,
      "grad_norm": 5.483222484588623,
      "learning_rate": 0.0007135663055089272,
      "loss": 0.6754,
      "step": 1928
    },
    {
      "epoch": 0.8764198091776465,
      "grad_norm": 5.927076816558838,
      "learning_rate": 0.0007134137036471844,
      "loss": 1.1382,
      "step": 1929
    },
    {
      "epoch": 0.8768741481144934,
      "grad_norm": 6.893660545349121,
      "learning_rate": 0.0007132611017854418,
      "loss": 1.3296,
      "step": 1930
    },
    {
      "epoch": 0.8773284870513403,
      "grad_norm": 5.253678798675537,
      "learning_rate": 0.0007131084999236991,
      "loss": 0.672,
      "step": 1931
    },
    {
      "epoch": 0.8777828259881872,
      "grad_norm": 7.308913230895996,
      "learning_rate": 0.0007129558980619563,
      "loss": 1.149,
      "step": 1932
    },
    {
      "epoch": 0.8782371649250341,
      "grad_norm": 7.520915508270264,
      "learning_rate": 0.0007128032962002137,
      "loss": 1.8067,
      "step": 1933
    },
    {
      "epoch": 0.878691503861881,
      "grad_norm": 3.790001392364502,
      "learning_rate": 0.0007126506943384709,
      "loss": 0.4597,
      "step": 1934
    },
    {
      "epoch": 0.8791458427987279,
      "grad_norm": 5.534965991973877,
      "learning_rate": 0.0007124980924767282,
      "loss": 0.8858,
      "step": 1935
    },
    {
      "epoch": 0.8796001817355747,
      "grad_norm": 5.704861640930176,
      "learning_rate": 0.0007123454906149856,
      "loss": 0.5513,
      "step": 1936
    },
    {
      "epoch": 0.8800545206724216,
      "grad_norm": 4.630179405212402,
      "learning_rate": 0.0007121928887532428,
      "loss": 0.7115,
      "step": 1937
    },
    {
      "epoch": 0.8805088596092685,
      "grad_norm": 7.922321796417236,
      "learning_rate": 0.0007120402868915002,
      "loss": 1.9029,
      "step": 1938
    },
    {
      "epoch": 0.8809631985461154,
      "grad_norm": 6.776327610015869,
      "learning_rate": 0.0007118876850297574,
      "loss": 0.9387,
      "step": 1939
    },
    {
      "epoch": 0.8814175374829623,
      "grad_norm": 3.4562690258026123,
      "learning_rate": 0.0007117350831680147,
      "loss": 0.3624,
      "step": 1940
    },
    {
      "epoch": 0.8818718764198091,
      "grad_norm": 5.7279372215271,
      "learning_rate": 0.0007115824813062718,
      "loss": 0.6521,
      "step": 1941
    },
    {
      "epoch": 0.8823262153566561,
      "grad_norm": 5.111639499664307,
      "learning_rate": 0.0007114298794445292,
      "loss": 0.9817,
      "step": 1942
    },
    {
      "epoch": 0.882780554293503,
      "grad_norm": 3.125272512435913,
      "learning_rate": 0.0007112772775827865,
      "loss": 0.3382,
      "step": 1943
    },
    {
      "epoch": 0.8832348932303499,
      "grad_norm": 5.420891761779785,
      "learning_rate": 0.0007111246757210437,
      "loss": 0.7491,
      "step": 1944
    },
    {
      "epoch": 0.8836892321671967,
      "grad_norm": 5.310026168823242,
      "learning_rate": 0.0007109720738593011,
      "loss": 1.1236,
      "step": 1945
    },
    {
      "epoch": 0.8841435711040436,
      "grad_norm": 5.4610443115234375,
      "learning_rate": 0.0007108194719975583,
      "loss": 0.8051,
      "step": 1946
    },
    {
      "epoch": 0.8845979100408905,
      "grad_norm": 4.037468433380127,
      "learning_rate": 0.0007106668701358157,
      "loss": 0.3783,
      "step": 1947
    },
    {
      "epoch": 0.8850522489777374,
      "grad_norm": 8.212574005126953,
      "learning_rate": 0.000710514268274073,
      "loss": 2.0641,
      "step": 1948
    },
    {
      "epoch": 0.8855065879145843,
      "grad_norm": 6.245050430297852,
      "learning_rate": 0.0007103616664123302,
      "loss": 0.6219,
      "step": 1949
    },
    {
      "epoch": 0.8859609268514311,
      "grad_norm": 6.279829502105713,
      "learning_rate": 0.0007102090645505876,
      "loss": 1.1376,
      "step": 1950
    },
    {
      "epoch": 0.886415265788278,
      "grad_norm": 5.423976898193359,
      "learning_rate": 0.0007100564626888448,
      "loss": 0.9512,
      "step": 1951
    },
    {
      "epoch": 0.886869604725125,
      "grad_norm": 5.565937519073486,
      "learning_rate": 0.0007099038608271021,
      "loss": 0.5866,
      "step": 1952
    },
    {
      "epoch": 0.8873239436619719,
      "grad_norm": 3.2887492179870605,
      "learning_rate": 0.0007097512589653595,
      "loss": 0.3798,
      "step": 1953
    },
    {
      "epoch": 0.8877782825988187,
      "grad_norm": 8.716856956481934,
      "learning_rate": 0.0007095986571036167,
      "loss": 1.2801,
      "step": 1954
    },
    {
      "epoch": 0.8882326215356656,
      "grad_norm": 5.98683500289917,
      "learning_rate": 0.000709446055241874,
      "loss": 0.6739,
      "step": 1955
    },
    {
      "epoch": 0.8886869604725125,
      "grad_norm": 3.947787284851074,
      "learning_rate": 0.0007092934533801313,
      "loss": 0.7065,
      "step": 1956
    },
    {
      "epoch": 0.8891412994093594,
      "grad_norm": 2.9420864582061768,
      "learning_rate": 0.0007091408515183886,
      "loss": 0.2127,
      "step": 1957
    },
    {
      "epoch": 0.8895956383462063,
      "grad_norm": 8.21181583404541,
      "learning_rate": 0.0007089882496566459,
      "loss": 1.4819,
      "step": 1958
    },
    {
      "epoch": 0.8900499772830531,
      "grad_norm": 3.9228971004486084,
      "learning_rate": 0.0007088356477949031,
      "loss": 0.3254,
      "step": 1959
    },
    {
      "epoch": 0.8905043162199,
      "grad_norm": 4.058672904968262,
      "learning_rate": 0.0007086830459331604,
      "loss": 0.4195,
      "step": 1960
    },
    {
      "epoch": 0.8909586551567469,
      "grad_norm": 4.327905178070068,
      "learning_rate": 0.0007085304440714176,
      "loss": 0.4077,
      "step": 1961
    },
    {
      "epoch": 0.8914129940935939,
      "grad_norm": 8.722265243530273,
      "learning_rate": 0.000708377842209675,
      "loss": 1.3232,
      "step": 1962
    },
    {
      "epoch": 0.8918673330304407,
      "grad_norm": 6.429074287414551,
      "learning_rate": 0.0007082252403479322,
      "loss": 0.9172,
      "step": 1963
    },
    {
      "epoch": 0.8923216719672876,
      "grad_norm": 2.5801069736480713,
      "learning_rate": 0.0007080726384861895,
      "loss": 0.1898,
      "step": 1964
    },
    {
      "epoch": 0.8927760109041345,
      "grad_norm": 3.4617929458618164,
      "learning_rate": 0.0007079200366244469,
      "loss": 1.1471,
      "step": 1965
    },
    {
      "epoch": 0.8932303498409814,
      "grad_norm": 7.024360179901123,
      "learning_rate": 0.0007077674347627041,
      "loss": 1.1965,
      "step": 1966
    },
    {
      "epoch": 0.8936846887778283,
      "grad_norm": 6.406704425811768,
      "learning_rate": 0.0007076148329009614,
      "loss": 1.2743,
      "step": 1967
    },
    {
      "epoch": 0.8941390277146751,
      "grad_norm": 7.127075672149658,
      "learning_rate": 0.0007074622310392187,
      "loss": 1.783,
      "step": 1968
    },
    {
      "epoch": 0.894593366651522,
      "grad_norm": 6.516239643096924,
      "learning_rate": 0.000707309629177476,
      "loss": 0.8316,
      "step": 1969
    },
    {
      "epoch": 0.8950477055883689,
      "grad_norm": 4.2204132080078125,
      "learning_rate": 0.0007071570273157333,
      "loss": 0.6816,
      "step": 1970
    },
    {
      "epoch": 0.8955020445252158,
      "grad_norm": 4.999958038330078,
      "learning_rate": 0.0007070044254539906,
      "loss": 0.8212,
      "step": 1971
    },
    {
      "epoch": 0.8959563834620627,
      "grad_norm": 3.891820192337036,
      "learning_rate": 0.0007068518235922479,
      "loss": 0.3277,
      "step": 1972
    },
    {
      "epoch": 0.8964107223989096,
      "grad_norm": 4.922902584075928,
      "learning_rate": 0.0007066992217305051,
      "loss": 0.5095,
      "step": 1973
    },
    {
      "epoch": 0.8968650613357565,
      "grad_norm": 4.895660400390625,
      "learning_rate": 0.0007065466198687625,
      "loss": 1.2226,
      "step": 1974
    },
    {
      "epoch": 0.8973194002726034,
      "grad_norm": 7.869164943695068,
      "learning_rate": 0.0007063940180070198,
      "loss": 1.1854,
      "step": 1975
    },
    {
      "epoch": 0.8977737392094502,
      "grad_norm": 6.767821788787842,
      "learning_rate": 0.000706241416145277,
      "loss": 1.1474,
      "step": 1976
    },
    {
      "epoch": 0.8982280781462971,
      "grad_norm": 6.22881555557251,
      "learning_rate": 0.0007060888142835343,
      "loss": 0.8912,
      "step": 1977
    },
    {
      "epoch": 0.898682417083144,
      "grad_norm": 6.987730503082275,
      "learning_rate": 0.0007059362124217915,
      "loss": 0.9931,
      "step": 1978
    },
    {
      "epoch": 0.8991367560199909,
      "grad_norm": 5.792884349822998,
      "learning_rate": 0.0007057836105600488,
      "loss": 1.2982,
      "step": 1979
    },
    {
      "epoch": 0.8995910949568378,
      "grad_norm": 7.034243106842041,
      "learning_rate": 0.0007056310086983061,
      "loss": 0.8764,
      "step": 1980
    },
    {
      "epoch": 0.9000454338936847,
      "grad_norm": 4.895236968994141,
      "learning_rate": 0.0007054784068365634,
      "loss": 0.8057,
      "step": 1981
    },
    {
      "epoch": 0.9004997728305316,
      "grad_norm": 5.484593868255615,
      "learning_rate": 0.0007053258049748207,
      "loss": 0.5558,
      "step": 1982
    },
    {
      "epoch": 0.9009541117673785,
      "grad_norm": 5.7938232421875,
      "learning_rate": 0.000705173203113078,
      "loss": 0.6774,
      "step": 1983
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 4.93589973449707,
      "learning_rate": 0.0007050206012513353,
      "loss": 0.5062,
      "step": 1984
    },
    {
      "epoch": 0.9018627896410722,
      "grad_norm": 2.7446393966674805,
      "learning_rate": 0.0007048679993895925,
      "loss": 0.7101,
      "step": 1985
    },
    {
      "epoch": 0.9023171285779191,
      "grad_norm": 5.69279670715332,
      "learning_rate": 0.0007047153975278499,
      "loss": 1.3304,
      "step": 1986
    },
    {
      "epoch": 0.902771467514766,
      "grad_norm": 6.147347450256348,
      "learning_rate": 0.0007045627956661072,
      "loss": 0.4917,
      "step": 1987
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 4.808304309844971,
      "learning_rate": 0.0007044101938043644,
      "loss": 0.7245,
      "step": 1988
    },
    {
      "epoch": 0.9036801453884598,
      "grad_norm": 3.0480692386627197,
      "learning_rate": 0.0007042575919426218,
      "loss": 0.5023,
      "step": 1989
    },
    {
      "epoch": 0.9041344843253066,
      "grad_norm": 4.809059143066406,
      "learning_rate": 0.000704104990080879,
      "loss": 0.7064,
      "step": 1990
    },
    {
      "epoch": 0.9045888232621536,
      "grad_norm": 5.870518207550049,
      "learning_rate": 0.0007039523882191363,
      "loss": 1.2094,
      "step": 1991
    },
    {
      "epoch": 0.9050431621990005,
      "grad_norm": 4.002080917358398,
      "learning_rate": 0.0007037997863573937,
      "loss": 0.5494,
      "step": 1992
    },
    {
      "epoch": 0.9054975011358474,
      "grad_norm": 4.997768878936768,
      "learning_rate": 0.0007036471844956509,
      "loss": 0.9387,
      "step": 1993
    },
    {
      "epoch": 0.9059518400726942,
      "grad_norm": 7.832595348358154,
      "learning_rate": 0.0007034945826339082,
      "loss": 0.8402,
      "step": 1994
    },
    {
      "epoch": 0.9064061790095411,
      "grad_norm": 7.167962551116943,
      "learning_rate": 0.0007033419807721654,
      "loss": 0.7674,
      "step": 1995
    },
    {
      "epoch": 0.906860517946388,
      "grad_norm": 4.659988880157471,
      "learning_rate": 0.0007031893789104227,
      "loss": 0.3989,
      "step": 1996
    },
    {
      "epoch": 0.9073148568832349,
      "grad_norm": 5.15620231628418,
      "learning_rate": 0.0007030367770486799,
      "loss": 0.8569,
      "step": 1997
    },
    {
      "epoch": 0.9077691958200818,
      "grad_norm": 5.403616905212402,
      "learning_rate": 0.0007028841751869373,
      "loss": 0.7395,
      "step": 1998
    },
    {
      "epoch": 0.9082235347569286,
      "grad_norm": 6.569815635681152,
      "learning_rate": 0.0007027315733251946,
      "loss": 0.7654,
      "step": 1999
    },
    {
      "epoch": 0.9086778736937755,
      "grad_norm": 6.170792579650879,
      "learning_rate": 0.0007025789714634518,
      "loss": 1.2942,
      "step": 2000
    },
    {
      "epoch": 0.9091322126306225,
      "grad_norm": 7.311605453491211,
      "learning_rate": 0.0007024263696017092,
      "loss": 1.2642,
      "step": 2001
    },
    {
      "epoch": 0.9095865515674694,
      "grad_norm": 5.122086048126221,
      "learning_rate": 0.0007022737677399664,
      "loss": 0.65,
      "step": 2002
    },
    {
      "epoch": 0.9100408905043162,
      "grad_norm": 7.785153388977051,
      "learning_rate": 0.0007021211658782237,
      "loss": 1.4613,
      "step": 2003
    },
    {
      "epoch": 0.9104952294411631,
      "grad_norm": 5.780588626861572,
      "learning_rate": 0.0007019685640164811,
      "loss": 1.1231,
      "step": 2004
    },
    {
      "epoch": 0.91094956837801,
      "grad_norm": 5.931447982788086,
      "learning_rate": 0.0007018159621547383,
      "loss": 0.6552,
      "step": 2005
    },
    {
      "epoch": 0.9114039073148569,
      "grad_norm": 4.565131187438965,
      "learning_rate": 0.0007016633602929956,
      "loss": 0.5371,
      "step": 2006
    },
    {
      "epoch": 0.9118582462517038,
      "grad_norm": 6.071425437927246,
      "learning_rate": 0.0007015107584312529,
      "loss": 1.0,
      "step": 2007
    },
    {
      "epoch": 0.9123125851885506,
      "grad_norm": 5.873400688171387,
      "learning_rate": 0.0007013581565695102,
      "loss": 1.1207,
      "step": 2008
    },
    {
      "epoch": 0.9127669241253975,
      "grad_norm": 4.6697306632995605,
      "learning_rate": 0.0007012055547077675,
      "loss": 0.3824,
      "step": 2009
    },
    {
      "epoch": 0.9132212630622444,
      "grad_norm": 3.4053189754486084,
      "learning_rate": 0.0007010529528460248,
      "loss": 0.8735,
      "step": 2010
    },
    {
      "epoch": 0.9136756019990914,
      "grad_norm": 6.547510623931885,
      "learning_rate": 0.0007009003509842821,
      "loss": 1.4251,
      "step": 2011
    },
    {
      "epoch": 0.9141299409359382,
      "grad_norm": 7.330219268798828,
      "learning_rate": 0.0007007477491225393,
      "loss": 1.3606,
      "step": 2012
    },
    {
      "epoch": 0.9145842798727851,
      "grad_norm": 5.19648551940918,
      "learning_rate": 0.0007005951472607967,
      "loss": 0.7697,
      "step": 2013
    },
    {
      "epoch": 0.915038618809632,
      "grad_norm": 4.557535171508789,
      "learning_rate": 0.0007004425453990538,
      "loss": 0.641,
      "step": 2014
    },
    {
      "epoch": 0.9154929577464789,
      "grad_norm": 5.209771156311035,
      "learning_rate": 0.0007002899435373111,
      "loss": 1.0701,
      "step": 2015
    },
    {
      "epoch": 0.9159472966833258,
      "grad_norm": 6.498602390289307,
      "learning_rate": 0.0007001373416755685,
      "loss": 1.3783,
      "step": 2016
    },
    {
      "epoch": 0.9164016356201726,
      "grad_norm": 3.1994919776916504,
      "learning_rate": 0.0006999847398138257,
      "loss": 0.2153,
      "step": 2017
    },
    {
      "epoch": 0.9168559745570195,
      "grad_norm": 4.121938228607178,
      "learning_rate": 0.000699832137952083,
      "loss": 0.4254,
      "step": 2018
    },
    {
      "epoch": 0.9173103134938664,
      "grad_norm": 5.196599960327148,
      "learning_rate": 0.0006996795360903403,
      "loss": 0.8877,
      "step": 2019
    },
    {
      "epoch": 0.9177646524307133,
      "grad_norm": 4.127929210662842,
      "learning_rate": 0.0006995269342285976,
      "loss": 0.4904,
      "step": 2020
    },
    {
      "epoch": 0.9182189913675602,
      "grad_norm": 7.0154032707214355,
      "learning_rate": 0.0006993743323668549,
      "loss": 1.6387,
      "step": 2021
    },
    {
      "epoch": 0.9186733303044071,
      "grad_norm": 10.962752342224121,
      "learning_rate": 0.0006992217305051122,
      "loss": 1.3821,
      "step": 2022
    },
    {
      "epoch": 0.919127669241254,
      "grad_norm": 5.344366550445557,
      "learning_rate": 0.0006990691286433695,
      "loss": 0.9465,
      "step": 2023
    },
    {
      "epoch": 0.9195820081781009,
      "grad_norm": 4.537943363189697,
      "learning_rate": 0.0006989165267816267,
      "loss": 1.1446,
      "step": 2024
    },
    {
      "epoch": 0.9200363471149478,
      "grad_norm": 6.657045841217041,
      "learning_rate": 0.0006987639249198841,
      "loss": 1.2017,
      "step": 2025
    },
    {
      "epoch": 0.9204906860517946,
      "grad_norm": 5.906113624572754,
      "learning_rate": 0.0006986113230581414,
      "loss": 0.8577,
      "step": 2026
    },
    {
      "epoch": 0.9209450249886415,
      "grad_norm": 7.203835487365723,
      "learning_rate": 0.0006984587211963986,
      "loss": 1.8921,
      "step": 2027
    },
    {
      "epoch": 0.9213993639254884,
      "grad_norm": 6.7763776779174805,
      "learning_rate": 0.000698306119334656,
      "loss": 0.9233,
      "step": 2028
    },
    {
      "epoch": 0.9218537028623353,
      "grad_norm": 5.944580554962158,
      "learning_rate": 0.0006981535174729132,
      "loss": 1.1253,
      "step": 2029
    },
    {
      "epoch": 0.9223080417991822,
      "grad_norm": 5.834329605102539,
      "learning_rate": 0.0006980009156111705,
      "loss": 0.7614,
      "step": 2030
    },
    {
      "epoch": 0.9227623807360291,
      "grad_norm": 6.8939409255981445,
      "learning_rate": 0.0006978483137494279,
      "loss": 1.1708,
      "step": 2031
    },
    {
      "epoch": 0.923216719672876,
      "grad_norm": 6.764132499694824,
      "learning_rate": 0.000697695711887685,
      "loss": 1.8177,
      "step": 2032
    },
    {
      "epoch": 0.9236710586097229,
      "grad_norm": 8.85224723815918,
      "learning_rate": 0.0006975431100259423,
      "loss": 1.6865,
      "step": 2033
    },
    {
      "epoch": 0.9241253975465697,
      "grad_norm": 5.285190582275391,
      "learning_rate": 0.0006973905081641996,
      "loss": 1.0406,
      "step": 2034
    },
    {
      "epoch": 0.9245797364834166,
      "grad_norm": 6.144356727600098,
      "learning_rate": 0.0006972379063024569,
      "loss": 0.9666,
      "step": 2035
    },
    {
      "epoch": 0.9250340754202635,
      "grad_norm": 7.395075798034668,
      "learning_rate": 0.0006970853044407141,
      "loss": 1.1379,
      "step": 2036
    },
    {
      "epoch": 0.9254884143571104,
      "grad_norm": 4.384398460388184,
      "learning_rate": 0.0006969327025789715,
      "loss": 0.7626,
      "step": 2037
    },
    {
      "epoch": 0.9259427532939573,
      "grad_norm": 8.474690437316895,
      "learning_rate": 0.0006967801007172288,
      "loss": 1.5839,
      "step": 2038
    },
    {
      "epoch": 0.9263970922308041,
      "grad_norm": 2.750568151473999,
      "learning_rate": 0.000696627498855486,
      "loss": 0.2265,
      "step": 2039
    },
    {
      "epoch": 0.9268514311676511,
      "grad_norm": 4.0753092765808105,
      "learning_rate": 0.0006964748969937434,
      "loss": 0.763,
      "step": 2040
    },
    {
      "epoch": 0.927305770104498,
      "grad_norm": 5.4336090087890625,
      "learning_rate": 0.0006963222951320006,
      "loss": 0.5569,
      "step": 2041
    },
    {
      "epoch": 0.9277601090413449,
      "grad_norm": 5.577949047088623,
      "learning_rate": 0.0006961696932702579,
      "loss": 0.7325,
      "step": 2042
    },
    {
      "epoch": 0.9282144479781917,
      "grad_norm": 4.524528980255127,
      "learning_rate": 0.0006960170914085153,
      "loss": 0.8214,
      "step": 2043
    },
    {
      "epoch": 0.9286687869150386,
      "grad_norm": 4.559654712677002,
      "learning_rate": 0.0006958644895467725,
      "loss": 1.2702,
      "step": 2044
    },
    {
      "epoch": 0.9291231258518855,
      "grad_norm": 8.559505462646484,
      "learning_rate": 0.0006957118876850298,
      "loss": 1.0067,
      "step": 2045
    },
    {
      "epoch": 0.9295774647887324,
      "grad_norm": 5.511291980743408,
      "learning_rate": 0.0006955592858232871,
      "loss": 0.9499,
      "step": 2046
    },
    {
      "epoch": 0.9300318037255793,
      "grad_norm": 5.140843868255615,
      "learning_rate": 0.0006954066839615444,
      "loss": 0.3305,
      "step": 2047
    },
    {
      "epoch": 0.9304861426624261,
      "grad_norm": 5.420475959777832,
      "learning_rate": 0.0006952540820998016,
      "loss": 0.7938,
      "step": 2048
    },
    {
      "epoch": 0.930940481599273,
      "grad_norm": 4.2873101234436035,
      "learning_rate": 0.000695101480238059,
      "loss": 0.4467,
      "step": 2049
    },
    {
      "epoch": 0.93139482053612,
      "grad_norm": 5.8762407302856445,
      "learning_rate": 0.0006949488783763162,
      "loss": 0.7616,
      "step": 2050
    },
    {
      "epoch": 0.9318491594729669,
      "grad_norm": 4.38421630859375,
      "learning_rate": 0.0006947962765145734,
      "loss": 0.5827,
      "step": 2051
    },
    {
      "epoch": 0.9323034984098137,
      "grad_norm": 3.084681987762451,
      "learning_rate": 0.0006946436746528308,
      "loss": 0.3552,
      "step": 2052
    },
    {
      "epoch": 0.9327578373466606,
      "grad_norm": 6.678351402282715,
      "learning_rate": 0.000694491072791088,
      "loss": 0.8168,
      "step": 2053
    },
    {
      "epoch": 0.9332121762835075,
      "grad_norm": 4.735074043273926,
      "learning_rate": 0.0006943384709293453,
      "loss": 0.6629,
      "step": 2054
    },
    {
      "epoch": 0.9336665152203544,
      "grad_norm": 6.248415470123291,
      "learning_rate": 0.0006941858690676027,
      "loss": 0.9084,
      "step": 2055
    },
    {
      "epoch": 0.9341208541572013,
      "grad_norm": 7.409706115722656,
      "learning_rate": 0.0006940332672058599,
      "loss": 0.9223,
      "step": 2056
    },
    {
      "epoch": 0.9345751930940481,
      "grad_norm": 4.350240707397461,
      "learning_rate": 0.0006938806653441172,
      "loss": 0.2766,
      "step": 2057
    },
    {
      "epoch": 0.935029532030895,
      "grad_norm": 7.194037914276123,
      "learning_rate": 0.0006937280634823745,
      "loss": 1.4067,
      "step": 2058
    },
    {
      "epoch": 0.9354838709677419,
      "grad_norm": 7.984813690185547,
      "learning_rate": 0.0006935754616206318,
      "loss": 1.1086,
      "step": 2059
    },
    {
      "epoch": 0.9359382099045889,
      "grad_norm": 5.023776054382324,
      "learning_rate": 0.000693422859758889,
      "loss": 0.8641,
      "step": 2060
    },
    {
      "epoch": 0.9363925488414357,
      "grad_norm": 6.6793317794799805,
      "learning_rate": 0.0006932702578971464,
      "loss": 0.9157,
      "step": 2061
    },
    {
      "epoch": 0.9368468877782826,
      "grad_norm": 4.723928451538086,
      "learning_rate": 0.0006931176560354037,
      "loss": 0.4177,
      "step": 2062
    },
    {
      "epoch": 0.9373012267151295,
      "grad_norm": 3.9985225200653076,
      "learning_rate": 0.0006929650541736609,
      "loss": 0.1684,
      "step": 2063
    },
    {
      "epoch": 0.9377555656519764,
      "grad_norm": 6.180245399475098,
      "learning_rate": 0.0006928124523119183,
      "loss": 1.0942,
      "step": 2064
    },
    {
      "epoch": 0.9382099045888233,
      "grad_norm": 6.659726142883301,
      "learning_rate": 0.0006926598504501755,
      "loss": 0.9402,
      "step": 2065
    },
    {
      "epoch": 0.9386642435256701,
      "grad_norm": 6.623106002807617,
      "learning_rate": 0.0006925072485884328,
      "loss": 0.7785,
      "step": 2066
    },
    {
      "epoch": 0.939118582462517,
      "grad_norm": 4.454534530639648,
      "learning_rate": 0.0006923546467266902,
      "loss": 0.4756,
      "step": 2067
    },
    {
      "epoch": 0.9395729213993639,
      "grad_norm": 4.622603416442871,
      "learning_rate": 0.0006922020448649474,
      "loss": 0.8102,
      "step": 2068
    },
    {
      "epoch": 0.9400272603362109,
      "grad_norm": 5.439027309417725,
      "learning_rate": 0.0006920494430032046,
      "loss": 0.7365,
      "step": 2069
    },
    {
      "epoch": 0.9404815992730577,
      "grad_norm": 8.632781028747559,
      "learning_rate": 0.0006918968411414619,
      "loss": 1.1332,
      "step": 2070
    },
    {
      "epoch": 0.9409359382099046,
      "grad_norm": 4.63347053527832,
      "learning_rate": 0.0006917442392797192,
      "loss": 0.715,
      "step": 2071
    },
    {
      "epoch": 0.9413902771467515,
      "grad_norm": 4.5464582443237305,
      "learning_rate": 0.0006915916374179764,
      "loss": 0.8656,
      "step": 2072
    },
    {
      "epoch": 0.9418446160835984,
      "grad_norm": 3.617842197418213,
      "learning_rate": 0.0006914390355562338,
      "loss": 0.5519,
      "step": 2073
    },
    {
      "epoch": 0.9422989550204453,
      "grad_norm": 5.072118759155273,
      "learning_rate": 0.0006912864336944911,
      "loss": 0.7861,
      "step": 2074
    },
    {
      "epoch": 0.9427532939572921,
      "grad_norm": 5.842392921447754,
      "learning_rate": 0.0006911338318327483,
      "loss": 1.0794,
      "step": 2075
    },
    {
      "epoch": 0.943207632894139,
      "grad_norm": 6.1749372482299805,
      "learning_rate": 0.0006909812299710057,
      "loss": 0.9633,
      "step": 2076
    },
    {
      "epoch": 0.9436619718309859,
      "grad_norm": 5.732656002044678,
      "learning_rate": 0.0006908286281092629,
      "loss": 0.4645,
      "step": 2077
    },
    {
      "epoch": 0.9441163107678328,
      "grad_norm": 6.043637752532959,
      "learning_rate": 0.0006906760262475202,
      "loss": 1.0011,
      "step": 2078
    },
    {
      "epoch": 0.9445706497046797,
      "grad_norm": 5.471591949462891,
      "learning_rate": 0.0006905234243857776,
      "loss": 0.3433,
      "step": 2079
    },
    {
      "epoch": 0.9450249886415266,
      "grad_norm": 6.425339698791504,
      "learning_rate": 0.0006903708225240348,
      "loss": 1.0389,
      "step": 2080
    },
    {
      "epoch": 0.9454793275783735,
      "grad_norm": 4.384903907775879,
      "learning_rate": 0.0006902182206622921,
      "loss": 0.356,
      "step": 2081
    },
    {
      "epoch": 0.9459336665152204,
      "grad_norm": 3.6266977787017822,
      "learning_rate": 0.0006900656188005494,
      "loss": 0.275,
      "step": 2082
    },
    {
      "epoch": 0.9463880054520672,
      "grad_norm": 5.804725646972656,
      "learning_rate": 0.0006899130169388067,
      "loss": 0.7105,
      "step": 2083
    },
    {
      "epoch": 0.9468423443889141,
      "grad_norm": 5.287444591522217,
      "learning_rate": 0.000689760415077064,
      "loss": 0.8226,
      "step": 2084
    },
    {
      "epoch": 0.947296683325761,
      "grad_norm": 4.906837463378906,
      "learning_rate": 0.0006896078132153213,
      "loss": 0.6542,
      "step": 2085
    },
    {
      "epoch": 0.9477510222626079,
      "grad_norm": 3.956883430480957,
      "learning_rate": 0.0006894552113535786,
      "loss": 0.7667,
      "step": 2086
    },
    {
      "epoch": 0.9482053611994548,
      "grad_norm": 5.840696811676025,
      "learning_rate": 0.0006893026094918357,
      "loss": 1.2249,
      "step": 2087
    },
    {
      "epoch": 0.9486597001363016,
      "grad_norm": 8.414755821228027,
      "learning_rate": 0.0006891500076300931,
      "loss": 0.8647,
      "step": 2088
    },
    {
      "epoch": 0.9491140390731486,
      "grad_norm": 2.8077991008758545,
      "learning_rate": 0.0006889974057683503,
      "loss": 0.4209,
      "step": 2089
    },
    {
      "epoch": 0.9495683780099955,
      "grad_norm": 5.932242393493652,
      "learning_rate": 0.0006888448039066076,
      "loss": 1.2927,
      "step": 2090
    },
    {
      "epoch": 0.9500227169468424,
      "grad_norm": 2.5040533542633057,
      "learning_rate": 0.000688692202044865,
      "loss": 0.1868,
      "step": 2091
    },
    {
      "epoch": 0.9504770558836892,
      "grad_norm": 6.4902448654174805,
      "learning_rate": 0.0006885396001831222,
      "loss": 0.5767,
      "step": 2092
    },
    {
      "epoch": 0.9509313948205361,
      "grad_norm": 9.10944652557373,
      "learning_rate": 0.0006883869983213795,
      "loss": 0.9808,
      "step": 2093
    },
    {
      "epoch": 0.951385733757383,
      "grad_norm": 5.4667887687683105,
      "learning_rate": 0.0006882343964596368,
      "loss": 0.6674,
      "step": 2094
    },
    {
      "epoch": 0.9518400726942299,
      "grad_norm": 7.508358001708984,
      "learning_rate": 0.0006880817945978941,
      "loss": 1.5717,
      "step": 2095
    },
    {
      "epoch": 0.9522944116310768,
      "grad_norm": 5.231922626495361,
      "learning_rate": 0.0006879291927361514,
      "loss": 0.7213,
      "step": 2096
    },
    {
      "epoch": 0.9527487505679236,
      "grad_norm": 5.3546247482299805,
      "learning_rate": 0.0006877765908744087,
      "loss": 0.5994,
      "step": 2097
    },
    {
      "epoch": 0.9532030895047705,
      "grad_norm": 9.389524459838867,
      "learning_rate": 0.000687623989012666,
      "loss": 1.4907,
      "step": 2098
    },
    {
      "epoch": 0.9536574284416175,
      "grad_norm": 10.425429344177246,
      "learning_rate": 0.0006874713871509232,
      "loss": 1.5236,
      "step": 2099
    },
    {
      "epoch": 0.9541117673784644,
      "grad_norm": 7.4511332511901855,
      "learning_rate": 0.0006873187852891806,
      "loss": 1.6015,
      "step": 2100
    },
    {
      "epoch": 0.9545661063153112,
      "grad_norm": 3.929046869277954,
      "learning_rate": 0.0006871661834274379,
      "loss": 0.7645,
      "step": 2101
    },
    {
      "epoch": 0.9550204452521581,
      "grad_norm": 5.187803745269775,
      "learning_rate": 0.0006870135815656951,
      "loss": 0.59,
      "step": 2102
    },
    {
      "epoch": 0.955474784189005,
      "grad_norm": 8.772862434387207,
      "learning_rate": 0.0006868609797039525,
      "loss": 1.9155,
      "step": 2103
    },
    {
      "epoch": 0.9559291231258519,
      "grad_norm": 6.9824371337890625,
      "learning_rate": 0.0006867083778422097,
      "loss": 0.8507,
      "step": 2104
    },
    {
      "epoch": 0.9563834620626988,
      "grad_norm": 7.279548645019531,
      "learning_rate": 0.0006865557759804669,
      "loss": 1.0064,
      "step": 2105
    },
    {
      "epoch": 0.9568378009995456,
      "grad_norm": 4.085732936859131,
      "learning_rate": 0.0006864031741187242,
      "loss": 0.4949,
      "step": 2106
    },
    {
      "epoch": 0.9572921399363925,
      "grad_norm": 4.949189186096191,
      "learning_rate": 0.0006862505722569815,
      "loss": 0.2295,
      "step": 2107
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 4.102118015289307,
      "learning_rate": 0.0006860979703952388,
      "loss": 0.3919,
      "step": 2108
    },
    {
      "epoch": 0.9582008178100864,
      "grad_norm": 5.828589916229248,
      "learning_rate": 0.0006859453685334961,
      "loss": 1.1843,
      "step": 2109
    },
    {
      "epoch": 0.9586551567469332,
      "grad_norm": 5.934272766113281,
      "learning_rate": 0.0006857927666717534,
      "loss": 0.8919,
      "step": 2110
    },
    {
      "epoch": 0.9591094956837801,
      "grad_norm": 7.086771011352539,
      "learning_rate": 0.0006856401648100106,
      "loss": 1.4282,
      "step": 2111
    },
    {
      "epoch": 0.959563834620627,
      "grad_norm": 9.648992538452148,
      "learning_rate": 0.000685487562948268,
      "loss": 1.1776,
      "step": 2112
    },
    {
      "epoch": 0.9600181735574739,
      "grad_norm": 5.124054908752441,
      "learning_rate": 0.0006853349610865253,
      "loss": 0.8797,
      "step": 2113
    },
    {
      "epoch": 0.9604725124943208,
      "grad_norm": 6.952889442443848,
      "learning_rate": 0.0006851823592247825,
      "loss": 0.7117,
      "step": 2114
    },
    {
      "epoch": 0.9609268514311676,
      "grad_norm": 4.106746196746826,
      "learning_rate": 0.0006850297573630399,
      "loss": 0.4156,
      "step": 2115
    },
    {
      "epoch": 0.9613811903680145,
      "grad_norm": 4.806198596954346,
      "learning_rate": 0.0006848771555012971,
      "loss": 0.905,
      "step": 2116
    },
    {
      "epoch": 0.9618355293048614,
      "grad_norm": 6.044643878936768,
      "learning_rate": 0.0006847245536395544,
      "loss": 0.8575,
      "step": 2117
    },
    {
      "epoch": 0.9622898682417084,
      "grad_norm": 5.374110221862793,
      "learning_rate": 0.0006845719517778118,
      "loss": 1.0996,
      "step": 2118
    },
    {
      "epoch": 0.9627442071785552,
      "grad_norm": 9.840723991394043,
      "learning_rate": 0.000684419349916069,
      "loss": 1.3672,
      "step": 2119
    },
    {
      "epoch": 0.9631985461154021,
      "grad_norm": 6.419771671295166,
      "learning_rate": 0.0006842667480543263,
      "loss": 1.977,
      "step": 2120
    },
    {
      "epoch": 0.963652885052249,
      "grad_norm": 4.817221641540527,
      "learning_rate": 0.0006841141461925836,
      "loss": 0.7205,
      "step": 2121
    },
    {
      "epoch": 0.9641072239890959,
      "grad_norm": 5.4574055671691895,
      "learning_rate": 0.0006839615443308409,
      "loss": 0.8091,
      "step": 2122
    },
    {
      "epoch": 0.9645615629259428,
      "grad_norm": 4.344059944152832,
      "learning_rate": 0.000683808942469098,
      "loss": 0.744,
      "step": 2123
    },
    {
      "epoch": 0.9650159018627896,
      "grad_norm": 3.786834478378296,
      "learning_rate": 0.0006836563406073554,
      "loss": 0.5783,
      "step": 2124
    },
    {
      "epoch": 0.9654702407996365,
      "grad_norm": 7.1932854652404785,
      "learning_rate": 0.0006835037387456127,
      "loss": 0.6766,
      "step": 2125
    },
    {
      "epoch": 0.9659245797364834,
      "grad_norm": 10.507695198059082,
      "learning_rate": 0.0006833511368838699,
      "loss": 0.908,
      "step": 2126
    },
    {
      "epoch": 0.9663789186733303,
      "grad_norm": 4.682920932769775,
      "learning_rate": 0.0006831985350221273,
      "loss": 0.4191,
      "step": 2127
    },
    {
      "epoch": 0.9668332576101772,
      "grad_norm": 5.872903823852539,
      "learning_rate": 0.0006830459331603845,
      "loss": 0.4046,
      "step": 2128
    },
    {
      "epoch": 0.9672875965470241,
      "grad_norm": 4.4036712646484375,
      "learning_rate": 0.0006828933312986418,
      "loss": 0.593,
      "step": 2129
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 6.087384223937988,
      "learning_rate": 0.0006827407294368992,
      "loss": 1.383,
      "step": 2130
    },
    {
      "epoch": 0.9681962744207179,
      "grad_norm": 2.363961696624756,
      "learning_rate": 0.0006825881275751564,
      "loss": 0.2128,
      "step": 2131
    },
    {
      "epoch": 0.9686506133575647,
      "grad_norm": 6.008056163787842,
      "learning_rate": 0.0006824355257134137,
      "loss": 0.6314,
      "step": 2132
    },
    {
      "epoch": 0.9691049522944116,
      "grad_norm": 6.250514984130859,
      "learning_rate": 0.000682282923851671,
      "loss": 0.9227,
      "step": 2133
    },
    {
      "epoch": 0.9695592912312585,
      "grad_norm": 9.159768104553223,
      "learning_rate": 0.0006821303219899283,
      "loss": 1.4347,
      "step": 2134
    },
    {
      "epoch": 0.9700136301681054,
      "grad_norm": 3.502830982208252,
      "learning_rate": 0.0006819777201281856,
      "loss": 0.418,
      "step": 2135
    },
    {
      "epoch": 0.9704679691049523,
      "grad_norm": 5.548464775085449,
      "learning_rate": 0.0006818251182664429,
      "loss": 0.5922,
      "step": 2136
    },
    {
      "epoch": 0.9709223080417991,
      "grad_norm": 5.369658946990967,
      "learning_rate": 0.0006816725164047002,
      "loss": 0.8621,
      "step": 2137
    },
    {
      "epoch": 0.9713766469786461,
      "grad_norm": 4.654785633087158,
      "learning_rate": 0.0006815199145429574,
      "loss": 0.7539,
      "step": 2138
    },
    {
      "epoch": 0.971830985915493,
      "grad_norm": 5.200718879699707,
      "learning_rate": 0.0006813673126812148,
      "loss": 0.8193,
      "step": 2139
    },
    {
      "epoch": 0.9722853248523399,
      "grad_norm": 7.549620628356934,
      "learning_rate": 0.000681214710819472,
      "loss": 1.4674,
      "step": 2140
    },
    {
      "epoch": 0.9727396637891867,
      "grad_norm": 4.965696334838867,
      "learning_rate": 0.0006810621089577293,
      "loss": 0.7305,
      "step": 2141
    },
    {
      "epoch": 0.9731940027260336,
      "grad_norm": 2.3229005336761475,
      "learning_rate": 0.0006809095070959866,
      "loss": 0.175,
      "step": 2142
    },
    {
      "epoch": 0.9736483416628805,
      "grad_norm": 5.4697041511535645,
      "learning_rate": 0.0006807569052342438,
      "loss": 1.1774,
      "step": 2143
    },
    {
      "epoch": 0.9741026805997274,
      "grad_norm": 5.711564540863037,
      "learning_rate": 0.0006806043033725011,
      "loss": 1.1203,
      "step": 2144
    },
    {
      "epoch": 0.9745570195365743,
      "grad_norm": 5.822653293609619,
      "learning_rate": 0.0006804517015107584,
      "loss": 0.7087,
      "step": 2145
    },
    {
      "epoch": 0.9750113584734211,
      "grad_norm": 8.34938907623291,
      "learning_rate": 0.0006802990996490157,
      "loss": 1.6725,
      "step": 2146
    },
    {
      "epoch": 0.975465697410268,
      "grad_norm": 4.827529430389404,
      "learning_rate": 0.000680146497787273,
      "loss": 0.4629,
      "step": 2147
    },
    {
      "epoch": 0.975920036347115,
      "grad_norm": 3.9937901496887207,
      "learning_rate": 0.0006799938959255303,
      "loss": 0.5219,
      "step": 2148
    },
    {
      "epoch": 0.9763743752839619,
      "grad_norm": 8.121403694152832,
      "learning_rate": 0.0006798412940637876,
      "loss": 0.7354,
      "step": 2149
    },
    {
      "epoch": 0.9768287142208087,
      "grad_norm": 9.27087688446045,
      "learning_rate": 0.0006796886922020448,
      "loss": 0.6956,
      "step": 2150
    },
    {
      "epoch": 0.9772830531576556,
      "grad_norm": 4.18889045715332,
      "learning_rate": 0.0006795360903403022,
      "loss": 0.8185,
      "step": 2151
    },
    {
      "epoch": 0.9777373920945025,
      "grad_norm": 3.8327982425689697,
      "learning_rate": 0.0006793834884785595,
      "loss": 0.4558,
      "step": 2152
    },
    {
      "epoch": 0.9781917310313494,
      "grad_norm": 6.747805595397949,
      "learning_rate": 0.0006792308866168167,
      "loss": 1.2008,
      "step": 2153
    },
    {
      "epoch": 0.9786460699681963,
      "grad_norm": 5.454352378845215,
      "learning_rate": 0.0006790782847550741,
      "loss": 0.5254,
      "step": 2154
    },
    {
      "epoch": 0.9791004089050431,
      "grad_norm": 6.983964920043945,
      "learning_rate": 0.0006789256828933313,
      "loss": 1.5219,
      "step": 2155
    },
    {
      "epoch": 0.97955474784189,
      "grad_norm": 6.95999813079834,
      "learning_rate": 0.0006787730810315886,
      "loss": 0.6064,
      "step": 2156
    },
    {
      "epoch": 0.980009086778737,
      "grad_norm": 7.03669548034668,
      "learning_rate": 0.000678620479169846,
      "loss": 0.9904,
      "step": 2157
    },
    {
      "epoch": 0.9804634257155839,
      "grad_norm": 8.695530891418457,
      "learning_rate": 0.0006784678773081032,
      "loss": 0.9068,
      "step": 2158
    },
    {
      "epoch": 0.9809177646524307,
      "grad_norm": 6.718227863311768,
      "learning_rate": 0.0006783152754463605,
      "loss": 0.8031,
      "step": 2159
    },
    {
      "epoch": 0.9813721035892776,
      "grad_norm": 6.190205097198486,
      "learning_rate": 0.0006781626735846177,
      "loss": 0.5617,
      "step": 2160
    },
    {
      "epoch": 0.9818264425261245,
      "grad_norm": 6.773263931274414,
      "learning_rate": 0.000678010071722875,
      "loss": 1.1928,
      "step": 2161
    },
    {
      "epoch": 0.9822807814629714,
      "grad_norm": 4.189763069152832,
      "learning_rate": 0.0006778574698611322,
      "loss": 0.589,
      "step": 2162
    },
    {
      "epoch": 0.9827351203998183,
      "grad_norm": 6.7655487060546875,
      "learning_rate": 0.0006777048679993896,
      "loss": 1.6225,
      "step": 2163
    },
    {
      "epoch": 0.9831894593366651,
      "grad_norm": 4.452278137207031,
      "learning_rate": 0.0006775522661376469,
      "loss": 0.5888,
      "step": 2164
    },
    {
      "epoch": 0.983643798273512,
      "grad_norm": 5.007771015167236,
      "learning_rate": 0.0006773996642759041,
      "loss": 0.6354,
      "step": 2165
    },
    {
      "epoch": 0.9840981372103589,
      "grad_norm": 8.073589324951172,
      "learning_rate": 0.0006772470624141615,
      "loss": 1.5732,
      "step": 2166
    },
    {
      "epoch": 0.9845524761472059,
      "grad_norm": 5.60862398147583,
      "learning_rate": 0.0006770944605524187,
      "loss": 0.4264,
      "step": 2167
    },
    {
      "epoch": 0.9850068150840527,
      "grad_norm": 1.9305694103240967,
      "learning_rate": 0.000676941858690676,
      "loss": 0.2353,
      "step": 2168
    },
    {
      "epoch": 0.9854611540208996,
      "grad_norm": 5.131899356842041,
      "learning_rate": 0.0006767892568289334,
      "loss": 0.5592,
      "step": 2169
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 3.165468454360962,
      "learning_rate": 0.0006766366549671906,
      "loss": 0.2763,
      "step": 2170
    },
    {
      "epoch": 0.9863698318945934,
      "grad_norm": 5.303527355194092,
      "learning_rate": 0.0006764840531054479,
      "loss": 0.9226,
      "step": 2171
    },
    {
      "epoch": 0.9868241708314403,
      "grad_norm": 7.085631370544434,
      "learning_rate": 0.0006763314512437052,
      "loss": 0.8222,
      "step": 2172
    },
    {
      "epoch": 0.9872785097682871,
      "grad_norm": 6.572265625,
      "learning_rate": 0.0006761788493819625,
      "loss": 1.2363,
      "step": 2173
    },
    {
      "epoch": 0.987732848705134,
      "grad_norm": 8.568450927734375,
      "learning_rate": 0.0006760262475202197,
      "loss": 1.2153,
      "step": 2174
    },
    {
      "epoch": 0.9881871876419809,
      "grad_norm": 7.019843101501465,
      "learning_rate": 0.0006758736456584771,
      "loss": 1.2834,
      "step": 2175
    },
    {
      "epoch": 0.9886415265788278,
      "grad_norm": 6.772791385650635,
      "learning_rate": 0.0006757210437967344,
      "loss": 1.2074,
      "step": 2176
    },
    {
      "epoch": 0.9890958655156747,
      "grad_norm": 6.941165924072266,
      "learning_rate": 0.0006755684419349916,
      "loss": 1.2768,
      "step": 2177
    },
    {
      "epoch": 0.9895502044525216,
      "grad_norm": 6.022774696350098,
      "learning_rate": 0.0006754158400732489,
      "loss": 1.4643,
      "step": 2178
    },
    {
      "epoch": 0.9900045433893685,
      "grad_norm": 6.705077171325684,
      "learning_rate": 0.0006752632382115061,
      "loss": 0.7066,
      "step": 2179
    },
    {
      "epoch": 0.9904588823262154,
      "grad_norm": 3.74477219581604,
      "learning_rate": 0.0006751106363497634,
      "loss": 0.2838,
      "step": 2180
    },
    {
      "epoch": 0.9909132212630622,
      "grad_norm": 5.143665790557861,
      "learning_rate": 0.0006749580344880208,
      "loss": 0.9862,
      "step": 2181
    },
    {
      "epoch": 0.9913675601999091,
      "grad_norm": 4.339573383331299,
      "learning_rate": 0.000674805432626278,
      "loss": 0.5373,
      "step": 2182
    },
    {
      "epoch": 0.991821899136756,
      "grad_norm": 5.761904716491699,
      "learning_rate": 0.0006746528307645353,
      "loss": 1.2256,
      "step": 2183
    },
    {
      "epoch": 0.9922762380736029,
      "grad_norm": 7.716029644012451,
      "learning_rate": 0.0006745002289027926,
      "loss": 0.5284,
      "step": 2184
    },
    {
      "epoch": 0.9927305770104498,
      "grad_norm": 4.326215744018555,
      "learning_rate": 0.0006743476270410499,
      "loss": 0.7082,
      "step": 2185
    },
    {
      "epoch": 0.9931849159472966,
      "grad_norm": 6.564938068389893,
      "learning_rate": 0.0006741950251793071,
      "loss": 1.4411,
      "step": 2186
    },
    {
      "epoch": 0.9936392548841436,
      "grad_norm": 6.798888206481934,
      "learning_rate": 0.0006740424233175645,
      "loss": 1.3383,
      "step": 2187
    },
    {
      "epoch": 0.9940935938209905,
      "grad_norm": 3.3701670169830322,
      "learning_rate": 0.0006738898214558218,
      "loss": 0.439,
      "step": 2188
    },
    {
      "epoch": 0.9945479327578374,
      "grad_norm": 4.399795055389404,
      "learning_rate": 0.000673737219594079,
      "loss": 1.225,
      "step": 2189
    },
    {
      "epoch": 0.9950022716946842,
      "grad_norm": 4.796753406524658,
      "learning_rate": 0.0006735846177323364,
      "loss": 0.8978,
      "step": 2190
    },
    {
      "epoch": 0.9954566106315311,
      "grad_norm": 7.221077919006348,
      "learning_rate": 0.0006734320158705936,
      "loss": 1.1108,
      "step": 2191
    },
    {
      "epoch": 0.995910949568378,
      "grad_norm": 4.793304443359375,
      "learning_rate": 0.000673279414008851,
      "loss": 1.0736,
      "step": 2192
    },
    {
      "epoch": 0.9963652885052249,
      "grad_norm": 3.594615936279297,
      "learning_rate": 0.0006731268121471083,
      "loss": 0.5576,
      "step": 2193
    },
    {
      "epoch": 0.9968196274420718,
      "grad_norm": 6.406000137329102,
      "learning_rate": 0.0006729742102853655,
      "loss": 0.7734,
      "step": 2194
    },
    {
      "epoch": 0.9972739663789186,
      "grad_norm": 4.660029888153076,
      "learning_rate": 0.0006728216084236229,
      "loss": 0.6914,
      "step": 2195
    },
    {
      "epoch": 0.9977283053157655,
      "grad_norm": 7.439437389373779,
      "learning_rate": 0.00067266900656188,
      "loss": 1.6483,
      "step": 2196
    },
    {
      "epoch": 0.9981826442526125,
      "grad_norm": 9.147397994995117,
      "learning_rate": 0.0006725164047001373,
      "loss": 2.5134,
      "step": 2197
    },
    {
      "epoch": 0.9986369831894594,
      "grad_norm": 6.8392791748046875,
      "learning_rate": 0.0006723638028383945,
      "loss": 1.4791,
      "step": 2198
    },
    {
      "epoch": 0.9990913221263062,
      "grad_norm": 6.224771499633789,
      "learning_rate": 0.0006722112009766519,
      "loss": 0.9093,
      "step": 2199
    },
    {
      "epoch": 0.9995456610631531,
      "grad_norm": 6.2813944816589355,
      "learning_rate": 0.0006720585991149092,
      "loss": 1.3066,
      "step": 2200
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.832905292510986,
      "learning_rate": 0.0006719059972531665,
      "loss": 0.2409,
      "step": 2201
    }
  ],
  "logging_steps": 1,
  "max_steps": 6603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.56255653888e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
