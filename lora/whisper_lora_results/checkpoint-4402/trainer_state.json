{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 4402,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00045433893684688776,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 8.0023,
      "step": 1
    },
    {
      "epoch": 0.0009086778736937755,
      "grad_norm": NaN,
      "learning_rate": 2e-05,
      "loss": 8.3776,
      "step": 2
    },
    {
      "epoch": 0.0013630168105406633,
      "grad_norm": NaN,
      "learning_rate": 4e-05,
      "loss": 8.2424,
      "step": 3
    },
    {
      "epoch": 0.001817355747387551,
      "grad_norm": 25.824222564697266,
      "learning_rate": 6e-05,
      "loss": 8.5806,
      "step": 4
    },
    {
      "epoch": 0.002271694684234439,
      "grad_norm": 19.027311325073242,
      "learning_rate": 8e-05,
      "loss": 7.2914,
      "step": 5
    },
    {
      "epoch": 0.0027260336210813267,
      "grad_norm": 26.578123092651367,
      "learning_rate": 0.0001,
      "loss": 9.4662,
      "step": 6
    },
    {
      "epoch": 0.0031803725579282144,
      "grad_norm": 25.66640853881836,
      "learning_rate": 0.00012,
      "loss": 8.0139,
      "step": 7
    },
    {
      "epoch": 0.003634711494775102,
      "grad_norm": 16.478614807128906,
      "learning_rate": 0.00014000000000000001,
      "loss": 7.829,
      "step": 8
    },
    {
      "epoch": 0.00408905043162199,
      "grad_norm": 17.312732696533203,
      "learning_rate": 0.00016,
      "loss": 6.7323,
      "step": 9
    },
    {
      "epoch": 0.004543389368468878,
      "grad_norm": 8.53313159942627,
      "learning_rate": 0.00017999999999999998,
      "loss": 6.2305,
      "step": 10
    },
    {
      "epoch": 0.004997728305315766,
      "grad_norm": 7.02564811706543,
      "learning_rate": 0.0002,
      "loss": 6.3462,
      "step": 11
    },
    {
      "epoch": 0.005452067242162653,
      "grad_norm": 5.80635404586792,
      "learning_rate": 0.00022,
      "loss": 5.7629,
      "step": 12
    },
    {
      "epoch": 0.005906406179009541,
      "grad_norm": 3.673982858657837,
      "learning_rate": 0.00024,
      "loss": 5.7599,
      "step": 13
    },
    {
      "epoch": 0.006360745115856429,
      "grad_norm": 3.477905750274658,
      "learning_rate": 0.00026000000000000003,
      "loss": 5.1999,
      "step": 14
    },
    {
      "epoch": 0.0068150840527033164,
      "grad_norm": 3.6834943294525146,
      "learning_rate": 0.00028000000000000003,
      "loss": 5.7998,
      "step": 15
    },
    {
      "epoch": 0.007269422989550204,
      "grad_norm": 4.192676067352295,
      "learning_rate": 0.0003,
      "loss": 4.9561,
      "step": 16
    },
    {
      "epoch": 0.007723761926397092,
      "grad_norm": 4.50867223739624,
      "learning_rate": 0.00032,
      "loss": 4.863,
      "step": 17
    },
    {
      "epoch": 0.00817810086324398,
      "grad_norm": 4.193966388702393,
      "learning_rate": 0.00034,
      "loss": 4.7476,
      "step": 18
    },
    {
      "epoch": 0.008632439800090867,
      "grad_norm": 24.335716247558594,
      "learning_rate": 0.00035999999999999997,
      "loss": 5.2606,
      "step": 19
    },
    {
      "epoch": 0.009086778736937756,
      "grad_norm": 5.41240930557251,
      "learning_rate": 0.00038,
      "loss": 4.5987,
      "step": 20
    },
    {
      "epoch": 0.009541117673784643,
      "grad_norm": 25.38216781616211,
      "learning_rate": 0.0004,
      "loss": 5.1304,
      "step": 21
    },
    {
      "epoch": 0.009995456610631531,
      "grad_norm": NaN,
      "learning_rate": 0.00042,
      "loss": 6.0407,
      "step": 22
    },
    {
      "epoch": 0.010449795547478418,
      "grad_norm": 34.17319107055664,
      "learning_rate": 0.00044,
      "loss": 6.8801,
      "step": 23
    },
    {
      "epoch": 0.010904134484325307,
      "grad_norm": 16.081523895263672,
      "learning_rate": 0.00046,
      "loss": 9.3468,
      "step": 24
    },
    {
      "epoch": 0.011358473421172195,
      "grad_norm": NaN,
      "learning_rate": 0.00048,
      "loss": 13.2617,
      "step": 25
    },
    {
      "epoch": 0.011812812358019082,
      "grad_norm": 121.80638122558594,
      "learning_rate": 0.0005,
      "loss": 12.4593,
      "step": 26
    },
    {
      "epoch": 0.01226715129486597,
      "grad_norm": 118.46707153320312,
      "learning_rate": 0.0005200000000000001,
      "loss": 14.8211,
      "step": 27
    },
    {
      "epoch": 0.012721490231712857,
      "grad_norm": 38.67084503173828,
      "learning_rate": 0.00054,
      "loss": 8.8595,
      "step": 28
    },
    {
      "epoch": 0.013175829168559746,
      "grad_norm": 129.6434783935547,
      "learning_rate": 0.0005600000000000001,
      "loss": 13.0564,
      "step": 29
    },
    {
      "epoch": 0.013630168105406633,
      "grad_norm": 152.58070373535156,
      "learning_rate": 0.00058,
      "loss": 13.632,
      "step": 30
    },
    {
      "epoch": 0.014084507042253521,
      "grad_norm": Infinity,
      "learning_rate": 0.0006,
      "loss": 15.2205,
      "step": 31
    },
    {
      "epoch": 0.014538845979100408,
      "grad_norm": 128.71963500976562,
      "learning_rate": 0.00062,
      "loss": 13.2465,
      "step": 32
    },
    {
      "epoch": 0.014993184915947297,
      "grad_norm": 67.63313293457031,
      "learning_rate": 0.00064,
      "loss": 10.1143,
      "step": 33
    },
    {
      "epoch": 0.015447523852794184,
      "grad_norm": Infinity,
      "learning_rate": 0.00066,
      "loss": 12.0679,
      "step": 34
    },
    {
      "epoch": 0.01590186278964107,
      "grad_norm": 44.58478546142578,
      "learning_rate": 0.00068,
      "loss": 12.7381,
      "step": 35
    },
    {
      "epoch": 0.01635620172648796,
      "grad_norm": 129.38858032226562,
      "learning_rate": 0.0007,
      "loss": 12.162,
      "step": 36
    },
    {
      "epoch": 0.016810540663334848,
      "grad_norm": 37.70536422729492,
      "learning_rate": 0.0007199999999999999,
      "loss": 11.085,
      "step": 37
    },
    {
      "epoch": 0.017264879600181735,
      "grad_norm": 138.8385009765625,
      "learning_rate": 0.00074,
      "loss": 10.5558,
      "step": 38
    },
    {
      "epoch": 0.017719218537028625,
      "grad_norm": 526.0149536132812,
      "learning_rate": 0.00076,
      "loss": 9.0135,
      "step": 39
    },
    {
      "epoch": 0.01817355747387551,
      "grad_norm": 29.567760467529297,
      "learning_rate": 0.0007800000000000001,
      "loss": 9.3933,
      "step": 40
    },
    {
      "epoch": 0.0186278964107224,
      "grad_norm": 12.656719207763672,
      "learning_rate": 0.0008,
      "loss": 9.9319,
      "step": 41
    },
    {
      "epoch": 0.019082235347569285,
      "grad_norm": 60.86611557006836,
      "learning_rate": 0.00082,
      "loss": 8.8919,
      "step": 42
    },
    {
      "epoch": 0.019536574284416176,
      "grad_norm": 20.097326278686523,
      "learning_rate": 0.00084,
      "loss": 9.1112,
      "step": 43
    },
    {
      "epoch": 0.019990913221263062,
      "grad_norm": 250.72438049316406,
      "learning_rate": 0.00086,
      "loss": 11.8096,
      "step": 44
    },
    {
      "epoch": 0.02044525215810995,
      "grad_norm": 112.64694213867188,
      "learning_rate": 0.00088,
      "loss": 11.3893,
      "step": 45
    },
    {
      "epoch": 0.020899591094956836,
      "grad_norm": 10.068092346191406,
      "learning_rate": 0.0009000000000000001,
      "loss": 7.7605,
      "step": 46
    },
    {
      "epoch": 0.021353930031803726,
      "grad_norm": 26.70404815673828,
      "learning_rate": 0.00092,
      "loss": 7.8564,
      "step": 47
    },
    {
      "epoch": 0.021808268968650613,
      "grad_norm": 209.85520935058594,
      "learning_rate": 0.00094,
      "loss": 10.3645,
      "step": 48
    },
    {
      "epoch": 0.0222626079054975,
      "grad_norm": 4.865591526031494,
      "learning_rate": 0.00096,
      "loss": 7.9674,
      "step": 49
    },
    {
      "epoch": 0.02271694684234439,
      "grad_norm": 43.300193786621094,
      "learning_rate": 0.00098,
      "loss": 10.9355,
      "step": 50
    },
    {
      "epoch": 0.023171285779191277,
      "grad_norm": 22.19122314453125,
      "learning_rate": 0.001,
      "loss": 9.4466,
      "step": 51
    },
    {
      "epoch": 0.023625624716038164,
      "grad_norm": 22.118881225585938,
      "learning_rate": 0.0009998473981382573,
      "loss": 9.6055,
      "step": 52
    },
    {
      "epoch": 0.02407996365288505,
      "grad_norm": 9.091259956359863,
      "learning_rate": 0.0009996947962765145,
      "loss": 10.048,
      "step": 53
    },
    {
      "epoch": 0.02453430258973194,
      "grad_norm": 16.8380126953125,
      "learning_rate": 0.000999542194414772,
      "loss": 11.4062,
      "step": 54
    },
    {
      "epoch": 0.024988641526578828,
      "grad_norm": 4.073796272277832,
      "learning_rate": 0.0009993895925530293,
      "loss": 10.9784,
      "step": 55
    },
    {
      "epoch": 0.025442980463425715,
      "grad_norm": 47.10187911987305,
      "learning_rate": 0.0009992369906912865,
      "loss": 8.6613,
      "step": 56
    },
    {
      "epoch": 0.025897319400272602,
      "grad_norm": 23.968469619750977,
      "learning_rate": 0.0009990843888295438,
      "loss": 9.575,
      "step": 57
    },
    {
      "epoch": 0.026351658337119492,
      "grad_norm": 119.49044799804688,
      "learning_rate": 0.000998931786967801,
      "loss": 9.5518,
      "step": 58
    },
    {
      "epoch": 0.02680599727396638,
      "grad_norm": 7.898088455200195,
      "learning_rate": 0.0009987791851060583,
      "loss": 8.6571,
      "step": 59
    },
    {
      "epoch": 0.027260336210813266,
      "grad_norm": 38.10663986206055,
      "learning_rate": 0.0009986265832443158,
      "loss": 6.4335,
      "step": 60
    },
    {
      "epoch": 0.027714675147660156,
      "grad_norm": 10.954629898071289,
      "learning_rate": 0.0009984739813825728,
      "loss": 7.4847,
      "step": 61
    },
    {
      "epoch": 0.028169014084507043,
      "grad_norm": 5.64437198638916,
      "learning_rate": 0.00099832137952083,
      "loss": 7.3038,
      "step": 62
    },
    {
      "epoch": 0.02862335302135393,
      "grad_norm": 6.3597822189331055,
      "learning_rate": 0.0009981687776590875,
      "loss": 6.3598,
      "step": 63
    },
    {
      "epoch": 0.029077691958200817,
      "grad_norm": 4.8628644943237305,
      "learning_rate": 0.0009980161757973448,
      "loss": 5.5706,
      "step": 64
    },
    {
      "epoch": 0.029532030895047707,
      "grad_norm": 4.439299583435059,
      "learning_rate": 0.000997863573935602,
      "loss": 5.6658,
      "step": 65
    },
    {
      "epoch": 0.029986369831894594,
      "grad_norm": 5.823978424072266,
      "learning_rate": 0.0009977109720738593,
      "loss": 5.7194,
      "step": 66
    },
    {
      "epoch": 0.03044070876874148,
      "grad_norm": 4.534936904907227,
      "learning_rate": 0.0009975583702121166,
      "loss": 4.2016,
      "step": 67
    },
    {
      "epoch": 0.030895047705588367,
      "grad_norm": 28.568662643432617,
      "learning_rate": 0.0009974057683503738,
      "loss": 4.9745,
      "step": 68
    },
    {
      "epoch": 0.03134938664243526,
      "grad_norm": 124.42140197753906,
      "learning_rate": 0.0009972531664886313,
      "loss": 5.4021,
      "step": 69
    },
    {
      "epoch": 0.03180372557928214,
      "grad_norm": 4.23909854888916,
      "learning_rate": 0.0009971005646268885,
      "loss": 5.1491,
      "step": 70
    },
    {
      "epoch": 0.03225806451612903,
      "grad_norm": 4.771326541900635,
      "learning_rate": 0.0009969479627651458,
      "loss": 4.5353,
      "step": 71
    },
    {
      "epoch": 0.03271240345297592,
      "grad_norm": 4.875744342803955,
      "learning_rate": 0.000996795360903403,
      "loss": 5.2877,
      "step": 72
    },
    {
      "epoch": 0.033166742389822805,
      "grad_norm": 4.559694766998291,
      "learning_rate": 0.0009966427590416603,
      "loss": 5.2964,
      "step": 73
    },
    {
      "epoch": 0.033621081326669695,
      "grad_norm": 4.016839981079102,
      "learning_rate": 0.0009964901571799176,
      "loss": 4.2866,
      "step": 74
    },
    {
      "epoch": 0.034075420263516586,
      "grad_norm": 5.4447922706604,
      "learning_rate": 0.000996337555318175,
      "loss": 4.6195,
      "step": 75
    },
    {
      "epoch": 0.03452975920036347,
      "grad_norm": 3.736846446990967,
      "learning_rate": 0.0009961849534564323,
      "loss": 4.4598,
      "step": 76
    },
    {
      "epoch": 0.03498409813721036,
      "grad_norm": 6.394325256347656,
      "learning_rate": 0.0009960323515946896,
      "loss": 4.386,
      "step": 77
    },
    {
      "epoch": 0.03543843707405725,
      "grad_norm": 3.9506008625030518,
      "learning_rate": 0.0009958797497329468,
      "loss": 6.0491,
      "step": 78
    },
    {
      "epoch": 0.03589277601090413,
      "grad_norm": 3.4684560298919678,
      "learning_rate": 0.000995727147871204,
      "loss": 4.7314,
      "step": 79
    },
    {
      "epoch": 0.03634711494775102,
      "grad_norm": 6.110920429229736,
      "learning_rate": 0.0009955745460094613,
      "loss": 6.2104,
      "step": 80
    },
    {
      "epoch": 0.03680145388459791,
      "grad_norm": 4.684608459472656,
      "learning_rate": 0.0009954219441477186,
      "loss": 5.209,
      "step": 81
    },
    {
      "epoch": 0.0372557928214448,
      "grad_norm": 6.747181415557861,
      "learning_rate": 0.0009952693422859758,
      "loss": 4.6792,
      "step": 82
    },
    {
      "epoch": 0.03771013175829169,
      "grad_norm": 4.552679538726807,
      "learning_rate": 0.000995116740424233,
      "loss": 5.3779,
      "step": 83
    },
    {
      "epoch": 0.03816447069513857,
      "grad_norm": 4.314069747924805,
      "learning_rate": 0.0009949641385624906,
      "loss": 4.5104,
      "step": 84
    },
    {
      "epoch": 0.03861880963198546,
      "grad_norm": 6.020544528961182,
      "learning_rate": 0.0009948115367007478,
      "loss": 6.0825,
      "step": 85
    },
    {
      "epoch": 0.03907314856883235,
      "grad_norm": 4.898756980895996,
      "learning_rate": 0.000994658934839005,
      "loss": 5.3171,
      "step": 86
    },
    {
      "epoch": 0.039527487505679235,
      "grad_norm": 4.783594131469727,
      "learning_rate": 0.0009945063329772623,
      "loss": 4.2888,
      "step": 87
    },
    {
      "epoch": 0.039981826442526125,
      "grad_norm": 5.4161224365234375,
      "learning_rate": 0.0009943537311155196,
      "loss": 3.889,
      "step": 88
    },
    {
      "epoch": 0.040436165379373015,
      "grad_norm": 14.11226749420166,
      "learning_rate": 0.0009942011292537768,
      "loss": 4.3414,
      "step": 89
    },
    {
      "epoch": 0.0408905043162199,
      "grad_norm": 5.149744510650635,
      "learning_rate": 0.0009940485273920343,
      "loss": 4.0926,
      "step": 90
    },
    {
      "epoch": 0.04134484325306679,
      "grad_norm": 6.219616889953613,
      "learning_rate": 0.0009938959255302916,
      "loss": 5.0,
      "step": 91
    },
    {
      "epoch": 0.04179918218991367,
      "grad_norm": 4.310451507568359,
      "learning_rate": 0.0009937433236685488,
      "loss": 3.888,
      "step": 92
    },
    {
      "epoch": 0.04225352112676056,
      "grad_norm": 6.236973762512207,
      "learning_rate": 0.000993590721806806,
      "loss": 3.3396,
      "step": 93
    },
    {
      "epoch": 0.04270786006360745,
      "grad_norm": 4.45758581161499,
      "learning_rate": 0.0009934381199450633,
      "loss": 4.9513,
      "step": 94
    },
    {
      "epoch": 0.043162199000454336,
      "grad_norm": 4.142629146575928,
      "learning_rate": 0.0009932855180833206,
      "loss": 4.1621,
      "step": 95
    },
    {
      "epoch": 0.04361653793730123,
      "grad_norm": 4.4617919921875,
      "learning_rate": 0.000993132916221578,
      "loss": 4.0225,
      "step": 96
    },
    {
      "epoch": 0.04407087687414812,
      "grad_norm": 3.3871145248413086,
      "learning_rate": 0.0009929803143598351,
      "loss": 4.771,
      "step": 97
    },
    {
      "epoch": 0.044525215810995,
      "grad_norm": 5.589167594909668,
      "learning_rate": 0.0009928277124980924,
      "loss": 4.5124,
      "step": 98
    },
    {
      "epoch": 0.04497955474784189,
      "grad_norm": 4.178234100341797,
      "learning_rate": 0.0009926751106363498,
      "loss": 4.8029,
      "step": 99
    },
    {
      "epoch": 0.04543389368468878,
      "grad_norm": 6.137523651123047,
      "learning_rate": 0.000992522508774607,
      "loss": 4.0348,
      "step": 100
    },
    {
      "epoch": 0.045888232621535664,
      "grad_norm": 6.350948810577393,
      "learning_rate": 0.0009923699069128644,
      "loss": 5.2349,
      "step": 101
    },
    {
      "epoch": 0.046342571558382555,
      "grad_norm": 2.972569704055786,
      "learning_rate": 0.0009922173050511216,
      "loss": 4.3659,
      "step": 102
    },
    {
      "epoch": 0.04679691049522944,
      "grad_norm": 18.598766326904297,
      "learning_rate": 0.0009920647031893789,
      "loss": 5.4895,
      "step": 103
    },
    {
      "epoch": 0.04725124943207633,
      "grad_norm": 2.7093234062194824,
      "learning_rate": 0.0009919121013276361,
      "loss": 3.8883,
      "step": 104
    },
    {
      "epoch": 0.04770558836892322,
      "grad_norm": 5.899909496307373,
      "learning_rate": 0.0009917594994658936,
      "loss": 5.7114,
      "step": 105
    },
    {
      "epoch": 0.0481599273057701,
      "grad_norm": 11.964067459106445,
      "learning_rate": 0.0009916068976041509,
      "loss": 5.3596,
      "step": 106
    },
    {
      "epoch": 0.04861426624261699,
      "grad_norm": 3.6703925132751465,
      "learning_rate": 0.0009914542957424081,
      "loss": 4.5971,
      "step": 107
    },
    {
      "epoch": 0.04906860517946388,
      "grad_norm": 5.376031875610352,
      "learning_rate": 0.0009913016938806654,
      "loss": 4.906,
      "step": 108
    },
    {
      "epoch": 0.049522944116310766,
      "grad_norm": 3.6859614849090576,
      "learning_rate": 0.0009911490920189226,
      "loss": 5.1568,
      "step": 109
    },
    {
      "epoch": 0.049977283053157656,
      "grad_norm": 4.364647388458252,
      "learning_rate": 0.0009909964901571799,
      "loss": 4.7153,
      "step": 110
    },
    {
      "epoch": 0.050431621990004546,
      "grad_norm": 13.444659233093262,
      "learning_rate": 0.0009908438882954374,
      "loss": 6.3371,
      "step": 111
    },
    {
      "epoch": 0.05088596092685143,
      "grad_norm": 12.527302742004395,
      "learning_rate": 0.0009906912864336946,
      "loss": 5.2483,
      "step": 112
    },
    {
      "epoch": 0.05134029986369832,
      "grad_norm": 1.416824221611023,
      "learning_rate": 0.0009905386845719519,
      "loss": 4.328,
      "step": 113
    },
    {
      "epoch": 0.051794638800545204,
      "grad_norm": 4.218554496765137,
      "learning_rate": 0.0009903860827102091,
      "loss": 4.3904,
      "step": 114
    },
    {
      "epoch": 0.052248977737392094,
      "grad_norm": 5.373893737792969,
      "learning_rate": 0.0009902334808484664,
      "loss": 5.1213,
      "step": 115
    },
    {
      "epoch": 0.052703316674238984,
      "grad_norm": 5.558022499084473,
      "learning_rate": 0.0009900808789867236,
      "loss": 4.2808,
      "step": 116
    },
    {
      "epoch": 0.05315765561108587,
      "grad_norm": 3.5679633617401123,
      "learning_rate": 0.000989928277124981,
      "loss": 5.6393,
      "step": 117
    },
    {
      "epoch": 0.05361199454793276,
      "grad_norm": 3.5981521606445312,
      "learning_rate": 0.0009897756752632382,
      "loss": 4.6271,
      "step": 118
    },
    {
      "epoch": 0.05406633348477965,
      "grad_norm": 3.3493311405181885,
      "learning_rate": 0.0009896230734014954,
      "loss": 4.1889,
      "step": 119
    },
    {
      "epoch": 0.05452067242162653,
      "grad_norm": 4.732795238494873,
      "learning_rate": 0.0009894704715397529,
      "loss": 5.1547,
      "step": 120
    },
    {
      "epoch": 0.05497501135847342,
      "grad_norm": 5.440881252288818,
      "learning_rate": 0.0009893178696780101,
      "loss": 4.1898,
      "step": 121
    },
    {
      "epoch": 0.05542935029532031,
      "grad_norm": 6.5941901206970215,
      "learning_rate": 0.0009891652678162674,
      "loss": 3.9222,
      "step": 122
    },
    {
      "epoch": 0.055883689232167195,
      "grad_norm": 3.764658212661743,
      "learning_rate": 0.0009890126659545246,
      "loss": 5.2241,
      "step": 123
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 5.435695171356201,
      "learning_rate": 0.000988860064092782,
      "loss": 5.3936,
      "step": 124
    },
    {
      "epoch": 0.05679236710586097,
      "grad_norm": 12.230674743652344,
      "learning_rate": 0.0009887074622310392,
      "loss": 3.7325,
      "step": 125
    },
    {
      "epoch": 0.05724670604270786,
      "grad_norm": 7.476783752441406,
      "learning_rate": 0.0009885548603692966,
      "loss": 4.1259,
      "step": 126
    },
    {
      "epoch": 0.05770104497955475,
      "grad_norm": 7.368002414703369,
      "learning_rate": 0.000988402258507554,
      "loss": 5.4554,
      "step": 127
    },
    {
      "epoch": 0.05815538391640163,
      "grad_norm": 3.7337493896484375,
      "learning_rate": 0.0009882496566458111,
      "loss": 4.0602,
      "step": 128
    },
    {
      "epoch": 0.05860972285324852,
      "grad_norm": 12.5004301071167,
      "learning_rate": 0.0009880970547840684,
      "loss": 4.8755,
      "step": 129
    },
    {
      "epoch": 0.059064061790095414,
      "grad_norm": 3.43620228767395,
      "learning_rate": 0.0009879444529223257,
      "loss": 3.4166,
      "step": 130
    },
    {
      "epoch": 0.0595184007269423,
      "grad_norm": 36.46588134765625,
      "learning_rate": 0.000987791851060583,
      "loss": 4.6218,
      "step": 131
    },
    {
      "epoch": 0.05997273966378919,
      "grad_norm": 8.751091957092285,
      "learning_rate": 0.0009876392491988404,
      "loss": 5.39,
      "step": 132
    },
    {
      "epoch": 0.06042707860063608,
      "grad_norm": 8.155447959899902,
      "learning_rate": 0.0009874866473370976,
      "loss": 4.5694,
      "step": 133
    },
    {
      "epoch": 0.06088141753748296,
      "grad_norm": 8.671798706054688,
      "learning_rate": 0.0009873340454753547,
      "loss": 5.0133,
      "step": 134
    },
    {
      "epoch": 0.06133575647432985,
      "grad_norm": 8.39020824432373,
      "learning_rate": 0.0009871814436136122,
      "loss": 4.3537,
      "step": 135
    },
    {
      "epoch": 0.061790095411176735,
      "grad_norm": 6.358360767364502,
      "learning_rate": 0.0009870288417518694,
      "loss": 4.2879,
      "step": 136
    },
    {
      "epoch": 0.062244434348023625,
      "grad_norm": 6.627678871154785,
      "learning_rate": 0.0009868762398901267,
      "loss": 3.6809,
      "step": 137
    },
    {
      "epoch": 0.06269877328487052,
      "grad_norm": 3.454014778137207,
      "learning_rate": 0.000986723638028384,
      "loss": 4.529,
      "step": 138
    },
    {
      "epoch": 0.0631531122217174,
      "grad_norm": 3.807094097137451,
      "learning_rate": 0.0009865710361666412,
      "loss": 3.8101,
      "step": 139
    },
    {
      "epoch": 0.06360745115856428,
      "grad_norm": 8.410943031311035,
      "learning_rate": 0.0009864184343048984,
      "loss": 4.2768,
      "step": 140
    },
    {
      "epoch": 0.06406179009541117,
      "grad_norm": 3.9943816661834717,
      "learning_rate": 0.000986265832443156,
      "loss": 3.8826,
      "step": 141
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 4.774807929992676,
      "learning_rate": 0.0009861132305814132,
      "loss": 2.8197,
      "step": 142
    },
    {
      "epoch": 0.06497046796910495,
      "grad_norm": 3.271233558654785,
      "learning_rate": 0.0009859606287196704,
      "loss": 3.6229,
      "step": 143
    },
    {
      "epoch": 0.06542480690595184,
      "grad_norm": 2.5481090545654297,
      "learning_rate": 0.0009858080268579277,
      "loss": 3.4013,
      "step": 144
    },
    {
      "epoch": 0.06587914584279873,
      "grad_norm": 8.705131530761719,
      "learning_rate": 0.000985655424996185,
      "loss": 4.6506,
      "step": 145
    },
    {
      "epoch": 0.06633348477964561,
      "grad_norm": 4.1458916664123535,
      "learning_rate": 0.0009855028231344422,
      "loss": 4.6174,
      "step": 146
    },
    {
      "epoch": 0.0667878237164925,
      "grad_norm": 4.918854236602783,
      "learning_rate": 0.0009853502212726997,
      "loss": 3.9963,
      "step": 147
    },
    {
      "epoch": 0.06724216265333939,
      "grad_norm": 5.896379470825195,
      "learning_rate": 0.000985197619410957,
      "loss": 3.2582,
      "step": 148
    },
    {
      "epoch": 0.06769650159018628,
      "grad_norm": 4.8517022132873535,
      "learning_rate": 0.0009850450175492142,
      "loss": 4.0775,
      "step": 149
    },
    {
      "epoch": 0.06815084052703317,
      "grad_norm": 4.007078170776367,
      "learning_rate": 0.0009848924156874714,
      "loss": 4.1657,
      "step": 150
    },
    {
      "epoch": 0.06860517946388005,
      "grad_norm": 4.619518280029297,
      "learning_rate": 0.0009847398138257287,
      "loss": 3.6541,
      "step": 151
    },
    {
      "epoch": 0.06905951840072694,
      "grad_norm": 5.512119770050049,
      "learning_rate": 0.000984587211963986,
      "loss": 3.4678,
      "step": 152
    },
    {
      "epoch": 0.06951385733757383,
      "grad_norm": 4.172642230987549,
      "learning_rate": 0.0009844346101022432,
      "loss": 3.8443,
      "step": 153
    },
    {
      "epoch": 0.06996819627442072,
      "grad_norm": 7.289047718048096,
      "learning_rate": 0.0009842820082405005,
      "loss": 3.9504,
      "step": 154
    },
    {
      "epoch": 0.07042253521126761,
      "grad_norm": 17.154674530029297,
      "learning_rate": 0.0009841294063787577,
      "loss": 3.243,
      "step": 155
    },
    {
      "epoch": 0.0708768741481145,
      "grad_norm": 4.262900352478027,
      "learning_rate": 0.0009839768045170152,
      "loss": 3.3881,
      "step": 156
    },
    {
      "epoch": 0.07133121308496138,
      "grad_norm": 10.729443550109863,
      "learning_rate": 0.0009838242026552725,
      "loss": 4.8154,
      "step": 157
    },
    {
      "epoch": 0.07178555202180827,
      "grad_norm": 37.1884880065918,
      "learning_rate": 0.0009836716007935297,
      "loss": 3.9405,
      "step": 158
    },
    {
      "epoch": 0.07223989095865516,
      "grad_norm": 4.59852409362793,
      "learning_rate": 0.000983518998931787,
      "loss": 2.7521,
      "step": 159
    },
    {
      "epoch": 0.07269422989550205,
      "grad_norm": 4.466158390045166,
      "learning_rate": 0.0009833663970700442,
      "loss": 4.0475,
      "step": 160
    },
    {
      "epoch": 0.07314856883234894,
      "grad_norm": 2.4084692001342773,
      "learning_rate": 0.0009832137952083015,
      "loss": 3.2675,
      "step": 161
    },
    {
      "epoch": 0.07360290776919581,
      "grad_norm": 4.966616153717041,
      "learning_rate": 0.000983061193346559,
      "loss": 5.1032,
      "step": 162
    },
    {
      "epoch": 0.0740572467060427,
      "grad_norm": 3.4630465507507324,
      "learning_rate": 0.0009829085914848162,
      "loss": 3.749,
      "step": 163
    },
    {
      "epoch": 0.0745115856428896,
      "grad_norm": 7.548681259155273,
      "learning_rate": 0.0009827559896230735,
      "loss": 4.1643,
      "step": 164
    },
    {
      "epoch": 0.07496592457973648,
      "grad_norm": 4.637468338012695,
      "learning_rate": 0.0009826033877613307,
      "loss": 4.6352,
      "step": 165
    },
    {
      "epoch": 0.07542026351658337,
      "grad_norm": 393.98614501953125,
      "learning_rate": 0.000982450785899588,
      "loss": 3.7115,
      "step": 166
    },
    {
      "epoch": 0.07587460245343026,
      "grad_norm": 5.763677597045898,
      "learning_rate": 0.0009822981840378452,
      "loss": 3.7971,
      "step": 167
    },
    {
      "epoch": 0.07632894139027714,
      "grad_norm": 3.4287073612213135,
      "learning_rate": 0.0009821455821761027,
      "loss": 4.2323,
      "step": 168
    },
    {
      "epoch": 0.07678328032712403,
      "grad_norm": 3.8936452865600586,
      "learning_rate": 0.00098199298031436,
      "loss": 4.6735,
      "step": 169
    },
    {
      "epoch": 0.07723761926397092,
      "grad_norm": 5.318809986114502,
      "learning_rate": 0.000981840378452617,
      "loss": 4.2489,
      "step": 170
    },
    {
      "epoch": 0.07769195820081781,
      "grad_norm": 8.321727752685547,
      "learning_rate": 0.0009816877765908745,
      "loss": 3.9057,
      "step": 171
    },
    {
      "epoch": 0.0781462971376647,
      "grad_norm": Infinity,
      "learning_rate": 0.0009815351747291317,
      "loss": 3.5756,
      "step": 172
    },
    {
      "epoch": 0.07860063607451158,
      "grad_norm": 3.2687430381774902,
      "learning_rate": 0.000981382572867389,
      "loss": 4.2894,
      "step": 173
    },
    {
      "epoch": 0.07905497501135847,
      "grad_norm": 5.355385780334473,
      "learning_rate": 0.0009812299710056462,
      "loss": 3.8189,
      "step": 174
    },
    {
      "epoch": 0.07950931394820536,
      "grad_norm": 8.080278396606445,
      "learning_rate": 0.0009810773691439035,
      "loss": 4.8145,
      "step": 175
    },
    {
      "epoch": 0.07996365288505225,
      "grad_norm": 5.8763427734375,
      "learning_rate": 0.0009809247672821608,
      "loss": 4.8995,
      "step": 176
    },
    {
      "epoch": 0.08041799182189914,
      "grad_norm": 4.652189254760742,
      "learning_rate": 0.0009807721654204182,
      "loss": 4.1373,
      "step": 177
    },
    {
      "epoch": 0.08087233075874603,
      "grad_norm": 4.947461128234863,
      "learning_rate": 0.0009806195635586755,
      "loss": 3.9209,
      "step": 178
    },
    {
      "epoch": 0.0813266696955929,
      "grad_norm": 6.76459264755249,
      "learning_rate": 0.0009804669616969327,
      "loss": 3.506,
      "step": 179
    },
    {
      "epoch": 0.0817810086324398,
      "grad_norm": 4.689905166625977,
      "learning_rate": 0.00098031435983519,
      "loss": 4.7262,
      "step": 180
    },
    {
      "epoch": 0.08223534756928669,
      "grad_norm": 23.074615478515625,
      "learning_rate": 0.0009801617579734473,
      "loss": 5.1583,
      "step": 181
    },
    {
      "epoch": 0.08268968650613358,
      "grad_norm": 2.692385196685791,
      "learning_rate": 0.0009800091561117045,
      "loss": 2.9736,
      "step": 182
    },
    {
      "epoch": 0.08314402544298047,
      "grad_norm": 4.335456371307373,
      "learning_rate": 0.000979856554249962,
      "loss": 3.1265,
      "step": 183
    },
    {
      "epoch": 0.08359836437982734,
      "grad_norm": 2.8782010078430176,
      "learning_rate": 0.0009797039523882192,
      "loss": 4.1927,
      "step": 184
    },
    {
      "epoch": 0.08405270331667423,
      "grad_norm": 4.179201126098633,
      "learning_rate": 0.0009795513505264765,
      "loss": 3.8804,
      "step": 185
    },
    {
      "epoch": 0.08450704225352113,
      "grad_norm": 4.887032508850098,
      "learning_rate": 0.0009793987486647338,
      "loss": 4.7505,
      "step": 186
    },
    {
      "epoch": 0.08496138119036802,
      "grad_norm": 5.900402545928955,
      "learning_rate": 0.000979246146802991,
      "loss": 3.1536,
      "step": 187
    },
    {
      "epoch": 0.0854157201272149,
      "grad_norm": 3.810967206954956,
      "learning_rate": 0.0009790935449412483,
      "loss": 2.6752,
      "step": 188
    },
    {
      "epoch": 0.0858700590640618,
      "grad_norm": 5.397327899932861,
      "learning_rate": 0.0009789409430795055,
      "loss": 3.947,
      "step": 189
    },
    {
      "epoch": 0.08632439800090867,
      "grad_norm": 3.058849334716797,
      "learning_rate": 0.0009787883412177628,
      "loss": 3.9808,
      "step": 190
    },
    {
      "epoch": 0.08677873693775556,
      "grad_norm": 4.307729721069336,
      "learning_rate": 0.00097863573935602,
      "loss": 3.5107,
      "step": 191
    },
    {
      "epoch": 0.08723307587460245,
      "grad_norm": 6.400062084197998,
      "learning_rate": 0.0009784831374942775,
      "loss": 4.9501,
      "step": 192
    },
    {
      "epoch": 0.08768741481144934,
      "grad_norm": 3.6810007095336914,
      "learning_rate": 0.0009783305356325348,
      "loss": 4.6354,
      "step": 193
    },
    {
      "epoch": 0.08814175374829623,
      "grad_norm": 5.11810302734375,
      "learning_rate": 0.000978177933770792,
      "loss": 4.2619,
      "step": 194
    },
    {
      "epoch": 0.08859609268514311,
      "grad_norm": 6.085944175720215,
      "learning_rate": 0.0009780253319090493,
      "loss": 3.658,
      "step": 195
    },
    {
      "epoch": 0.08905043162199,
      "grad_norm": 13.353829383850098,
      "learning_rate": 0.0009778727300473065,
      "loss": 3.7545,
      "step": 196
    },
    {
      "epoch": 0.08950477055883689,
      "grad_norm": 5.361572742462158,
      "learning_rate": 0.0009777201281855638,
      "loss": 4.0232,
      "step": 197
    },
    {
      "epoch": 0.08995910949568378,
      "grad_norm": 7.019571304321289,
      "learning_rate": 0.0009775675263238213,
      "loss": 3.2647,
      "step": 198
    },
    {
      "epoch": 0.09041344843253067,
      "grad_norm": 3.7887091636657715,
      "learning_rate": 0.0009774149244620785,
      "loss": 2.7459,
      "step": 199
    },
    {
      "epoch": 0.09086778736937756,
      "grad_norm": 4.476651191711426,
      "learning_rate": 0.0009772623226003358,
      "loss": 4.352,
      "step": 200
    },
    {
      "epoch": 0.09132212630622444,
      "grad_norm": 5.926980018615723,
      "learning_rate": 0.000977109720738593,
      "loss": 3.151,
      "step": 201
    },
    {
      "epoch": 0.09177646524307133,
      "grad_norm": 5.531089782714844,
      "learning_rate": 0.0009769571188768503,
      "loss": 3.8806,
      "step": 202
    },
    {
      "epoch": 0.09223080417991822,
      "grad_norm": 4.885206699371338,
      "learning_rate": 0.0009768045170151075,
      "loss": 4.3888,
      "step": 203
    },
    {
      "epoch": 0.09268514311676511,
      "grad_norm": 5.224144458770752,
      "learning_rate": 0.000976651915153365,
      "loss": 3.1822,
      "step": 204
    },
    {
      "epoch": 0.093139482053612,
      "grad_norm": 6.5018815994262695,
      "learning_rate": 0.0009764993132916223,
      "loss": 3.73,
      "step": 205
    },
    {
      "epoch": 0.09359382099045888,
      "grad_norm": 4.337461471557617,
      "learning_rate": 0.0009763467114298795,
      "loss": 2.4075,
      "step": 206
    },
    {
      "epoch": 0.09404815992730577,
      "grad_norm": 11.796463966369629,
      "learning_rate": 0.0009761941095681367,
      "loss": 3.2493,
      "step": 207
    },
    {
      "epoch": 0.09450249886415266,
      "grad_norm": 4.554281711578369,
      "learning_rate": 0.000976041507706394,
      "loss": 3.8821,
      "step": 208
    },
    {
      "epoch": 0.09495683780099955,
      "grad_norm": 6.080577373504639,
      "learning_rate": 0.0009758889058446513,
      "loss": 4.0082,
      "step": 209
    },
    {
      "epoch": 0.09541117673784644,
      "grad_norm": 5.826722621917725,
      "learning_rate": 0.0009757363039829086,
      "loss": 3.9422,
      "step": 210
    },
    {
      "epoch": 0.09586551567469333,
      "grad_norm": 6.585216045379639,
      "learning_rate": 0.0009755837021211659,
      "loss": 3.9036,
      "step": 211
    },
    {
      "epoch": 0.0963198546115402,
      "grad_norm": 21.507322311401367,
      "learning_rate": 0.0009754311002594232,
      "loss": 4.807,
      "step": 212
    },
    {
      "epoch": 0.0967741935483871,
      "grad_norm": 4.999459266662598,
      "learning_rate": 0.0009752784983976804,
      "loss": 3.6513,
      "step": 213
    },
    {
      "epoch": 0.09722853248523398,
      "grad_norm": 3.4748528003692627,
      "learning_rate": 0.0009751258965359378,
      "loss": 4.3142,
      "step": 214
    },
    {
      "epoch": 0.09768287142208087,
      "grad_norm": 5.2093915939331055,
      "learning_rate": 0.0009749732946741951,
      "loss": 3.3314,
      "step": 215
    },
    {
      "epoch": 0.09813721035892777,
      "grad_norm": 5.477571964263916,
      "learning_rate": 0.0009748206928124523,
      "loss": 4.4049,
      "step": 216
    },
    {
      "epoch": 0.09859154929577464,
      "grad_norm": 92.94070434570312,
      "learning_rate": 0.0009746680909507097,
      "loss": 2.7441,
      "step": 217
    },
    {
      "epoch": 0.09904588823262153,
      "grad_norm": 3.993104934692383,
      "learning_rate": 0.0009745154890889669,
      "loss": 4.6918,
      "step": 218
    },
    {
      "epoch": 0.09950022716946842,
      "grad_norm": 10.868013381958008,
      "learning_rate": 0.0009743628872272242,
      "loss": 2.665,
      "step": 219
    },
    {
      "epoch": 0.09995456610631531,
      "grad_norm": 6.979304790496826,
      "learning_rate": 0.0009742102853654816,
      "loss": 4.2163,
      "step": 220
    },
    {
      "epoch": 0.1004089050431622,
      "grad_norm": 4.047073841094971,
      "learning_rate": 0.0009740576835037388,
      "loss": 3.0042,
      "step": 221
    },
    {
      "epoch": 0.10086324398000909,
      "grad_norm": 5.500774383544922,
      "learning_rate": 0.0009739050816419961,
      "loss": 4.5916,
      "step": 222
    },
    {
      "epoch": 0.10131758291685597,
      "grad_norm": 22.360429763793945,
      "learning_rate": 0.0009737524797802534,
      "loss": 2.4437,
      "step": 223
    },
    {
      "epoch": 0.10177192185370286,
      "grad_norm": 5.232440948486328,
      "learning_rate": 0.0009735998779185107,
      "loss": 4.0674,
      "step": 224
    },
    {
      "epoch": 0.10222626079054975,
      "grad_norm": 15.816807746887207,
      "learning_rate": 0.0009734472760567678,
      "loss": 2.4081,
      "step": 225
    },
    {
      "epoch": 0.10268059972739664,
      "grad_norm": 7.023444652557373,
      "learning_rate": 0.0009732946741950252,
      "loss": 3.5343,
      "step": 226
    },
    {
      "epoch": 0.10313493866424353,
      "grad_norm": 8.967727661132812,
      "learning_rate": 0.0009731420723332825,
      "loss": 2.6659,
      "step": 227
    },
    {
      "epoch": 0.10358927760109041,
      "grad_norm": 13.048861503601074,
      "learning_rate": 0.0009729894704715397,
      "loss": 3.3763,
      "step": 228
    },
    {
      "epoch": 0.1040436165379373,
      "grad_norm": 8.219457626342773,
      "learning_rate": 0.0009728368686097971,
      "loss": 2.3,
      "step": 229
    },
    {
      "epoch": 0.10449795547478419,
      "grad_norm": 8.147345542907715,
      "learning_rate": 0.0009726842667480543,
      "loss": 2.6034,
      "step": 230
    },
    {
      "epoch": 0.10495229441163108,
      "grad_norm": 11.596321105957031,
      "learning_rate": 0.0009725316648863116,
      "loss": 2.2113,
      "step": 231
    },
    {
      "epoch": 0.10540663334847797,
      "grad_norm": 5.11240816116333,
      "learning_rate": 0.000972379063024569,
      "loss": 2.5929,
      "step": 232
    },
    {
      "epoch": 0.10586097228532486,
      "grad_norm": 5.214093208312988,
      "learning_rate": 0.0009722264611628262,
      "loss": 2.3972,
      "step": 233
    },
    {
      "epoch": 0.10631531122217174,
      "grad_norm": 7.715131759643555,
      "learning_rate": 0.0009720738593010835,
      "loss": 2.6193,
      "step": 234
    },
    {
      "epoch": 0.10676965015901863,
      "grad_norm": 7.7353010177612305,
      "learning_rate": 0.0009719212574393408,
      "loss": 2.8781,
      "step": 235
    },
    {
      "epoch": 0.10722398909586552,
      "grad_norm": 17.7259464263916,
      "learning_rate": 0.0009717686555775981,
      "loss": 2.2655,
      "step": 236
    },
    {
      "epoch": 0.1076783280327124,
      "grad_norm": 9.315775871276855,
      "learning_rate": 0.0009716160537158553,
      "loss": 2.162,
      "step": 237
    },
    {
      "epoch": 0.1081326669695593,
      "grad_norm": 19.49567222595215,
      "learning_rate": 0.0009714634518541127,
      "loss": 2.1241,
      "step": 238
    },
    {
      "epoch": 0.10858700590640617,
      "grad_norm": 13.681622505187988,
      "learning_rate": 0.00097131084999237,
      "loss": 3.5965,
      "step": 239
    },
    {
      "epoch": 0.10904134484325306,
      "grad_norm": 9.754660606384277,
      "learning_rate": 0.0009711582481306272,
      "loss": 2.1734,
      "step": 240
    },
    {
      "epoch": 0.10949568378009995,
      "grad_norm": 5.145036697387695,
      "learning_rate": 0.0009710056462688846,
      "loss": 1.1966,
      "step": 241
    },
    {
      "epoch": 0.10995002271694684,
      "grad_norm": 9.710577011108398,
      "learning_rate": 0.0009708530444071418,
      "loss": 1.3841,
      "step": 242
    },
    {
      "epoch": 0.11040436165379373,
      "grad_norm": 6.591537952423096,
      "learning_rate": 0.000970700442545399,
      "loss": 1.8608,
      "step": 243
    },
    {
      "epoch": 0.11085870059064062,
      "grad_norm": 6.7716803550720215,
      "learning_rate": 0.0009705478406836564,
      "loss": 1.7105,
      "step": 244
    },
    {
      "epoch": 0.1113130395274875,
      "grad_norm": 9.092998504638672,
      "learning_rate": 0.0009703952388219136,
      "loss": 2.5472,
      "step": 245
    },
    {
      "epoch": 0.11176737846433439,
      "grad_norm": 12.923064231872559,
      "learning_rate": 0.0009702426369601709,
      "loss": 2.7263,
      "step": 246
    },
    {
      "epoch": 0.11222171740118128,
      "grad_norm": 10.285985946655273,
      "learning_rate": 0.0009700900350984282,
      "loss": 2.7098,
      "step": 247
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 4.863156318664551,
      "learning_rate": 0.0009699374332366855,
      "loss": 1.7458,
      "step": 248
    },
    {
      "epoch": 0.11313039527487506,
      "grad_norm": 6.0615410804748535,
      "learning_rate": 0.0009697848313749427,
      "loss": 1.8858,
      "step": 249
    },
    {
      "epoch": 0.11358473421172194,
      "grad_norm": 6.759240627288818,
      "learning_rate": 0.0009696322295132001,
      "loss": 2.2782,
      "step": 250
    },
    {
      "epoch": 0.11403907314856883,
      "grad_norm": 4.964827537536621,
      "learning_rate": 0.0009694796276514574,
      "loss": 1.0873,
      "step": 251
    },
    {
      "epoch": 0.11449341208541572,
      "grad_norm": 6.755768775939941,
      "learning_rate": 0.0009693270257897146,
      "loss": 1.6664,
      "step": 252
    },
    {
      "epoch": 0.11494775102226261,
      "grad_norm": 5.244935035705566,
      "learning_rate": 0.000969174423927972,
      "loss": 1.301,
      "step": 253
    },
    {
      "epoch": 0.1154020899591095,
      "grad_norm": 8.03216552734375,
      "learning_rate": 0.0009690218220662292,
      "loss": 2.082,
      "step": 254
    },
    {
      "epoch": 0.11585642889595639,
      "grad_norm": 3.4233405590057373,
      "learning_rate": 0.0009688692202044865,
      "loss": 1.2097,
      "step": 255
    },
    {
      "epoch": 0.11631076783280327,
      "grad_norm": 6.953229904174805,
      "learning_rate": 0.0009687166183427439,
      "loss": 2.3142,
      "step": 256
    },
    {
      "epoch": 0.11676510676965016,
      "grad_norm": 12.756695747375488,
      "learning_rate": 0.0009685640164810011,
      "loss": 2.375,
      "step": 257
    },
    {
      "epoch": 0.11721944570649705,
      "grad_norm": 1.88507080078125,
      "learning_rate": 0.0009684114146192584,
      "loss": 0.795,
      "step": 258
    },
    {
      "epoch": 0.11767378464334394,
      "grad_norm": 6.357827663421631,
      "learning_rate": 0.0009682588127575157,
      "loss": 1.6194,
      "step": 259
    },
    {
      "epoch": 0.11812812358019083,
      "grad_norm": 3.229525566101074,
      "learning_rate": 0.000968106210895773,
      "loss": 1.1368,
      "step": 260
    },
    {
      "epoch": 0.1185824625170377,
      "grad_norm": 7.686180114746094,
      "learning_rate": 0.0009679536090340302,
      "loss": 1.3899,
      "step": 261
    },
    {
      "epoch": 0.1190368014538846,
      "grad_norm": 6.135477542877197,
      "learning_rate": 0.0009678010071722875,
      "loss": 1.9438,
      "step": 262
    },
    {
      "epoch": 0.11949114039073148,
      "grad_norm": 5.925567150115967,
      "learning_rate": 0.0009676484053105448,
      "loss": 1.8923,
      "step": 263
    },
    {
      "epoch": 0.11994547932757837,
      "grad_norm": 7.040816307067871,
      "learning_rate": 0.000967495803448802,
      "loss": 1.4019,
      "step": 264
    },
    {
      "epoch": 0.12039981826442527,
      "grad_norm": 5.518520355224609,
      "learning_rate": 0.0009673432015870594,
      "loss": 1.7695,
      "step": 265
    },
    {
      "epoch": 0.12085415720127216,
      "grad_norm": 6.107264518737793,
      "learning_rate": 0.0009671905997253167,
      "loss": 2.3963,
      "step": 266
    },
    {
      "epoch": 0.12130849613811903,
      "grad_norm": 4.519526958465576,
      "learning_rate": 0.0009670379978635739,
      "loss": 2.1478,
      "step": 267
    },
    {
      "epoch": 0.12176283507496592,
      "grad_norm": 3.4286012649536133,
      "learning_rate": 0.0009668853960018313,
      "loss": 1.1449,
      "step": 268
    },
    {
      "epoch": 0.12221717401181281,
      "grad_norm": 4.617491245269775,
      "learning_rate": 0.0009667327941400885,
      "loss": 1.7138,
      "step": 269
    },
    {
      "epoch": 0.1226715129486597,
      "grad_norm": 7.1106648445129395,
      "learning_rate": 0.0009665801922783458,
      "loss": 1.4312,
      "step": 270
    },
    {
      "epoch": 0.1231258518855066,
      "grad_norm": 4.849409580230713,
      "learning_rate": 0.0009664275904166031,
      "loss": 1.9531,
      "step": 271
    },
    {
      "epoch": 0.12358019082235347,
      "grad_norm": 4.902888298034668,
      "learning_rate": 0.0009662749885548604,
      "loss": 1.5832,
      "step": 272
    },
    {
      "epoch": 0.12403452975920036,
      "grad_norm": 5.298555850982666,
      "learning_rate": 0.0009661223866931177,
      "loss": 1.3189,
      "step": 273
    },
    {
      "epoch": 0.12448886869604725,
      "grad_norm": 3.0021321773529053,
      "learning_rate": 0.000965969784831375,
      "loss": 0.762,
      "step": 274
    },
    {
      "epoch": 0.12494320763289414,
      "grad_norm": 10.084877967834473,
      "learning_rate": 0.0009658171829696323,
      "loss": 1.8804,
      "step": 275
    },
    {
      "epoch": 0.12539754656974103,
      "grad_norm": 3.7184712886810303,
      "learning_rate": 0.0009656645811078895,
      "loss": 1.4398,
      "step": 276
    },
    {
      "epoch": 0.1258518855065879,
      "grad_norm": 4.975409030914307,
      "learning_rate": 0.0009655119792461469,
      "loss": 1.2895,
      "step": 277
    },
    {
      "epoch": 0.1263062244434348,
      "grad_norm": 4.711068630218506,
      "learning_rate": 0.0009653593773844042,
      "loss": 1.2868,
      "step": 278
    },
    {
      "epoch": 0.1267605633802817,
      "grad_norm": 4.77157735824585,
      "learning_rate": 0.0009652067755226614,
      "loss": 1.4741,
      "step": 279
    },
    {
      "epoch": 0.12721490231712856,
      "grad_norm": 4.753172874450684,
      "learning_rate": 0.0009650541736609187,
      "loss": 1.6691,
      "step": 280
    },
    {
      "epoch": 0.12766924125397547,
      "grad_norm": 5.105186462402344,
      "learning_rate": 0.0009649015717991759,
      "loss": 2.6583,
      "step": 281
    },
    {
      "epoch": 0.12812358019082234,
      "grad_norm": 3.5031063556671143,
      "learning_rate": 0.0009647489699374332,
      "loss": 1.6488,
      "step": 282
    },
    {
      "epoch": 0.12857791912766925,
      "grad_norm": 5.482685089111328,
      "learning_rate": 0.0009645963680756906,
      "loss": 1.8768,
      "step": 283
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 4.130162239074707,
      "learning_rate": 0.0009644437662139478,
      "loss": 2.0376,
      "step": 284
    },
    {
      "epoch": 0.12948659700136303,
      "grad_norm": 4.079968452453613,
      "learning_rate": 0.0009642911643522051,
      "loss": 1.3714,
      "step": 285
    },
    {
      "epoch": 0.1299409359382099,
      "grad_norm": 5.442225933074951,
      "learning_rate": 0.0009641385624904624,
      "loss": 2.5749,
      "step": 286
    },
    {
      "epoch": 0.13039527487505678,
      "grad_norm": 3.0847272872924805,
      "learning_rate": 0.0009639859606287197,
      "loss": 1.2828,
      "step": 287
    },
    {
      "epoch": 0.1308496138119037,
      "grad_norm": 5.267121315002441,
      "learning_rate": 0.0009638333587669769,
      "loss": 2.3027,
      "step": 288
    },
    {
      "epoch": 0.13130395274875056,
      "grad_norm": 3.9282617568969727,
      "learning_rate": 0.0009636807569052343,
      "loss": 2.1601,
      "step": 289
    },
    {
      "epoch": 0.13175829168559747,
      "grad_norm": 4.273280143737793,
      "learning_rate": 0.0009635281550434916,
      "loss": 1.261,
      "step": 290
    },
    {
      "epoch": 0.13221263062244434,
      "grad_norm": 6.7581329345703125,
      "learning_rate": 0.0009633755531817488,
      "loss": 1.7971,
      "step": 291
    },
    {
      "epoch": 0.13266696955929122,
      "grad_norm": 6.447543621063232,
      "learning_rate": 0.0009632229513200062,
      "loss": 2.1721,
      "step": 292
    },
    {
      "epoch": 0.13312130849613812,
      "grad_norm": 5.7428388595581055,
      "learning_rate": 0.0009630703494582634,
      "loss": 2.0216,
      "step": 293
    },
    {
      "epoch": 0.133575647432985,
      "grad_norm": 4.247249603271484,
      "learning_rate": 0.0009629177475965207,
      "loss": 1.9773,
      "step": 294
    },
    {
      "epoch": 0.1340299863698319,
      "grad_norm": 4.143215179443359,
      "learning_rate": 0.0009627651457347781,
      "loss": 1.3673,
      "step": 295
    },
    {
      "epoch": 0.13448432530667878,
      "grad_norm": 5.418733596801758,
      "learning_rate": 0.0009626125438730353,
      "loss": 1.8236,
      "step": 296
    },
    {
      "epoch": 0.13493866424352566,
      "grad_norm": 4.523118495941162,
      "learning_rate": 0.0009624599420112926,
      "loss": 1.3294,
      "step": 297
    },
    {
      "epoch": 0.13539300318037256,
      "grad_norm": 5.696187973022461,
      "learning_rate": 0.0009623073401495498,
      "loss": 1.9017,
      "step": 298
    },
    {
      "epoch": 0.13584734211721944,
      "grad_norm": 3.8294601440429688,
      "learning_rate": 0.0009621547382878071,
      "loss": 1.2499,
      "step": 299
    },
    {
      "epoch": 0.13630168105406634,
      "grad_norm": 4.698948860168457,
      "learning_rate": 0.0009620021364260643,
      "loss": 2.1312,
      "step": 300
    },
    {
      "epoch": 0.13675601999091322,
      "grad_norm": 5.377554893493652,
      "learning_rate": 0.0009618495345643217,
      "loss": 1.8425,
      "step": 301
    },
    {
      "epoch": 0.1372103589277601,
      "grad_norm": 6.004270076751709,
      "learning_rate": 0.000961696932702579,
      "loss": 2.4913,
      "step": 302
    },
    {
      "epoch": 0.137664697864607,
      "grad_norm": 4.0011420249938965,
      "learning_rate": 0.0009615443308408362,
      "loss": 1.2079,
      "step": 303
    },
    {
      "epoch": 0.13811903680145388,
      "grad_norm": 3.1162526607513428,
      "learning_rate": 0.0009613917289790936,
      "loss": 0.8489,
      "step": 304
    },
    {
      "epoch": 0.13857337573830078,
      "grad_norm": 3.202921152114868,
      "learning_rate": 0.0009612391271173508,
      "loss": 1.4815,
      "step": 305
    },
    {
      "epoch": 0.13902771467514766,
      "grad_norm": 4.1892409324646,
      "learning_rate": 0.0009610865252556081,
      "loss": 1.8213,
      "step": 306
    },
    {
      "epoch": 0.13948205361199456,
      "grad_norm": 3.7600250244140625,
      "learning_rate": 0.0009609339233938655,
      "loss": 1.5397,
      "step": 307
    },
    {
      "epoch": 0.13993639254884144,
      "grad_norm": 10.17154312133789,
      "learning_rate": 0.0009607813215321227,
      "loss": 2.6686,
      "step": 308
    },
    {
      "epoch": 0.1403907314856883,
      "grad_norm": 6.5317792892456055,
      "learning_rate": 0.00096062871967038,
      "loss": 1.6185,
      "step": 309
    },
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 5.431933879852295,
      "learning_rate": 0.0009604761178086373,
      "loss": 1.5038,
      "step": 310
    },
    {
      "epoch": 0.1412994093593821,
      "grad_norm": 3.023481607437134,
      "learning_rate": 0.0009603235159468946,
      "loss": 1.1049,
      "step": 311
    },
    {
      "epoch": 0.141753748296229,
      "grad_norm": 2.909053087234497,
      "learning_rate": 0.0009601709140851519,
      "loss": 0.8892,
      "step": 312
    },
    {
      "epoch": 0.14220808723307587,
      "grad_norm": 5.58517599105835,
      "learning_rate": 0.0009600183122234092,
      "loss": 1.4781,
      "step": 313
    },
    {
      "epoch": 0.14266242616992275,
      "grad_norm": 4.75494909286499,
      "learning_rate": 0.0009598657103616665,
      "loss": 1.6158,
      "step": 314
    },
    {
      "epoch": 0.14311676510676966,
      "grad_norm": 4.936613082885742,
      "learning_rate": 0.0009597131084999237,
      "loss": 1.2898,
      "step": 315
    },
    {
      "epoch": 0.14357110404361653,
      "grad_norm": 6.368581295013428,
      "learning_rate": 0.000959560506638181,
      "loss": 2.2102,
      "step": 316
    },
    {
      "epoch": 0.14402544298046344,
      "grad_norm": 5.877938747406006,
      "learning_rate": 0.0009594079047764382,
      "loss": 2.1716,
      "step": 317
    },
    {
      "epoch": 0.1444797819173103,
      "grad_norm": 6.3772053718566895,
      "learning_rate": 0.0009592553029146955,
      "loss": 1.5077,
      "step": 318
    },
    {
      "epoch": 0.1449341208541572,
      "grad_norm": 3.435494899749756,
      "learning_rate": 0.0009591027010529529,
      "loss": 1.4913,
      "step": 319
    },
    {
      "epoch": 0.1453884597910041,
      "grad_norm": 3.8153128623962402,
      "learning_rate": 0.0009589500991912101,
      "loss": 1.2667,
      "step": 320
    },
    {
      "epoch": 0.14584279872785097,
      "grad_norm": 5.131373405456543,
      "learning_rate": 0.0009587974973294674,
      "loss": 1.9779,
      "step": 321
    },
    {
      "epoch": 0.14629713766469787,
      "grad_norm": 5.150716304779053,
      "learning_rate": 0.0009586448954677247,
      "loss": 1.968,
      "step": 322
    },
    {
      "epoch": 0.14675147660154475,
      "grad_norm": 6.0059494972229,
      "learning_rate": 0.000958492293605982,
      "loss": 1.8916,
      "step": 323
    },
    {
      "epoch": 0.14720581553839163,
      "grad_norm": 4.621043682098389,
      "learning_rate": 0.0009583396917442393,
      "loss": 1.5227,
      "step": 324
    },
    {
      "epoch": 0.14766015447523853,
      "grad_norm": 5.760458469390869,
      "learning_rate": 0.0009581870898824966,
      "loss": 0.9451,
      "step": 325
    },
    {
      "epoch": 0.1481144934120854,
      "grad_norm": 4.9935479164123535,
      "learning_rate": 0.0009580344880207539,
      "loss": 2.2242,
      "step": 326
    },
    {
      "epoch": 0.1485688323489323,
      "grad_norm": 4.3562331199646,
      "learning_rate": 0.0009578818861590111,
      "loss": 1.4898,
      "step": 327
    },
    {
      "epoch": 0.1490231712857792,
      "grad_norm": 3.6777570247650146,
      "learning_rate": 0.0009577292842972685,
      "loss": 0.9235,
      "step": 328
    },
    {
      "epoch": 0.1494775102226261,
      "grad_norm": 4.099028587341309,
      "learning_rate": 0.0009575766824355258,
      "loss": 1.0073,
      "step": 329
    },
    {
      "epoch": 0.14993184915947297,
      "grad_norm": 3.791944980621338,
      "learning_rate": 0.000957424080573783,
      "loss": 1.3769,
      "step": 330
    },
    {
      "epoch": 0.15038618809631984,
      "grad_norm": 5.003060817718506,
      "learning_rate": 0.0009572714787120404,
      "loss": 1.8799,
      "step": 331
    },
    {
      "epoch": 0.15084052703316675,
      "grad_norm": 6.193656921386719,
      "learning_rate": 0.0009571188768502976,
      "loss": 2.1635,
      "step": 332
    },
    {
      "epoch": 0.15129486597001363,
      "grad_norm": 6.927994728088379,
      "learning_rate": 0.0009569662749885549,
      "loss": 1.9274,
      "step": 333
    },
    {
      "epoch": 0.15174920490686053,
      "grad_norm": 4.5294013023376465,
      "learning_rate": 0.0009568136731268121,
      "loss": 1.0982,
      "step": 334
    },
    {
      "epoch": 0.1522035438437074,
      "grad_norm": 6.519195556640625,
      "learning_rate": 0.0009566610712650694,
      "loss": 1.5222,
      "step": 335
    },
    {
      "epoch": 0.15265788278055428,
      "grad_norm": 4.4936747550964355,
      "learning_rate": 0.0009565084694033267,
      "loss": 1.7168,
      "step": 336
    },
    {
      "epoch": 0.1531122217174012,
      "grad_norm": 3.4830853939056396,
      "learning_rate": 0.000956355867541584,
      "loss": 1.2097,
      "step": 337
    },
    {
      "epoch": 0.15356656065424806,
      "grad_norm": 4.848855018615723,
      "learning_rate": 0.0009562032656798413,
      "loss": 2.0716,
      "step": 338
    },
    {
      "epoch": 0.15402089959109497,
      "grad_norm": 9.708375930786133,
      "learning_rate": 0.0009560506638180985,
      "loss": 1.6628,
      "step": 339
    },
    {
      "epoch": 0.15447523852794184,
      "grad_norm": 5.300631523132324,
      "learning_rate": 0.0009558980619563559,
      "loss": 1.5746,
      "step": 340
    },
    {
      "epoch": 0.15492957746478872,
      "grad_norm": 6.133340358734131,
      "learning_rate": 0.0009557454600946132,
      "loss": 2.181,
      "step": 341
    },
    {
      "epoch": 0.15538391640163562,
      "grad_norm": 5.502054214477539,
      "learning_rate": 0.0009555928582328704,
      "loss": 2.2198,
      "step": 342
    },
    {
      "epoch": 0.1558382553384825,
      "grad_norm": 5.86580753326416,
      "learning_rate": 0.0009554402563711278,
      "loss": 1.1812,
      "step": 343
    },
    {
      "epoch": 0.1562925942753294,
      "grad_norm": 7.142019748687744,
      "learning_rate": 0.000955287654509385,
      "loss": 2.0566,
      "step": 344
    },
    {
      "epoch": 0.15674693321217628,
      "grad_norm": 4.58322286605835,
      "learning_rate": 0.0009551350526476423,
      "loss": 2.0312,
      "step": 345
    },
    {
      "epoch": 0.15720127214902316,
      "grad_norm": 4.734084129333496,
      "learning_rate": 0.0009549824507858997,
      "loss": 1.072,
      "step": 346
    },
    {
      "epoch": 0.15765561108587006,
      "grad_norm": 3.4882612228393555,
      "learning_rate": 0.0009548298489241569,
      "loss": 1.1598,
      "step": 347
    },
    {
      "epoch": 0.15810995002271694,
      "grad_norm": 5.3633198738098145,
      "learning_rate": 0.0009546772470624142,
      "loss": 1.3874,
      "step": 348
    },
    {
      "epoch": 0.15856428895956384,
      "grad_norm": 4.552365303039551,
      "learning_rate": 0.0009545246452006715,
      "loss": 0.8377,
      "step": 349
    },
    {
      "epoch": 0.15901862789641072,
      "grad_norm": 5.839667797088623,
      "learning_rate": 0.0009543720433389288,
      "loss": 2.1463,
      "step": 350
    },
    {
      "epoch": 0.15947296683325762,
      "grad_norm": 7.1999030113220215,
      "learning_rate": 0.000954219441477186,
      "loss": 1.9613,
      "step": 351
    },
    {
      "epoch": 0.1599273057701045,
      "grad_norm": 3.627861261367798,
      "learning_rate": 0.0009540668396154434,
      "loss": 1.4151,
      "step": 352
    },
    {
      "epoch": 0.16038164470695138,
      "grad_norm": 4.5662078857421875,
      "learning_rate": 0.0009539142377537006,
      "loss": 1.3432,
      "step": 353
    },
    {
      "epoch": 0.16083598364379828,
      "grad_norm": 4.794084548950195,
      "learning_rate": 0.0009537616358919578,
      "loss": 1.2252,
      "step": 354
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 6.940950870513916,
      "learning_rate": 0.0009536090340302152,
      "loss": 2.3509,
      "step": 355
    },
    {
      "epoch": 0.16174466151749206,
      "grad_norm": 4.2744140625,
      "learning_rate": 0.0009534564321684724,
      "loss": 1.3867,
      "step": 356
    },
    {
      "epoch": 0.16219900045433894,
      "grad_norm": 6.7544708251953125,
      "learning_rate": 0.0009533038303067297,
      "loss": 2.4108,
      "step": 357
    },
    {
      "epoch": 0.1626533393911858,
      "grad_norm": 5.51371955871582,
      "learning_rate": 0.0009531512284449871,
      "loss": 1.9005,
      "step": 358
    },
    {
      "epoch": 0.16310767832803272,
      "grad_norm": 7.359863758087158,
      "learning_rate": 0.0009529986265832443,
      "loss": 1.8119,
      "step": 359
    },
    {
      "epoch": 0.1635620172648796,
      "grad_norm": 6.958871841430664,
      "learning_rate": 0.0009528460247215016,
      "loss": 1.54,
      "step": 360
    },
    {
      "epoch": 0.1640163562017265,
      "grad_norm": 3.6794283390045166,
      "learning_rate": 0.0009526934228597589,
      "loss": 1.0328,
      "step": 361
    },
    {
      "epoch": 0.16447069513857338,
      "grad_norm": 3.9978232383728027,
      "learning_rate": 0.0009525408209980162,
      "loss": 1.4197,
      "step": 362
    },
    {
      "epoch": 0.16492503407542025,
      "grad_norm": 3.645087480545044,
      "learning_rate": 0.0009523882191362734,
      "loss": 1.1157,
      "step": 363
    },
    {
      "epoch": 0.16537937301226716,
      "grad_norm": 5.046257972717285,
      "learning_rate": 0.0009522356172745308,
      "loss": 1.3622,
      "step": 364
    },
    {
      "epoch": 0.16583371194911403,
      "grad_norm": 6.543088912963867,
      "learning_rate": 0.0009520830154127881,
      "loss": 1.8488,
      "step": 365
    },
    {
      "epoch": 0.16628805088596094,
      "grad_norm": 4.333006381988525,
      "learning_rate": 0.0009519304135510453,
      "loss": 1.3312,
      "step": 366
    },
    {
      "epoch": 0.1667423898228078,
      "grad_norm": 4.138701438903809,
      "learning_rate": 0.0009517778116893027,
      "loss": 1.0467,
      "step": 367
    },
    {
      "epoch": 0.1671967287596547,
      "grad_norm": 7.115666389465332,
      "learning_rate": 0.00095162520982756,
      "loss": 1.3021,
      "step": 368
    },
    {
      "epoch": 0.1676510676965016,
      "grad_norm": 4.082321643829346,
      "learning_rate": 0.0009514726079658172,
      "loss": 1.6254,
      "step": 369
    },
    {
      "epoch": 0.16810540663334847,
      "grad_norm": 5.0034685134887695,
      "learning_rate": 0.0009513200061040746,
      "loss": 1.3705,
      "step": 370
    },
    {
      "epoch": 0.16855974557019537,
      "grad_norm": 6.105103015899658,
      "learning_rate": 0.0009511674042423317,
      "loss": 1.9385,
      "step": 371
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 4.43419075012207,
      "learning_rate": 0.000951014802380589,
      "loss": 1.4608,
      "step": 372
    },
    {
      "epoch": 0.16946842344388915,
      "grad_norm": 4.995471954345703,
      "learning_rate": 0.0009508622005188463,
      "loss": 1.2076,
      "step": 373
    },
    {
      "epoch": 0.16992276238073603,
      "grad_norm": 8.26581859588623,
      "learning_rate": 0.0009507095986571036,
      "loss": 3.2799,
      "step": 374
    },
    {
      "epoch": 0.1703771013175829,
      "grad_norm": 7.827980995178223,
      "learning_rate": 0.0009505569967953608,
      "loss": 3.2027,
      "step": 375
    },
    {
      "epoch": 0.1708314402544298,
      "grad_norm": 9.408897399902344,
      "learning_rate": 0.0009504043949336182,
      "loss": 1.0714,
      "step": 376
    },
    {
      "epoch": 0.1712857791912767,
      "grad_norm": 5.053788185119629,
      "learning_rate": 0.0009502517930718755,
      "loss": 1.2166,
      "step": 377
    },
    {
      "epoch": 0.1717401181281236,
      "grad_norm": 5.507104396820068,
      "learning_rate": 0.0009500991912101327,
      "loss": 1.7135,
      "step": 378
    },
    {
      "epoch": 0.17219445706497047,
      "grad_norm": 3.9690065383911133,
      "learning_rate": 0.0009499465893483901,
      "loss": 1.7665,
      "step": 379
    },
    {
      "epoch": 0.17264879600181735,
      "grad_norm": 5.33690881729126,
      "learning_rate": 0.0009497939874866473,
      "loss": 2.0596,
      "step": 380
    },
    {
      "epoch": 0.17310313493866425,
      "grad_norm": 4.316205024719238,
      "learning_rate": 0.0009496413856249046,
      "loss": 1.522,
      "step": 381
    },
    {
      "epoch": 0.17355747387551113,
      "grad_norm": 2.7867448329925537,
      "learning_rate": 0.000949488783763162,
      "loss": 1.2558,
      "step": 382
    },
    {
      "epoch": 0.17401181281235803,
      "grad_norm": 3.978105068206787,
      "learning_rate": 0.0009493361819014192,
      "loss": 1.0527,
      "step": 383
    },
    {
      "epoch": 0.1744661517492049,
      "grad_norm": 3.791734457015991,
      "learning_rate": 0.0009491835800396765,
      "loss": 1.4442,
      "step": 384
    },
    {
      "epoch": 0.17492049068605178,
      "grad_norm": 2.4090046882629395,
      "learning_rate": 0.0009490309781779338,
      "loss": 0.8097,
      "step": 385
    },
    {
      "epoch": 0.1753748296228987,
      "grad_norm": 4.924672603607178,
      "learning_rate": 0.0009488783763161911,
      "loss": 1.5682,
      "step": 386
    },
    {
      "epoch": 0.17582916855974556,
      "grad_norm": 6.532632827758789,
      "learning_rate": 0.0009487257744544484,
      "loss": 1.7224,
      "step": 387
    },
    {
      "epoch": 0.17628350749659247,
      "grad_norm": 4.940225124359131,
      "learning_rate": 0.0009485731725927057,
      "loss": 1.1896,
      "step": 388
    },
    {
      "epoch": 0.17673784643343934,
      "grad_norm": 4.2179975509643555,
      "learning_rate": 0.0009484205707309629,
      "loss": 1.2505,
      "step": 389
    },
    {
      "epoch": 0.17719218537028622,
      "grad_norm": 2.875326156616211,
      "learning_rate": 0.0009482679688692201,
      "loss": 1.0426,
      "step": 390
    },
    {
      "epoch": 0.17764652430713312,
      "grad_norm": 6.014885425567627,
      "learning_rate": 0.0009481153670074775,
      "loss": 1.7501,
      "step": 391
    },
    {
      "epoch": 0.17810086324398,
      "grad_norm": 6.904287815093994,
      "learning_rate": 0.0009479627651457348,
      "loss": 1.3953,
      "step": 392
    },
    {
      "epoch": 0.1785552021808269,
      "grad_norm": 3.87725830078125,
      "learning_rate": 0.000947810163283992,
      "loss": 1.576,
      "step": 393
    },
    {
      "epoch": 0.17900954111767378,
      "grad_norm": 4.728071212768555,
      "learning_rate": 0.0009476575614222494,
      "loss": 2.1511,
      "step": 394
    },
    {
      "epoch": 0.17946388005452069,
      "grad_norm": 5.006023406982422,
      "learning_rate": 0.0009475049595605066,
      "loss": 1.5623,
      "step": 395
    },
    {
      "epoch": 0.17991821899136756,
      "grad_norm": 4.96576452255249,
      "learning_rate": 0.0009473523576987639,
      "loss": 2.081,
      "step": 396
    },
    {
      "epoch": 0.18037255792821444,
      "grad_norm": 7.731472015380859,
      "learning_rate": 0.0009471997558370212,
      "loss": 1.5883,
      "step": 397
    },
    {
      "epoch": 0.18082689686506134,
      "grad_norm": 5.842845916748047,
      "learning_rate": 0.0009470471539752785,
      "loss": 1.5457,
      "step": 398
    },
    {
      "epoch": 0.18128123580190822,
      "grad_norm": 8.413512229919434,
      "learning_rate": 0.0009468945521135358,
      "loss": 1.4936,
      "step": 399
    },
    {
      "epoch": 0.18173557473875512,
      "grad_norm": 7.487287998199463,
      "learning_rate": 0.0009467419502517931,
      "loss": 2.2145,
      "step": 400
    },
    {
      "epoch": 0.182189913675602,
      "grad_norm": 5.437649726867676,
      "learning_rate": 0.0009465893483900504,
      "loss": 1.7753,
      "step": 401
    },
    {
      "epoch": 0.18264425261244888,
      "grad_norm": 6.131345272064209,
      "learning_rate": 0.0009464367465283076,
      "loss": 1.4985,
      "step": 402
    },
    {
      "epoch": 0.18309859154929578,
      "grad_norm": 2.614121437072754,
      "learning_rate": 0.000946284144666565,
      "loss": 1.1369,
      "step": 403
    },
    {
      "epoch": 0.18355293048614266,
      "grad_norm": 7.6424970626831055,
      "learning_rate": 0.0009461315428048223,
      "loss": 1.9627,
      "step": 404
    },
    {
      "epoch": 0.18400726942298956,
      "grad_norm": 5.5819268226623535,
      "learning_rate": 0.0009459789409430795,
      "loss": 1.2637,
      "step": 405
    },
    {
      "epoch": 0.18446160835983644,
      "grad_norm": 4.559139728546143,
      "learning_rate": 0.0009458263390813369,
      "loss": 1.3853,
      "step": 406
    },
    {
      "epoch": 0.18491594729668331,
      "grad_norm": 3.6235055923461914,
      "learning_rate": 0.000945673737219594,
      "loss": 0.9448,
      "step": 407
    },
    {
      "epoch": 0.18537028623353022,
      "grad_norm": 5.552212715148926,
      "learning_rate": 0.0009455211353578513,
      "loss": 1.6926,
      "step": 408
    },
    {
      "epoch": 0.1858246251703771,
      "grad_norm": 4.262753486633301,
      "learning_rate": 0.0009453685334961087,
      "loss": 1.1928,
      "step": 409
    },
    {
      "epoch": 0.186278964107224,
      "grad_norm": 4.875225067138672,
      "learning_rate": 0.0009452159316343659,
      "loss": 1.331,
      "step": 410
    },
    {
      "epoch": 0.18673330304407088,
      "grad_norm": 5.551540374755859,
      "learning_rate": 0.0009450633297726232,
      "loss": 1.6461,
      "step": 411
    },
    {
      "epoch": 0.18718764198091775,
      "grad_norm": 7.446421146392822,
      "learning_rate": 0.0009449107279108805,
      "loss": 1.3242,
      "step": 412
    },
    {
      "epoch": 0.18764198091776466,
      "grad_norm": 7.013265609741211,
      "learning_rate": 0.0009447581260491378,
      "loss": 1.6484,
      "step": 413
    },
    {
      "epoch": 0.18809631985461153,
      "grad_norm": 4.385944843292236,
      "learning_rate": 0.000944605524187395,
      "loss": 1.0595,
      "step": 414
    },
    {
      "epoch": 0.18855065879145844,
      "grad_norm": 3.26851487159729,
      "learning_rate": 0.0009444529223256524,
      "loss": 1.0926,
      "step": 415
    },
    {
      "epoch": 0.1890049977283053,
      "grad_norm": 5.806207180023193,
      "learning_rate": 0.0009443003204639097,
      "loss": 2.5344,
      "step": 416
    },
    {
      "epoch": 0.18945933666515222,
      "grad_norm": 5.659852504730225,
      "learning_rate": 0.0009441477186021669,
      "loss": 1.9432,
      "step": 417
    },
    {
      "epoch": 0.1899136756019991,
      "grad_norm": 5.077337741851807,
      "learning_rate": 0.0009439951167404243,
      "loss": 1.6054,
      "step": 418
    },
    {
      "epoch": 0.19036801453884597,
      "grad_norm": 5.186604022979736,
      "learning_rate": 0.0009438425148786815,
      "loss": 2.5691,
      "step": 419
    },
    {
      "epoch": 0.19082235347569287,
      "grad_norm": 6.871816635131836,
      "learning_rate": 0.0009436899130169388,
      "loss": 1.4415,
      "step": 420
    },
    {
      "epoch": 0.19127669241253975,
      "grad_norm": 4.514774799346924,
      "learning_rate": 0.0009435373111551962,
      "loss": 1.474,
      "step": 421
    },
    {
      "epoch": 0.19173103134938665,
      "grad_norm": 2.7210142612457275,
      "learning_rate": 0.0009433847092934534,
      "loss": 1.2143,
      "step": 422
    },
    {
      "epoch": 0.19218537028623353,
      "grad_norm": 6.983016490936279,
      "learning_rate": 0.0009432321074317108,
      "loss": 1.4631,
      "step": 423
    },
    {
      "epoch": 0.1926397092230804,
      "grad_norm": 7.309233665466309,
      "learning_rate": 0.000943079505569968,
      "loss": 2.0752,
      "step": 424
    },
    {
      "epoch": 0.1930940481599273,
      "grad_norm": 2.8916566371917725,
      "learning_rate": 0.0009429269037082253,
      "loss": 0.6985,
      "step": 425
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 5.820022106170654,
      "learning_rate": 0.0009427743018464824,
      "loss": 2.4693,
      "step": 426
    },
    {
      "epoch": 0.1940027260336211,
      "grad_norm": 7.536548137664795,
      "learning_rate": 0.0009426216999847398,
      "loss": 2.2119,
      "step": 427
    },
    {
      "epoch": 0.19445706497046797,
      "grad_norm": 4.727935791015625,
      "learning_rate": 0.0009424690981229971,
      "loss": 1.3794,
      "step": 428
    },
    {
      "epoch": 0.19491140390731485,
      "grad_norm": 6.465019702911377,
      "learning_rate": 0.0009423164962612543,
      "loss": 1.9791,
      "step": 429
    },
    {
      "epoch": 0.19536574284416175,
      "grad_norm": 5.451267242431641,
      "learning_rate": 0.0009421638943995117,
      "loss": 1.7393,
      "step": 430
    },
    {
      "epoch": 0.19582008178100863,
      "grad_norm": 7.749899387359619,
      "learning_rate": 0.0009420112925377689,
      "loss": 2.6558,
      "step": 431
    },
    {
      "epoch": 0.19627442071785553,
      "grad_norm": 4.837016582489014,
      "learning_rate": 0.0009418586906760263,
      "loss": 1.2864,
      "step": 432
    },
    {
      "epoch": 0.1967287596547024,
      "grad_norm": 4.787230968475342,
      "learning_rate": 0.0009417060888142836,
      "loss": 1.4421,
      "step": 433
    },
    {
      "epoch": 0.19718309859154928,
      "grad_norm": 4.947290420532227,
      "learning_rate": 0.0009415534869525408,
      "loss": 1.1981,
      "step": 434
    },
    {
      "epoch": 0.1976374375283962,
      "grad_norm": 5.157255172729492,
      "learning_rate": 0.0009414008850907982,
      "loss": 1.2748,
      "step": 435
    },
    {
      "epoch": 0.19809177646524306,
      "grad_norm": 8.211413383483887,
      "learning_rate": 0.0009412482832290554,
      "loss": 1.7366,
      "step": 436
    },
    {
      "epoch": 0.19854611540208997,
      "grad_norm": 4.752699851989746,
      "learning_rate": 0.0009410956813673127,
      "loss": 1.5452,
      "step": 437
    },
    {
      "epoch": 0.19900045433893684,
      "grad_norm": 4.8603596687316895,
      "learning_rate": 0.0009409430795055701,
      "loss": 1.8112,
      "step": 438
    },
    {
      "epoch": 0.19945479327578375,
      "grad_norm": 6.218367099761963,
      "learning_rate": 0.0009407904776438273,
      "loss": 2.2075,
      "step": 439
    },
    {
      "epoch": 0.19990913221263062,
      "grad_norm": 2.5570054054260254,
      "learning_rate": 0.0009406378757820846,
      "loss": 0.7072,
      "step": 440
    },
    {
      "epoch": 0.2003634711494775,
      "grad_norm": 4.988033294677734,
      "learning_rate": 0.0009404852739203419,
      "loss": 2.3541,
      "step": 441
    },
    {
      "epoch": 0.2008178100863244,
      "grad_norm": 4.045705795288086,
      "learning_rate": 0.0009403326720585992,
      "loss": 0.9018,
      "step": 442
    },
    {
      "epoch": 0.20127214902317128,
      "grad_norm": 7.6420159339904785,
      "learning_rate": 0.0009401800701968565,
      "loss": 1.9201,
      "step": 443
    },
    {
      "epoch": 0.20172648796001819,
      "grad_norm": 9.172426223754883,
      "learning_rate": 0.0009400274683351137,
      "loss": 0.9007,
      "step": 444
    },
    {
      "epoch": 0.20218082689686506,
      "grad_norm": 3.9377553462982178,
      "learning_rate": 0.000939874866473371,
      "loss": 1.2728,
      "step": 445
    },
    {
      "epoch": 0.20263516583371194,
      "grad_norm": 5.961678981781006,
      "learning_rate": 0.0009397222646116282,
      "loss": 1.3516,
      "step": 446
    },
    {
      "epoch": 0.20308950477055884,
      "grad_norm": 7.464356422424316,
      "learning_rate": 0.0009395696627498856,
      "loss": 1.9587,
      "step": 447
    },
    {
      "epoch": 0.20354384370740572,
      "grad_norm": 5.066741466522217,
      "learning_rate": 0.0009394170608881428,
      "loss": 1.8978,
      "step": 448
    },
    {
      "epoch": 0.20399818264425262,
      "grad_norm": 8.800253868103027,
      "learning_rate": 0.0009392644590264001,
      "loss": 2.306,
      "step": 449
    },
    {
      "epoch": 0.2044525215810995,
      "grad_norm": 4.4850640296936035,
      "learning_rate": 0.0009391118571646575,
      "loss": 1.2291,
      "step": 450
    },
    {
      "epoch": 0.20490686051794638,
      "grad_norm": 7.148253440856934,
      "learning_rate": 0.0009389592553029147,
      "loss": 1.8733,
      "step": 451
    },
    {
      "epoch": 0.20536119945479328,
      "grad_norm": 3.8067727088928223,
      "learning_rate": 0.000938806653441172,
      "loss": 1.5931,
      "step": 452
    },
    {
      "epoch": 0.20581553839164016,
      "grad_norm": 2.8213534355163574,
      "learning_rate": 0.0009386540515794293,
      "loss": 0.7292,
      "step": 453
    },
    {
      "epoch": 0.20626987732848706,
      "grad_norm": 5.033578395843506,
      "learning_rate": 0.0009385014497176866,
      "loss": 1.351,
      "step": 454
    },
    {
      "epoch": 0.20672421626533394,
      "grad_norm": 7.203210353851318,
      "learning_rate": 0.0009383488478559439,
      "loss": 1.9794,
      "step": 455
    },
    {
      "epoch": 0.20717855520218081,
      "grad_norm": 6.388524532318115,
      "learning_rate": 0.0009381962459942012,
      "loss": 2.3093,
      "step": 456
    },
    {
      "epoch": 0.20763289413902772,
      "grad_norm": 8.489537239074707,
      "learning_rate": 0.0009380436441324585,
      "loss": 1.4224,
      "step": 457
    },
    {
      "epoch": 0.2080872330758746,
      "grad_norm": 6.995753765106201,
      "learning_rate": 0.0009378910422707157,
      "loss": 1.8721,
      "step": 458
    },
    {
      "epoch": 0.2085415720127215,
      "grad_norm": 6.279966831207275,
      "learning_rate": 0.0009377384404089731,
      "loss": 1.3,
      "step": 459
    },
    {
      "epoch": 0.20899591094956838,
      "grad_norm": 5.739799499511719,
      "learning_rate": 0.0009375858385472304,
      "loss": 1.5835,
      "step": 460
    },
    {
      "epoch": 0.20945024988641528,
      "grad_norm": 6.339118480682373,
      "learning_rate": 0.0009374332366854876,
      "loss": 1.6173,
      "step": 461
    },
    {
      "epoch": 0.20990458882326216,
      "grad_norm": 7.6310200691223145,
      "learning_rate": 0.0009372806348237449,
      "loss": 2.0345,
      "step": 462
    },
    {
      "epoch": 0.21035892776010903,
      "grad_norm": 5.157326698303223,
      "learning_rate": 0.0009371280329620021,
      "loss": 2.051,
      "step": 463
    },
    {
      "epoch": 0.21081326669695594,
      "grad_norm": 3.588669538497925,
      "learning_rate": 0.0009369754311002594,
      "loss": 1.1911,
      "step": 464
    },
    {
      "epoch": 0.2112676056338028,
      "grad_norm": 4.343991279602051,
      "learning_rate": 0.0009368228292385167,
      "loss": 1.7756,
      "step": 465
    },
    {
      "epoch": 0.21172194457064972,
      "grad_norm": 5.403534889221191,
      "learning_rate": 0.000936670227376774,
      "loss": 1.8344,
      "step": 466
    },
    {
      "epoch": 0.2121762835074966,
      "grad_norm": 4.733722686767578,
      "learning_rate": 0.0009365176255150313,
      "loss": 1.6148,
      "step": 467
    },
    {
      "epoch": 0.21263062244434347,
      "grad_norm": 4.588109493255615,
      "learning_rate": 0.0009363650236532886,
      "loss": 1.3201,
      "step": 468
    },
    {
      "epoch": 0.21308496138119037,
      "grad_norm": 5.961502552032471,
      "learning_rate": 0.0009362124217915459,
      "loss": 1.4417,
      "step": 469
    },
    {
      "epoch": 0.21353930031803725,
      "grad_norm": 6.888246536254883,
      "learning_rate": 0.0009360598199298031,
      "loss": 1.8016,
      "step": 470
    },
    {
      "epoch": 0.21399363925488415,
      "grad_norm": 6.24324369430542,
      "learning_rate": 0.0009359072180680605,
      "loss": 2.0089,
      "step": 471
    },
    {
      "epoch": 0.21444797819173103,
      "grad_norm": 4.939812183380127,
      "learning_rate": 0.0009357546162063178,
      "loss": 1.6091,
      "step": 472
    },
    {
      "epoch": 0.2149023171285779,
      "grad_norm": 4.994645595550537,
      "learning_rate": 0.000935602014344575,
      "loss": 1.1565,
      "step": 473
    },
    {
      "epoch": 0.2153566560654248,
      "grad_norm": 4.653181552886963,
      "learning_rate": 0.0009354494124828324,
      "loss": 1.1146,
      "step": 474
    },
    {
      "epoch": 0.2158109950022717,
      "grad_norm": 4.534937381744385,
      "learning_rate": 0.0009352968106210896,
      "loss": 1.5195,
      "step": 475
    },
    {
      "epoch": 0.2162653339391186,
      "grad_norm": 6.994931221008301,
      "learning_rate": 0.0009351442087593469,
      "loss": 1.5121,
      "step": 476
    },
    {
      "epoch": 0.21671967287596547,
      "grad_norm": 7.108386039733887,
      "learning_rate": 0.0009349916068976043,
      "loss": 2.3431,
      "step": 477
    },
    {
      "epoch": 0.21717401181281235,
      "grad_norm": 5.635793685913086,
      "learning_rate": 0.0009348390050358615,
      "loss": 1.5281,
      "step": 478
    },
    {
      "epoch": 0.21762835074965925,
      "grad_norm": 3.227243185043335,
      "learning_rate": 0.0009346864031741188,
      "loss": 1.0244,
      "step": 479
    },
    {
      "epoch": 0.21808268968650613,
      "grad_norm": 5.443169593811035,
      "learning_rate": 0.000934533801312376,
      "loss": 2.0097,
      "step": 480
    },
    {
      "epoch": 0.21853702862335303,
      "grad_norm": 4.196178913116455,
      "learning_rate": 0.0009343811994506333,
      "loss": 0.8677,
      "step": 481
    },
    {
      "epoch": 0.2189913675601999,
      "grad_norm": 5.010131359100342,
      "learning_rate": 0.0009342285975888905,
      "loss": 1.1281,
      "step": 482
    },
    {
      "epoch": 0.2194457064970468,
      "grad_norm": 4.863934516906738,
      "learning_rate": 0.0009340759957271479,
      "loss": 1.1286,
      "step": 483
    },
    {
      "epoch": 0.2199000454338937,
      "grad_norm": 6.517072677612305,
      "learning_rate": 0.0009339233938654052,
      "loss": 1.7019,
      "step": 484
    },
    {
      "epoch": 0.22035438437074056,
      "grad_norm": 3.062812089920044,
      "learning_rate": 0.0009337707920036624,
      "loss": 0.884,
      "step": 485
    },
    {
      "epoch": 0.22080872330758747,
      "grad_norm": 5.0589728355407715,
      "learning_rate": 0.0009336181901419198,
      "loss": 2.0554,
      "step": 486
    },
    {
      "epoch": 0.22126306224443434,
      "grad_norm": 5.991934776306152,
      "learning_rate": 0.000933465588280177,
      "loss": 2.6971,
      "step": 487
    },
    {
      "epoch": 0.22171740118128125,
      "grad_norm": 4.487901210784912,
      "learning_rate": 0.0009333129864184343,
      "loss": 1.1536,
      "step": 488
    },
    {
      "epoch": 0.22217174011812812,
      "grad_norm": 4.74009895324707,
      "learning_rate": 0.0009331603845566917,
      "loss": 1.5551,
      "step": 489
    },
    {
      "epoch": 0.222626079054975,
      "grad_norm": 5.753303050994873,
      "learning_rate": 0.0009330077826949489,
      "loss": 2.2021,
      "step": 490
    },
    {
      "epoch": 0.2230804179918219,
      "grad_norm": 4.603237628936768,
      "learning_rate": 0.0009328551808332062,
      "loss": 0.8779,
      "step": 491
    },
    {
      "epoch": 0.22353475692866878,
      "grad_norm": 3.4563276767730713,
      "learning_rate": 0.0009327025789714635,
      "loss": 1.3759,
      "step": 492
    },
    {
      "epoch": 0.22398909586551569,
      "grad_norm": 4.847369194030762,
      "learning_rate": 0.0009325499771097208,
      "loss": 1.4832,
      "step": 493
    },
    {
      "epoch": 0.22444343480236256,
      "grad_norm": 7.879354000091553,
      "learning_rate": 0.000932397375247978,
      "loss": 1.6465,
      "step": 494
    },
    {
      "epoch": 0.22489777373920944,
      "grad_norm": 2.858377695083618,
      "learning_rate": 0.0009322447733862354,
      "loss": 0.8818,
      "step": 495
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 4.103867053985596,
      "learning_rate": 0.0009320921715244927,
      "loss": 1.1715,
      "step": 496
    },
    {
      "epoch": 0.22580645161290322,
      "grad_norm": 4.983833312988281,
      "learning_rate": 0.0009319395696627499,
      "loss": 1.978,
      "step": 497
    },
    {
      "epoch": 0.22626079054975012,
      "grad_norm": 5.14265251159668,
      "learning_rate": 0.0009317869678010073,
      "loss": 1.3398,
      "step": 498
    },
    {
      "epoch": 0.226715129486597,
      "grad_norm": 4.244718074798584,
      "learning_rate": 0.0009316343659392644,
      "loss": 0.7031,
      "step": 499
    },
    {
      "epoch": 0.22716946842344388,
      "grad_norm": 4.052267551422119,
      "learning_rate": 0.0009314817640775217,
      "loss": 1.3007,
      "step": 500
    },
    {
      "epoch": 0.22762380736029078,
      "grad_norm": 3.4163036346435547,
      "learning_rate": 0.0009313291622157791,
      "loss": 0.7556,
      "step": 501
    },
    {
      "epoch": 0.22807814629713766,
      "grad_norm": 4.7087860107421875,
      "learning_rate": 0.0009311765603540363,
      "loss": 1.235,
      "step": 502
    },
    {
      "epoch": 0.22853248523398456,
      "grad_norm": 5.557741165161133,
      "learning_rate": 0.0009310239584922936,
      "loss": 2.3006,
      "step": 503
    },
    {
      "epoch": 0.22898682417083144,
      "grad_norm": 6.312621593475342,
      "learning_rate": 0.0009308713566305509,
      "loss": 1.7725,
      "step": 504
    },
    {
      "epoch": 0.22944116310767831,
      "grad_norm": 7.39268684387207,
      "learning_rate": 0.0009307187547688082,
      "loss": 1.9056,
      "step": 505
    },
    {
      "epoch": 0.22989550204452522,
      "grad_norm": 3.0886428356170654,
      "learning_rate": 0.0009305661529070654,
      "loss": 0.9491,
      "step": 506
    },
    {
      "epoch": 0.2303498409813721,
      "grad_norm": 4.250034809112549,
      "learning_rate": 0.0009304135510453228,
      "loss": 0.9298,
      "step": 507
    },
    {
      "epoch": 0.230804179918219,
      "grad_norm": 3.524555206298828,
      "learning_rate": 0.0009302609491835801,
      "loss": 1.5024,
      "step": 508
    },
    {
      "epoch": 0.23125851885506588,
      "grad_norm": 4.884903907775879,
      "learning_rate": 0.0009301083473218373,
      "loss": 1.6223,
      "step": 509
    },
    {
      "epoch": 0.23171285779191278,
      "grad_norm": 6.04000186920166,
      "learning_rate": 0.0009299557454600947,
      "loss": 1.4292,
      "step": 510
    },
    {
      "epoch": 0.23216719672875966,
      "grad_norm": 4.823760986328125,
      "learning_rate": 0.000929803143598352,
      "loss": 1.2106,
      "step": 511
    },
    {
      "epoch": 0.23262153566560653,
      "grad_norm": 3.3610918521881104,
      "learning_rate": 0.0009296505417366092,
      "loss": 1.5498,
      "step": 512
    },
    {
      "epoch": 0.23307587460245344,
      "grad_norm": 9.057700157165527,
      "learning_rate": 0.0009294979398748666,
      "loss": 1.916,
      "step": 513
    },
    {
      "epoch": 0.2335302135393003,
      "grad_norm": 3.9425299167633057,
      "learning_rate": 0.0009293453380131238,
      "loss": 1.3938,
      "step": 514
    },
    {
      "epoch": 0.23398455247614722,
      "grad_norm": 5.285974025726318,
      "learning_rate": 0.0009291927361513811,
      "loss": 1.9874,
      "step": 515
    },
    {
      "epoch": 0.2344388914129941,
      "grad_norm": 4.023134231567383,
      "learning_rate": 0.0009290401342896384,
      "loss": 0.8728,
      "step": 516
    },
    {
      "epoch": 0.23489323034984097,
      "grad_norm": 7.8974928855896,
      "learning_rate": 0.0009288875324278956,
      "loss": 1.6332,
      "step": 517
    },
    {
      "epoch": 0.23534756928668787,
      "grad_norm": 7.250007152557373,
      "learning_rate": 0.0009287349305661529,
      "loss": 2.14,
      "step": 518
    },
    {
      "epoch": 0.23580190822353475,
      "grad_norm": 5.450918674468994,
      "learning_rate": 0.0009285823287044102,
      "loss": 1.3023,
      "step": 519
    },
    {
      "epoch": 0.23625624716038165,
      "grad_norm": 6.659681797027588,
      "learning_rate": 0.0009284297268426675,
      "loss": 1.7903,
      "step": 520
    },
    {
      "epoch": 0.23671058609722853,
      "grad_norm": 9.149643898010254,
      "learning_rate": 0.0009282771249809247,
      "loss": 2.3153,
      "step": 521
    },
    {
      "epoch": 0.2371649250340754,
      "grad_norm": 4.200259685516357,
      "learning_rate": 0.0009281245231191821,
      "loss": 1.4539,
      "step": 522
    },
    {
      "epoch": 0.2376192639709223,
      "grad_norm": 6.091450214385986,
      "learning_rate": 0.0009279719212574393,
      "loss": 2.5699,
      "step": 523
    },
    {
      "epoch": 0.2380736029077692,
      "grad_norm": 3.177809000015259,
      "learning_rate": 0.0009278193193956966,
      "loss": 0.8812,
      "step": 524
    },
    {
      "epoch": 0.2385279418446161,
      "grad_norm": 4.273008346557617,
      "learning_rate": 0.000927666717533954,
      "loss": 1.3761,
      "step": 525
    },
    {
      "epoch": 0.23898228078146297,
      "grad_norm": 2.9500999450683594,
      "learning_rate": 0.0009275141156722112,
      "loss": 0.7171,
      "step": 526
    },
    {
      "epoch": 0.23943661971830985,
      "grad_norm": 4.709161281585693,
      "learning_rate": 0.0009273615138104685,
      "loss": 1.1487,
      "step": 527
    },
    {
      "epoch": 0.23989095865515675,
      "grad_norm": 6.275514125823975,
      "learning_rate": 0.0009272089119487258,
      "loss": 2.0007,
      "step": 528
    },
    {
      "epoch": 0.24034529759200363,
      "grad_norm": 3.549431800842285,
      "learning_rate": 0.0009270563100869831,
      "loss": 1.5391,
      "step": 529
    },
    {
      "epoch": 0.24079963652885053,
      "grad_norm": 3.827634334564209,
      "learning_rate": 0.0009269037082252404,
      "loss": 2.0854,
      "step": 530
    },
    {
      "epoch": 0.2412539754656974,
      "grad_norm": 3.044224262237549,
      "learning_rate": 0.0009267511063634977,
      "loss": 0.8641,
      "step": 531
    },
    {
      "epoch": 0.2417083144025443,
      "grad_norm": 4.237382888793945,
      "learning_rate": 0.000926598504501755,
      "loss": 0.8488,
      "step": 532
    },
    {
      "epoch": 0.2421626533393912,
      "grad_norm": 6.293590545654297,
      "learning_rate": 0.0009264459026400122,
      "loss": 1.7358,
      "step": 533
    },
    {
      "epoch": 0.24261699227623806,
      "grad_norm": 4.809879302978516,
      "learning_rate": 0.0009262933007782696,
      "loss": 1.1807,
      "step": 534
    },
    {
      "epoch": 0.24307133121308497,
      "grad_norm": 4.56773567199707,
      "learning_rate": 0.0009261406989165268,
      "loss": 1.4164,
      "step": 535
    },
    {
      "epoch": 0.24352567014993184,
      "grad_norm": 4.453222274780273,
      "learning_rate": 0.000925988097054784,
      "loss": 1.759,
      "step": 536
    },
    {
      "epoch": 0.24398000908677875,
      "grad_norm": 4.046647548675537,
      "learning_rate": 0.0009258354951930414,
      "loss": 0.8062,
      "step": 537
    },
    {
      "epoch": 0.24443434802362562,
      "grad_norm": 5.971732139587402,
      "learning_rate": 0.0009256828933312986,
      "loss": 1.7187,
      "step": 538
    },
    {
      "epoch": 0.2448886869604725,
      "grad_norm": 4.943645000457764,
      "learning_rate": 0.0009255302914695559,
      "loss": 0.962,
      "step": 539
    },
    {
      "epoch": 0.2453430258973194,
      "grad_norm": 2.518033504486084,
      "learning_rate": 0.0009253776896078133,
      "loss": 1.064,
      "step": 540
    },
    {
      "epoch": 0.24579736483416628,
      "grad_norm": 8.027234077453613,
      "learning_rate": 0.0009252250877460705,
      "loss": 2.5086,
      "step": 541
    },
    {
      "epoch": 0.2462517037710132,
      "grad_norm": 5.246140003204346,
      "learning_rate": 0.0009250724858843278,
      "loss": 1.5558,
      "step": 542
    },
    {
      "epoch": 0.24670604270786006,
      "grad_norm": 5.483332633972168,
      "learning_rate": 0.0009249198840225851,
      "loss": 2.0179,
      "step": 543
    },
    {
      "epoch": 0.24716038164470694,
      "grad_norm": 3.19100022315979,
      "learning_rate": 0.0009247672821608424,
      "loss": 0.7591,
      "step": 544
    },
    {
      "epoch": 0.24761472058155384,
      "grad_norm": 6.662031650543213,
      "learning_rate": 0.0009246146802990996,
      "loss": 1.5568,
      "step": 545
    },
    {
      "epoch": 0.24806905951840072,
      "grad_norm": 5.204570293426514,
      "learning_rate": 0.000924462078437357,
      "loss": 0.8228,
      "step": 546
    },
    {
      "epoch": 0.24852339845524762,
      "grad_norm": 5.730715274810791,
      "learning_rate": 0.0009243094765756143,
      "loss": 1.5326,
      "step": 547
    },
    {
      "epoch": 0.2489777373920945,
      "grad_norm": 6.679883003234863,
      "learning_rate": 0.0009241568747138715,
      "loss": 1.529,
      "step": 548
    },
    {
      "epoch": 0.24943207632894138,
      "grad_norm": 10.029292106628418,
      "learning_rate": 0.0009240042728521289,
      "loss": 1.7187,
      "step": 549
    },
    {
      "epoch": 0.24988641526578828,
      "grad_norm": 5.180072784423828,
      "learning_rate": 0.0009238516709903861,
      "loss": 1.4721,
      "step": 550
    },
    {
      "epoch": 0.2503407542026352,
      "grad_norm": 3.6068618297576904,
      "learning_rate": 0.0009236990691286434,
      "loss": 0.528,
      "step": 551
    },
    {
      "epoch": 0.25079509313948206,
      "grad_norm": 4.126284599304199,
      "learning_rate": 0.0009235464672669008,
      "loss": 1.4289,
      "step": 552
    },
    {
      "epoch": 0.25124943207632894,
      "grad_norm": 8.755053520202637,
      "learning_rate": 0.0009233938654051579,
      "loss": 1.3203,
      "step": 553
    },
    {
      "epoch": 0.2517037710131758,
      "grad_norm": 5.032867431640625,
      "learning_rate": 0.0009232412635434152,
      "loss": 1.4822,
      "step": 554
    },
    {
      "epoch": 0.2521581099500227,
      "grad_norm": 6.3786396980285645,
      "learning_rate": 0.0009230886616816725,
      "loss": 0.8053,
      "step": 555
    },
    {
      "epoch": 0.2526124488868696,
      "grad_norm": 5.1681671142578125,
      "learning_rate": 0.0009229360598199298,
      "loss": 2.4012,
      "step": 556
    },
    {
      "epoch": 0.2530667878237165,
      "grad_norm": 4.100263595581055,
      "learning_rate": 0.000922783457958187,
      "loss": 0.8291,
      "step": 557
    },
    {
      "epoch": 0.2535211267605634,
      "grad_norm": 3.3364055156707764,
      "learning_rate": 0.0009226308560964444,
      "loss": 0.5106,
      "step": 558
    },
    {
      "epoch": 0.25397546569741025,
      "grad_norm": 4.887160301208496,
      "learning_rate": 0.0009224782542347017,
      "loss": 1.0372,
      "step": 559
    },
    {
      "epoch": 0.25442980463425713,
      "grad_norm": 4.523820877075195,
      "learning_rate": 0.0009223256523729589,
      "loss": 1.1551,
      "step": 560
    },
    {
      "epoch": 0.25488414357110406,
      "grad_norm": 5.33235502243042,
      "learning_rate": 0.0009221730505112163,
      "loss": 1.3016,
      "step": 561
    },
    {
      "epoch": 0.25533848250795094,
      "grad_norm": 6.062936305999756,
      "learning_rate": 0.0009220204486494735,
      "loss": 1.3854,
      "step": 562
    },
    {
      "epoch": 0.2557928214447978,
      "grad_norm": 6.7851433753967285,
      "learning_rate": 0.0009218678467877308,
      "loss": 2.0811,
      "step": 563
    },
    {
      "epoch": 0.2562471603816447,
      "grad_norm": 5.023318290710449,
      "learning_rate": 0.0009217152449259882,
      "loss": 1.1233,
      "step": 564
    },
    {
      "epoch": 0.2567014993184916,
      "grad_norm": 5.412562370300293,
      "learning_rate": 0.0009215626430642454,
      "loss": 1.226,
      "step": 565
    },
    {
      "epoch": 0.2571558382553385,
      "grad_norm": 5.4040913581848145,
      "learning_rate": 0.0009214100412025027,
      "loss": 1.1275,
      "step": 566
    },
    {
      "epoch": 0.2576101771921854,
      "grad_norm": 4.76725959777832,
      "learning_rate": 0.00092125743934076,
      "loss": 1.6946,
      "step": 567
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 4.946346759796143,
      "learning_rate": 0.0009211048374790173,
      "loss": 1.3524,
      "step": 568
    },
    {
      "epoch": 0.2585188550658791,
      "grad_norm": 5.389051914215088,
      "learning_rate": 0.0009209522356172746,
      "loss": 1.7749,
      "step": 569
    },
    {
      "epoch": 0.25897319400272606,
      "grad_norm": 5.0096755027771,
      "learning_rate": 0.0009207996337555319,
      "loss": 1.3946,
      "step": 570
    },
    {
      "epoch": 0.25942753293957294,
      "grad_norm": 5.072806358337402,
      "learning_rate": 0.0009206470318937892,
      "loss": 2.0729,
      "step": 571
    },
    {
      "epoch": 0.2598818718764198,
      "grad_norm": 4.723791122436523,
      "learning_rate": 0.0009204944300320463,
      "loss": 1.3054,
      "step": 572
    },
    {
      "epoch": 0.2603362108132667,
      "grad_norm": 5.575528144836426,
      "learning_rate": 0.0009203418281703037,
      "loss": 1.5758,
      "step": 573
    },
    {
      "epoch": 0.26079054975011356,
      "grad_norm": 6.84596061706543,
      "learning_rate": 0.0009201892263085609,
      "loss": 1.7747,
      "step": 574
    },
    {
      "epoch": 0.2612448886869605,
      "grad_norm": 8.100935935974121,
      "learning_rate": 0.0009200366244468182,
      "loss": 0.7659,
      "step": 575
    },
    {
      "epoch": 0.2616992276238074,
      "grad_norm": 5.519986629486084,
      "learning_rate": 0.0009198840225850756,
      "loss": 2.2464,
      "step": 576
    },
    {
      "epoch": 0.26215356656065425,
      "grad_norm": 4.773331642150879,
      "learning_rate": 0.0009197314207233328,
      "loss": 1.33,
      "step": 577
    },
    {
      "epoch": 0.2626079054975011,
      "grad_norm": 5.48815393447876,
      "learning_rate": 0.0009195788188615901,
      "loss": 1.2504,
      "step": 578
    },
    {
      "epoch": 0.263062244434348,
      "grad_norm": 4.384014129638672,
      "learning_rate": 0.0009194262169998474,
      "loss": 0.918,
      "step": 579
    },
    {
      "epoch": 0.26351658337119493,
      "grad_norm": 4.061789512634277,
      "learning_rate": 0.0009192736151381047,
      "loss": 1.2569,
      "step": 580
    },
    {
      "epoch": 0.2639709223080418,
      "grad_norm": 7.078236103057861,
      "learning_rate": 0.000919121013276362,
      "loss": 1.0958,
      "step": 581
    },
    {
      "epoch": 0.2644252612448887,
      "grad_norm": 4.4202117919921875,
      "learning_rate": 0.0009189684114146193,
      "loss": 1.1517,
      "step": 582
    },
    {
      "epoch": 0.26487960018173556,
      "grad_norm": 4.816877841949463,
      "learning_rate": 0.0009188158095528766,
      "loss": 1.5249,
      "step": 583
    },
    {
      "epoch": 0.26533393911858244,
      "grad_norm": 5.253818035125732,
      "learning_rate": 0.0009186632076911338,
      "loss": 1.8154,
      "step": 584
    },
    {
      "epoch": 0.26578827805542937,
      "grad_norm": 7.396517276763916,
      "learning_rate": 0.0009185106058293912,
      "loss": 2.1731,
      "step": 585
    },
    {
      "epoch": 0.26624261699227625,
      "grad_norm": 4.9590301513671875,
      "learning_rate": 0.0009183580039676485,
      "loss": 0.8487,
      "step": 586
    },
    {
      "epoch": 0.2666969559291231,
      "grad_norm": 6.355540752410889,
      "learning_rate": 0.0009182054021059057,
      "loss": 2.0731,
      "step": 587
    },
    {
      "epoch": 0.26715129486597,
      "grad_norm": 4.560359954833984,
      "learning_rate": 0.0009180528002441631,
      "loss": 1.2604,
      "step": 588
    },
    {
      "epoch": 0.2676056338028169,
      "grad_norm": 7.685898303985596,
      "learning_rate": 0.0009179001983824203,
      "loss": 2.0147,
      "step": 589
    },
    {
      "epoch": 0.2680599727396638,
      "grad_norm": 4.235246181488037,
      "learning_rate": 0.0009177475965206775,
      "loss": 0.9788,
      "step": 590
    },
    {
      "epoch": 0.2685143116765107,
      "grad_norm": 4.622025489807129,
      "learning_rate": 0.0009175949946589348,
      "loss": 1.0884,
      "step": 591
    },
    {
      "epoch": 0.26896865061335756,
      "grad_norm": 7.128083229064941,
      "learning_rate": 0.0009174423927971921,
      "loss": 1.599,
      "step": 592
    },
    {
      "epoch": 0.26942298955020444,
      "grad_norm": 3.528709888458252,
      "learning_rate": 0.0009172897909354494,
      "loss": 0.8349,
      "step": 593
    },
    {
      "epoch": 0.2698773284870513,
      "grad_norm": 3.4778661727905273,
      "learning_rate": 0.0009171371890737067,
      "loss": 0.8394,
      "step": 594
    },
    {
      "epoch": 0.27033166742389825,
      "grad_norm": 3.74234676361084,
      "learning_rate": 0.000916984587211964,
      "loss": 1.3801,
      "step": 595
    },
    {
      "epoch": 0.2707860063607451,
      "grad_norm": 5.004946231842041,
      "learning_rate": 0.0009168319853502212,
      "loss": 1.0736,
      "step": 596
    },
    {
      "epoch": 0.271240345297592,
      "grad_norm": 6.021921634674072,
      "learning_rate": 0.0009166793834884786,
      "loss": 2.0274,
      "step": 597
    },
    {
      "epoch": 0.2716946842344389,
      "grad_norm": 4.134975910186768,
      "learning_rate": 0.0009165267816267359,
      "loss": 1.258,
      "step": 598
    },
    {
      "epoch": 0.27214902317128575,
      "grad_norm": 8.249998092651367,
      "learning_rate": 0.0009163741797649931,
      "loss": 1.2967,
      "step": 599
    },
    {
      "epoch": 0.2726033621081327,
      "grad_norm": 5.583903789520264,
      "learning_rate": 0.0009162215779032505,
      "loss": 1.0022,
      "step": 600
    },
    {
      "epoch": 0.27305770104497956,
      "grad_norm": 5.472000598907471,
      "learning_rate": 0.0009160689760415077,
      "loss": 1.8403,
      "step": 601
    },
    {
      "epoch": 0.27351203998182644,
      "grad_norm": 7.100162029266357,
      "learning_rate": 0.000915916374179765,
      "loss": 2.2491,
      "step": 602
    },
    {
      "epoch": 0.2739663789186733,
      "grad_norm": 6.804991722106934,
      "learning_rate": 0.0009157637723180224,
      "loss": 1.4767,
      "step": 603
    },
    {
      "epoch": 0.2744207178555202,
      "grad_norm": 3.161573886871338,
      "learning_rate": 0.0009156111704562796,
      "loss": 0.3372,
      "step": 604
    },
    {
      "epoch": 0.2748750567923671,
      "grad_norm": 4.5229973793029785,
      "learning_rate": 0.0009154585685945369,
      "loss": 1.0128,
      "step": 605
    },
    {
      "epoch": 0.275329395729214,
      "grad_norm": 5.653120040893555,
      "learning_rate": 0.0009153059667327942,
      "loss": 1.336,
      "step": 606
    },
    {
      "epoch": 0.2757837346660609,
      "grad_norm": 2.4268925189971924,
      "learning_rate": 0.0009151533648710515,
      "loss": 0.6291,
      "step": 607
    },
    {
      "epoch": 0.27623807360290775,
      "grad_norm": 1.5728938579559326,
      "learning_rate": 0.0009150007630093086,
      "loss": 0.2365,
      "step": 608
    },
    {
      "epoch": 0.2766924125397547,
      "grad_norm": 5.880955696105957,
      "learning_rate": 0.000914848161147566,
      "loss": 1.3506,
      "step": 609
    },
    {
      "epoch": 0.27714675147660156,
      "grad_norm": 3.3033854961395264,
      "learning_rate": 0.0009146955592858233,
      "loss": 0.7932,
      "step": 610
    },
    {
      "epoch": 0.27760109041344844,
      "grad_norm": 8.630804061889648,
      "learning_rate": 0.0009145429574240805,
      "loss": 2.1612,
      "step": 611
    },
    {
      "epoch": 0.2780554293502953,
      "grad_norm": 7.046489238739014,
      "learning_rate": 0.0009143903555623379,
      "loss": 1.8017,
      "step": 612
    },
    {
      "epoch": 0.2785097682871422,
      "grad_norm": 4.609836578369141,
      "learning_rate": 0.0009142377537005951,
      "loss": 1.0406,
      "step": 613
    },
    {
      "epoch": 0.2789641072239891,
      "grad_norm": 6.363504409790039,
      "learning_rate": 0.0009140851518388524,
      "loss": 1.6503,
      "step": 614
    },
    {
      "epoch": 0.279418446160836,
      "grad_norm": 5.37036657333374,
      "learning_rate": 0.0009139325499771098,
      "loss": 0.8727,
      "step": 615
    },
    {
      "epoch": 0.2798727850976829,
      "grad_norm": 6.577094554901123,
      "learning_rate": 0.000913779948115367,
      "loss": 1.4743,
      "step": 616
    },
    {
      "epoch": 0.28032712403452975,
      "grad_norm": 5.413032531738281,
      "learning_rate": 0.0009136273462536243,
      "loss": 0.7801,
      "step": 617
    },
    {
      "epoch": 0.2807814629713766,
      "grad_norm": 4.95162296295166,
      "learning_rate": 0.0009134747443918816,
      "loss": 1.1429,
      "step": 618
    },
    {
      "epoch": 0.28123580190822356,
      "grad_norm": 5.99307918548584,
      "learning_rate": 0.0009133221425301389,
      "loss": 1.0337,
      "step": 619
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 7.192525386810303,
      "learning_rate": 0.0009131695406683961,
      "loss": 1.4794,
      "step": 620
    },
    {
      "epoch": 0.2821444797819173,
      "grad_norm": 5.389986991882324,
      "learning_rate": 0.0009130169388066535,
      "loss": 1.5985,
      "step": 621
    },
    {
      "epoch": 0.2825988187187642,
      "grad_norm": 6.388218879699707,
      "learning_rate": 0.0009128643369449108,
      "loss": 1.39,
      "step": 622
    },
    {
      "epoch": 0.28305315765561107,
      "grad_norm": 7.198439598083496,
      "learning_rate": 0.000912711735083168,
      "loss": 2.0477,
      "step": 623
    },
    {
      "epoch": 0.283507496592458,
      "grad_norm": 6.589672565460205,
      "learning_rate": 0.0009125591332214254,
      "loss": 1.0592,
      "step": 624
    },
    {
      "epoch": 0.2839618355293049,
      "grad_norm": 7.428500175476074,
      "learning_rate": 0.0009124065313596826,
      "loss": 1.7249,
      "step": 625
    },
    {
      "epoch": 0.28441617446615175,
      "grad_norm": 2.6205573081970215,
      "learning_rate": 0.0009122539294979398,
      "loss": 0.709,
      "step": 626
    },
    {
      "epoch": 0.2848705134029986,
      "grad_norm": 8.424569129943848,
      "learning_rate": 0.0009121013276361972,
      "loss": 2.031,
      "step": 627
    },
    {
      "epoch": 0.2853248523398455,
      "grad_norm": 6.911533355712891,
      "learning_rate": 0.0009119487257744544,
      "loss": 1.0313,
      "step": 628
    },
    {
      "epoch": 0.28577919127669243,
      "grad_norm": 7.072881698608398,
      "learning_rate": 0.0009117961239127117,
      "loss": 2.3812,
      "step": 629
    },
    {
      "epoch": 0.2862335302135393,
      "grad_norm": 5.830142974853516,
      "learning_rate": 0.000911643522050969,
      "loss": 1.522,
      "step": 630
    },
    {
      "epoch": 0.2866878691503862,
      "grad_norm": 5.957951545715332,
      "learning_rate": 0.0009114909201892263,
      "loss": 1.3728,
      "step": 631
    },
    {
      "epoch": 0.28714220808723306,
      "grad_norm": 4.633702754974365,
      "learning_rate": 0.0009113383183274835,
      "loss": 1.134,
      "step": 632
    },
    {
      "epoch": 0.28759654702407994,
      "grad_norm": 4.490708351135254,
      "learning_rate": 0.0009111857164657409,
      "loss": 1.1693,
      "step": 633
    },
    {
      "epoch": 0.2880508859609269,
      "grad_norm": 5.500217437744141,
      "learning_rate": 0.0009110331146039982,
      "loss": 1.699,
      "step": 634
    },
    {
      "epoch": 0.28850522489777375,
      "grad_norm": 5.468127250671387,
      "learning_rate": 0.0009108805127422554,
      "loss": 1.4934,
      "step": 635
    },
    {
      "epoch": 0.2889595638346206,
      "grad_norm": 4.226212978363037,
      "learning_rate": 0.0009107279108805128,
      "loss": 0.9285,
      "step": 636
    },
    {
      "epoch": 0.2894139027714675,
      "grad_norm": 6.202568531036377,
      "learning_rate": 0.00091057530901877,
      "loss": 1.3562,
      "step": 637
    },
    {
      "epoch": 0.2898682417083144,
      "grad_norm": 4.931166648864746,
      "learning_rate": 0.0009104227071570273,
      "loss": 1.7361,
      "step": 638
    },
    {
      "epoch": 0.2903225806451613,
      "grad_norm": 5.546168327331543,
      "learning_rate": 0.0009102701052952847,
      "loss": 1.4409,
      "step": 639
    },
    {
      "epoch": 0.2907769195820082,
      "grad_norm": 4.295429706573486,
      "learning_rate": 0.0009101175034335419,
      "loss": 1.3949,
      "step": 640
    },
    {
      "epoch": 0.29123125851885506,
      "grad_norm": 5.603832244873047,
      "learning_rate": 0.0009099649015717992,
      "loss": 1.2488,
      "step": 641
    },
    {
      "epoch": 0.29168559745570194,
      "grad_norm": 3.2638766765594482,
      "learning_rate": 0.0009098122997100565,
      "loss": 0.7774,
      "step": 642
    },
    {
      "epoch": 0.2921399363925488,
      "grad_norm": 4.885572910308838,
      "learning_rate": 0.0009096596978483138,
      "loss": 1.5704,
      "step": 643
    },
    {
      "epoch": 0.29259427532939575,
      "grad_norm": 7.38744592666626,
      "learning_rate": 0.0009095070959865711,
      "loss": 2.2956,
      "step": 644
    },
    {
      "epoch": 0.2930486142662426,
      "grad_norm": 2.9849119186401367,
      "learning_rate": 0.0009093544941248283,
      "loss": 0.6937,
      "step": 645
    },
    {
      "epoch": 0.2935029532030895,
      "grad_norm": 4.190621376037598,
      "learning_rate": 0.0009092018922630856,
      "loss": 1.3973,
      "step": 646
    },
    {
      "epoch": 0.2939572921399364,
      "grad_norm": 6.036888599395752,
      "learning_rate": 0.0009090492904013428,
      "loss": 1.2425,
      "step": 647
    },
    {
      "epoch": 0.29441163107678325,
      "grad_norm": 5.346428871154785,
      "learning_rate": 0.0009088966885396002,
      "loss": 1.2555,
      "step": 648
    },
    {
      "epoch": 0.2948659700136302,
      "grad_norm": 5.6636576652526855,
      "learning_rate": 0.0009087440866778574,
      "loss": 1.3117,
      "step": 649
    },
    {
      "epoch": 0.29532030895047706,
      "grad_norm": 4.384967803955078,
      "learning_rate": 0.0009085914848161147,
      "loss": 0.9854,
      "step": 650
    },
    {
      "epoch": 0.29577464788732394,
      "grad_norm": 6.367961883544922,
      "learning_rate": 0.0009084388829543721,
      "loss": 1.6923,
      "step": 651
    },
    {
      "epoch": 0.2962289868241708,
      "grad_norm": 6.390758514404297,
      "learning_rate": 0.0009082862810926293,
      "loss": 1.4467,
      "step": 652
    },
    {
      "epoch": 0.29668332576101775,
      "grad_norm": 5.711019515991211,
      "learning_rate": 0.0009081336792308866,
      "loss": 1.4172,
      "step": 653
    },
    {
      "epoch": 0.2971376646978646,
      "grad_norm": 7.737837791442871,
      "learning_rate": 0.000907981077369144,
      "loss": 1.3,
      "step": 654
    },
    {
      "epoch": 0.2975920036347115,
      "grad_norm": 7.054724216461182,
      "learning_rate": 0.0009078284755074012,
      "loss": 1.6923,
      "step": 655
    },
    {
      "epoch": 0.2980463425715584,
      "grad_norm": 10.139739990234375,
      "learning_rate": 0.0009076758736456585,
      "loss": 1.1614,
      "step": 656
    },
    {
      "epoch": 0.29850068150840525,
      "grad_norm": 5.842681884765625,
      "learning_rate": 0.0009075232717839158,
      "loss": 0.8983,
      "step": 657
    },
    {
      "epoch": 0.2989550204452522,
      "grad_norm": 4.3310394287109375,
      "learning_rate": 0.0009073706699221731,
      "loss": 1.4263,
      "step": 658
    },
    {
      "epoch": 0.29940935938209906,
      "grad_norm": 4.853456497192383,
      "learning_rate": 0.0009072180680604303,
      "loss": 1.3812,
      "step": 659
    },
    {
      "epoch": 0.29986369831894594,
      "grad_norm": 7.142138481140137,
      "learning_rate": 0.0009070654661986877,
      "loss": 1.5735,
      "step": 660
    },
    {
      "epoch": 0.3003180372557928,
      "grad_norm": 6.551620960235596,
      "learning_rate": 0.000906912864336945,
      "loss": 1.7851,
      "step": 661
    },
    {
      "epoch": 0.3007723761926397,
      "grad_norm": 4.533819198608398,
      "learning_rate": 0.0009067602624752022,
      "loss": 1.9254,
      "step": 662
    },
    {
      "epoch": 0.3012267151294866,
      "grad_norm": 4.494215488433838,
      "learning_rate": 0.0009066076606134595,
      "loss": 1.3916,
      "step": 663
    },
    {
      "epoch": 0.3016810540663335,
      "grad_norm": 3.2439515590667725,
      "learning_rate": 0.0009064550587517167,
      "loss": 0.7816,
      "step": 664
    },
    {
      "epoch": 0.3021353930031804,
      "grad_norm": 5.469624996185303,
      "learning_rate": 0.000906302456889974,
      "loss": 1.414,
      "step": 665
    },
    {
      "epoch": 0.30258973194002725,
      "grad_norm": 6.507264614105225,
      "learning_rate": 0.0009061498550282314,
      "loss": 1.4308,
      "step": 666
    },
    {
      "epoch": 0.3030440708768741,
      "grad_norm": 7.361557483673096,
      "learning_rate": 0.0009059972531664886,
      "loss": 1.8834,
      "step": 667
    },
    {
      "epoch": 0.30349840981372106,
      "grad_norm": 5.164240837097168,
      "learning_rate": 0.0009058446513047459,
      "loss": 0.7788,
      "step": 668
    },
    {
      "epoch": 0.30395274875056794,
      "grad_norm": 8.768113136291504,
      "learning_rate": 0.0009056920494430032,
      "loss": 1.8272,
      "step": 669
    },
    {
      "epoch": 0.3044070876874148,
      "grad_norm": 4.383128643035889,
      "learning_rate": 0.0009055394475812605,
      "loss": 1.4038,
      "step": 670
    },
    {
      "epoch": 0.3048614266242617,
      "grad_norm": 5.78069543838501,
      "learning_rate": 0.0009053868457195177,
      "loss": 1.7492,
      "step": 671
    },
    {
      "epoch": 0.30531576556110857,
      "grad_norm": 5.917608261108398,
      "learning_rate": 0.0009052342438577751,
      "loss": 1.7623,
      "step": 672
    },
    {
      "epoch": 0.3057701044979555,
      "grad_norm": 6.333732604980469,
      "learning_rate": 0.0009050816419960324,
      "loss": 2.1098,
      "step": 673
    },
    {
      "epoch": 0.3062244434348024,
      "grad_norm": 4.973388671875,
      "learning_rate": 0.0009049290401342896,
      "loss": 1.1607,
      "step": 674
    },
    {
      "epoch": 0.30667878237164925,
      "grad_norm": 2.311116933822632,
      "learning_rate": 0.000904776438272547,
      "loss": 0.5885,
      "step": 675
    },
    {
      "epoch": 0.3071331213084961,
      "grad_norm": 5.2455525398254395,
      "learning_rate": 0.0009046238364108042,
      "loss": 1.0814,
      "step": 676
    },
    {
      "epoch": 0.307587460245343,
      "grad_norm": 4.60811185836792,
      "learning_rate": 0.0009044712345490616,
      "loss": 1.0617,
      "step": 677
    },
    {
      "epoch": 0.30804179918218993,
      "grad_norm": 5.379466533660889,
      "learning_rate": 0.0009043186326873189,
      "loss": 1.5779,
      "step": 678
    },
    {
      "epoch": 0.3084961381190368,
      "grad_norm": 4.111013889312744,
      "learning_rate": 0.0009041660308255761,
      "loss": 1.3002,
      "step": 679
    },
    {
      "epoch": 0.3089504770558837,
      "grad_norm": 5.266077995300293,
      "learning_rate": 0.0009040134289638335,
      "loss": 1.4072,
      "step": 680
    },
    {
      "epoch": 0.30940481599273056,
      "grad_norm": 4.921057224273682,
      "learning_rate": 0.0009038608271020906,
      "loss": 1.3558,
      "step": 681
    },
    {
      "epoch": 0.30985915492957744,
      "grad_norm": 4.8222880363464355,
      "learning_rate": 0.0009037082252403479,
      "loss": 1.4584,
      "step": 682
    },
    {
      "epoch": 0.3103134938664244,
      "grad_norm": 3.070258378982544,
      "learning_rate": 0.0009035556233786051,
      "loss": 0.455,
      "step": 683
    },
    {
      "epoch": 0.31076783280327125,
      "grad_norm": 5.135226249694824,
      "learning_rate": 0.0009034030215168625,
      "loss": 1.1599,
      "step": 684
    },
    {
      "epoch": 0.3112221717401181,
      "grad_norm": 6.606522560119629,
      "learning_rate": 0.0009032504196551198,
      "loss": 1.5742,
      "step": 685
    },
    {
      "epoch": 0.311676510676965,
      "grad_norm": 4.983480930328369,
      "learning_rate": 0.0009030978177933771,
      "loss": 1.3242,
      "step": 686
    },
    {
      "epoch": 0.3121308496138119,
      "grad_norm": 5.344476699829102,
      "learning_rate": 0.0009029452159316344,
      "loss": 1.0436,
      "step": 687
    },
    {
      "epoch": 0.3125851885506588,
      "grad_norm": 5.968101501464844,
      "learning_rate": 0.0009027926140698916,
      "loss": 1.3507,
      "step": 688
    },
    {
      "epoch": 0.3130395274875057,
      "grad_norm": 5.695165634155273,
      "learning_rate": 0.000902640012208149,
      "loss": 1.3303,
      "step": 689
    },
    {
      "epoch": 0.31349386642435256,
      "grad_norm": 5.3923797607421875,
      "learning_rate": 0.0009024874103464063,
      "loss": 1.8582,
      "step": 690
    },
    {
      "epoch": 0.31394820536119944,
      "grad_norm": 8.225834846496582,
      "learning_rate": 0.0009023348084846635,
      "loss": 1.2481,
      "step": 691
    },
    {
      "epoch": 0.3144025442980463,
      "grad_norm": 4.872406005859375,
      "learning_rate": 0.0009021822066229209,
      "loss": 1.0433,
      "step": 692
    },
    {
      "epoch": 0.31485688323489325,
      "grad_norm": 5.271617889404297,
      "learning_rate": 0.0009020296047611781,
      "loss": 1.1346,
      "step": 693
    },
    {
      "epoch": 0.3153112221717401,
      "grad_norm": 3.7310118675231934,
      "learning_rate": 0.0009018770028994354,
      "loss": 0.6126,
      "step": 694
    },
    {
      "epoch": 0.315765561108587,
      "grad_norm": 4.534595966339111,
      "learning_rate": 0.0009017244010376928,
      "loss": 1.8715,
      "step": 695
    },
    {
      "epoch": 0.3162199000454339,
      "grad_norm": 7.9659624099731445,
      "learning_rate": 0.00090157179917595,
      "loss": 1.2456,
      "step": 696
    },
    {
      "epoch": 0.3166742389822808,
      "grad_norm": 2.746971607208252,
      "learning_rate": 0.0009014191973142073,
      "loss": 0.1998,
      "step": 697
    },
    {
      "epoch": 0.3171285779191277,
      "grad_norm": 4.802375793457031,
      "learning_rate": 0.0009012665954524646,
      "loss": 1.5132,
      "step": 698
    },
    {
      "epoch": 0.31758291685597456,
      "grad_norm": 4.404576778411865,
      "learning_rate": 0.0009011139935907218,
      "loss": 0.9696,
      "step": 699
    },
    {
      "epoch": 0.31803725579282144,
      "grad_norm": 4.736330509185791,
      "learning_rate": 0.000900961391728979,
      "loss": 1.0385,
      "step": 700
    },
    {
      "epoch": 0.3184915947296683,
      "grad_norm": 4.363767623901367,
      "learning_rate": 0.0009008087898672364,
      "loss": 1.2364,
      "step": 701
    },
    {
      "epoch": 0.31894593366651525,
      "grad_norm": 3.9340288639068604,
      "learning_rate": 0.0009006561880054937,
      "loss": 0.6827,
      "step": 702
    },
    {
      "epoch": 0.3194002726033621,
      "grad_norm": 6.679687976837158,
      "learning_rate": 0.0009005035861437509,
      "loss": 1.5536,
      "step": 703
    },
    {
      "epoch": 0.319854611540209,
      "grad_norm": 7.124037265777588,
      "learning_rate": 0.0009003509842820083,
      "loss": 1.5573,
      "step": 704
    },
    {
      "epoch": 0.3203089504770559,
      "grad_norm": 10.348806381225586,
      "learning_rate": 0.0009001983824202655,
      "loss": 1.1839,
      "step": 705
    },
    {
      "epoch": 0.32076328941390275,
      "grad_norm": 7.161068439483643,
      "learning_rate": 0.0009000457805585228,
      "loss": 0.9171,
      "step": 706
    },
    {
      "epoch": 0.3212176283507497,
      "grad_norm": 5.507075309753418,
      "learning_rate": 0.0008998931786967802,
      "loss": 1.051,
      "step": 707
    },
    {
      "epoch": 0.32167196728759656,
      "grad_norm": 5.178337097167969,
      "learning_rate": 0.0008997405768350374,
      "loss": 1.146,
      "step": 708
    },
    {
      "epoch": 0.32212630622444344,
      "grad_norm": 7.401108741760254,
      "learning_rate": 0.0008995879749732947,
      "loss": 1.0331,
      "step": 709
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 8.693160057067871,
      "learning_rate": 0.000899435373111552,
      "loss": 1.9827,
      "step": 710
    },
    {
      "epoch": 0.3230349840981372,
      "grad_norm": 4.160111427307129,
      "learning_rate": 0.0008992827712498093,
      "loss": 1.0471,
      "step": 711
    },
    {
      "epoch": 0.3234893230349841,
      "grad_norm": 5.818142890930176,
      "learning_rate": 0.0008991301693880666,
      "loss": 1.8707,
      "step": 712
    },
    {
      "epoch": 0.323943661971831,
      "grad_norm": 7.5347065925598145,
      "learning_rate": 0.0008989775675263239,
      "loss": 1.625,
      "step": 713
    },
    {
      "epoch": 0.3243980009086779,
      "grad_norm": 7.068375110626221,
      "learning_rate": 0.0008988249656645812,
      "loss": 2.4389,
      "step": 714
    },
    {
      "epoch": 0.32485233984552475,
      "grad_norm": 4.19640588760376,
      "learning_rate": 0.0008986723638028384,
      "loss": 1.0454,
      "step": 715
    },
    {
      "epoch": 0.3253066787823716,
      "grad_norm": 5.324029922485352,
      "learning_rate": 0.0008985197619410958,
      "loss": 1.2791,
      "step": 716
    },
    {
      "epoch": 0.32576101771921856,
      "grad_norm": 5.373900890350342,
      "learning_rate": 0.000898367160079353,
      "loss": 0.9585,
      "step": 717
    },
    {
      "epoch": 0.32621535665606544,
      "grad_norm": 6.265317916870117,
      "learning_rate": 0.0008982145582176102,
      "loss": 1.0317,
      "step": 718
    },
    {
      "epoch": 0.3266696955929123,
      "grad_norm": 4.960346221923828,
      "learning_rate": 0.0008980619563558676,
      "loss": 1.25,
      "step": 719
    },
    {
      "epoch": 0.3271240345297592,
      "grad_norm": 5.012333393096924,
      "learning_rate": 0.0008979093544941248,
      "loss": 1.0481,
      "step": 720
    },
    {
      "epoch": 0.32757837346660607,
      "grad_norm": 4.924685955047607,
      "learning_rate": 0.0008977567526323821,
      "loss": 0.9619,
      "step": 721
    },
    {
      "epoch": 0.328032712403453,
      "grad_norm": 6.789018154144287,
      "learning_rate": 0.0008976041507706394,
      "loss": 1.07,
      "step": 722
    },
    {
      "epoch": 0.3284870513402999,
      "grad_norm": 4.378010272979736,
      "learning_rate": 0.0008974515489088967,
      "loss": 1.3325,
      "step": 723
    },
    {
      "epoch": 0.32894139027714675,
      "grad_norm": 5.990737438201904,
      "learning_rate": 0.000897298947047154,
      "loss": 0.9738,
      "step": 724
    },
    {
      "epoch": 0.3293957292139936,
      "grad_norm": 6.337378978729248,
      "learning_rate": 0.0008971463451854113,
      "loss": 1.2698,
      "step": 725
    },
    {
      "epoch": 0.3298500681508405,
      "grad_norm": 7.552202224731445,
      "learning_rate": 0.0008969937433236686,
      "loss": 1.6657,
      "step": 726
    },
    {
      "epoch": 0.33030440708768743,
      "grad_norm": 5.4488115310668945,
      "learning_rate": 0.0008968411414619258,
      "loss": 1.807,
      "step": 727
    },
    {
      "epoch": 0.3307587460245343,
      "grad_norm": 3.7958643436431885,
      "learning_rate": 0.0008966885396001832,
      "loss": 0.8535,
      "step": 728
    },
    {
      "epoch": 0.3312130849613812,
      "grad_norm": 5.843702793121338,
      "learning_rate": 0.0008965359377384405,
      "loss": 0.9989,
      "step": 729
    },
    {
      "epoch": 0.33166742389822806,
      "grad_norm": 6.102712631225586,
      "learning_rate": 0.0008963833358766977,
      "loss": 1.6119,
      "step": 730
    },
    {
      "epoch": 0.33212176283507494,
      "grad_norm": 2.939974546432495,
      "learning_rate": 0.0008962307340149551,
      "loss": 0.7871,
      "step": 731
    },
    {
      "epoch": 0.3325761017719219,
      "grad_norm": 4.828990459442139,
      "learning_rate": 0.0008960781321532123,
      "loss": 0.7795,
      "step": 732
    },
    {
      "epoch": 0.33303044070876875,
      "grad_norm": 6.127473831176758,
      "learning_rate": 0.0008959255302914696,
      "loss": 1.1245,
      "step": 733
    },
    {
      "epoch": 0.3334847796456156,
      "grad_norm": 5.2644734382629395,
      "learning_rate": 0.000895772928429727,
      "loss": 1.8222,
      "step": 734
    },
    {
      "epoch": 0.3339391185824625,
      "grad_norm": 4.509850025177002,
      "learning_rate": 0.0008956203265679842,
      "loss": 2.5741,
      "step": 735
    },
    {
      "epoch": 0.3343934575193094,
      "grad_norm": 4.659401893615723,
      "learning_rate": 0.0008954677247062414,
      "loss": 0.7392,
      "step": 736
    },
    {
      "epoch": 0.3348477964561563,
      "grad_norm": 7.231908321380615,
      "learning_rate": 0.0008953151228444987,
      "loss": 1.4515,
      "step": 737
    },
    {
      "epoch": 0.3353021353930032,
      "grad_norm": 5.600584030151367,
      "learning_rate": 0.000895162520982756,
      "loss": 1.5619,
      "step": 738
    },
    {
      "epoch": 0.33575647432985006,
      "grad_norm": 6.941690921783447,
      "learning_rate": 0.0008950099191210132,
      "loss": 2.0901,
      "step": 739
    },
    {
      "epoch": 0.33621081326669694,
      "grad_norm": 7.821681022644043,
      "learning_rate": 0.0008948573172592706,
      "loss": 1.814,
      "step": 740
    },
    {
      "epoch": 0.3366651522035438,
      "grad_norm": 4.955844879150391,
      "learning_rate": 0.0008947047153975279,
      "loss": 1.5052,
      "step": 741
    },
    {
      "epoch": 0.33711949114039075,
      "grad_norm": 5.747659206390381,
      "learning_rate": 0.0008945521135357851,
      "loss": 2.0925,
      "step": 742
    },
    {
      "epoch": 0.3375738300772376,
      "grad_norm": 5.617347717285156,
      "learning_rate": 0.0008943995116740425,
      "loss": 1.4257,
      "step": 743
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 3.721226215362549,
      "learning_rate": 0.0008942469098122997,
      "loss": 1.0654,
      "step": 744
    },
    {
      "epoch": 0.3384825079509314,
      "grad_norm": 5.194368839263916,
      "learning_rate": 0.000894094307950557,
      "loss": 1.3531,
      "step": 745
    },
    {
      "epoch": 0.3389368468877783,
      "grad_norm": 3.6242027282714844,
      "learning_rate": 0.0008939417060888144,
      "loss": 1.0402,
      "step": 746
    },
    {
      "epoch": 0.3393911858246252,
      "grad_norm": 6.254701137542725,
      "learning_rate": 0.0008937891042270716,
      "loss": 1.5445,
      "step": 747
    },
    {
      "epoch": 0.33984552476147206,
      "grad_norm": 7.0447869300842285,
      "learning_rate": 0.0008936365023653289,
      "loss": 0.9114,
      "step": 748
    },
    {
      "epoch": 0.34029986369831894,
      "grad_norm": 6.379082202911377,
      "learning_rate": 0.0008934839005035862,
      "loss": 1.7807,
      "step": 749
    },
    {
      "epoch": 0.3407542026351658,
      "grad_norm": 5.904658317565918,
      "learning_rate": 0.0008933312986418435,
      "loss": 1.3471,
      "step": 750
    },
    {
      "epoch": 0.34120854157201275,
      "grad_norm": 5.320494174957275,
      "learning_rate": 0.0008931786967801007,
      "loss": 0.8985,
      "step": 751
    },
    {
      "epoch": 0.3416628805088596,
      "grad_norm": 3.576894760131836,
      "learning_rate": 0.0008930260949183581,
      "loss": 0.7627,
      "step": 752
    },
    {
      "epoch": 0.3421172194457065,
      "grad_norm": 7.7538323402404785,
      "learning_rate": 0.0008928734930566154,
      "loss": 0.5025,
      "step": 753
    },
    {
      "epoch": 0.3425715583825534,
      "grad_norm": 11.220712661743164,
      "learning_rate": 0.0008927208911948725,
      "loss": 3.6933,
      "step": 754
    },
    {
      "epoch": 0.34302589731940025,
      "grad_norm": 3.9716765880584717,
      "learning_rate": 0.0008925682893331299,
      "loss": 0.5121,
      "step": 755
    },
    {
      "epoch": 0.3434802362562472,
      "grad_norm": 7.0838775634765625,
      "learning_rate": 0.0008924156874713871,
      "loss": 1.2683,
      "step": 756
    },
    {
      "epoch": 0.34393457519309406,
      "grad_norm": 8.348575592041016,
      "learning_rate": 0.0008922630856096444,
      "loss": 2.5024,
      "step": 757
    },
    {
      "epoch": 0.34438891412994094,
      "grad_norm": 7.98122501373291,
      "learning_rate": 0.0008921104837479018,
      "loss": 1.9194,
      "step": 758
    },
    {
      "epoch": 0.3448432530667878,
      "grad_norm": 4.48157262802124,
      "learning_rate": 0.000891957881886159,
      "loss": 1.4625,
      "step": 759
    },
    {
      "epoch": 0.3452975920036347,
      "grad_norm": 4.723053932189941,
      "learning_rate": 0.0008918052800244163,
      "loss": 0.4183,
      "step": 760
    },
    {
      "epoch": 0.3457519309404816,
      "grad_norm": 6.398916721343994,
      "learning_rate": 0.0008916526781626736,
      "loss": 1.2318,
      "step": 761
    },
    {
      "epoch": 0.3462062698773285,
      "grad_norm": 7.294674396514893,
      "learning_rate": 0.0008915000763009309,
      "loss": 1.7608,
      "step": 762
    },
    {
      "epoch": 0.3466606088141754,
      "grad_norm": 5.144762992858887,
      "learning_rate": 0.0008913474744391881,
      "loss": 1.8781,
      "step": 763
    },
    {
      "epoch": 0.34711494775102225,
      "grad_norm": 2.862628698348999,
      "learning_rate": 0.0008911948725774455,
      "loss": 0.8013,
      "step": 764
    },
    {
      "epoch": 0.34756928668786913,
      "grad_norm": 6.13777494430542,
      "learning_rate": 0.0008910422707157028,
      "loss": 0.9416,
      "step": 765
    },
    {
      "epoch": 0.34802362562471606,
      "grad_norm": 5.450795650482178,
      "learning_rate": 0.00089088966885396,
      "loss": 1.7573,
      "step": 766
    },
    {
      "epoch": 0.34847796456156294,
      "grad_norm": 7.270195007324219,
      "learning_rate": 0.0008907370669922174,
      "loss": 1.5138,
      "step": 767
    },
    {
      "epoch": 0.3489323034984098,
      "grad_norm": 4.835293292999268,
      "learning_rate": 0.0008905844651304746,
      "loss": 0.9317,
      "step": 768
    },
    {
      "epoch": 0.3493866424352567,
      "grad_norm": 4.970389366149902,
      "learning_rate": 0.0008904318632687319,
      "loss": 0.4813,
      "step": 769
    },
    {
      "epoch": 0.34984098137210357,
      "grad_norm": 5.852909564971924,
      "learning_rate": 0.0008902792614069893,
      "loss": 1.3894,
      "step": 770
    },
    {
      "epoch": 0.3502953203089505,
      "grad_norm": 4.468610763549805,
      "learning_rate": 0.0008901266595452465,
      "loss": 1.1838,
      "step": 771
    },
    {
      "epoch": 0.3507496592457974,
      "grad_norm": 4.038933277130127,
      "learning_rate": 0.0008899740576835038,
      "loss": 1.2799,
      "step": 772
    },
    {
      "epoch": 0.35120399818264425,
      "grad_norm": 4.139795303344727,
      "learning_rate": 0.000889821455821761,
      "loss": 0.9444,
      "step": 773
    },
    {
      "epoch": 0.3516583371194911,
      "grad_norm": 7.196143627166748,
      "learning_rate": 0.0008896688539600183,
      "loss": 1.0944,
      "step": 774
    },
    {
      "epoch": 0.352112676056338,
      "grad_norm": 7.289319038391113,
      "learning_rate": 0.0008895162520982755,
      "loss": 1.5625,
      "step": 775
    },
    {
      "epoch": 0.35256701499318494,
      "grad_norm": 5.9315409660339355,
      "learning_rate": 0.0008893636502365329,
      "loss": 1.0123,
      "step": 776
    },
    {
      "epoch": 0.3530213539300318,
      "grad_norm": 4.678885459899902,
      "learning_rate": 0.0008892110483747902,
      "loss": 0.8268,
      "step": 777
    },
    {
      "epoch": 0.3534756928668787,
      "grad_norm": 5.393078804016113,
      "learning_rate": 0.0008890584465130474,
      "loss": 1.6318,
      "step": 778
    },
    {
      "epoch": 0.35393003180372556,
      "grad_norm": 6.044681549072266,
      "learning_rate": 0.0008889058446513048,
      "loss": 1.6828,
      "step": 779
    },
    {
      "epoch": 0.35438437074057244,
      "grad_norm": 4.718715190887451,
      "learning_rate": 0.000888753242789562,
      "loss": 0.8647,
      "step": 780
    },
    {
      "epoch": 0.3548387096774194,
      "grad_norm": 4.99652624130249,
      "learning_rate": 0.0008886006409278193,
      "loss": 1.3498,
      "step": 781
    },
    {
      "epoch": 0.35529304861426625,
      "grad_norm": 15.826578140258789,
      "learning_rate": 0.0008884480390660767,
      "loss": 1.0379,
      "step": 782
    },
    {
      "epoch": 0.3557473875511131,
      "grad_norm": 6.590808868408203,
      "learning_rate": 0.0008882954372043339,
      "loss": 0.7933,
      "step": 783
    },
    {
      "epoch": 0.35620172648796,
      "grad_norm": 6.815788745880127,
      "learning_rate": 0.0008881428353425912,
      "loss": 1.8551,
      "step": 784
    },
    {
      "epoch": 0.3566560654248069,
      "grad_norm": 8.120882034301758,
      "learning_rate": 0.0008879902334808485,
      "loss": 1.666,
      "step": 785
    },
    {
      "epoch": 0.3571104043616538,
      "grad_norm": 6.315911769866943,
      "learning_rate": 0.0008878376316191058,
      "loss": 1.5484,
      "step": 786
    },
    {
      "epoch": 0.3575647432985007,
      "grad_norm": 6.714621067047119,
      "learning_rate": 0.0008876850297573631,
      "loss": 1.8189,
      "step": 787
    },
    {
      "epoch": 0.35801908223534756,
      "grad_norm": 4.559648513793945,
      "learning_rate": 0.0008875324278956204,
      "loss": 1.3818,
      "step": 788
    },
    {
      "epoch": 0.35847342117219444,
      "grad_norm": 5.749180793762207,
      "learning_rate": 0.0008873798260338777,
      "loss": 1.7525,
      "step": 789
    },
    {
      "epoch": 0.35892776010904137,
      "grad_norm": 6.948873043060303,
      "learning_rate": 0.0008872272241721349,
      "loss": 1.1463,
      "step": 790
    },
    {
      "epoch": 0.35938209904588825,
      "grad_norm": 5.125828742980957,
      "learning_rate": 0.0008870746223103922,
      "loss": 1.5909,
      "step": 791
    },
    {
      "epoch": 0.3598364379827351,
      "grad_norm": 2.661564350128174,
      "learning_rate": 0.0008869220204486495,
      "loss": 0.5803,
      "step": 792
    },
    {
      "epoch": 0.360290776919582,
      "grad_norm": 4.652291297912598,
      "learning_rate": 0.0008867694185869067,
      "loss": 1.1044,
      "step": 793
    },
    {
      "epoch": 0.3607451158564289,
      "grad_norm": 4.028030872344971,
      "learning_rate": 0.0008866168167251641,
      "loss": 0.8222,
      "step": 794
    },
    {
      "epoch": 0.3611994547932758,
      "grad_norm": 3.693725824356079,
      "learning_rate": 0.0008864642148634213,
      "loss": 1.1521,
      "step": 795
    },
    {
      "epoch": 0.3616537937301227,
      "grad_norm": 6.290371417999268,
      "learning_rate": 0.0008863116130016786,
      "loss": 1.7265,
      "step": 796
    },
    {
      "epoch": 0.36210813266696956,
      "grad_norm": 4.38830041885376,
      "learning_rate": 0.000886159011139936,
      "loss": 1.1135,
      "step": 797
    },
    {
      "epoch": 0.36256247160381644,
      "grad_norm": 6.2588210105896,
      "learning_rate": 0.0008860064092781932,
      "loss": 2.1466,
      "step": 798
    },
    {
      "epoch": 0.3630168105406633,
      "grad_norm": 4.5905632972717285,
      "learning_rate": 0.0008858538074164505,
      "loss": 1.1098,
      "step": 799
    },
    {
      "epoch": 0.36347114947751025,
      "grad_norm": 8.847102165222168,
      "learning_rate": 0.0008857012055547078,
      "loss": 1.9507,
      "step": 800
    },
    {
      "epoch": 0.3639254884143571,
      "grad_norm": 4.761922359466553,
      "learning_rate": 0.0008855486036929651,
      "loss": 1.698,
      "step": 801
    },
    {
      "epoch": 0.364379827351204,
      "grad_norm": 4.210709095001221,
      "learning_rate": 0.0008853960018312223,
      "loss": 1.3085,
      "step": 802
    },
    {
      "epoch": 0.3648341662880509,
      "grad_norm": 4.917298316955566,
      "learning_rate": 0.0008852433999694797,
      "loss": 0.8398,
      "step": 803
    },
    {
      "epoch": 0.36528850522489775,
      "grad_norm": 4.388576984405518,
      "learning_rate": 0.000885090798107737,
      "loss": 1.3174,
      "step": 804
    },
    {
      "epoch": 0.3657428441617447,
      "grad_norm": 5.961156368255615,
      "learning_rate": 0.0008849381962459942,
      "loss": 1.8372,
      "step": 805
    },
    {
      "epoch": 0.36619718309859156,
      "grad_norm": 4.9643096923828125,
      "learning_rate": 0.0008847855943842516,
      "loss": 0.8173,
      "step": 806
    },
    {
      "epoch": 0.36665152203543844,
      "grad_norm": 5.263026237487793,
      "learning_rate": 0.0008846329925225088,
      "loss": 1.2083,
      "step": 807
    },
    {
      "epoch": 0.3671058609722853,
      "grad_norm": 5.512087345123291,
      "learning_rate": 0.0008844803906607661,
      "loss": 0.9233,
      "step": 808
    },
    {
      "epoch": 0.3675601999091322,
      "grad_norm": 4.0441975593566895,
      "learning_rate": 0.0008843277887990234,
      "loss": 0.4826,
      "step": 809
    },
    {
      "epoch": 0.3680145388459791,
      "grad_norm": 7.15964412689209,
      "learning_rate": 0.0008841751869372806,
      "loss": 1.4408,
      "step": 810
    },
    {
      "epoch": 0.368468877782826,
      "grad_norm": 5.7863450050354,
      "learning_rate": 0.0008840225850755379,
      "loss": 0.8129,
      "step": 811
    },
    {
      "epoch": 0.3689232167196729,
      "grad_norm": 8.042669296264648,
      "learning_rate": 0.0008838699832137952,
      "loss": 1.0594,
      "step": 812
    },
    {
      "epoch": 0.36937755565651975,
      "grad_norm": 8.159673690795898,
      "learning_rate": 0.0008837173813520525,
      "loss": 1.1793,
      "step": 813
    },
    {
      "epoch": 0.36983189459336663,
      "grad_norm": 4.126338005065918,
      "learning_rate": 0.0008835647794903097,
      "loss": 0.6974,
      "step": 814
    },
    {
      "epoch": 0.37028623353021356,
      "grad_norm": 5.75996732711792,
      "learning_rate": 0.0008834121776285671,
      "loss": 0.7462,
      "step": 815
    },
    {
      "epoch": 0.37074057246706044,
      "grad_norm": 5.039966106414795,
      "learning_rate": 0.0008832595757668244,
      "loss": 0.8713,
      "step": 816
    },
    {
      "epoch": 0.3711949114039073,
      "grad_norm": 8.12533950805664,
      "learning_rate": 0.0008831069739050816,
      "loss": 0.9585,
      "step": 817
    },
    {
      "epoch": 0.3716492503407542,
      "grad_norm": 5.671520709991455,
      "learning_rate": 0.000882954372043339,
      "loss": 1.0822,
      "step": 818
    },
    {
      "epoch": 0.37210358927760107,
      "grad_norm": 6.228645324707031,
      "learning_rate": 0.0008828017701815962,
      "loss": 0.7568,
      "step": 819
    },
    {
      "epoch": 0.372557928214448,
      "grad_norm": 5.231075286865234,
      "learning_rate": 0.0008826491683198535,
      "loss": 1.2278,
      "step": 820
    },
    {
      "epoch": 0.3730122671512949,
      "grad_norm": 6.376522064208984,
      "learning_rate": 0.0008824965664581109,
      "loss": 1.3174,
      "step": 821
    },
    {
      "epoch": 0.37346660608814175,
      "grad_norm": 4.5015130043029785,
      "learning_rate": 0.0008823439645963681,
      "loss": 1.1981,
      "step": 822
    },
    {
      "epoch": 0.3739209450249886,
      "grad_norm": 3.693192481994629,
      "learning_rate": 0.0008821913627346254,
      "loss": 0.4418,
      "step": 823
    },
    {
      "epoch": 0.3743752839618355,
      "grad_norm": 7.570511341094971,
      "learning_rate": 0.0008820387608728827,
      "loss": 1.6689,
      "step": 824
    },
    {
      "epoch": 0.37482962289868244,
      "grad_norm": 4.831517696380615,
      "learning_rate": 0.00088188615901114,
      "loss": 0.7746,
      "step": 825
    },
    {
      "epoch": 0.3752839618355293,
      "grad_norm": 6.0755133628845215,
      "learning_rate": 0.0008817335571493973,
      "loss": 1.2679,
      "step": 826
    },
    {
      "epoch": 0.3757383007723762,
      "grad_norm": 3.312351942062378,
      "learning_rate": 0.0008815809552876545,
      "loss": 0.6349,
      "step": 827
    },
    {
      "epoch": 0.37619263970922306,
      "grad_norm": 5.031361103057861,
      "learning_rate": 0.0008814283534259118,
      "loss": 1.0149,
      "step": 828
    },
    {
      "epoch": 0.37664697864606994,
      "grad_norm": 4.737961292266846,
      "learning_rate": 0.000881275751564169,
      "loss": 0.8932,
      "step": 829
    },
    {
      "epoch": 0.3771013175829169,
      "grad_norm": 5.504453182220459,
      "learning_rate": 0.0008811231497024264,
      "loss": 0.7911,
      "step": 830
    },
    {
      "epoch": 0.37755565651976375,
      "grad_norm": 6.514984607696533,
      "learning_rate": 0.0008809705478406836,
      "loss": 1.8092,
      "step": 831
    },
    {
      "epoch": 0.3780099954566106,
      "grad_norm": 6.701390266418457,
      "learning_rate": 0.0008808179459789409,
      "loss": 2.2639,
      "step": 832
    },
    {
      "epoch": 0.3784643343934575,
      "grad_norm": 7.493356227874756,
      "learning_rate": 0.0008806653441171983,
      "loss": 1.7451,
      "step": 833
    },
    {
      "epoch": 0.37891867333030443,
      "grad_norm": 5.3426079750061035,
      "learning_rate": 0.0008805127422554555,
      "loss": 1.4081,
      "step": 834
    },
    {
      "epoch": 0.3793730122671513,
      "grad_norm": 5.853097438812256,
      "learning_rate": 0.0008803601403937128,
      "loss": 1.2593,
      "step": 835
    },
    {
      "epoch": 0.3798273512039982,
      "grad_norm": 5.506092071533203,
      "learning_rate": 0.0008802075385319701,
      "loss": 1.0256,
      "step": 836
    },
    {
      "epoch": 0.38028169014084506,
      "grad_norm": 5.42415189743042,
      "learning_rate": 0.0008800549366702274,
      "loss": 1.5649,
      "step": 837
    },
    {
      "epoch": 0.38073602907769194,
      "grad_norm": 6.9796247482299805,
      "learning_rate": 0.0008799023348084847,
      "loss": 1.4461,
      "step": 838
    },
    {
      "epoch": 0.38119036801453887,
      "grad_norm": 6.075656890869141,
      "learning_rate": 0.000879749732946742,
      "loss": 1.5696,
      "step": 839
    },
    {
      "epoch": 0.38164470695138575,
      "grad_norm": 5.317514896392822,
      "learning_rate": 0.0008795971310849993,
      "loss": 1.1687,
      "step": 840
    },
    {
      "epoch": 0.3820990458882326,
      "grad_norm": 3.9273173809051514,
      "learning_rate": 0.0008794445292232565,
      "loss": 0.4521,
      "step": 841
    },
    {
      "epoch": 0.3825533848250795,
      "grad_norm": 5.507394313812256,
      "learning_rate": 0.0008792919273615139,
      "loss": 1.3065,
      "step": 842
    },
    {
      "epoch": 0.3830077237619264,
      "grad_norm": 3.2257485389709473,
      "learning_rate": 0.0008791393254997712,
      "loss": 0.7686,
      "step": 843
    },
    {
      "epoch": 0.3834620626987733,
      "grad_norm": 7.638891220092773,
      "learning_rate": 0.0008789867236380284,
      "loss": 2.0235,
      "step": 844
    },
    {
      "epoch": 0.3839164016356202,
      "grad_norm": 5.754235744476318,
      "learning_rate": 0.0008788341217762858,
      "loss": 0.9875,
      "step": 845
    },
    {
      "epoch": 0.38437074057246706,
      "grad_norm": 8.527482986450195,
      "learning_rate": 0.0008786815199145429,
      "loss": 1.2834,
      "step": 846
    },
    {
      "epoch": 0.38482507950931394,
      "grad_norm": 7.179009914398193,
      "learning_rate": 0.0008785289180528002,
      "loss": 1.9082,
      "step": 847
    },
    {
      "epoch": 0.3852794184461608,
      "grad_norm": 8.557568550109863,
      "learning_rate": 0.0008783763161910575,
      "loss": 1.0206,
      "step": 848
    },
    {
      "epoch": 0.38573375738300775,
      "grad_norm": 5.695643901824951,
      "learning_rate": 0.0008782237143293148,
      "loss": 1.2159,
      "step": 849
    },
    {
      "epoch": 0.3861880963198546,
      "grad_norm": 4.623987197875977,
      "learning_rate": 0.0008780711124675721,
      "loss": 1.2844,
      "step": 850
    },
    {
      "epoch": 0.3866424352567015,
      "grad_norm": 5.601416110992432,
      "learning_rate": 0.0008779185106058294,
      "loss": 1.8982,
      "step": 851
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 7.359823226928711,
      "learning_rate": 0.0008777659087440867,
      "loss": 1.6388,
      "step": 852
    },
    {
      "epoch": 0.38755111313039525,
      "grad_norm": 4.786874771118164,
      "learning_rate": 0.0008776133068823439,
      "loss": 0.8866,
      "step": 853
    },
    {
      "epoch": 0.3880054520672422,
      "grad_norm": 5.887673854827881,
      "learning_rate": 0.0008774607050206013,
      "loss": 0.774,
      "step": 854
    },
    {
      "epoch": 0.38845979100408906,
      "grad_norm": 6.920986175537109,
      "learning_rate": 0.0008773081031588586,
      "loss": 2.1393,
      "step": 855
    },
    {
      "epoch": 0.38891412994093594,
      "grad_norm": 4.936712265014648,
      "learning_rate": 0.0008771555012971158,
      "loss": 1.0528,
      "step": 856
    },
    {
      "epoch": 0.3893684688777828,
      "grad_norm": 5.552990913391113,
      "learning_rate": 0.0008770028994353732,
      "loss": 1.588,
      "step": 857
    },
    {
      "epoch": 0.3898228078146297,
      "grad_norm": 7.129572868347168,
      "learning_rate": 0.0008768502975736304,
      "loss": 1.02,
      "step": 858
    },
    {
      "epoch": 0.3902771467514766,
      "grad_norm": 5.0391316413879395,
      "learning_rate": 0.0008766976957118877,
      "loss": 1.2694,
      "step": 859
    },
    {
      "epoch": 0.3907314856883235,
      "grad_norm": 8.063055992126465,
      "learning_rate": 0.000876545093850145,
      "loss": 1.7669,
      "step": 860
    },
    {
      "epoch": 0.3911858246251704,
      "grad_norm": 7.054976463317871,
      "learning_rate": 0.0008763924919884023,
      "loss": 1.2926,
      "step": 861
    },
    {
      "epoch": 0.39164016356201725,
      "grad_norm": 5.328644752502441,
      "learning_rate": 0.0008762398901266596,
      "loss": 1.2329,
      "step": 862
    },
    {
      "epoch": 0.39209450249886413,
      "grad_norm": 3.397826910018921,
      "learning_rate": 0.0008760872882649169,
      "loss": 0.7181,
      "step": 863
    },
    {
      "epoch": 0.39254884143571106,
      "grad_norm": 3.349475860595703,
      "learning_rate": 0.0008759346864031741,
      "loss": 0.7142,
      "step": 864
    },
    {
      "epoch": 0.39300318037255794,
      "grad_norm": 7.951776027679443,
      "learning_rate": 0.0008757820845414313,
      "loss": 1.4526,
      "step": 865
    },
    {
      "epoch": 0.3934575193094048,
      "grad_norm": 5.146064281463623,
      "learning_rate": 0.0008756294826796887,
      "loss": 0.5813,
      "step": 866
    },
    {
      "epoch": 0.3939118582462517,
      "grad_norm": 4.871695041656494,
      "learning_rate": 0.000875476880817946,
      "loss": 0.9001,
      "step": 867
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 4.13064432144165,
      "learning_rate": 0.0008753242789562032,
      "loss": 0.8851,
      "step": 868
    },
    {
      "epoch": 0.3948205361199455,
      "grad_norm": 8.221821784973145,
      "learning_rate": 0.0008751716770944606,
      "loss": 2.8987,
      "step": 869
    },
    {
      "epoch": 0.3952748750567924,
      "grad_norm": 6.750838756561279,
      "learning_rate": 0.0008750190752327178,
      "loss": 1.2941,
      "step": 870
    },
    {
      "epoch": 0.39572921399363925,
      "grad_norm": 5.362662315368652,
      "learning_rate": 0.0008748664733709751,
      "loss": 1.6922,
      "step": 871
    },
    {
      "epoch": 0.3961835529304861,
      "grad_norm": 4.805442810058594,
      "learning_rate": 0.0008747138715092325,
      "loss": 1.0973,
      "step": 872
    },
    {
      "epoch": 0.396637891867333,
      "grad_norm": 5.9169535636901855,
      "learning_rate": 0.0008745612696474897,
      "loss": 1.4078,
      "step": 873
    },
    {
      "epoch": 0.39709223080417994,
      "grad_norm": 3.709094524383545,
      "learning_rate": 0.000874408667785747,
      "loss": 1.0237,
      "step": 874
    },
    {
      "epoch": 0.3975465697410268,
      "grad_norm": 6.004878520965576,
      "learning_rate": 0.0008742560659240043,
      "loss": 2.2223,
      "step": 875
    },
    {
      "epoch": 0.3980009086778737,
      "grad_norm": 2.8631155490875244,
      "learning_rate": 0.0008741034640622616,
      "loss": 0.4194,
      "step": 876
    },
    {
      "epoch": 0.39845524761472056,
      "grad_norm": 4.2946271896362305,
      "learning_rate": 0.0008739508622005188,
      "loss": 1.3251,
      "step": 877
    },
    {
      "epoch": 0.3989095865515675,
      "grad_norm": 5.739575386047363,
      "learning_rate": 0.0008737982603387762,
      "loss": 1.5933,
      "step": 878
    },
    {
      "epoch": 0.3993639254884144,
      "grad_norm": 7.450702667236328,
      "learning_rate": 0.0008736456584770335,
      "loss": 2.1154,
      "step": 879
    },
    {
      "epoch": 0.39981826442526125,
      "grad_norm": 3.881716251373291,
      "learning_rate": 0.0008734930566152907,
      "loss": 0.8806,
      "step": 880
    },
    {
      "epoch": 0.4002726033621081,
      "grad_norm": 7.835771083831787,
      "learning_rate": 0.0008733404547535481,
      "loss": 1.6434,
      "step": 881
    },
    {
      "epoch": 0.400726942298955,
      "grad_norm": 5.30209493637085,
      "learning_rate": 0.0008731878528918052,
      "loss": 1.2209,
      "step": 882
    },
    {
      "epoch": 0.40118128123580193,
      "grad_norm": 4.560446739196777,
      "learning_rate": 0.0008730352510300625,
      "loss": 1.0081,
      "step": 883
    },
    {
      "epoch": 0.4016356201726488,
      "grad_norm": 5.753317356109619,
      "learning_rate": 0.0008728826491683199,
      "loss": 1.7058,
      "step": 884
    },
    {
      "epoch": 0.4020899591094957,
      "grad_norm": 3.2562057971954346,
      "learning_rate": 0.0008727300473065771,
      "loss": 0.5335,
      "step": 885
    },
    {
      "epoch": 0.40254429804634256,
      "grad_norm": 7.66619348526001,
      "learning_rate": 0.0008725774454448344,
      "loss": 1.7394,
      "step": 886
    },
    {
      "epoch": 0.40299863698318944,
      "grad_norm": 5.992953777313232,
      "learning_rate": 0.0008724248435830917,
      "loss": 1.6394,
      "step": 887
    },
    {
      "epoch": 0.40345297592003637,
      "grad_norm": 3.056389093399048,
      "learning_rate": 0.000872272241721349,
      "loss": 0.3788,
      "step": 888
    },
    {
      "epoch": 0.40390731485688325,
      "grad_norm": 3.3203887939453125,
      "learning_rate": 0.0008721196398596062,
      "loss": 0.4388,
      "step": 889
    },
    {
      "epoch": 0.4043616537937301,
      "grad_norm": 3.465618371963501,
      "learning_rate": 0.0008719670379978636,
      "loss": 0.9635,
      "step": 890
    },
    {
      "epoch": 0.404815992730577,
      "grad_norm": 4.590958118438721,
      "learning_rate": 0.0008718144361361209,
      "loss": 1.3972,
      "step": 891
    },
    {
      "epoch": 0.4052703316674239,
      "grad_norm": 6.1119184494018555,
      "learning_rate": 0.0008716618342743781,
      "loss": 1.0774,
      "step": 892
    },
    {
      "epoch": 0.4057246706042708,
      "grad_norm": 2.7479076385498047,
      "learning_rate": 0.0008715092324126355,
      "loss": 0.6592,
      "step": 893
    },
    {
      "epoch": 0.4061790095411177,
      "grad_norm": 5.125417709350586,
      "learning_rate": 0.0008713566305508927,
      "loss": 1.2492,
      "step": 894
    },
    {
      "epoch": 0.40663334847796456,
      "grad_norm": 5.337588787078857,
      "learning_rate": 0.00087120402868915,
      "loss": 1.9068,
      "step": 895
    },
    {
      "epoch": 0.40708768741481144,
      "grad_norm": 7.3836669921875,
      "learning_rate": 0.0008710514268274074,
      "loss": 1.8366,
      "step": 896
    },
    {
      "epoch": 0.4075420263516583,
      "grad_norm": 8.530547142028809,
      "learning_rate": 0.0008708988249656646,
      "loss": 2.0357,
      "step": 897
    },
    {
      "epoch": 0.40799636528850525,
      "grad_norm": 7.696071147918701,
      "learning_rate": 0.0008707462231039219,
      "loss": 1.7091,
      "step": 898
    },
    {
      "epoch": 0.4084507042253521,
      "grad_norm": 5.895552158355713,
      "learning_rate": 0.0008705936212421792,
      "loss": 1.5901,
      "step": 899
    },
    {
      "epoch": 0.408905043162199,
      "grad_norm": 6.105013847351074,
      "learning_rate": 0.0008704410193804364,
      "loss": 1.9,
      "step": 900
    },
    {
      "epoch": 0.4093593820990459,
      "grad_norm": 6.181286811828613,
      "learning_rate": 0.0008702884175186936,
      "loss": 2.2476,
      "step": 901
    },
    {
      "epoch": 0.40981372103589275,
      "grad_norm": 4.8901591300964355,
      "learning_rate": 0.000870135815656951,
      "loss": 1.2356,
      "step": 902
    },
    {
      "epoch": 0.4102680599727397,
      "grad_norm": 5.564911365509033,
      "learning_rate": 0.0008699832137952083,
      "loss": 1.7435,
      "step": 903
    },
    {
      "epoch": 0.41072239890958656,
      "grad_norm": 4.7510247230529785,
      "learning_rate": 0.0008698306119334655,
      "loss": 1.3415,
      "step": 904
    },
    {
      "epoch": 0.41117673784643344,
      "grad_norm": 8.729755401611328,
      "learning_rate": 0.0008696780100717229,
      "loss": 1.8198,
      "step": 905
    },
    {
      "epoch": 0.4116310767832803,
      "grad_norm": 5.696547508239746,
      "learning_rate": 0.0008695254082099801,
      "loss": 0.7062,
      "step": 906
    },
    {
      "epoch": 0.4120854157201272,
      "grad_norm": 6.156518459320068,
      "learning_rate": 0.0008693728063482374,
      "loss": 1.2757,
      "step": 907
    },
    {
      "epoch": 0.4125397546569741,
      "grad_norm": 6.9368815422058105,
      "learning_rate": 0.0008692202044864948,
      "loss": 1.4089,
      "step": 908
    },
    {
      "epoch": 0.412994093593821,
      "grad_norm": 8.933419227600098,
      "learning_rate": 0.000869067602624752,
      "loss": 1.2313,
      "step": 909
    },
    {
      "epoch": 0.4134484325306679,
      "grad_norm": 4.753856658935547,
      "learning_rate": 0.0008689150007630093,
      "loss": 0.8183,
      "step": 910
    },
    {
      "epoch": 0.41390277146751475,
      "grad_norm": 7.750436782836914,
      "learning_rate": 0.0008687623989012666,
      "loss": 1.529,
      "step": 911
    },
    {
      "epoch": 0.41435711040436163,
      "grad_norm": 6.572569370269775,
      "learning_rate": 0.0008686097970395239,
      "loss": 1.346,
      "step": 912
    },
    {
      "epoch": 0.41481144934120856,
      "grad_norm": 5.664178848266602,
      "learning_rate": 0.0008684571951777812,
      "loss": 1.2673,
      "step": 913
    },
    {
      "epoch": 0.41526578827805544,
      "grad_norm": 6.847504615783691,
      "learning_rate": 0.0008683045933160385,
      "loss": 2.7972,
      "step": 914
    },
    {
      "epoch": 0.4157201272149023,
      "grad_norm": 4.16949462890625,
      "learning_rate": 0.0008681519914542958,
      "loss": 0.5847,
      "step": 915
    },
    {
      "epoch": 0.4161744661517492,
      "grad_norm": 5.101690769195557,
      "learning_rate": 0.000867999389592553,
      "loss": 1.5828,
      "step": 916
    },
    {
      "epoch": 0.41662880508859607,
      "grad_norm": 3.6873440742492676,
      "learning_rate": 0.0008678467877308104,
      "loss": 0.811,
      "step": 917
    },
    {
      "epoch": 0.417083144025443,
      "grad_norm": 4.957457542419434,
      "learning_rate": 0.0008676941858690677,
      "loss": 1.2381,
      "step": 918
    },
    {
      "epoch": 0.4175374829622899,
      "grad_norm": 7.953632354736328,
      "learning_rate": 0.0008675415840073248,
      "loss": 1.8537,
      "step": 919
    },
    {
      "epoch": 0.41799182189913675,
      "grad_norm": 5.226185321807861,
      "learning_rate": 0.0008673889821455822,
      "loss": 0.8752,
      "step": 920
    },
    {
      "epoch": 0.4184461608359836,
      "grad_norm": 4.199342250823975,
      "learning_rate": 0.0008672363802838394,
      "loss": 1.0391,
      "step": 921
    },
    {
      "epoch": 0.41890049977283056,
      "grad_norm": 5.26807975769043,
      "learning_rate": 0.0008670837784220967,
      "loss": 1.1868,
      "step": 922
    },
    {
      "epoch": 0.41935483870967744,
      "grad_norm": 5.914754390716553,
      "learning_rate": 0.000866931176560354,
      "loss": 0.9474,
      "step": 923
    },
    {
      "epoch": 0.4198091776465243,
      "grad_norm": 5.258687973022461,
      "learning_rate": 0.0008667785746986113,
      "loss": 1.2182,
      "step": 924
    },
    {
      "epoch": 0.4202635165833712,
      "grad_norm": 7.529735565185547,
      "learning_rate": 0.0008666259728368686,
      "loss": 1.7517,
      "step": 925
    },
    {
      "epoch": 0.42071785552021806,
      "grad_norm": 2.809624195098877,
      "learning_rate": 0.0008664733709751259,
      "loss": 1.205,
      "step": 926
    },
    {
      "epoch": 0.421172194457065,
      "grad_norm": 6.192967414855957,
      "learning_rate": 0.0008663207691133832,
      "loss": 1.5507,
      "step": 927
    },
    {
      "epoch": 0.4216265333939119,
      "grad_norm": 6.6328959465026855,
      "learning_rate": 0.0008661681672516404,
      "loss": 0.8576,
      "step": 928
    },
    {
      "epoch": 0.42208087233075875,
      "grad_norm": 5.689620018005371,
      "learning_rate": 0.0008660155653898978,
      "loss": 1.043,
      "step": 929
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 5.4058003425598145,
      "learning_rate": 0.0008658629635281551,
      "loss": 1.1403,
      "step": 930
    },
    {
      "epoch": 0.4229895502044525,
      "grad_norm": 5.170819282531738,
      "learning_rate": 0.0008657103616664124,
      "loss": 1.4704,
      "step": 931
    },
    {
      "epoch": 0.42344388914129943,
      "grad_norm": 5.411779880523682,
      "learning_rate": 0.0008655577598046697,
      "loss": 1.2149,
      "step": 932
    },
    {
      "epoch": 0.4238982280781463,
      "grad_norm": 6.52142858505249,
      "learning_rate": 0.0008654051579429269,
      "loss": 1.3392,
      "step": 933
    },
    {
      "epoch": 0.4243525670149932,
      "grad_norm": 4.843770503997803,
      "learning_rate": 0.0008652525560811843,
      "loss": 0.9096,
      "step": 934
    },
    {
      "epoch": 0.42480690595184006,
      "grad_norm": 6.689940929412842,
      "learning_rate": 0.0008650999542194416,
      "loss": 1.6074,
      "step": 935
    },
    {
      "epoch": 0.42526124488868694,
      "grad_norm": 4.926410675048828,
      "learning_rate": 0.0008649473523576988,
      "loss": 0.9179,
      "step": 936
    },
    {
      "epoch": 0.42571558382553387,
      "grad_norm": 5.093119144439697,
      "learning_rate": 0.000864794750495956,
      "loss": 1.662,
      "step": 937
    },
    {
      "epoch": 0.42616992276238075,
      "grad_norm": 7.606739521026611,
      "learning_rate": 0.0008646421486342133,
      "loss": 2.168,
      "step": 938
    },
    {
      "epoch": 0.4266242616992276,
      "grad_norm": 6.797503471374512,
      "learning_rate": 0.0008644895467724706,
      "loss": 1.9714,
      "step": 939
    },
    {
      "epoch": 0.4270786006360745,
      "grad_norm": 6.741391181945801,
      "learning_rate": 0.000864336944910728,
      "loss": 1.1171,
      "step": 940
    },
    {
      "epoch": 0.4275329395729214,
      "grad_norm": 9.400270462036133,
      "learning_rate": 0.0008641843430489852,
      "loss": 1.03,
      "step": 941
    },
    {
      "epoch": 0.4279872785097683,
      "grad_norm": 4.27142333984375,
      "learning_rate": 0.0008640317411872425,
      "loss": 0.6237,
      "step": 942
    },
    {
      "epoch": 0.4284416174466152,
      "grad_norm": 6.776236057281494,
      "learning_rate": 0.0008638791393254998,
      "loss": 1.5474,
      "step": 943
    },
    {
      "epoch": 0.42889595638346206,
      "grad_norm": 6.66179084777832,
      "learning_rate": 0.0008637265374637571,
      "loss": 0.9864,
      "step": 944
    },
    {
      "epoch": 0.42935029532030894,
      "grad_norm": 5.889345645904541,
      "learning_rate": 0.0008635739356020143,
      "loss": 0.869,
      "step": 945
    },
    {
      "epoch": 0.4298046342571558,
      "grad_norm": 7.195096969604492,
      "learning_rate": 0.0008634213337402717,
      "loss": 0.7958,
      "step": 946
    },
    {
      "epoch": 0.43025897319400275,
      "grad_norm": 3.841414213180542,
      "learning_rate": 0.000863268731878529,
      "loss": 0.4608,
      "step": 947
    },
    {
      "epoch": 0.4307133121308496,
      "grad_norm": 4.008823871612549,
      "learning_rate": 0.0008631161300167862,
      "loss": 0.4375,
      "step": 948
    },
    {
      "epoch": 0.4311676510676965,
      "grad_norm": 6.528337001800537,
      "learning_rate": 0.0008629635281550436,
      "loss": 1.5188,
      "step": 949
    },
    {
      "epoch": 0.4316219900045434,
      "grad_norm": 5.091749668121338,
      "learning_rate": 0.0008628109262933008,
      "loss": 1.5312,
      "step": 950
    },
    {
      "epoch": 0.43207632894139025,
      "grad_norm": 5.9130425453186035,
      "learning_rate": 0.0008626583244315581,
      "loss": 1.0513,
      "step": 951
    },
    {
      "epoch": 0.4325306678782372,
      "grad_norm": 7.981223106384277,
      "learning_rate": 0.0008625057225698155,
      "loss": 1.5899,
      "step": 952
    },
    {
      "epoch": 0.43298500681508406,
      "grad_norm": 4.713680744171143,
      "learning_rate": 0.0008623531207080727,
      "loss": 1.0954,
      "step": 953
    },
    {
      "epoch": 0.43343934575193094,
      "grad_norm": 3.2080931663513184,
      "learning_rate": 0.00086220051884633,
      "loss": 0.2,
      "step": 954
    },
    {
      "epoch": 0.4338936846887778,
      "grad_norm": 4.3739166259765625,
      "learning_rate": 0.0008620479169845872,
      "loss": 0.8257,
      "step": 955
    },
    {
      "epoch": 0.4343480236256247,
      "grad_norm": 6.5156121253967285,
      "learning_rate": 0.0008618953151228445,
      "loss": 2.4876,
      "step": 956
    },
    {
      "epoch": 0.4348023625624716,
      "grad_norm": 7.063384056091309,
      "learning_rate": 0.0008617427132611017,
      "loss": 1.7366,
      "step": 957
    },
    {
      "epoch": 0.4352567014993185,
      "grad_norm": 4.522094249725342,
      "learning_rate": 0.0008615901113993591,
      "loss": 1.1107,
      "step": 958
    },
    {
      "epoch": 0.4357110404361654,
      "grad_norm": 4.6341776847839355,
      "learning_rate": 0.0008614375095376164,
      "loss": 1.1924,
      "step": 959
    },
    {
      "epoch": 0.43616537937301225,
      "grad_norm": 6.497812271118164,
      "learning_rate": 0.0008612849076758736,
      "loss": 0.8585,
      "step": 960
    },
    {
      "epoch": 0.43661971830985913,
      "grad_norm": 6.07902193069458,
      "learning_rate": 0.000861132305814131,
      "loss": 1.19,
      "step": 961
    },
    {
      "epoch": 0.43707405724670606,
      "grad_norm": 3.8967456817626953,
      "learning_rate": 0.0008609797039523882,
      "loss": 0.7496,
      "step": 962
    },
    {
      "epoch": 0.43752839618355294,
      "grad_norm": 7.391471862792969,
      "learning_rate": 0.0008608271020906455,
      "loss": 1.0022,
      "step": 963
    },
    {
      "epoch": 0.4379827351203998,
      "grad_norm": 6.980207443237305,
      "learning_rate": 0.0008606745002289029,
      "loss": 0.8355,
      "step": 964
    },
    {
      "epoch": 0.4384370740572467,
      "grad_norm": 4.290483474731445,
      "learning_rate": 0.0008605218983671601,
      "loss": 0.8613,
      "step": 965
    },
    {
      "epoch": 0.4388914129940936,
      "grad_norm": 5.353556156158447,
      "learning_rate": 0.0008603692965054174,
      "loss": 0.8788,
      "step": 966
    },
    {
      "epoch": 0.4393457519309405,
      "grad_norm": 5.208590030670166,
      "learning_rate": 0.0008602166946436747,
      "loss": 1.5826,
      "step": 967
    },
    {
      "epoch": 0.4398000908677874,
      "grad_norm": 6.183323383331299,
      "learning_rate": 0.000860064092781932,
      "loss": 1.175,
      "step": 968
    },
    {
      "epoch": 0.44025442980463425,
      "grad_norm": 5.985810279846191,
      "learning_rate": 0.0008599114909201893,
      "loss": 0.8497,
      "step": 969
    },
    {
      "epoch": 0.4407087687414811,
      "grad_norm": 5.852529525756836,
      "learning_rate": 0.0008597588890584466,
      "loss": 0.8327,
      "step": 970
    },
    {
      "epoch": 0.44116310767832806,
      "grad_norm": 5.094454288482666,
      "learning_rate": 0.0008596062871967039,
      "loss": 0.6737,
      "step": 971
    },
    {
      "epoch": 0.44161744661517494,
      "grad_norm": 7.0584797859191895,
      "learning_rate": 0.0008594536853349611,
      "loss": 1.3817,
      "step": 972
    },
    {
      "epoch": 0.4420717855520218,
      "grad_norm": 5.17043924331665,
      "learning_rate": 0.0008593010834732184,
      "loss": 0.7932,
      "step": 973
    },
    {
      "epoch": 0.4425261244888687,
      "grad_norm": 4.956395149230957,
      "learning_rate": 0.0008591484816114756,
      "loss": 0.919,
      "step": 974
    },
    {
      "epoch": 0.44298046342571556,
      "grad_norm": 5.982621669769287,
      "learning_rate": 0.0008589958797497329,
      "loss": 1.1108,
      "step": 975
    },
    {
      "epoch": 0.4434348023625625,
      "grad_norm": 6.408397674560547,
      "learning_rate": 0.0008588432778879903,
      "loss": 1.2061,
      "step": 976
    },
    {
      "epoch": 0.4438891412994094,
      "grad_norm": 7.211100101470947,
      "learning_rate": 0.0008586906760262475,
      "loss": 1.1162,
      "step": 977
    },
    {
      "epoch": 0.44434348023625625,
      "grad_norm": 6.028185844421387,
      "learning_rate": 0.0008585380741645048,
      "loss": 1.9802,
      "step": 978
    },
    {
      "epoch": 0.4447978191731031,
      "grad_norm": 6.788867950439453,
      "learning_rate": 0.0008583854723027621,
      "loss": 1.5684,
      "step": 979
    },
    {
      "epoch": 0.44525215810995,
      "grad_norm": 7.551860332489014,
      "learning_rate": 0.0008582328704410194,
      "loss": 1.8892,
      "step": 980
    },
    {
      "epoch": 0.44570649704679693,
      "grad_norm": 8.397481918334961,
      "learning_rate": 0.0008580802685792767,
      "loss": 1.4161,
      "step": 981
    },
    {
      "epoch": 0.4461608359836438,
      "grad_norm": 5.002186298370361,
      "learning_rate": 0.000857927666717534,
      "loss": 0.9486,
      "step": 982
    },
    {
      "epoch": 0.4466151749204907,
      "grad_norm": 9.530433654785156,
      "learning_rate": 0.0008577750648557913,
      "loss": 1.7995,
      "step": 983
    },
    {
      "epoch": 0.44706951385733756,
      "grad_norm": 7.6902055740356445,
      "learning_rate": 0.0008576224629940485,
      "loss": 1.1942,
      "step": 984
    },
    {
      "epoch": 0.44752385279418444,
      "grad_norm": 4.214831829071045,
      "learning_rate": 0.0008574698611323059,
      "loss": 1.5227,
      "step": 985
    },
    {
      "epoch": 0.44797819173103137,
      "grad_norm": 6.611765384674072,
      "learning_rate": 0.0008573172592705632,
      "loss": 1.4416,
      "step": 986
    },
    {
      "epoch": 0.44843253066787825,
      "grad_norm": 6.718103408813477,
      "learning_rate": 0.0008571646574088204,
      "loss": 1.2857,
      "step": 987
    },
    {
      "epoch": 0.4488868696047251,
      "grad_norm": 6.35604190826416,
      "learning_rate": 0.0008570120555470778,
      "loss": 1.4592,
      "step": 988
    },
    {
      "epoch": 0.449341208541572,
      "grad_norm": 4.740074157714844,
      "learning_rate": 0.000856859453685335,
      "loss": 0.4984,
      "step": 989
    },
    {
      "epoch": 0.4497955474784189,
      "grad_norm": 2.1703381538391113,
      "learning_rate": 0.0008567068518235923,
      "loss": 0.2979,
      "step": 990
    },
    {
      "epoch": 0.4502498864152658,
      "grad_norm": 11.879271507263184,
      "learning_rate": 0.0008565542499618497,
      "loss": 1.7121,
      "step": 991
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 6.456932067871094,
      "learning_rate": 0.0008564016481001068,
      "loss": 1.4912,
      "step": 992
    },
    {
      "epoch": 0.45115856428895956,
      "grad_norm": 5.193727493286133,
      "learning_rate": 0.0008562490462383641,
      "loss": 1.2286,
      "step": 993
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 4.299095630645752,
      "learning_rate": 0.0008560964443766214,
      "loss": 0.4361,
      "step": 994
    },
    {
      "epoch": 0.4520672421626533,
      "grad_norm": 4.750974178314209,
      "learning_rate": 0.0008559438425148787,
      "loss": 0.9665,
      "step": 995
    },
    {
      "epoch": 0.45252158109950025,
      "grad_norm": 6.530337810516357,
      "learning_rate": 0.0008557912406531359,
      "loss": 2.0856,
      "step": 996
    },
    {
      "epoch": 0.4529759200363471,
      "grad_norm": 4.6142072677612305,
      "learning_rate": 0.0008556386387913933,
      "loss": 1.1444,
      "step": 997
    },
    {
      "epoch": 0.453430258973194,
      "grad_norm": 4.032329082489014,
      "learning_rate": 0.0008554860369296506,
      "loss": 0.883,
      "step": 998
    },
    {
      "epoch": 0.4538845979100409,
      "grad_norm": 6.458658218383789,
      "learning_rate": 0.0008553334350679078,
      "loss": 1.5612,
      "step": 999
    },
    {
      "epoch": 0.45433893684688775,
      "grad_norm": 6.138057708740234,
      "learning_rate": 0.0008551808332061652,
      "loss": 1.2868,
      "step": 1000
    },
    {
      "epoch": 0.4547932757837347,
      "grad_norm": 6.380071640014648,
      "learning_rate": 0.0008550282313444224,
      "loss": 0.8025,
      "step": 1001
    },
    {
      "epoch": 0.45524761472058156,
      "grad_norm": 4.58574914932251,
      "learning_rate": 0.0008548756294826797,
      "loss": 1.3185,
      "step": 1002
    },
    {
      "epoch": 0.45570195365742844,
      "grad_norm": 5.173280715942383,
      "learning_rate": 0.000854723027620937,
      "loss": 0.5902,
      "step": 1003
    },
    {
      "epoch": 0.4561562925942753,
      "grad_norm": 5.617477893829346,
      "learning_rate": 0.0008545704257591943,
      "loss": 1.4722,
      "step": 1004
    },
    {
      "epoch": 0.4566106315311222,
      "grad_norm": 9.842035293579102,
      "learning_rate": 0.0008544178238974516,
      "loss": 1.6373,
      "step": 1005
    },
    {
      "epoch": 0.4570649704679691,
      "grad_norm": 7.32267951965332,
      "learning_rate": 0.0008542652220357089,
      "loss": 1.781,
      "step": 1006
    },
    {
      "epoch": 0.457519309404816,
      "grad_norm": 5.314257621765137,
      "learning_rate": 0.0008541126201739662,
      "loss": 0.9008,
      "step": 1007
    },
    {
      "epoch": 0.4579736483416629,
      "grad_norm": 5.942941665649414,
      "learning_rate": 0.0008539600183122234,
      "loss": 1.5513,
      "step": 1008
    },
    {
      "epoch": 0.45842798727850975,
      "grad_norm": 5.07900857925415,
      "learning_rate": 0.0008538074164504808,
      "loss": 1.5369,
      "step": 1009
    },
    {
      "epoch": 0.45888232621535663,
      "grad_norm": 6.6903076171875,
      "learning_rate": 0.000853654814588738,
      "loss": 1.8422,
      "step": 1010
    },
    {
      "epoch": 0.45933666515220356,
      "grad_norm": 2.4380643367767334,
      "learning_rate": 0.0008535022127269952,
      "loss": 0.4334,
      "step": 1011
    },
    {
      "epoch": 0.45979100408905044,
      "grad_norm": 3.6741573810577393,
      "learning_rate": 0.0008533496108652526,
      "loss": 0.4538,
      "step": 1012
    },
    {
      "epoch": 0.4602453430258973,
      "grad_norm": 7.181878089904785,
      "learning_rate": 0.0008531970090035098,
      "loss": 1.63,
      "step": 1013
    },
    {
      "epoch": 0.4606996819627442,
      "grad_norm": 7.914843559265137,
      "learning_rate": 0.0008530444071417671,
      "loss": 1.5862,
      "step": 1014
    },
    {
      "epoch": 0.4611540208995911,
      "grad_norm": 5.013667583465576,
      "learning_rate": 0.0008528918052800245,
      "loss": 1.1057,
      "step": 1015
    },
    {
      "epoch": 0.461608359836438,
      "grad_norm": 4.393171787261963,
      "learning_rate": 0.0008527392034182817,
      "loss": 0.8305,
      "step": 1016
    },
    {
      "epoch": 0.4620626987732849,
      "grad_norm": 3.380945920944214,
      "learning_rate": 0.000852586601556539,
      "loss": 0.4266,
      "step": 1017
    },
    {
      "epoch": 0.46251703771013175,
      "grad_norm": 6.450294017791748,
      "learning_rate": 0.0008524339996947963,
      "loss": 1.1641,
      "step": 1018
    },
    {
      "epoch": 0.4629713766469786,
      "grad_norm": 6.865687370300293,
      "learning_rate": 0.0008522813978330536,
      "loss": 1.0847,
      "step": 1019
    },
    {
      "epoch": 0.46342571558382556,
      "grad_norm": 4.857980728149414,
      "learning_rate": 0.0008521287959713108,
      "loss": 0.8739,
      "step": 1020
    },
    {
      "epoch": 0.46388005452067244,
      "grad_norm": 5.361921310424805,
      "learning_rate": 0.0008519761941095682,
      "loss": 0.7581,
      "step": 1021
    },
    {
      "epoch": 0.4643343934575193,
      "grad_norm": 5.7912163734436035,
      "learning_rate": 0.0008518235922478255,
      "loss": 1.2129,
      "step": 1022
    },
    {
      "epoch": 0.4647887323943662,
      "grad_norm": 9.393000602722168,
      "learning_rate": 0.0008516709903860827,
      "loss": 2.8299,
      "step": 1023
    },
    {
      "epoch": 0.46524307133121307,
      "grad_norm": 6.408736705780029,
      "learning_rate": 0.0008515183885243401,
      "loss": 1.2587,
      "step": 1024
    },
    {
      "epoch": 0.46569741026806,
      "grad_norm": 3.2343766689300537,
      "learning_rate": 0.0008513657866625973,
      "loss": 0.4129,
      "step": 1025
    },
    {
      "epoch": 0.4661517492049069,
      "grad_norm": 5.080480575561523,
      "learning_rate": 0.0008512131848008546,
      "loss": 1.2855,
      "step": 1026
    },
    {
      "epoch": 0.46660608814175375,
      "grad_norm": 5.9554972648620605,
      "learning_rate": 0.000851060582939112,
      "loss": 0.7575,
      "step": 1027
    },
    {
      "epoch": 0.4670604270786006,
      "grad_norm": 5.896932125091553,
      "learning_rate": 0.0008509079810773691,
      "loss": 1.4098,
      "step": 1028
    },
    {
      "epoch": 0.4675147660154475,
      "grad_norm": 6.0760016441345215,
      "learning_rate": 0.0008507553792156264,
      "loss": 0.8272,
      "step": 1029
    },
    {
      "epoch": 0.46796910495229443,
      "grad_norm": 6.415463924407959,
      "learning_rate": 0.0008506027773538837,
      "loss": 1.2483,
      "step": 1030
    },
    {
      "epoch": 0.4684234438891413,
      "grad_norm": 4.7757697105407715,
      "learning_rate": 0.000850450175492141,
      "loss": 0.8672,
      "step": 1031
    },
    {
      "epoch": 0.4688777828259882,
      "grad_norm": 7.364598274230957,
      "learning_rate": 0.0008502975736303982,
      "loss": 1.6919,
      "step": 1032
    },
    {
      "epoch": 0.46933212176283506,
      "grad_norm": 6.956244468688965,
      "learning_rate": 0.0008501449717686556,
      "loss": 2.1183,
      "step": 1033
    },
    {
      "epoch": 0.46978646069968194,
      "grad_norm": 4.836253643035889,
      "learning_rate": 0.0008499923699069129,
      "loss": 0.8949,
      "step": 1034
    },
    {
      "epoch": 0.4702407996365289,
      "grad_norm": 6.674531936645508,
      "learning_rate": 0.0008498397680451701,
      "loss": 1.9401,
      "step": 1035
    },
    {
      "epoch": 0.47069513857337575,
      "grad_norm": 4.968378067016602,
      "learning_rate": 0.0008496871661834275,
      "loss": 1.3732,
      "step": 1036
    },
    {
      "epoch": 0.4711494775102226,
      "grad_norm": 9.626267433166504,
      "learning_rate": 0.0008495345643216847,
      "loss": 1.0569,
      "step": 1037
    },
    {
      "epoch": 0.4716038164470695,
      "grad_norm": 5.505126476287842,
      "learning_rate": 0.000849381962459942,
      "loss": 1.2286,
      "step": 1038
    },
    {
      "epoch": 0.4720581553839164,
      "grad_norm": 9.345490455627441,
      "learning_rate": 0.0008492293605981994,
      "loss": 1.6806,
      "step": 1039
    },
    {
      "epoch": 0.4725124943207633,
      "grad_norm": 3.725044012069702,
      "learning_rate": 0.0008490767587364566,
      "loss": 1.2942,
      "step": 1040
    },
    {
      "epoch": 0.4729668332576102,
      "grad_norm": 7.118791103363037,
      "learning_rate": 0.0008489241568747139,
      "loss": 1.2514,
      "step": 1041
    },
    {
      "epoch": 0.47342117219445706,
      "grad_norm": 6.948042869567871,
      "learning_rate": 0.0008487715550129712,
      "loss": 1.6835,
      "step": 1042
    },
    {
      "epoch": 0.47387551113130394,
      "grad_norm": 5.219738483428955,
      "learning_rate": 0.0008486189531512285,
      "loss": 1.0965,
      "step": 1043
    },
    {
      "epoch": 0.4743298500681508,
      "grad_norm": 5.083114147186279,
      "learning_rate": 0.0008484663512894858,
      "loss": 0.8009,
      "step": 1044
    },
    {
      "epoch": 0.47478418900499775,
      "grad_norm": 7.639681816101074,
      "learning_rate": 0.0008483137494277431,
      "loss": 0.897,
      "step": 1045
    },
    {
      "epoch": 0.4752385279418446,
      "grad_norm": 3.6835720539093018,
      "learning_rate": 0.0008481611475660003,
      "loss": 0.617,
      "step": 1046
    },
    {
      "epoch": 0.4756928668786915,
      "grad_norm": 7.283473491668701,
      "learning_rate": 0.0008480085457042575,
      "loss": 1.6139,
      "step": 1047
    },
    {
      "epoch": 0.4761472058155384,
      "grad_norm": 9.359955787658691,
      "learning_rate": 0.0008478559438425149,
      "loss": 1.3503,
      "step": 1048
    },
    {
      "epoch": 0.47660154475238525,
      "grad_norm": 3.748080015182495,
      "learning_rate": 0.0008477033419807721,
      "loss": 0.5581,
      "step": 1049
    },
    {
      "epoch": 0.4770558836892322,
      "grad_norm": 4.528615474700928,
      "learning_rate": 0.0008475507401190294,
      "loss": 0.6533,
      "step": 1050
    },
    {
      "epoch": 0.47751022262607906,
      "grad_norm": 3.3676633834838867,
      "learning_rate": 0.0008473981382572868,
      "loss": 0.747,
      "step": 1051
    },
    {
      "epoch": 0.47796456156292594,
      "grad_norm": 9.729265213012695,
      "learning_rate": 0.000847245536395544,
      "loss": 1.6583,
      "step": 1052
    },
    {
      "epoch": 0.4784189004997728,
      "grad_norm": 7.407498836517334,
      "learning_rate": 0.0008470929345338013,
      "loss": 1.0277,
      "step": 1053
    },
    {
      "epoch": 0.4788732394366197,
      "grad_norm": 5.873044967651367,
      "learning_rate": 0.0008469403326720586,
      "loss": 1.3591,
      "step": 1054
    },
    {
      "epoch": 0.4793275783734666,
      "grad_norm": 9.989470481872559,
      "learning_rate": 0.0008467877308103159,
      "loss": 1.1562,
      "step": 1055
    },
    {
      "epoch": 0.4797819173103135,
      "grad_norm": 6.393779277801514,
      "learning_rate": 0.0008466351289485732,
      "loss": 1.4065,
      "step": 1056
    },
    {
      "epoch": 0.4802362562471604,
      "grad_norm": 5.903590202331543,
      "learning_rate": 0.0008464825270868305,
      "loss": 0.8985,
      "step": 1057
    },
    {
      "epoch": 0.48069059518400725,
      "grad_norm": 4.940035820007324,
      "learning_rate": 0.0008463299252250878,
      "loss": 1.2563,
      "step": 1058
    },
    {
      "epoch": 0.4811449341208542,
      "grad_norm": 6.836970806121826,
      "learning_rate": 0.000846177323363345,
      "loss": 1.6439,
      "step": 1059
    },
    {
      "epoch": 0.48159927305770106,
      "grad_norm": 5.041175365447998,
      "learning_rate": 0.0008460247215016024,
      "loss": 1.1691,
      "step": 1060
    },
    {
      "epoch": 0.48205361199454794,
      "grad_norm": 6.179610252380371,
      "learning_rate": 0.0008458721196398597,
      "loss": 0.9738,
      "step": 1061
    },
    {
      "epoch": 0.4825079509313948,
      "grad_norm": 6.3361053466796875,
      "learning_rate": 0.0008457195177781169,
      "loss": 1.6368,
      "step": 1062
    },
    {
      "epoch": 0.4829622898682417,
      "grad_norm": 8.360262870788574,
      "learning_rate": 0.0008455669159163743,
      "loss": 1.4182,
      "step": 1063
    },
    {
      "epoch": 0.4834166288050886,
      "grad_norm": 5.877464771270752,
      "learning_rate": 0.0008454143140546315,
      "loss": 1.5698,
      "step": 1064
    },
    {
      "epoch": 0.4838709677419355,
      "grad_norm": 3.9838290214538574,
      "learning_rate": 0.0008452617121928887,
      "loss": 0.6296,
      "step": 1065
    },
    {
      "epoch": 0.4843253066787824,
      "grad_norm": 6.497221946716309,
      "learning_rate": 0.000845109110331146,
      "loss": 1.5373,
      "step": 1066
    },
    {
      "epoch": 0.48477964561562925,
      "grad_norm": 4.204268455505371,
      "learning_rate": 0.0008449565084694033,
      "loss": 0.5592,
      "step": 1067
    },
    {
      "epoch": 0.4852339845524761,
      "grad_norm": 7.243320465087891,
      "learning_rate": 0.0008448039066076606,
      "loss": 1.1569,
      "step": 1068
    },
    {
      "epoch": 0.48568832348932306,
      "grad_norm": 7.655782222747803,
      "learning_rate": 0.0008446513047459179,
      "loss": 1.6234,
      "step": 1069
    },
    {
      "epoch": 0.48614266242616994,
      "grad_norm": 3.4744296073913574,
      "learning_rate": 0.0008444987028841752,
      "loss": 0.7883,
      "step": 1070
    },
    {
      "epoch": 0.4865970013630168,
      "grad_norm": 5.540771007537842,
      "learning_rate": 0.0008443461010224324,
      "loss": 0.9551,
      "step": 1071
    },
    {
      "epoch": 0.4870513402998637,
      "grad_norm": 6.8821001052856445,
      "learning_rate": 0.0008441934991606898,
      "loss": 1.5747,
      "step": 1072
    },
    {
      "epoch": 0.48750567923671057,
      "grad_norm": 7.079697608947754,
      "learning_rate": 0.0008440408972989471,
      "loss": 1.4531,
      "step": 1073
    },
    {
      "epoch": 0.4879600181735575,
      "grad_norm": 3.9076895713806152,
      "learning_rate": 0.0008438882954372043,
      "loss": 0.5766,
      "step": 1074
    },
    {
      "epoch": 0.4884143571104044,
      "grad_norm": 7.287618637084961,
      "learning_rate": 0.0008437356935754617,
      "loss": 0.7804,
      "step": 1075
    },
    {
      "epoch": 0.48886869604725125,
      "grad_norm": 6.263627052307129,
      "learning_rate": 0.0008435830917137189,
      "loss": 0.8192,
      "step": 1076
    },
    {
      "epoch": 0.4893230349840981,
      "grad_norm": 8.775127410888672,
      "learning_rate": 0.0008434304898519762,
      "loss": 1.4447,
      "step": 1077
    },
    {
      "epoch": 0.489777373920945,
      "grad_norm": 6.353113174438477,
      "learning_rate": 0.0008432778879902336,
      "loss": 1.1747,
      "step": 1078
    },
    {
      "epoch": 0.49023171285779193,
      "grad_norm": 6.539178371429443,
      "learning_rate": 0.0008431252861284908,
      "loss": 2.0047,
      "step": 1079
    },
    {
      "epoch": 0.4906860517946388,
      "grad_norm": 4.7845139503479,
      "learning_rate": 0.0008429726842667481,
      "loss": 0.6835,
      "step": 1080
    },
    {
      "epoch": 0.4911403907314857,
      "grad_norm": 3.2114803791046143,
      "learning_rate": 0.0008428200824050054,
      "loss": 0.7961,
      "step": 1081
    },
    {
      "epoch": 0.49159472966833256,
      "grad_norm": 8.696600914001465,
      "learning_rate": 0.0008426674805432627,
      "loss": 1.8851,
      "step": 1082
    },
    {
      "epoch": 0.49204906860517944,
      "grad_norm": 7.021031856536865,
      "learning_rate": 0.0008425148786815198,
      "loss": 0.9314,
      "step": 1083
    },
    {
      "epoch": 0.4925034075420264,
      "grad_norm": 17.19550132751465,
      "learning_rate": 0.0008423622768197772,
      "loss": 1.7705,
      "step": 1084
    },
    {
      "epoch": 0.49295774647887325,
      "grad_norm": 4.5938920974731445,
      "learning_rate": 0.0008422096749580345,
      "loss": 0.645,
      "step": 1085
    },
    {
      "epoch": 0.4934120854157201,
      "grad_norm": 5.021948337554932,
      "learning_rate": 0.0008420570730962917,
      "loss": 1.7244,
      "step": 1086
    },
    {
      "epoch": 0.493866424352567,
      "grad_norm": 6.16664981842041,
      "learning_rate": 0.0008419044712345491,
      "loss": 0.713,
      "step": 1087
    },
    {
      "epoch": 0.4943207632894139,
      "grad_norm": 6.615296363830566,
      "learning_rate": 0.0008417518693728063,
      "loss": 1.769,
      "step": 1088
    },
    {
      "epoch": 0.4947751022262608,
      "grad_norm": 7.072011947631836,
      "learning_rate": 0.0008415992675110636,
      "loss": 1.3384,
      "step": 1089
    },
    {
      "epoch": 0.4952294411631077,
      "grad_norm": 8.322399139404297,
      "learning_rate": 0.000841446665649321,
      "loss": 2.1046,
      "step": 1090
    },
    {
      "epoch": 0.49568378009995456,
      "grad_norm": 7.297142028808594,
      "learning_rate": 0.0008412940637875782,
      "loss": 1.1217,
      "step": 1091
    },
    {
      "epoch": 0.49613811903680144,
      "grad_norm": 5.8210673332214355,
      "learning_rate": 0.0008411414619258355,
      "loss": 1.0339,
      "step": 1092
    },
    {
      "epoch": 0.4965924579736483,
      "grad_norm": 5.07891845703125,
      "learning_rate": 0.0008409888600640928,
      "loss": 1.0438,
      "step": 1093
    },
    {
      "epoch": 0.49704679691049525,
      "grad_norm": 5.2758636474609375,
      "learning_rate": 0.0008408362582023501,
      "loss": 0.5346,
      "step": 1094
    },
    {
      "epoch": 0.4975011358473421,
      "grad_norm": 7.319534778594971,
      "learning_rate": 0.0008406836563406074,
      "loss": 1.594,
      "step": 1095
    },
    {
      "epoch": 0.497955474784189,
      "grad_norm": 5.124091148376465,
      "learning_rate": 0.0008405310544788647,
      "loss": 0.6298,
      "step": 1096
    },
    {
      "epoch": 0.4984098137210359,
      "grad_norm": 5.642143726348877,
      "learning_rate": 0.000840378452617122,
      "loss": 1.2714,
      "step": 1097
    },
    {
      "epoch": 0.49886415265788275,
      "grad_norm": 6.534734725952148,
      "learning_rate": 0.0008402258507553792,
      "loss": 0.9762,
      "step": 1098
    },
    {
      "epoch": 0.4993184915947297,
      "grad_norm": 6.664302349090576,
      "learning_rate": 0.0008400732488936366,
      "loss": 2.434,
      "step": 1099
    },
    {
      "epoch": 0.49977283053157656,
      "grad_norm": 7.810424327850342,
      "learning_rate": 0.0008399206470318939,
      "loss": 1.1881,
      "step": 1100
    },
    {
      "epoch": 0.5002271694684235,
      "grad_norm": 6.587212085723877,
      "learning_rate": 0.000839768045170151,
      "loss": 1.2737,
      "step": 1101
    },
    {
      "epoch": 0.5006815084052704,
      "grad_norm": 5.747220993041992,
      "learning_rate": 0.0008396154433084084,
      "loss": 1.4719,
      "step": 1102
    },
    {
      "epoch": 0.5011358473421172,
      "grad_norm": 6.571447372436523,
      "learning_rate": 0.0008394628414466656,
      "loss": 1.0354,
      "step": 1103
    },
    {
      "epoch": 0.5015901862789641,
      "grad_norm": 7.127504825592041,
      "learning_rate": 0.0008393102395849229,
      "loss": 1.1939,
      "step": 1104
    },
    {
      "epoch": 0.502044525215811,
      "grad_norm": 7.037901878356934,
      "learning_rate": 0.0008391576377231802,
      "loss": 1.576,
      "step": 1105
    },
    {
      "epoch": 0.5024988641526579,
      "grad_norm": 4.822382926940918,
      "learning_rate": 0.0008390050358614375,
      "loss": 1.3234,
      "step": 1106
    },
    {
      "epoch": 0.5029532030895048,
      "grad_norm": 5.6152262687683105,
      "learning_rate": 0.0008388524339996948,
      "loss": 0.9346,
      "step": 1107
    },
    {
      "epoch": 0.5034075420263516,
      "grad_norm": 7.399785995483398,
      "learning_rate": 0.0008386998321379521,
      "loss": 1.6886,
      "step": 1108
    },
    {
      "epoch": 0.5038618809631985,
      "grad_norm": 8.636186599731445,
      "learning_rate": 0.0008385472302762094,
      "loss": 1.324,
      "step": 1109
    },
    {
      "epoch": 0.5043162199000454,
      "grad_norm": 6.949794769287109,
      "learning_rate": 0.0008383946284144666,
      "loss": 2.395,
      "step": 1110
    },
    {
      "epoch": 0.5047705588368924,
      "grad_norm": 5.682286739349365,
      "learning_rate": 0.000838242026552724,
      "loss": 1.0192,
      "step": 1111
    },
    {
      "epoch": 0.5052248977737392,
      "grad_norm": 8.152521133422852,
      "learning_rate": 0.0008380894246909813,
      "loss": 1.4793,
      "step": 1112
    },
    {
      "epoch": 0.5056792367105861,
      "grad_norm": 5.671889781951904,
      "learning_rate": 0.0008379368228292385,
      "loss": 1.525,
      "step": 1113
    },
    {
      "epoch": 0.506133575647433,
      "grad_norm": 3.8371431827545166,
      "learning_rate": 0.0008377842209674959,
      "loss": 1.1724,
      "step": 1114
    },
    {
      "epoch": 0.5065879145842799,
      "grad_norm": 5.2708420753479,
      "learning_rate": 0.0008376316191057531,
      "loss": 1.6699,
      "step": 1115
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 3.933154821395874,
      "learning_rate": 0.0008374790172440104,
      "loss": 0.5225,
      "step": 1116
    },
    {
      "epoch": 0.5074965924579736,
      "grad_norm": 9.281172752380371,
      "learning_rate": 0.0008373264153822678,
      "loss": 1.5916,
      "step": 1117
    },
    {
      "epoch": 0.5079509313948205,
      "grad_norm": 3.2661590576171875,
      "learning_rate": 0.000837173813520525,
      "loss": 0.3318,
      "step": 1118
    },
    {
      "epoch": 0.5084052703316674,
      "grad_norm": 5.06362771987915,
      "learning_rate": 0.0008370212116587822,
      "loss": 1.1835,
      "step": 1119
    },
    {
      "epoch": 0.5088596092685143,
      "grad_norm": 7.5969953536987305,
      "learning_rate": 0.0008368686097970395,
      "loss": 1.5246,
      "step": 1120
    },
    {
      "epoch": 0.5093139482053612,
      "grad_norm": 5.2594895362854,
      "learning_rate": 0.0008367160079352968,
      "loss": 1.2345,
      "step": 1121
    },
    {
      "epoch": 0.5097682871422081,
      "grad_norm": 5.665925979614258,
      "learning_rate": 0.000836563406073554,
      "loss": 1.1251,
      "step": 1122
    },
    {
      "epoch": 0.510222626079055,
      "grad_norm": 7.767950534820557,
      "learning_rate": 0.0008364108042118114,
      "loss": 0.8699,
      "step": 1123
    },
    {
      "epoch": 0.5106769650159019,
      "grad_norm": 4.963507175445557,
      "learning_rate": 0.0008362582023500687,
      "loss": 1.0921,
      "step": 1124
    },
    {
      "epoch": 0.5111313039527488,
      "grad_norm": 6.4657063484191895,
      "learning_rate": 0.0008361056004883259,
      "loss": 1.4953,
      "step": 1125
    },
    {
      "epoch": 0.5115856428895956,
      "grad_norm": 3.8104875087738037,
      "learning_rate": 0.0008359529986265833,
      "loss": 0.657,
      "step": 1126
    },
    {
      "epoch": 0.5120399818264425,
      "grad_norm": 5.053524017333984,
      "learning_rate": 0.0008358003967648405,
      "loss": 1.0779,
      "step": 1127
    },
    {
      "epoch": 0.5124943207632894,
      "grad_norm": 7.919530868530273,
      "learning_rate": 0.0008356477949030978,
      "loss": 1.1535,
      "step": 1128
    },
    {
      "epoch": 0.5129486597001363,
      "grad_norm": 6.943070888519287,
      "learning_rate": 0.0008354951930413552,
      "loss": 1.2327,
      "step": 1129
    },
    {
      "epoch": 0.5134029986369832,
      "grad_norm": 4.677811145782471,
      "learning_rate": 0.0008353425911796124,
      "loss": 0.5897,
      "step": 1130
    },
    {
      "epoch": 0.5138573375738301,
      "grad_norm": 3.222764015197754,
      "learning_rate": 0.0008351899893178697,
      "loss": 0.5926,
      "step": 1131
    },
    {
      "epoch": 0.514311676510677,
      "grad_norm": 8.361616134643555,
      "learning_rate": 0.000835037387456127,
      "loss": 1.0879,
      "step": 1132
    },
    {
      "epoch": 0.5147660154475239,
      "grad_norm": 5.022936820983887,
      "learning_rate": 0.0008348847855943843,
      "loss": 0.8763,
      "step": 1133
    },
    {
      "epoch": 0.5152203543843707,
      "grad_norm": 4.736138343811035,
      "learning_rate": 0.0008347321837326415,
      "loss": 1.1375,
      "step": 1134
    },
    {
      "epoch": 0.5156746933212176,
      "grad_norm": 6.446150302886963,
      "learning_rate": 0.0008345795818708989,
      "loss": 0.9612,
      "step": 1135
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 5.588617324829102,
      "learning_rate": 0.0008344269800091562,
      "loss": 0.7953,
      "step": 1136
    },
    {
      "epoch": 0.5165833711949114,
      "grad_norm": 4.436561107635498,
      "learning_rate": 0.0008342743781474134,
      "loss": 1.5106,
      "step": 1137
    },
    {
      "epoch": 0.5170377101317583,
      "grad_norm": 6.238348007202148,
      "learning_rate": 0.0008341217762856707,
      "loss": 1.6455,
      "step": 1138
    },
    {
      "epoch": 0.5174920490686051,
      "grad_norm": 3.2097113132476807,
      "learning_rate": 0.0008339691744239279,
      "loss": 0.7943,
      "step": 1139
    },
    {
      "epoch": 0.5179463880054521,
      "grad_norm": 5.05265998840332,
      "learning_rate": 0.0008338165725621852,
      "loss": 1.8043,
      "step": 1140
    },
    {
      "epoch": 0.518400726942299,
      "grad_norm": 6.3679656982421875,
      "learning_rate": 0.0008336639707004426,
      "loss": 2.1458,
      "step": 1141
    },
    {
      "epoch": 0.5188550658791459,
      "grad_norm": 7.319841384887695,
      "learning_rate": 0.0008335113688386998,
      "loss": 1.3739,
      "step": 1142
    },
    {
      "epoch": 0.5193094048159927,
      "grad_norm": 3.1988539695739746,
      "learning_rate": 0.0008333587669769571,
      "loss": 0.3195,
      "step": 1143
    },
    {
      "epoch": 0.5197637437528396,
      "grad_norm": 6.674330234527588,
      "learning_rate": 0.0008332061651152144,
      "loss": 0.7525,
      "step": 1144
    },
    {
      "epoch": 0.5202180826896865,
      "grad_norm": 6.5078630447387695,
      "learning_rate": 0.0008330535632534717,
      "loss": 1.3959,
      "step": 1145
    },
    {
      "epoch": 0.5206724216265334,
      "grad_norm": 5.118145942687988,
      "learning_rate": 0.000832900961391729,
      "loss": 0.8671,
      "step": 1146
    },
    {
      "epoch": 0.5211267605633803,
      "grad_norm": 5.7324090003967285,
      "learning_rate": 0.0008327483595299863,
      "loss": 2.0736,
      "step": 1147
    },
    {
      "epoch": 0.5215810995002271,
      "grad_norm": 6.452836990356445,
      "learning_rate": 0.0008325957576682436,
      "loss": 1.4646,
      "step": 1148
    },
    {
      "epoch": 0.522035438437074,
      "grad_norm": 6.395796298980713,
      "learning_rate": 0.0008324431558065008,
      "loss": 0.639,
      "step": 1149
    },
    {
      "epoch": 0.522489777373921,
      "grad_norm": 7.015491485595703,
      "learning_rate": 0.0008322905539447582,
      "loss": 1.8082,
      "step": 1150
    },
    {
      "epoch": 0.5229441163107679,
      "grad_norm": 5.786884307861328,
      "learning_rate": 0.0008321379520830154,
      "loss": 1.9942,
      "step": 1151
    },
    {
      "epoch": 0.5233984552476147,
      "grad_norm": 6.377841472625732,
      "learning_rate": 0.0008319853502212727,
      "loss": 1.7593,
      "step": 1152
    },
    {
      "epoch": 0.5238527941844616,
      "grad_norm": 3.702681064605713,
      "learning_rate": 0.0008318327483595301,
      "loss": 0.6748,
      "step": 1153
    },
    {
      "epoch": 0.5243071331213085,
      "grad_norm": 6.831049919128418,
      "learning_rate": 0.0008316801464977873,
      "loss": 0.9887,
      "step": 1154
    },
    {
      "epoch": 0.5247614720581554,
      "grad_norm": 6.707858085632324,
      "learning_rate": 0.0008315275446360446,
      "loss": 2.1175,
      "step": 1155
    },
    {
      "epoch": 0.5252158109950023,
      "grad_norm": 8.513182640075684,
      "learning_rate": 0.0008313749427743018,
      "loss": 0.6443,
      "step": 1156
    },
    {
      "epoch": 0.5256701499318491,
      "grad_norm": 7.185368061065674,
      "learning_rate": 0.0008312223409125591,
      "loss": 1.3889,
      "step": 1157
    },
    {
      "epoch": 0.526124488868696,
      "grad_norm": 5.176680088043213,
      "learning_rate": 0.0008310697390508163,
      "loss": 1.0179,
      "step": 1158
    },
    {
      "epoch": 0.5265788278055429,
      "grad_norm": 4.861725807189941,
      "learning_rate": 0.0008309171371890737,
      "loss": 0.4887,
      "step": 1159
    },
    {
      "epoch": 0.5270331667423899,
      "grad_norm": 7.278319835662842,
      "learning_rate": 0.000830764535327331,
      "loss": 1.2909,
      "step": 1160
    },
    {
      "epoch": 0.5274875056792367,
      "grad_norm": 17.27720832824707,
      "learning_rate": 0.0008306119334655882,
      "loss": 1.8176,
      "step": 1161
    },
    {
      "epoch": 0.5279418446160836,
      "grad_norm": 5.87204122543335,
      "learning_rate": 0.0008304593316038456,
      "loss": 1.4236,
      "step": 1162
    },
    {
      "epoch": 0.5283961835529305,
      "grad_norm": 6.304206848144531,
      "learning_rate": 0.0008303067297421028,
      "loss": 1.7068,
      "step": 1163
    },
    {
      "epoch": 0.5288505224897774,
      "grad_norm": 4.693657875061035,
      "learning_rate": 0.0008301541278803601,
      "loss": 0.6465,
      "step": 1164
    },
    {
      "epoch": 0.5293048614266243,
      "grad_norm": 6.963006496429443,
      "learning_rate": 0.0008300015260186175,
      "loss": 2.3457,
      "step": 1165
    },
    {
      "epoch": 0.5297592003634711,
      "grad_norm": 6.1390461921691895,
      "learning_rate": 0.0008298489241568747,
      "loss": 1.7583,
      "step": 1166
    },
    {
      "epoch": 0.530213539300318,
      "grad_norm": 5.272864818572998,
      "learning_rate": 0.000829696322295132,
      "loss": 0.7739,
      "step": 1167
    },
    {
      "epoch": 0.5306678782371649,
      "grad_norm": 4.5414347648620605,
      "learning_rate": 0.0008295437204333893,
      "loss": 1.3345,
      "step": 1168
    },
    {
      "epoch": 0.5311222171740119,
      "grad_norm": 9.698951721191406,
      "learning_rate": 0.0008293911185716466,
      "loss": 1.9161,
      "step": 1169
    },
    {
      "epoch": 0.5315765561108587,
      "grad_norm": 7.420163631439209,
      "learning_rate": 0.0008292385167099039,
      "loss": 1.7181,
      "step": 1170
    },
    {
      "epoch": 0.5320308950477056,
      "grad_norm": 9.188798904418945,
      "learning_rate": 0.0008290859148481612,
      "loss": 0.7597,
      "step": 1171
    },
    {
      "epoch": 0.5324852339845525,
      "grad_norm": 4.536269664764404,
      "learning_rate": 0.0008289333129864185,
      "loss": 0.4638,
      "step": 1172
    },
    {
      "epoch": 0.5329395729213994,
      "grad_norm": 4.662639617919922,
      "learning_rate": 0.0008287807111246757,
      "loss": 1.0449,
      "step": 1173
    },
    {
      "epoch": 0.5333939118582463,
      "grad_norm": 6.209084510803223,
      "learning_rate": 0.000828628109262933,
      "loss": 0.8974,
      "step": 1174
    },
    {
      "epoch": 0.5338482507950931,
      "grad_norm": 4.942394733428955,
      "learning_rate": 0.0008284755074011902,
      "loss": 1.1607,
      "step": 1175
    },
    {
      "epoch": 0.53430258973194,
      "grad_norm": 6.583189487457275,
      "learning_rate": 0.0008283229055394475,
      "loss": 1.5189,
      "step": 1176
    },
    {
      "epoch": 0.5347569286687869,
      "grad_norm": 4.831203937530518,
      "learning_rate": 0.0008281703036777049,
      "loss": 1.679,
      "step": 1177
    },
    {
      "epoch": 0.5352112676056338,
      "grad_norm": 13.487860679626465,
      "learning_rate": 0.0008280177018159621,
      "loss": 1.8475,
      "step": 1178
    },
    {
      "epoch": 0.5356656065424807,
      "grad_norm": 6.313546180725098,
      "learning_rate": 0.0008278650999542194,
      "loss": 1.3374,
      "step": 1179
    },
    {
      "epoch": 0.5361199454793276,
      "grad_norm": 4.745636940002441,
      "learning_rate": 0.0008277124980924767,
      "loss": 1.2737,
      "step": 1180
    },
    {
      "epoch": 0.5365742844161745,
      "grad_norm": 6.67850399017334,
      "learning_rate": 0.000827559896230734,
      "loss": 0.9239,
      "step": 1181
    },
    {
      "epoch": 0.5370286233530214,
      "grad_norm": 6.6718034744262695,
      "learning_rate": 0.0008274072943689913,
      "loss": 1.4754,
      "step": 1182
    },
    {
      "epoch": 0.5374829622898682,
      "grad_norm": 6.956282138824463,
      "learning_rate": 0.0008272546925072486,
      "loss": 2.2666,
      "step": 1183
    },
    {
      "epoch": 0.5379373012267151,
      "grad_norm": 8.297225952148438,
      "learning_rate": 0.0008271020906455059,
      "loss": 1.5886,
      "step": 1184
    },
    {
      "epoch": 0.538391640163562,
      "grad_norm": 9.301739692687988,
      "learning_rate": 0.0008269494887837632,
      "loss": 0.9983,
      "step": 1185
    },
    {
      "epoch": 0.5388459791004089,
      "grad_norm": 6.073221683502197,
      "learning_rate": 0.0008267968869220205,
      "loss": 1.1862,
      "step": 1186
    },
    {
      "epoch": 0.5393003180372558,
      "grad_norm": 2.7531702518463135,
      "learning_rate": 0.0008266442850602778,
      "loss": 0.3537,
      "step": 1187
    },
    {
      "epoch": 0.5397546569741026,
      "grad_norm": 4.274366855621338,
      "learning_rate": 0.0008264916831985351,
      "loss": 0.7858,
      "step": 1188
    },
    {
      "epoch": 0.5402089959109496,
      "grad_norm": 6.951312065124512,
      "learning_rate": 0.0008263390813367924,
      "loss": 1.4619,
      "step": 1189
    },
    {
      "epoch": 0.5406633348477965,
      "grad_norm": 6.464057922363281,
      "learning_rate": 0.0008261864794750496,
      "loss": 0.9225,
      "step": 1190
    },
    {
      "epoch": 0.5411176737846434,
      "grad_norm": 6.679666996002197,
      "learning_rate": 0.000826033877613307,
      "loss": 0.9605,
      "step": 1191
    },
    {
      "epoch": 0.5415720127214902,
      "grad_norm": 4.815444469451904,
      "learning_rate": 0.0008258812757515642,
      "loss": 0.8086,
      "step": 1192
    },
    {
      "epoch": 0.5420263516583371,
      "grad_norm": 6.160940170288086,
      "learning_rate": 0.0008257286738898214,
      "loss": 2.3253,
      "step": 1193
    },
    {
      "epoch": 0.542480690595184,
      "grad_norm": 3.3507702350616455,
      "learning_rate": 0.0008255760720280788,
      "loss": 0.2128,
      "step": 1194
    },
    {
      "epoch": 0.5429350295320309,
      "grad_norm": 3.254721164703369,
      "learning_rate": 0.000825423470166336,
      "loss": 0.5995,
      "step": 1195
    },
    {
      "epoch": 0.5433893684688778,
      "grad_norm": 6.513477802276611,
      "learning_rate": 0.0008252708683045933,
      "loss": 1.3237,
      "step": 1196
    },
    {
      "epoch": 0.5438437074057246,
      "grad_norm": 3.6162710189819336,
      "learning_rate": 0.0008251182664428506,
      "loss": 0.7148,
      "step": 1197
    },
    {
      "epoch": 0.5442980463425715,
      "grad_norm": 5.494419097900391,
      "learning_rate": 0.0008249656645811079,
      "loss": 0.5587,
      "step": 1198
    },
    {
      "epoch": 0.5447523852794185,
      "grad_norm": 3.4626121520996094,
      "learning_rate": 0.0008248130627193652,
      "loss": 0.2397,
      "step": 1199
    },
    {
      "epoch": 0.5452067242162654,
      "grad_norm": 5.082940101623535,
      "learning_rate": 0.0008246604608576225,
      "loss": 0.6891,
      "step": 1200
    },
    {
      "epoch": 0.5456610631531122,
      "grad_norm": 3.9760026931762695,
      "learning_rate": 0.0008245078589958798,
      "loss": 0.7846,
      "step": 1201
    },
    {
      "epoch": 0.5461154020899591,
      "grad_norm": 5.9122490882873535,
      "learning_rate": 0.000824355257134137,
      "loss": 0.5325,
      "step": 1202
    },
    {
      "epoch": 0.546569741026806,
      "grad_norm": 5.3019208908081055,
      "learning_rate": 0.0008242026552723944,
      "loss": 1.1994,
      "step": 1203
    },
    {
      "epoch": 0.5470240799636529,
      "grad_norm": 5.0390944480896,
      "learning_rate": 0.0008240500534106517,
      "loss": 0.8099,
      "step": 1204
    },
    {
      "epoch": 0.5474784189004998,
      "grad_norm": 5.425317287445068,
      "learning_rate": 0.0008238974515489089,
      "loss": 1.157,
      "step": 1205
    },
    {
      "epoch": 0.5479327578373466,
      "grad_norm": 6.418102264404297,
      "learning_rate": 0.0008237448496871663,
      "loss": 1.3961,
      "step": 1206
    },
    {
      "epoch": 0.5483870967741935,
      "grad_norm": 9.10991096496582,
      "learning_rate": 0.0008235922478254235,
      "loss": 1.5522,
      "step": 1207
    },
    {
      "epoch": 0.5488414357110404,
      "grad_norm": 5.911365032196045,
      "learning_rate": 0.0008234396459636808,
      "loss": 1.1203,
      "step": 1208
    },
    {
      "epoch": 0.5492957746478874,
      "grad_norm": 8.328518867492676,
      "learning_rate": 0.0008232870441019382,
      "loss": 1.8095,
      "step": 1209
    },
    {
      "epoch": 0.5497501135847342,
      "grad_norm": 7.175042152404785,
      "learning_rate": 0.0008231344422401954,
      "loss": 1.5219,
      "step": 1210
    },
    {
      "epoch": 0.5502044525215811,
      "grad_norm": 6.133046627044678,
      "learning_rate": 0.0008229818403784526,
      "loss": 0.9384,
      "step": 1211
    },
    {
      "epoch": 0.550658791458428,
      "grad_norm": 5.517056941986084,
      "learning_rate": 0.0008228292385167099,
      "loss": 0.8382,
      "step": 1212
    },
    {
      "epoch": 0.5511131303952749,
      "grad_norm": 7.617383003234863,
      "learning_rate": 0.0008226766366549672,
      "loss": 1.714,
      "step": 1213
    },
    {
      "epoch": 0.5515674693321218,
      "grad_norm": 9.26024055480957,
      "learning_rate": 0.0008225240347932244,
      "loss": 2.0166,
      "step": 1214
    },
    {
      "epoch": 0.5520218082689686,
      "grad_norm": 6.745128631591797,
      "learning_rate": 0.0008223714329314818,
      "loss": 1.2403,
      "step": 1215
    },
    {
      "epoch": 0.5524761472058155,
      "grad_norm": 4.949755668640137,
      "learning_rate": 0.0008222188310697391,
      "loss": 0.8338,
      "step": 1216
    },
    {
      "epoch": 0.5529304861426624,
      "grad_norm": 3.5189788341522217,
      "learning_rate": 0.0008220662292079963,
      "loss": 0.5103,
      "step": 1217
    },
    {
      "epoch": 0.5533848250795094,
      "grad_norm": 5.342153549194336,
      "learning_rate": 0.0008219136273462537,
      "loss": 0.9218,
      "step": 1218
    },
    {
      "epoch": 0.5538391640163562,
      "grad_norm": 13.7302827835083,
      "learning_rate": 0.0008217610254845109,
      "loss": 1.039,
      "step": 1219
    },
    {
      "epoch": 0.5542935029532031,
      "grad_norm": 4.3553996086120605,
      "learning_rate": 0.0008216084236227682,
      "loss": 0.5273,
      "step": 1220
    },
    {
      "epoch": 0.55474784189005,
      "grad_norm": 7.788926124572754,
      "learning_rate": 0.0008214558217610256,
      "loss": 1.5554,
      "step": 1221
    },
    {
      "epoch": 0.5552021808268969,
      "grad_norm": 5.65952730178833,
      "learning_rate": 0.0008213032198992828,
      "loss": 1.0313,
      "step": 1222
    },
    {
      "epoch": 0.5556565197637438,
      "grad_norm": 6.534519672393799,
      "learning_rate": 0.0008211506180375401,
      "loss": 2.0169,
      "step": 1223
    },
    {
      "epoch": 0.5561108587005906,
      "grad_norm": 4.024695873260498,
      "learning_rate": 0.0008209980161757974,
      "loss": 0.7382,
      "step": 1224
    },
    {
      "epoch": 0.5565651976374375,
      "grad_norm": 3.806488513946533,
      "learning_rate": 0.0008208454143140547,
      "loss": 0.8968,
      "step": 1225
    },
    {
      "epoch": 0.5570195365742844,
      "grad_norm": 6.14376974105835,
      "learning_rate": 0.000820692812452312,
      "loss": 0.9687,
      "step": 1226
    },
    {
      "epoch": 0.5574738755111313,
      "grad_norm": 7.916499614715576,
      "learning_rate": 0.0008205402105905693,
      "loss": 1.6188,
      "step": 1227
    },
    {
      "epoch": 0.5579282144479782,
      "grad_norm": 7.786999702453613,
      "learning_rate": 0.0008203876087288266,
      "loss": 0.6235,
      "step": 1228
    },
    {
      "epoch": 0.5583825533848251,
      "grad_norm": 5.394393444061279,
      "learning_rate": 0.0008202350068670837,
      "loss": 0.9468,
      "step": 1229
    },
    {
      "epoch": 0.558836892321672,
      "grad_norm": 3.7229905128479004,
      "learning_rate": 0.0008200824050053411,
      "loss": 0.7669,
      "step": 1230
    },
    {
      "epoch": 0.5592912312585189,
      "grad_norm": 7.466763973236084,
      "learning_rate": 0.0008199298031435983,
      "loss": 1.4316,
      "step": 1231
    },
    {
      "epoch": 0.5597455701953657,
      "grad_norm": 6.7799601554870605,
      "learning_rate": 0.0008197772012818556,
      "loss": 1.9075,
      "step": 1232
    },
    {
      "epoch": 0.5601999091322126,
      "grad_norm": 4.002566337585449,
      "learning_rate": 0.000819624599420113,
      "loss": 0.9072,
      "step": 1233
    },
    {
      "epoch": 0.5606542480690595,
      "grad_norm": 5.971920967102051,
      "learning_rate": 0.0008194719975583702,
      "loss": 0.5071,
      "step": 1234
    },
    {
      "epoch": 0.5611085870059064,
      "grad_norm": 6.923343181610107,
      "learning_rate": 0.0008193193956966275,
      "loss": 0.8392,
      "step": 1235
    },
    {
      "epoch": 0.5615629259427533,
      "grad_norm": 7.6955885887146,
      "learning_rate": 0.0008191667938348848,
      "loss": 1.3326,
      "step": 1236
    },
    {
      "epoch": 0.5620172648796001,
      "grad_norm": 4.405864715576172,
      "learning_rate": 0.0008190141919731421,
      "loss": 1.1797,
      "step": 1237
    },
    {
      "epoch": 0.5624716038164471,
      "grad_norm": 5.175873756408691,
      "learning_rate": 0.0008188615901113994,
      "loss": 0.875,
      "step": 1238
    },
    {
      "epoch": 0.562925942753294,
      "grad_norm": 6.434118747711182,
      "learning_rate": 0.0008187089882496567,
      "loss": 0.9034,
      "step": 1239
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 3.1608283519744873,
      "learning_rate": 0.000818556386387914,
      "loss": 0.4508,
      "step": 1240
    },
    {
      "epoch": 0.5638346206269877,
      "grad_norm": 6.271414756774902,
      "learning_rate": 0.0008184037845261712,
      "loss": 1.4124,
      "step": 1241
    },
    {
      "epoch": 0.5642889595638346,
      "grad_norm": 5.498674392700195,
      "learning_rate": 0.0008182511826644286,
      "loss": 1.3382,
      "step": 1242
    },
    {
      "epoch": 0.5647432985006815,
      "grad_norm": 6.5768351554870605,
      "learning_rate": 0.0008180985808026859,
      "loss": 1.266,
      "step": 1243
    },
    {
      "epoch": 0.5651976374375284,
      "grad_norm": 6.520320415496826,
      "learning_rate": 0.0008179459789409431,
      "loss": 0.9217,
      "step": 1244
    },
    {
      "epoch": 0.5656519763743753,
      "grad_norm": 4.976265907287598,
      "learning_rate": 0.0008177933770792005,
      "loss": 1.1161,
      "step": 1245
    },
    {
      "epoch": 0.5661063153112221,
      "grad_norm": 6.112398147583008,
      "learning_rate": 0.0008176407752174577,
      "loss": 1.4038,
      "step": 1246
    },
    {
      "epoch": 0.566560654248069,
      "grad_norm": 8.129387855529785,
      "learning_rate": 0.0008174881733557149,
      "loss": 1.5316,
      "step": 1247
    },
    {
      "epoch": 0.567014993184916,
      "grad_norm": 5.897031784057617,
      "learning_rate": 0.0008173355714939722,
      "loss": 1.1407,
      "step": 1248
    },
    {
      "epoch": 0.5674693321217629,
      "grad_norm": 5.369862079620361,
      "learning_rate": 0.0008171829696322295,
      "loss": 0.9827,
      "step": 1249
    },
    {
      "epoch": 0.5679236710586097,
      "grad_norm": 6.0362958908081055,
      "learning_rate": 0.0008170303677704868,
      "loss": 0.3878,
      "step": 1250
    },
    {
      "epoch": 0.5683780099954566,
      "grad_norm": 3.661496877670288,
      "learning_rate": 0.0008168777659087441,
      "loss": 0.9978,
      "step": 1251
    },
    {
      "epoch": 0.5688323489323035,
      "grad_norm": 5.139003276824951,
      "learning_rate": 0.0008167251640470014,
      "loss": 0.6848,
      "step": 1252
    },
    {
      "epoch": 0.5692866878691504,
      "grad_norm": 7.862900733947754,
      "learning_rate": 0.0008165725621852586,
      "loss": 1.0405,
      "step": 1253
    },
    {
      "epoch": 0.5697410268059973,
      "grad_norm": 16.890684127807617,
      "learning_rate": 0.000816419960323516,
      "loss": 1.089,
      "step": 1254
    },
    {
      "epoch": 0.5701953657428441,
      "grad_norm": 7.233978271484375,
      "learning_rate": 0.0008162673584617733,
      "loss": 1.4882,
      "step": 1255
    },
    {
      "epoch": 0.570649704679691,
      "grad_norm": 6.825947284698486,
      "learning_rate": 0.0008161147566000305,
      "loss": 1.0065,
      "step": 1256
    },
    {
      "epoch": 0.5711040436165379,
      "grad_norm": 6.668953895568848,
      "learning_rate": 0.0008159621547382879,
      "loss": 1.0159,
      "step": 1257
    },
    {
      "epoch": 0.5715583825533849,
      "grad_norm": 7.447165012359619,
      "learning_rate": 0.0008158095528765451,
      "loss": 1.0214,
      "step": 1258
    },
    {
      "epoch": 0.5720127214902317,
      "grad_norm": 7.2910990715026855,
      "learning_rate": 0.0008156569510148024,
      "loss": 0.8842,
      "step": 1259
    },
    {
      "epoch": 0.5724670604270786,
      "grad_norm": 5.445218086242676,
      "learning_rate": 0.0008155043491530598,
      "loss": 0.7926,
      "step": 1260
    },
    {
      "epoch": 0.5729213993639255,
      "grad_norm": 7.814641952514648,
      "learning_rate": 0.000815351747291317,
      "loss": 1.1274,
      "step": 1261
    },
    {
      "epoch": 0.5733757383007724,
      "grad_norm": 7.4753875732421875,
      "learning_rate": 0.0008151991454295743,
      "loss": 1.5286,
      "step": 1262
    },
    {
      "epoch": 0.5738300772376193,
      "grad_norm": 5.316451549530029,
      "learning_rate": 0.0008150465435678316,
      "loss": 1.1217,
      "step": 1263
    },
    {
      "epoch": 0.5742844161744661,
      "grad_norm": 5.877849578857422,
      "learning_rate": 0.0008148939417060889,
      "loss": 1.0975,
      "step": 1264
    },
    {
      "epoch": 0.574738755111313,
      "grad_norm": 7.696946144104004,
      "learning_rate": 0.000814741339844346,
      "loss": 1.7816,
      "step": 1265
    },
    {
      "epoch": 0.5751930940481599,
      "grad_norm": 2.7154650688171387,
      "learning_rate": 0.0008145887379826034,
      "loss": 0.3643,
      "step": 1266
    },
    {
      "epoch": 0.5756474329850069,
      "grad_norm": 5.1808977127075195,
      "learning_rate": 0.0008144361361208607,
      "loss": 1.1688,
      "step": 1267
    },
    {
      "epoch": 0.5761017719218537,
      "grad_norm": 5.9901957511901855,
      "learning_rate": 0.0008142835342591179,
      "loss": 1.2481,
      "step": 1268
    },
    {
      "epoch": 0.5765561108587006,
      "grad_norm": 5.425498008728027,
      "learning_rate": 0.0008141309323973753,
      "loss": 1.3147,
      "step": 1269
    },
    {
      "epoch": 0.5770104497955475,
      "grad_norm": 6.687105178833008,
      "learning_rate": 0.0008139783305356325,
      "loss": 1.4879,
      "step": 1270
    },
    {
      "epoch": 0.5774647887323944,
      "grad_norm": 4.831216335296631,
      "learning_rate": 0.0008138257286738898,
      "loss": 0.8243,
      "step": 1271
    },
    {
      "epoch": 0.5779191276692413,
      "grad_norm": 6.691722869873047,
      "learning_rate": 0.0008136731268121472,
      "loss": 1.2698,
      "step": 1272
    },
    {
      "epoch": 0.5783734666060881,
      "grad_norm": 135.3623809814453,
      "learning_rate": 0.0008135205249504044,
      "loss": 1.2937,
      "step": 1273
    },
    {
      "epoch": 0.578827805542935,
      "grad_norm": 7.011422634124756,
      "learning_rate": 0.0008133679230886617,
      "loss": 1.8289,
      "step": 1274
    },
    {
      "epoch": 0.5792821444797819,
      "grad_norm": 9.466841697692871,
      "learning_rate": 0.000813215321226919,
      "loss": 0.6793,
      "step": 1275
    },
    {
      "epoch": 0.5797364834166288,
      "grad_norm": 8.610464096069336,
      "learning_rate": 0.0008130627193651763,
      "loss": 1.5084,
      "step": 1276
    },
    {
      "epoch": 0.5801908223534757,
      "grad_norm": 5.0471625328063965,
      "learning_rate": 0.0008129101175034335,
      "loss": 0.5818,
      "step": 1277
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 4.7672271728515625,
      "learning_rate": 0.0008127575156416909,
      "loss": 0.9364,
      "step": 1278
    },
    {
      "epoch": 0.5810995002271695,
      "grad_norm": 3.6336934566497803,
      "learning_rate": 0.0008126049137799482,
      "loss": 0.556,
      "step": 1279
    },
    {
      "epoch": 0.5815538391640164,
      "grad_norm": 5.421381950378418,
      "learning_rate": 0.0008124523119182054,
      "loss": 1.2465,
      "step": 1280
    },
    {
      "epoch": 0.5820081781008632,
      "grad_norm": 7.01259183883667,
      "learning_rate": 0.0008122997100564628,
      "loss": 1.1732,
      "step": 1281
    },
    {
      "epoch": 0.5824625170377101,
      "grad_norm": 6.30565071105957,
      "learning_rate": 0.00081214710819472,
      "loss": 0.8785,
      "step": 1282
    },
    {
      "epoch": 0.582916855974557,
      "grad_norm": 6.582678318023682,
      "learning_rate": 0.0008119945063329773,
      "loss": 1.1427,
      "step": 1283
    },
    {
      "epoch": 0.5833711949114039,
      "grad_norm": 6.8486175537109375,
      "learning_rate": 0.0008118419044712346,
      "loss": 1.0795,
      "step": 1284
    },
    {
      "epoch": 0.5838255338482508,
      "grad_norm": 6.883326530456543,
      "learning_rate": 0.0008116893026094918,
      "loss": 1.7116,
      "step": 1285
    },
    {
      "epoch": 0.5842798727850976,
      "grad_norm": 7.0595622062683105,
      "learning_rate": 0.0008115367007477491,
      "loss": 1.7128,
      "step": 1286
    },
    {
      "epoch": 0.5847342117219446,
      "grad_norm": 6.186928749084473,
      "learning_rate": 0.0008113840988860064,
      "loss": 1.3797,
      "step": 1287
    },
    {
      "epoch": 0.5851885506587915,
      "grad_norm": 6.214039325714111,
      "learning_rate": 0.0008112314970242637,
      "loss": 1.3521,
      "step": 1288
    },
    {
      "epoch": 0.5856428895956384,
      "grad_norm": 5.564186096191406,
      "learning_rate": 0.000811078895162521,
      "loss": 0.8776,
      "step": 1289
    },
    {
      "epoch": 0.5860972285324852,
      "grad_norm": 7.293272495269775,
      "learning_rate": 0.0008109262933007783,
      "loss": 1.5496,
      "step": 1290
    },
    {
      "epoch": 0.5865515674693321,
      "grad_norm": 5.1530070304870605,
      "learning_rate": 0.0008107736914390356,
      "loss": 0.6217,
      "step": 1291
    },
    {
      "epoch": 0.587005906406179,
      "grad_norm": 6.410775184631348,
      "learning_rate": 0.0008106210895772928,
      "loss": 1.0615,
      "step": 1292
    },
    {
      "epoch": 0.5874602453430259,
      "grad_norm": 6.223714351654053,
      "learning_rate": 0.0008104684877155502,
      "loss": 1.1997,
      "step": 1293
    },
    {
      "epoch": 0.5879145842798728,
      "grad_norm": 6.520752906799316,
      "learning_rate": 0.0008103158858538074,
      "loss": 1.5266,
      "step": 1294
    },
    {
      "epoch": 0.5883689232167196,
      "grad_norm": 6.416818141937256,
      "learning_rate": 0.0008101632839920647,
      "loss": 1.4006,
      "step": 1295
    },
    {
      "epoch": 0.5888232621535665,
      "grad_norm": 3.686058282852173,
      "learning_rate": 0.0008100106821303221,
      "loss": 0.6443,
      "step": 1296
    },
    {
      "epoch": 0.5892776010904135,
      "grad_norm": 5.8341875076293945,
      "learning_rate": 0.0008098580802685793,
      "loss": 1.3254,
      "step": 1297
    },
    {
      "epoch": 0.5897319400272604,
      "grad_norm": 5.7881364822387695,
      "learning_rate": 0.0008097054784068366,
      "loss": 0.9228,
      "step": 1298
    },
    {
      "epoch": 0.5901862789641072,
      "grad_norm": 4.518800258636475,
      "learning_rate": 0.0008095528765450939,
      "loss": 0.5747,
      "step": 1299
    },
    {
      "epoch": 0.5906406179009541,
      "grad_norm": 6.2555012702941895,
      "learning_rate": 0.0008094002746833512,
      "loss": 1.6169,
      "step": 1300
    },
    {
      "epoch": 0.591094956837801,
      "grad_norm": 2.8178484439849854,
      "learning_rate": 0.0008092476728216085,
      "loss": 0.5721,
      "step": 1301
    },
    {
      "epoch": 0.5915492957746479,
      "grad_norm": 7.749699115753174,
      "learning_rate": 0.0008090950709598657,
      "loss": 2.427,
      "step": 1302
    },
    {
      "epoch": 0.5920036347114948,
      "grad_norm": 9.012787818908691,
      "learning_rate": 0.000808942469098123,
      "loss": 1.473,
      "step": 1303
    },
    {
      "epoch": 0.5924579736483416,
      "grad_norm": 5.610328674316406,
      "learning_rate": 0.0008087898672363802,
      "loss": 1.1548,
      "step": 1304
    },
    {
      "epoch": 0.5929123125851885,
      "grad_norm": 6.9034504890441895,
      "learning_rate": 0.0008086372653746376,
      "loss": 1.299,
      "step": 1305
    },
    {
      "epoch": 0.5933666515220355,
      "grad_norm": 5.955702781677246,
      "learning_rate": 0.0008084846635128948,
      "loss": 1.6329,
      "step": 1306
    },
    {
      "epoch": 0.5938209904588824,
      "grad_norm": 5.2477192878723145,
      "learning_rate": 0.0008083320616511521,
      "loss": 0.6427,
      "step": 1307
    },
    {
      "epoch": 0.5942753293957292,
      "grad_norm": 5.981815814971924,
      "learning_rate": 0.0008081794597894095,
      "loss": 1.4448,
      "step": 1308
    },
    {
      "epoch": 0.5947296683325761,
      "grad_norm": 4.528540134429932,
      "learning_rate": 0.0008080268579276667,
      "loss": 0.8158,
      "step": 1309
    },
    {
      "epoch": 0.595184007269423,
      "grad_norm": 5.4870076179504395,
      "learning_rate": 0.000807874256065924,
      "loss": 1.2084,
      "step": 1310
    },
    {
      "epoch": 0.5956383462062699,
      "grad_norm": 9.952052116394043,
      "learning_rate": 0.0008077216542041813,
      "loss": 1.7544,
      "step": 1311
    },
    {
      "epoch": 0.5960926851431168,
      "grad_norm": 7.670504570007324,
      "learning_rate": 0.0008075690523424386,
      "loss": 1.6137,
      "step": 1312
    },
    {
      "epoch": 0.5965470240799636,
      "grad_norm": 5.205831527709961,
      "learning_rate": 0.0008074164504806959,
      "loss": 1.0316,
      "step": 1313
    },
    {
      "epoch": 0.5970013630168105,
      "grad_norm": 8.450263977050781,
      "learning_rate": 0.0008072638486189532,
      "loss": 1.7017,
      "step": 1314
    },
    {
      "epoch": 0.5974557019536574,
      "grad_norm": 4.336009979248047,
      "learning_rate": 0.0008071112467572105,
      "loss": 0.8639,
      "step": 1315
    },
    {
      "epoch": 0.5979100408905044,
      "grad_norm": 7.516353607177734,
      "learning_rate": 0.0008069586448954677,
      "loss": 1.5313,
      "step": 1316
    },
    {
      "epoch": 0.5983643798273512,
      "grad_norm": 5.664214134216309,
      "learning_rate": 0.0008068060430337251,
      "loss": 1.4991,
      "step": 1317
    },
    {
      "epoch": 0.5988187187641981,
      "grad_norm": 7.5650811195373535,
      "learning_rate": 0.0008066534411719824,
      "loss": 0.856,
      "step": 1318
    },
    {
      "epoch": 0.599273057701045,
      "grad_norm": 4.182690143585205,
      "learning_rate": 0.0008065008393102396,
      "loss": 0.6543,
      "step": 1319
    },
    {
      "epoch": 0.5997273966378919,
      "grad_norm": 5.345774173736572,
      "learning_rate": 0.0008063482374484969,
      "loss": 1.8945,
      "step": 1320
    },
    {
      "epoch": 0.6001817355747388,
      "grad_norm": 4.078822612762451,
      "learning_rate": 0.0008061956355867541,
      "loss": 1.0504,
      "step": 1321
    },
    {
      "epoch": 0.6006360745115856,
      "grad_norm": 4.63715124130249,
      "learning_rate": 0.0008060430337250114,
      "loss": 1.111,
      "step": 1322
    },
    {
      "epoch": 0.6010904134484325,
      "grad_norm": 6.130924224853516,
      "learning_rate": 0.0008058904318632687,
      "loss": 1.5079,
      "step": 1323
    },
    {
      "epoch": 0.6015447523852794,
      "grad_norm": 6.413631916046143,
      "learning_rate": 0.000805737830001526,
      "loss": 2.005,
      "step": 1324
    },
    {
      "epoch": 0.6019990913221263,
      "grad_norm": 3.857496500015259,
      "learning_rate": 0.0008055852281397833,
      "loss": 0.6019,
      "step": 1325
    },
    {
      "epoch": 0.6024534302589732,
      "grad_norm": 6.786165714263916,
      "learning_rate": 0.0008054326262780406,
      "loss": 0.6747,
      "step": 1326
    },
    {
      "epoch": 0.6029077691958201,
      "grad_norm": 6.860152244567871,
      "learning_rate": 0.0008052800244162979,
      "loss": 1.1176,
      "step": 1327
    },
    {
      "epoch": 0.603362108132667,
      "grad_norm": 7.444455623626709,
      "learning_rate": 0.0008051274225545551,
      "loss": 1.9436,
      "step": 1328
    },
    {
      "epoch": 0.6038164470695139,
      "grad_norm": 7.655731678009033,
      "learning_rate": 0.0008049748206928125,
      "loss": 1.4623,
      "step": 1329
    },
    {
      "epoch": 0.6042707860063607,
      "grad_norm": 2.7612476348876953,
      "learning_rate": 0.0008048222188310698,
      "loss": 0.427,
      "step": 1330
    },
    {
      "epoch": 0.6047251249432076,
      "grad_norm": 6.44251012802124,
      "learning_rate": 0.000804669616969327,
      "loss": 1.3309,
      "step": 1331
    },
    {
      "epoch": 0.6051794638800545,
      "grad_norm": 5.426219940185547,
      "learning_rate": 0.0008045170151075844,
      "loss": 0.7048,
      "step": 1332
    },
    {
      "epoch": 0.6056338028169014,
      "grad_norm": 5.859844207763672,
      "learning_rate": 0.0008043644132458416,
      "loss": 1.1779,
      "step": 1333
    },
    {
      "epoch": 0.6060881417537483,
      "grad_norm": 4.717188835144043,
      "learning_rate": 0.0008042118113840989,
      "loss": 0.9108,
      "step": 1334
    },
    {
      "epoch": 0.6065424806905951,
      "grad_norm": 6.960063457489014,
      "learning_rate": 0.0008040592095223563,
      "loss": 0.4815,
      "step": 1335
    },
    {
      "epoch": 0.6069968196274421,
      "grad_norm": 5.074649333953857,
      "learning_rate": 0.0008039066076606135,
      "loss": 1.0845,
      "step": 1336
    },
    {
      "epoch": 0.607451158564289,
      "grad_norm": 8.223470687866211,
      "learning_rate": 0.0008037540057988708,
      "loss": 2.088,
      "step": 1337
    },
    {
      "epoch": 0.6079054975011359,
      "grad_norm": 5.972082138061523,
      "learning_rate": 0.000803601403937128,
      "loss": 1.0045,
      "step": 1338
    },
    {
      "epoch": 0.6083598364379827,
      "grad_norm": 5.990791320800781,
      "learning_rate": 0.0008034488020753853,
      "loss": 1.4569,
      "step": 1339
    },
    {
      "epoch": 0.6088141753748296,
      "grad_norm": 7.417426586151123,
      "learning_rate": 0.0008032962002136425,
      "loss": 0.794,
      "step": 1340
    },
    {
      "epoch": 0.6092685143116765,
      "grad_norm": 5.831076622009277,
      "learning_rate": 0.0008031435983518999,
      "loss": 1.2372,
      "step": 1341
    },
    {
      "epoch": 0.6097228532485234,
      "grad_norm": 5.57807731628418,
      "learning_rate": 0.0008029909964901572,
      "loss": 0.9567,
      "step": 1342
    },
    {
      "epoch": 0.6101771921853703,
      "grad_norm": 3.2722156047821045,
      "learning_rate": 0.0008028383946284144,
      "loss": 0.2963,
      "step": 1343
    },
    {
      "epoch": 0.6106315311222171,
      "grad_norm": 7.051586627960205,
      "learning_rate": 0.0008026857927666718,
      "loss": 1.602,
      "step": 1344
    },
    {
      "epoch": 0.611085870059064,
      "grad_norm": 6.264026641845703,
      "learning_rate": 0.000802533190904929,
      "loss": 1.048,
      "step": 1345
    },
    {
      "epoch": 0.611540208995911,
      "grad_norm": 5.663254737854004,
      "learning_rate": 0.0008023805890431863,
      "loss": 0.8167,
      "step": 1346
    },
    {
      "epoch": 0.6119945479327579,
      "grad_norm": 5.417767524719238,
      "learning_rate": 0.0008022279871814437,
      "loss": 1.258,
      "step": 1347
    },
    {
      "epoch": 0.6124488868696047,
      "grad_norm": 5.060525417327881,
      "learning_rate": 0.0008020753853197009,
      "loss": 0.7135,
      "step": 1348
    },
    {
      "epoch": 0.6129032258064516,
      "grad_norm": 9.488626480102539,
      "learning_rate": 0.0008019227834579582,
      "loss": 1.5387,
      "step": 1349
    },
    {
      "epoch": 0.6133575647432985,
      "grad_norm": 5.014365196228027,
      "learning_rate": 0.0008017701815962155,
      "loss": 0.8847,
      "step": 1350
    },
    {
      "epoch": 0.6138119036801454,
      "grad_norm": 5.703094005584717,
      "learning_rate": 0.0008016175797344728,
      "loss": 0.8571,
      "step": 1351
    },
    {
      "epoch": 0.6142662426169923,
      "grad_norm": 6.202407360076904,
      "learning_rate": 0.00080146497787273,
      "loss": 1.0533,
      "step": 1352
    },
    {
      "epoch": 0.6147205815538391,
      "grad_norm": 3.1154391765594482,
      "learning_rate": 0.0008013123760109874,
      "loss": 0.3302,
      "step": 1353
    },
    {
      "epoch": 0.615174920490686,
      "grad_norm": 9.458215713500977,
      "learning_rate": 0.0008011597741492447,
      "loss": 0.9946,
      "step": 1354
    },
    {
      "epoch": 0.615629259427533,
      "grad_norm": 5.511579513549805,
      "learning_rate": 0.0008010071722875019,
      "loss": 0.5526,
      "step": 1355
    },
    {
      "epoch": 0.6160835983643799,
      "grad_norm": 6.057048320770264,
      "learning_rate": 0.0008008545704257593,
      "loss": 0.4071,
      "step": 1356
    },
    {
      "epoch": 0.6165379373012267,
      "grad_norm": 4.5907769203186035,
      "learning_rate": 0.0008007019685640164,
      "loss": 1.2564,
      "step": 1357
    },
    {
      "epoch": 0.6169922762380736,
      "grad_norm": 5.483401775360107,
      "learning_rate": 0.0008005493667022737,
      "loss": 1.6652,
      "step": 1358
    },
    {
      "epoch": 0.6174466151749205,
      "grad_norm": 6.2452392578125,
      "learning_rate": 0.0008003967648405311,
      "loss": 0.9617,
      "step": 1359
    },
    {
      "epoch": 0.6179009541117674,
      "grad_norm": 4.959096431732178,
      "learning_rate": 0.0008002441629787883,
      "loss": 0.8546,
      "step": 1360
    },
    {
      "epoch": 0.6183552930486143,
      "grad_norm": 5.823307991027832,
      "learning_rate": 0.0008000915611170456,
      "loss": 1.0947,
      "step": 1361
    },
    {
      "epoch": 0.6188096319854611,
      "grad_norm": 6.209987163543701,
      "learning_rate": 0.0007999389592553029,
      "loss": 0.8287,
      "step": 1362
    },
    {
      "epoch": 0.619263970922308,
      "grad_norm": 5.136107444763184,
      "learning_rate": 0.0007997863573935602,
      "loss": 1.1176,
      "step": 1363
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 6.723048210144043,
      "learning_rate": 0.0007996337555318175,
      "loss": 1.6752,
      "step": 1364
    },
    {
      "epoch": 0.6201726487960019,
      "grad_norm": 5.681275844573975,
      "learning_rate": 0.0007994811536700748,
      "loss": 1.1357,
      "step": 1365
    },
    {
      "epoch": 0.6206269877328487,
      "grad_norm": 6.596471309661865,
      "learning_rate": 0.0007993285518083321,
      "loss": 1.057,
      "step": 1366
    },
    {
      "epoch": 0.6210813266696956,
      "grad_norm": 3.7734224796295166,
      "learning_rate": 0.0007991759499465893,
      "loss": 0.5056,
      "step": 1367
    },
    {
      "epoch": 0.6215356656065425,
      "grad_norm": 7.0453104972839355,
      "learning_rate": 0.0007990233480848467,
      "loss": 1.1784,
      "step": 1368
    },
    {
      "epoch": 0.6219900045433894,
      "grad_norm": 4.920443058013916,
      "learning_rate": 0.000798870746223104,
      "loss": 1.3006,
      "step": 1369
    },
    {
      "epoch": 0.6224443434802363,
      "grad_norm": 6.466322898864746,
      "learning_rate": 0.0007987181443613612,
      "loss": 1.2372,
      "step": 1370
    },
    {
      "epoch": 0.6228986824170831,
      "grad_norm": 7.534252643585205,
      "learning_rate": 0.0007985655424996186,
      "loss": 1.8052,
      "step": 1371
    },
    {
      "epoch": 0.62335302135393,
      "grad_norm": 7.078782081604004,
      "learning_rate": 0.0007984129406378758,
      "loss": 1.6658,
      "step": 1372
    },
    {
      "epoch": 0.6238073602907769,
      "grad_norm": 9.594943046569824,
      "learning_rate": 0.0007982603387761331,
      "loss": 1.2632,
      "step": 1373
    },
    {
      "epoch": 0.6242616992276238,
      "grad_norm": 7.196069717407227,
      "learning_rate": 0.0007981077369143905,
      "loss": 0.8855,
      "step": 1374
    },
    {
      "epoch": 0.6247160381644707,
      "grad_norm": 5.710907459259033,
      "learning_rate": 0.0007979551350526476,
      "loss": 0.594,
      "step": 1375
    },
    {
      "epoch": 0.6251703771013176,
      "grad_norm": 4.289903163909912,
      "learning_rate": 0.0007978025331909049,
      "loss": 1.0955,
      "step": 1376
    },
    {
      "epoch": 0.6256247160381645,
      "grad_norm": 5.825680732727051,
      "learning_rate": 0.0007976499313291622,
      "loss": 0.5891,
      "step": 1377
    },
    {
      "epoch": 0.6260790549750114,
      "grad_norm": 3.9199907779693604,
      "learning_rate": 0.0007974973294674195,
      "loss": 0.7235,
      "step": 1378
    },
    {
      "epoch": 0.6265333939118582,
      "grad_norm": 3.767669916152954,
      "learning_rate": 0.0007973447276056767,
      "loss": 0.3258,
      "step": 1379
    },
    {
      "epoch": 0.6269877328487051,
      "grad_norm": 4.285425186157227,
      "learning_rate": 0.0007971921257439341,
      "loss": 0.548,
      "step": 1380
    },
    {
      "epoch": 0.627442071785552,
      "grad_norm": 3.5331737995147705,
      "learning_rate": 0.0007970395238821914,
      "loss": 0.4491,
      "step": 1381
    },
    {
      "epoch": 0.6278964107223989,
      "grad_norm": 6.446519374847412,
      "learning_rate": 0.0007968869220204486,
      "loss": 1.3006,
      "step": 1382
    },
    {
      "epoch": 0.6283507496592458,
      "grad_norm": 8.172139167785645,
      "learning_rate": 0.000796734320158706,
      "loss": 1.771,
      "step": 1383
    },
    {
      "epoch": 0.6288050885960926,
      "grad_norm": 4.228353023529053,
      "learning_rate": 0.0007965817182969632,
      "loss": 0.6265,
      "step": 1384
    },
    {
      "epoch": 0.6292594275329396,
      "grad_norm": 7.055757999420166,
      "learning_rate": 0.0007964291164352205,
      "loss": 1.4252,
      "step": 1385
    },
    {
      "epoch": 0.6297137664697865,
      "grad_norm": 5.32549524307251,
      "learning_rate": 0.0007962765145734779,
      "loss": 1.22,
      "step": 1386
    },
    {
      "epoch": 0.6301681054066334,
      "grad_norm": 5.628271102905273,
      "learning_rate": 0.0007961239127117351,
      "loss": 1.0192,
      "step": 1387
    },
    {
      "epoch": 0.6306224443434802,
      "grad_norm": 5.718201637268066,
      "learning_rate": 0.0007959713108499924,
      "loss": 1.6616,
      "step": 1388
    },
    {
      "epoch": 0.6310767832803271,
      "grad_norm": 3.283595561981201,
      "learning_rate": 0.0007958187089882497,
      "loss": 0.3505,
      "step": 1389
    },
    {
      "epoch": 0.631531122217174,
      "grad_norm": 6.083111763000488,
      "learning_rate": 0.000795666107126507,
      "loss": 1.1207,
      "step": 1390
    },
    {
      "epoch": 0.6319854611540209,
      "grad_norm": 5.485536098480225,
      "learning_rate": 0.0007955135052647642,
      "loss": 1.0057,
      "step": 1391
    },
    {
      "epoch": 0.6324398000908678,
      "grad_norm": 4.74648380279541,
      "learning_rate": 0.0007953609034030216,
      "loss": 0.8247,
      "step": 1392
    },
    {
      "epoch": 0.6328941390277146,
      "grad_norm": 8.810240745544434,
      "learning_rate": 0.0007952083015412788,
      "loss": 1.7624,
      "step": 1393
    },
    {
      "epoch": 0.6333484779645616,
      "grad_norm": 6.062629699707031,
      "learning_rate": 0.000795055699679536,
      "loss": 2.0413,
      "step": 1394
    },
    {
      "epoch": 0.6338028169014085,
      "grad_norm": 5.3618855476379395,
      "learning_rate": 0.0007949030978177934,
      "loss": 1.0277,
      "step": 1395
    },
    {
      "epoch": 0.6342571558382554,
      "grad_norm": 5.8260393142700195,
      "learning_rate": 0.0007947504959560506,
      "loss": 1.1445,
      "step": 1396
    },
    {
      "epoch": 0.6347114947751022,
      "grad_norm": 6.546052932739258,
      "learning_rate": 0.0007945978940943079,
      "loss": 1.2671,
      "step": 1397
    },
    {
      "epoch": 0.6351658337119491,
      "grad_norm": 3.6654515266418457,
      "learning_rate": 0.0007944452922325653,
      "loss": 0.4309,
      "step": 1398
    },
    {
      "epoch": 0.635620172648796,
      "grad_norm": 9.943631172180176,
      "learning_rate": 0.0007942926903708225,
      "loss": 0.7706,
      "step": 1399
    },
    {
      "epoch": 0.6360745115856429,
      "grad_norm": 3.4915106296539307,
      "learning_rate": 0.0007941400885090798,
      "loss": 0.5856,
      "step": 1400
    },
    {
      "epoch": 0.6365288505224898,
      "grad_norm": 5.764359951019287,
      "learning_rate": 0.0007939874866473371,
      "loss": 1.2559,
      "step": 1401
    },
    {
      "epoch": 0.6369831894593366,
      "grad_norm": 5.755843639373779,
      "learning_rate": 0.0007938348847855944,
      "loss": 1.1239,
      "step": 1402
    },
    {
      "epoch": 0.6374375283961835,
      "grad_norm": 1.3969237804412842,
      "learning_rate": 0.0007936822829238516,
      "loss": 0.0715,
      "step": 1403
    },
    {
      "epoch": 0.6378918673330305,
      "grad_norm": 8.127410888671875,
      "learning_rate": 0.000793529681062109,
      "loss": 0.6025,
      "step": 1404
    },
    {
      "epoch": 0.6383462062698774,
      "grad_norm": 5.827754020690918,
      "learning_rate": 0.0007933770792003663,
      "loss": 1.6998,
      "step": 1405
    },
    {
      "epoch": 0.6388005452067242,
      "grad_norm": 4.133856773376465,
      "learning_rate": 0.0007932244773386235,
      "loss": 0.5807,
      "step": 1406
    },
    {
      "epoch": 0.6392548841435711,
      "grad_norm": 4.753881931304932,
      "learning_rate": 0.0007930718754768809,
      "loss": 0.8609,
      "step": 1407
    },
    {
      "epoch": 0.639709223080418,
      "grad_norm": 4.866656303405762,
      "learning_rate": 0.0007929192736151381,
      "loss": 0.6177,
      "step": 1408
    },
    {
      "epoch": 0.6401635620172649,
      "grad_norm": 8.758550643920898,
      "learning_rate": 0.0007927666717533954,
      "loss": 1.5523,
      "step": 1409
    },
    {
      "epoch": 0.6406179009541118,
      "grad_norm": 2.951582193374634,
      "learning_rate": 0.0007926140698916528,
      "loss": 0.4994,
      "step": 1410
    },
    {
      "epoch": 0.6410722398909586,
      "grad_norm": 6.293062686920166,
      "learning_rate": 0.0007924614680299099,
      "loss": 1.7727,
      "step": 1411
    },
    {
      "epoch": 0.6415265788278055,
      "grad_norm": 4.464162826538086,
      "learning_rate": 0.0007923088661681672,
      "loss": 1.2461,
      "step": 1412
    },
    {
      "epoch": 0.6419809177646524,
      "grad_norm": 5.647894382476807,
      "learning_rate": 0.0007921562643064245,
      "loss": 1.7729,
      "step": 1413
    },
    {
      "epoch": 0.6424352567014994,
      "grad_norm": 3.2501158714294434,
      "learning_rate": 0.0007920036624446818,
      "loss": 0.8582,
      "step": 1414
    },
    {
      "epoch": 0.6428895956383462,
      "grad_norm": 9.438764572143555,
      "learning_rate": 0.000791851060582939,
      "loss": 1.0518,
      "step": 1415
    },
    {
      "epoch": 0.6433439345751931,
      "grad_norm": 4.167375087738037,
      "learning_rate": 0.0007916984587211964,
      "loss": 0.5139,
      "step": 1416
    },
    {
      "epoch": 0.64379827351204,
      "grad_norm": 4.920766830444336,
      "learning_rate": 0.0007915458568594537,
      "loss": 0.6679,
      "step": 1417
    },
    {
      "epoch": 0.6442526124488869,
      "grad_norm": 5.576142311096191,
      "learning_rate": 0.0007913932549977109,
      "loss": 1.1112,
      "step": 1418
    },
    {
      "epoch": 0.6447069513857338,
      "grad_norm": 5.950145244598389,
      "learning_rate": 0.0007912406531359683,
      "loss": 0.5672,
      "step": 1419
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 7.0068678855896,
      "learning_rate": 0.0007910880512742255,
      "loss": 0.9249,
      "step": 1420
    },
    {
      "epoch": 0.6456156292594275,
      "grad_norm": 6.743326187133789,
      "learning_rate": 0.0007909354494124828,
      "loss": 0.9449,
      "step": 1421
    },
    {
      "epoch": 0.6460699681962744,
      "grad_norm": 6.350657939910889,
      "learning_rate": 0.0007907828475507402,
      "loss": 1.7957,
      "step": 1422
    },
    {
      "epoch": 0.6465243071331213,
      "grad_norm": 9.829829216003418,
      "learning_rate": 0.0007906302456889974,
      "loss": 1.4843,
      "step": 1423
    },
    {
      "epoch": 0.6469786460699682,
      "grad_norm": 5.616198539733887,
      "learning_rate": 0.0007904776438272547,
      "loss": 1.296,
      "step": 1424
    },
    {
      "epoch": 0.6474329850068151,
      "grad_norm": 5.9122090339660645,
      "learning_rate": 0.000790325041965512,
      "loss": 1.0046,
      "step": 1425
    },
    {
      "epoch": 0.647887323943662,
      "grad_norm": 6.836793422698975,
      "learning_rate": 0.0007901724401037693,
      "loss": 1.4868,
      "step": 1426
    },
    {
      "epoch": 0.6483416628805089,
      "grad_norm": 5.500484943389893,
      "learning_rate": 0.0007900198382420266,
      "loss": 1.3434,
      "step": 1427
    },
    {
      "epoch": 0.6487960018173557,
      "grad_norm": 5.411383152008057,
      "learning_rate": 0.0007898672363802839,
      "loss": 1.1458,
      "step": 1428
    },
    {
      "epoch": 0.6492503407542026,
      "grad_norm": 7.5787835121154785,
      "learning_rate": 0.0007897146345185412,
      "loss": 2.1217,
      "step": 1429
    },
    {
      "epoch": 0.6497046796910495,
      "grad_norm": 4.120079517364502,
      "learning_rate": 0.0007895620326567983,
      "loss": 0.3922,
      "step": 1430
    },
    {
      "epoch": 0.6501590186278964,
      "grad_norm": 6.109663963317871,
      "learning_rate": 0.0007894094307950557,
      "loss": 1.2149,
      "step": 1431
    },
    {
      "epoch": 0.6506133575647433,
      "grad_norm": 4.440192699432373,
      "learning_rate": 0.000789256828933313,
      "loss": 0.5231,
      "step": 1432
    },
    {
      "epoch": 0.6510676965015901,
      "grad_norm": 8.766824722290039,
      "learning_rate": 0.0007891042270715702,
      "loss": 1.4329,
      "step": 1433
    },
    {
      "epoch": 0.6515220354384371,
      "grad_norm": 5.94691801071167,
      "learning_rate": 0.0007889516252098276,
      "loss": 1.1446,
      "step": 1434
    },
    {
      "epoch": 0.651976374375284,
      "grad_norm": 4.759953498840332,
      "learning_rate": 0.0007887990233480848,
      "loss": 1.2445,
      "step": 1435
    },
    {
      "epoch": 0.6524307133121309,
      "grad_norm": 6.440614223480225,
      "learning_rate": 0.0007886464214863421,
      "loss": 1.9847,
      "step": 1436
    },
    {
      "epoch": 0.6528850522489777,
      "grad_norm": 5.059123992919922,
      "learning_rate": 0.0007884938196245994,
      "loss": 0.738,
      "step": 1437
    },
    {
      "epoch": 0.6533393911858246,
      "grad_norm": 10.837761878967285,
      "learning_rate": 0.0007883412177628567,
      "loss": 2.0845,
      "step": 1438
    },
    {
      "epoch": 0.6537937301226715,
      "grad_norm": 7.2627644538879395,
      "learning_rate": 0.0007881886159011141,
      "loss": 1.2267,
      "step": 1439
    },
    {
      "epoch": 0.6542480690595184,
      "grad_norm": 4.1303019523620605,
      "learning_rate": 0.0007880360140393713,
      "loss": 0.9608,
      "step": 1440
    },
    {
      "epoch": 0.6547024079963653,
      "grad_norm": 3.623751163482666,
      "learning_rate": 0.0007878834121776286,
      "loss": 0.4298,
      "step": 1441
    },
    {
      "epoch": 0.6551567469332121,
      "grad_norm": 9.190818786621094,
      "learning_rate": 0.000787730810315886,
      "loss": 1.2648,
      "step": 1442
    },
    {
      "epoch": 0.6556110858700591,
      "grad_norm": 5.1518778800964355,
      "learning_rate": 0.0007875782084541432,
      "loss": 1.3192,
      "step": 1443
    },
    {
      "epoch": 0.656065424806906,
      "grad_norm": 3.5116569995880127,
      "learning_rate": 0.0007874256065924005,
      "loss": 0.5517,
      "step": 1444
    },
    {
      "epoch": 0.6565197637437529,
      "grad_norm": 9.495993614196777,
      "learning_rate": 0.0007872730047306578,
      "loss": 1.6873,
      "step": 1445
    },
    {
      "epoch": 0.6569741026805997,
      "grad_norm": 3.444287061691284,
      "learning_rate": 0.0007871204028689151,
      "loss": 0.4371,
      "step": 1446
    },
    {
      "epoch": 0.6574284416174466,
      "grad_norm": 4.256266117095947,
      "learning_rate": 0.0007869678010071723,
      "loss": 0.7089,
      "step": 1447
    },
    {
      "epoch": 0.6578827805542935,
      "grad_norm": 6.797482967376709,
      "learning_rate": 0.0007868151991454296,
      "loss": 1.4317,
      "step": 1448
    },
    {
      "epoch": 0.6583371194911404,
      "grad_norm": 5.82199764251709,
      "learning_rate": 0.0007866625972836868,
      "loss": 1.6526,
      "step": 1449
    },
    {
      "epoch": 0.6587914584279873,
      "grad_norm": 5.155043601989746,
      "learning_rate": 0.0007865099954219441,
      "loss": 0.6916,
      "step": 1450
    },
    {
      "epoch": 0.6592457973648341,
      "grad_norm": 5.637386322021484,
      "learning_rate": 0.0007863573935602015,
      "loss": 0.7454,
      "step": 1451
    },
    {
      "epoch": 0.659700136301681,
      "grad_norm": 5.094969272613525,
      "learning_rate": 0.0007862047916984587,
      "loss": 0.5969,
      "step": 1452
    },
    {
      "epoch": 0.660154475238528,
      "grad_norm": 7.282687187194824,
      "learning_rate": 0.000786052189836716,
      "loss": 1.0657,
      "step": 1453
    },
    {
      "epoch": 0.6606088141753749,
      "grad_norm": 5.214996814727783,
      "learning_rate": 0.0007858995879749733,
      "loss": 0.9292,
      "step": 1454
    },
    {
      "epoch": 0.6610631531122217,
      "grad_norm": 5.580793380737305,
      "learning_rate": 0.0007857469861132306,
      "loss": 0.5234,
      "step": 1455
    },
    {
      "epoch": 0.6615174920490686,
      "grad_norm": 6.12857723236084,
      "learning_rate": 0.0007855943842514879,
      "loss": 1.0119,
      "step": 1456
    },
    {
      "epoch": 0.6619718309859155,
      "grad_norm": 6.981881618499756,
      "learning_rate": 0.0007854417823897452,
      "loss": 0.9887,
      "step": 1457
    },
    {
      "epoch": 0.6624261699227624,
      "grad_norm": 9.628764152526855,
      "learning_rate": 0.0007852891805280025,
      "loss": 1.2945,
      "step": 1458
    },
    {
      "epoch": 0.6628805088596093,
      "grad_norm": 8.11777114868164,
      "learning_rate": 0.0007851365786662597,
      "loss": 1.4057,
      "step": 1459
    },
    {
      "epoch": 0.6633348477964561,
      "grad_norm": 7.322922229766846,
      "learning_rate": 0.0007849839768045171,
      "loss": 1.0113,
      "step": 1460
    },
    {
      "epoch": 0.663789186733303,
      "grad_norm": 5.016434192657471,
      "learning_rate": 0.0007848313749427744,
      "loss": 1.0668,
      "step": 1461
    },
    {
      "epoch": 0.6642435256701499,
      "grad_norm": 3.883370876312256,
      "learning_rate": 0.0007846787730810316,
      "loss": 0.4982,
      "step": 1462
    },
    {
      "epoch": 0.6646978646069969,
      "grad_norm": 5.175708770751953,
      "learning_rate": 0.000784526171219289,
      "loss": 0.8169,
      "step": 1463
    },
    {
      "epoch": 0.6651522035438437,
      "grad_norm": 7.206801414489746,
      "learning_rate": 0.0007843735693575462,
      "loss": 1.5054,
      "step": 1464
    },
    {
      "epoch": 0.6656065424806906,
      "grad_norm": 4.503102779388428,
      "learning_rate": 0.0007842209674958035,
      "loss": 0.4972,
      "step": 1465
    },
    {
      "epoch": 0.6660608814175375,
      "grad_norm": 5.49851131439209,
      "learning_rate": 0.0007840683656340607,
      "loss": 1.5189,
      "step": 1466
    },
    {
      "epoch": 0.6665152203543844,
      "grad_norm": 5.620057582855225,
      "learning_rate": 0.000783915763772318,
      "loss": 0.9887,
      "step": 1467
    },
    {
      "epoch": 0.6669695592912313,
      "grad_norm": 4.410693645477295,
      "learning_rate": 0.0007837631619105753,
      "loss": 0.5424,
      "step": 1468
    },
    {
      "epoch": 0.6674238982280781,
      "grad_norm": 4.28333044052124,
      "learning_rate": 0.0007836105600488326,
      "loss": 0.7419,
      "step": 1469
    },
    {
      "epoch": 0.667878237164925,
      "grad_norm": 9.179056167602539,
      "learning_rate": 0.0007834579581870899,
      "loss": 0.8312,
      "step": 1470
    },
    {
      "epoch": 0.6683325761017719,
      "grad_norm": 6.452798843383789,
      "learning_rate": 0.0007833053563253471,
      "loss": 0.9159,
      "step": 1471
    },
    {
      "epoch": 0.6687869150386188,
      "grad_norm": 5.789968490600586,
      "learning_rate": 0.0007831527544636045,
      "loss": 0.7285,
      "step": 1472
    },
    {
      "epoch": 0.6692412539754657,
      "grad_norm": 7.148740291595459,
      "learning_rate": 0.0007830001526018618,
      "loss": 1.1279,
      "step": 1473
    },
    {
      "epoch": 0.6696955929123126,
      "grad_norm": 6.354269027709961,
      "learning_rate": 0.000782847550740119,
      "loss": 1.5954,
      "step": 1474
    },
    {
      "epoch": 0.6701499318491595,
      "grad_norm": 7.1886186599731445,
      "learning_rate": 0.0007826949488783764,
      "loss": 0.6978,
      "step": 1475
    },
    {
      "epoch": 0.6706042707860064,
      "grad_norm": 9.729846000671387,
      "learning_rate": 0.0007825423470166336,
      "loss": 1.1452,
      "step": 1476
    },
    {
      "epoch": 0.6710586097228532,
      "grad_norm": 6.579062461853027,
      "learning_rate": 0.0007823897451548909,
      "loss": 0.9163,
      "step": 1477
    },
    {
      "epoch": 0.6715129486597001,
      "grad_norm": 6.972972869873047,
      "learning_rate": 0.0007822371432931483,
      "loss": 1.5189,
      "step": 1478
    },
    {
      "epoch": 0.671967287596547,
      "grad_norm": 8.208090782165527,
      "learning_rate": 0.0007820845414314055,
      "loss": 1.654,
      "step": 1479
    },
    {
      "epoch": 0.6724216265333939,
      "grad_norm": 10.716583251953125,
      "learning_rate": 0.0007819319395696628,
      "loss": 0.701,
      "step": 1480
    },
    {
      "epoch": 0.6728759654702408,
      "grad_norm": 3.6777091026306152,
      "learning_rate": 0.0007817793377079201,
      "loss": 0.7026,
      "step": 1481
    },
    {
      "epoch": 0.6733303044070876,
      "grad_norm": 5.356451034545898,
      "learning_rate": 0.0007816267358461774,
      "loss": 0.9798,
      "step": 1482
    },
    {
      "epoch": 0.6737846433439346,
      "grad_norm": 6.674968719482422,
      "learning_rate": 0.0007814741339844347,
      "loss": 0.9207,
      "step": 1483
    },
    {
      "epoch": 0.6742389822807815,
      "grad_norm": 6.04261589050293,
      "learning_rate": 0.0007813215321226919,
      "loss": 1.1735,
      "step": 1484
    },
    {
      "epoch": 0.6746933212176284,
      "grad_norm": 9.377655029296875,
      "learning_rate": 0.0007811689302609492,
      "loss": 1.288,
      "step": 1485
    },
    {
      "epoch": 0.6751476601544752,
      "grad_norm": 7.524163722991943,
      "learning_rate": 0.0007810163283992064,
      "loss": 2.0197,
      "step": 1486
    },
    {
      "epoch": 0.6756019990913221,
      "grad_norm": 5.156660556793213,
      "learning_rate": 0.0007808637265374638,
      "loss": 0.7673,
      "step": 1487
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 6.030290603637695,
      "learning_rate": 0.000780711124675721,
      "loss": 0.8745,
      "step": 1488
    },
    {
      "epoch": 0.6765106769650159,
      "grad_norm": 6.245311737060547,
      "learning_rate": 0.0007805585228139783,
      "loss": 1.3397,
      "step": 1489
    },
    {
      "epoch": 0.6769650159018628,
      "grad_norm": 4.7669572830200195,
      "learning_rate": 0.0007804059209522357,
      "loss": 0.4862,
      "step": 1490
    },
    {
      "epoch": 0.6774193548387096,
      "grad_norm": 6.157237529754639,
      "learning_rate": 0.0007802533190904929,
      "loss": 1.0786,
      "step": 1491
    },
    {
      "epoch": 0.6778736937755566,
      "grad_norm": 7.469592094421387,
      "learning_rate": 0.0007801007172287502,
      "loss": 1.4169,
      "step": 1492
    },
    {
      "epoch": 0.6783280327124035,
      "grad_norm": 6.10805082321167,
      "learning_rate": 0.0007799481153670075,
      "loss": 0.9854,
      "step": 1493
    },
    {
      "epoch": 0.6787823716492504,
      "grad_norm": 7.521082878112793,
      "learning_rate": 0.0007797955135052648,
      "loss": 1.8691,
      "step": 1494
    },
    {
      "epoch": 0.6792367105860972,
      "grad_norm": 8.136005401611328,
      "learning_rate": 0.000779642911643522,
      "loss": 1.7169,
      "step": 1495
    },
    {
      "epoch": 0.6796910495229441,
      "grad_norm": 6.511387348175049,
      "learning_rate": 0.0007794903097817794,
      "loss": 0.6014,
      "step": 1496
    },
    {
      "epoch": 0.680145388459791,
      "grad_norm": 9.368496894836426,
      "learning_rate": 0.0007793377079200367,
      "loss": 0.8594,
      "step": 1497
    },
    {
      "epoch": 0.6805997273966379,
      "grad_norm": 4.296799182891846,
      "learning_rate": 0.0007791851060582939,
      "loss": 0.5524,
      "step": 1498
    },
    {
      "epoch": 0.6810540663334848,
      "grad_norm": 14.663606643676758,
      "learning_rate": 0.0007790325041965513,
      "loss": 1.2876,
      "step": 1499
    },
    {
      "epoch": 0.6815084052703316,
      "grad_norm": 6.059589862823486,
      "learning_rate": 0.0007788799023348086,
      "loss": 1.0711,
      "step": 1500
    },
    {
      "epoch": 0.6819627442071785,
      "grad_norm": 9.492984771728516,
      "learning_rate": 0.0007787273004730658,
      "loss": 0.848,
      "step": 1501
    },
    {
      "epoch": 0.6824170831440255,
      "grad_norm": 7.135290145874023,
      "learning_rate": 0.0007785746986113232,
      "loss": 1.5994,
      "step": 1502
    },
    {
      "epoch": 0.6828714220808724,
      "grad_norm": 6.599969387054443,
      "learning_rate": 0.0007784220967495803,
      "loss": 0.8878,
      "step": 1503
    },
    {
      "epoch": 0.6833257610177192,
      "grad_norm": 4.767058372497559,
      "learning_rate": 0.0007782694948878376,
      "loss": 0.7764,
      "step": 1504
    },
    {
      "epoch": 0.6837800999545661,
      "grad_norm": 9.062597274780273,
      "learning_rate": 0.0007781168930260949,
      "loss": 2.1517,
      "step": 1505
    },
    {
      "epoch": 0.684234438891413,
      "grad_norm": 5.226751327514648,
      "learning_rate": 0.0007779642911643522,
      "loss": 0.7559,
      "step": 1506
    },
    {
      "epoch": 0.6846887778282599,
      "grad_norm": 9.25583553314209,
      "learning_rate": 0.0007778116893026095,
      "loss": 1.0533,
      "step": 1507
    },
    {
      "epoch": 0.6851431167651068,
      "grad_norm": 3.965822458267212,
      "learning_rate": 0.0007776590874408668,
      "loss": 0.7256,
      "step": 1508
    },
    {
      "epoch": 0.6855974557019536,
      "grad_norm": 5.797781467437744,
      "learning_rate": 0.0007775064855791241,
      "loss": 1.2128,
      "step": 1509
    },
    {
      "epoch": 0.6860517946388005,
      "grad_norm": 4.342081546783447,
      "learning_rate": 0.0007773538837173813,
      "loss": 0.7311,
      "step": 1510
    },
    {
      "epoch": 0.6865061335756474,
      "grad_norm": 4.3423848152160645,
      "learning_rate": 0.0007772012818556387,
      "loss": 0.7665,
      "step": 1511
    },
    {
      "epoch": 0.6869604725124944,
      "grad_norm": 6.515761852264404,
      "learning_rate": 0.000777048679993896,
      "loss": 1.1413,
      "step": 1512
    },
    {
      "epoch": 0.6874148114493412,
      "grad_norm": 5.433284759521484,
      "learning_rate": 0.0007768960781321532,
      "loss": 0.7827,
      "step": 1513
    },
    {
      "epoch": 0.6878691503861881,
      "grad_norm": 7.485538482666016,
      "learning_rate": 0.0007767434762704106,
      "loss": 1.1826,
      "step": 1514
    },
    {
      "epoch": 0.688323489323035,
      "grad_norm": 5.362311363220215,
      "learning_rate": 0.0007765908744086678,
      "loss": 1.4854,
      "step": 1515
    },
    {
      "epoch": 0.6887778282598819,
      "grad_norm": 8.92164134979248,
      "learning_rate": 0.0007764382725469251,
      "loss": 1.5214,
      "step": 1516
    },
    {
      "epoch": 0.6892321671967288,
      "grad_norm": 6.739617824554443,
      "learning_rate": 0.0007762856706851825,
      "loss": 0.9598,
      "step": 1517
    },
    {
      "epoch": 0.6896865061335756,
      "grad_norm": 4.788415431976318,
      "learning_rate": 0.0007761330688234397,
      "loss": 0.9576,
      "step": 1518
    },
    {
      "epoch": 0.6901408450704225,
      "grad_norm": 6.1607985496521,
      "learning_rate": 0.000775980466961697,
      "loss": 1.6307,
      "step": 1519
    },
    {
      "epoch": 0.6905951840072694,
      "grad_norm": 7.770327568054199,
      "learning_rate": 0.0007758278650999543,
      "loss": 2.059,
      "step": 1520
    },
    {
      "epoch": 0.6910495229441163,
      "grad_norm": 4.683786392211914,
      "learning_rate": 0.0007756752632382115,
      "loss": 1.0407,
      "step": 1521
    },
    {
      "epoch": 0.6915038618809632,
      "grad_norm": 4.092618942260742,
      "learning_rate": 0.0007755226613764687,
      "loss": 0.7087,
      "step": 1522
    },
    {
      "epoch": 0.6919582008178101,
      "grad_norm": 5.251509666442871,
      "learning_rate": 0.0007753700595147261,
      "loss": 1.6426,
      "step": 1523
    },
    {
      "epoch": 0.692412539754657,
      "grad_norm": 5.172855377197266,
      "learning_rate": 0.0007752174576529834,
      "loss": 1.5765,
      "step": 1524
    },
    {
      "epoch": 0.6928668786915039,
      "grad_norm": 7.6597490310668945,
      "learning_rate": 0.0007750648557912406,
      "loss": 1.4523,
      "step": 1525
    },
    {
      "epoch": 0.6933212176283507,
      "grad_norm": 4.652515888214111,
      "learning_rate": 0.000774912253929498,
      "loss": 1.4269,
      "step": 1526
    },
    {
      "epoch": 0.6937755565651976,
      "grad_norm": 7.290930271148682,
      "learning_rate": 0.0007747596520677552,
      "loss": 1.113,
      "step": 1527
    },
    {
      "epoch": 0.6942298955020445,
      "grad_norm": 5.931819915771484,
      "learning_rate": 0.0007746070502060125,
      "loss": 0.9715,
      "step": 1528
    },
    {
      "epoch": 0.6946842344388914,
      "grad_norm": 6.531838893890381,
      "learning_rate": 0.0007744544483442699,
      "loss": 0.9047,
      "step": 1529
    },
    {
      "epoch": 0.6951385733757383,
      "grad_norm": 6.730010986328125,
      "learning_rate": 0.0007743018464825271,
      "loss": 1.1914,
      "step": 1530
    },
    {
      "epoch": 0.6955929123125852,
      "grad_norm": 7.018351078033447,
      "learning_rate": 0.0007741492446207844,
      "loss": 0.9429,
      "step": 1531
    },
    {
      "epoch": 0.6960472512494321,
      "grad_norm": 7.287688732147217,
      "learning_rate": 0.0007739966427590417,
      "loss": 0.7757,
      "step": 1532
    },
    {
      "epoch": 0.696501590186279,
      "grad_norm": 9.200973510742188,
      "learning_rate": 0.000773844040897299,
      "loss": 1.7328,
      "step": 1533
    },
    {
      "epoch": 0.6969559291231259,
      "grad_norm": 4.618945121765137,
      "learning_rate": 0.0007736914390355562,
      "loss": 0.7445,
      "step": 1534
    },
    {
      "epoch": 0.6974102680599727,
      "grad_norm": 5.345422267913818,
      "learning_rate": 0.0007735388371738136,
      "loss": 1.2869,
      "step": 1535
    },
    {
      "epoch": 0.6978646069968196,
      "grad_norm": 4.401554107666016,
      "learning_rate": 0.0007733862353120709,
      "loss": 1.3004,
      "step": 1536
    },
    {
      "epoch": 0.6983189459336665,
      "grad_norm": 7.883388519287109,
      "learning_rate": 0.0007732336334503281,
      "loss": 1.2593,
      "step": 1537
    },
    {
      "epoch": 0.6987732848705134,
      "grad_norm": 3.0849945545196533,
      "learning_rate": 0.0007730810315885855,
      "loss": 0.4453,
      "step": 1538
    },
    {
      "epoch": 0.6992276238073603,
      "grad_norm": 4.450264930725098,
      "learning_rate": 0.0007729284297268426,
      "loss": 0.8562,
      "step": 1539
    },
    {
      "epoch": 0.6996819627442071,
      "grad_norm": 3.762633800506592,
      "learning_rate": 0.0007727758278650999,
      "loss": 0.5024,
      "step": 1540
    },
    {
      "epoch": 0.7001363016810541,
      "grad_norm": 6.823173522949219,
      "learning_rate": 0.0007726232260033573,
      "loss": 1.8143,
      "step": 1541
    },
    {
      "epoch": 0.700590640617901,
      "grad_norm": 2.8456997871398926,
      "learning_rate": 0.0007724706241416145,
      "loss": 0.4872,
      "step": 1542
    },
    {
      "epoch": 0.7010449795547479,
      "grad_norm": 6.853029251098633,
      "learning_rate": 0.0007723180222798718,
      "loss": 1.0588,
      "step": 1543
    },
    {
      "epoch": 0.7014993184915947,
      "grad_norm": 4.821033477783203,
      "learning_rate": 0.0007721654204181291,
      "loss": 1.1164,
      "step": 1544
    },
    {
      "epoch": 0.7019536574284416,
      "grad_norm": 8.11875057220459,
      "learning_rate": 0.0007720128185563864,
      "loss": 1.3042,
      "step": 1545
    },
    {
      "epoch": 0.7024079963652885,
      "grad_norm": 6.086859226226807,
      "learning_rate": 0.0007718602166946436,
      "loss": 1.0809,
      "step": 1546
    },
    {
      "epoch": 0.7028623353021354,
      "grad_norm": 5.140821933746338,
      "learning_rate": 0.000771707614832901,
      "loss": 0.7947,
      "step": 1547
    },
    {
      "epoch": 0.7033166742389823,
      "grad_norm": 9.196961402893066,
      "learning_rate": 0.0007715550129711583,
      "loss": 2.5656,
      "step": 1548
    },
    {
      "epoch": 0.7037710131758291,
      "grad_norm": 6.393149375915527,
      "learning_rate": 0.0007714024111094155,
      "loss": 1.0148,
      "step": 1549
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 7.553123474121094,
      "learning_rate": 0.0007712498092476729,
      "loss": 1.7205,
      "step": 1550
    },
    {
      "epoch": 0.704679691049523,
      "grad_norm": 7.0878190994262695,
      "learning_rate": 0.0007710972073859301,
      "loss": 1.2792,
      "step": 1551
    },
    {
      "epoch": 0.7051340299863699,
      "grad_norm": 8.402579307556152,
      "learning_rate": 0.0007709446055241874,
      "loss": 1.5116,
      "step": 1552
    },
    {
      "epoch": 0.7055883689232167,
      "grad_norm": 4.504055023193359,
      "learning_rate": 0.0007707920036624448,
      "loss": 0.5619,
      "step": 1553
    },
    {
      "epoch": 0.7060427078600636,
      "grad_norm": 4.677282810211182,
      "learning_rate": 0.000770639401800702,
      "loss": 0.6349,
      "step": 1554
    },
    {
      "epoch": 0.7064970467969105,
      "grad_norm": 8.145313262939453,
      "learning_rate": 0.0007704867999389593,
      "loss": 0.5449,
      "step": 1555
    },
    {
      "epoch": 0.7069513857337574,
      "grad_norm": 4.115911483764648,
      "learning_rate": 0.0007703341980772166,
      "loss": 0.8061,
      "step": 1556
    },
    {
      "epoch": 0.7074057246706043,
      "grad_norm": 8.55687141418457,
      "learning_rate": 0.0007701815962154738,
      "loss": 1.7611,
      "step": 1557
    },
    {
      "epoch": 0.7078600636074511,
      "grad_norm": 6.850009918212891,
      "learning_rate": 0.000770028994353731,
      "loss": 1.5924,
      "step": 1558
    },
    {
      "epoch": 0.708314402544298,
      "grad_norm": 6.343586444854736,
      "learning_rate": 0.0007698763924919884,
      "loss": 0.8532,
      "step": 1559
    },
    {
      "epoch": 0.7087687414811449,
      "grad_norm": 6.01372766494751,
      "learning_rate": 0.0007697237906302457,
      "loss": 0.8644,
      "step": 1560
    },
    {
      "epoch": 0.7092230804179919,
      "grad_norm": 4.733336448669434,
      "learning_rate": 0.0007695711887685029,
      "loss": 0.8007,
      "step": 1561
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 6.03196907043457,
      "learning_rate": 0.0007694185869067603,
      "loss": 1.1461,
      "step": 1562
    },
    {
      "epoch": 0.7101317582916856,
      "grad_norm": 4.771724700927734,
      "learning_rate": 0.0007692659850450175,
      "loss": 0.7205,
      "step": 1563
    },
    {
      "epoch": 0.7105860972285325,
      "grad_norm": 6.774246692657471,
      "learning_rate": 0.0007691133831832748,
      "loss": 1.4306,
      "step": 1564
    },
    {
      "epoch": 0.7110404361653794,
      "grad_norm": 5.846048831939697,
      "learning_rate": 0.0007689607813215322,
      "loss": 0.9516,
      "step": 1565
    },
    {
      "epoch": 0.7114947751022263,
      "grad_norm": 5.837342262268066,
      "learning_rate": 0.0007688081794597894,
      "loss": 1.1761,
      "step": 1566
    },
    {
      "epoch": 0.7119491140390731,
      "grad_norm": 4.686714172363281,
      "learning_rate": 0.0007686555775980467,
      "loss": 0.5,
      "step": 1567
    },
    {
      "epoch": 0.71240345297592,
      "grad_norm": 4.85838508605957,
      "learning_rate": 0.000768502975736304,
      "loss": 0.8804,
      "step": 1568
    },
    {
      "epoch": 0.7128577919127669,
      "grad_norm": 4.354681968688965,
      "learning_rate": 0.0007683503738745613,
      "loss": 0.3498,
      "step": 1569
    },
    {
      "epoch": 0.7133121308496138,
      "grad_norm": 4.168893814086914,
      "learning_rate": 0.0007681977720128186,
      "loss": 0.5305,
      "step": 1570
    },
    {
      "epoch": 0.7137664697864607,
      "grad_norm": 5.924803733825684,
      "learning_rate": 0.0007680451701510759,
      "loss": 1.2703,
      "step": 1571
    },
    {
      "epoch": 0.7142208087233076,
      "grad_norm": 6.737338066101074,
      "learning_rate": 0.0007678925682893332,
      "loss": 0.9048,
      "step": 1572
    },
    {
      "epoch": 0.7146751476601545,
      "grad_norm": 6.255313396453857,
      "learning_rate": 0.0007677399664275904,
      "loss": 0.7105,
      "step": 1573
    },
    {
      "epoch": 0.7151294865970014,
      "grad_norm": 6.282099723815918,
      "learning_rate": 0.0007675873645658478,
      "loss": 0.9504,
      "step": 1574
    },
    {
      "epoch": 0.7155838255338483,
      "grad_norm": 7.160847187042236,
      "learning_rate": 0.0007674347627041051,
      "loss": 1.345,
      "step": 1575
    },
    {
      "epoch": 0.7160381644706951,
      "grad_norm": 4.949448108673096,
      "learning_rate": 0.0007672821608423622,
      "loss": 0.8307,
      "step": 1576
    },
    {
      "epoch": 0.716492503407542,
      "grad_norm": 7.166744232177734,
      "learning_rate": 0.0007671295589806196,
      "loss": 0.9721,
      "step": 1577
    },
    {
      "epoch": 0.7169468423443889,
      "grad_norm": 6.6720709800720215,
      "learning_rate": 0.0007669769571188768,
      "loss": 1.0444,
      "step": 1578
    },
    {
      "epoch": 0.7174011812812358,
      "grad_norm": 4.1474151611328125,
      "learning_rate": 0.0007668243552571341,
      "loss": 0.5663,
      "step": 1579
    },
    {
      "epoch": 0.7178555202180827,
      "grad_norm": 5.961559295654297,
      "learning_rate": 0.0007666717533953914,
      "loss": 1.3907,
      "step": 1580
    },
    {
      "epoch": 0.7183098591549296,
      "grad_norm": 3.3881473541259766,
      "learning_rate": 0.0007665191515336487,
      "loss": 0.282,
      "step": 1581
    },
    {
      "epoch": 0.7187641980917765,
      "grad_norm": 5.4837164878845215,
      "learning_rate": 0.000766366549671906,
      "loss": 0.4787,
      "step": 1582
    },
    {
      "epoch": 0.7192185370286234,
      "grad_norm": 3.0646378993988037,
      "learning_rate": 0.0007662139478101633,
      "loss": 0.2425,
      "step": 1583
    },
    {
      "epoch": 0.7196728759654702,
      "grad_norm": 5.724050045013428,
      "learning_rate": 0.0007660613459484206,
      "loss": 1.0488,
      "step": 1584
    },
    {
      "epoch": 0.7201272149023171,
      "grad_norm": 3.9141883850097656,
      "learning_rate": 0.0007659087440866778,
      "loss": 0.5948,
      "step": 1585
    },
    {
      "epoch": 0.720581553839164,
      "grad_norm": 6.572970867156982,
      "learning_rate": 0.0007657561422249352,
      "loss": 0.8082,
      "step": 1586
    },
    {
      "epoch": 0.7210358927760109,
      "grad_norm": 7.3082146644592285,
      "learning_rate": 0.0007656035403631925,
      "loss": 1.5664,
      "step": 1587
    },
    {
      "epoch": 0.7214902317128578,
      "grad_norm": 9.14210319519043,
      "learning_rate": 0.0007654509385014497,
      "loss": 1.0212,
      "step": 1588
    },
    {
      "epoch": 0.7219445706497046,
      "grad_norm": 5.716622829437256,
      "learning_rate": 0.0007652983366397071,
      "loss": 1.2407,
      "step": 1589
    },
    {
      "epoch": 0.7223989095865516,
      "grad_norm": 5.185332775115967,
      "learning_rate": 0.0007651457347779643,
      "loss": 1.1018,
      "step": 1590
    },
    {
      "epoch": 0.7228532485233985,
      "grad_norm": 6.5982770919799805,
      "learning_rate": 0.0007649931329162216,
      "loss": 0.6491,
      "step": 1591
    },
    {
      "epoch": 0.7233075874602454,
      "grad_norm": 7.9192585945129395,
      "learning_rate": 0.000764840531054479,
      "loss": 0.4611,
      "step": 1592
    },
    {
      "epoch": 0.7237619263970922,
      "grad_norm": 4.177964210510254,
      "learning_rate": 0.0007646879291927362,
      "loss": 0.5758,
      "step": 1593
    },
    {
      "epoch": 0.7242162653339391,
      "grad_norm": 7.277731418609619,
      "learning_rate": 0.0007645353273309934,
      "loss": 1.4928,
      "step": 1594
    },
    {
      "epoch": 0.724670604270786,
      "grad_norm": 8.003886222839355,
      "learning_rate": 0.0007643827254692507,
      "loss": 1.749,
      "step": 1595
    },
    {
      "epoch": 0.7251249432076329,
      "grad_norm": 5.956092834472656,
      "learning_rate": 0.000764230123607508,
      "loss": 0.6622,
      "step": 1596
    },
    {
      "epoch": 0.7255792821444798,
      "grad_norm": 5.664785385131836,
      "learning_rate": 0.0007640775217457652,
      "loss": 1.0564,
      "step": 1597
    },
    {
      "epoch": 0.7260336210813266,
      "grad_norm": 4.484902381896973,
      "learning_rate": 0.0007639249198840226,
      "loss": 0.7678,
      "step": 1598
    },
    {
      "epoch": 0.7264879600181735,
      "grad_norm": 5.584417819976807,
      "learning_rate": 0.0007637723180222799,
      "loss": 0.4762,
      "step": 1599
    },
    {
      "epoch": 0.7269422989550205,
      "grad_norm": 8.49598217010498,
      "learning_rate": 0.0007636197161605371,
      "loss": 1.9802,
      "step": 1600
    },
    {
      "epoch": 0.7273966378918674,
      "grad_norm": 6.948309421539307,
      "learning_rate": 0.0007634671142987945,
      "loss": 0.695,
      "step": 1601
    },
    {
      "epoch": 0.7278509768287142,
      "grad_norm": 5.079593181610107,
      "learning_rate": 0.0007633145124370517,
      "loss": 0.8935,
      "step": 1602
    },
    {
      "epoch": 0.7283053157655611,
      "grad_norm": 6.517108917236328,
      "learning_rate": 0.000763161910575309,
      "loss": 0.8205,
      "step": 1603
    },
    {
      "epoch": 0.728759654702408,
      "grad_norm": 4.810179710388184,
      "learning_rate": 0.0007630093087135664,
      "loss": 1.0391,
      "step": 1604
    },
    {
      "epoch": 0.7292139936392549,
      "grad_norm": 12.650346755981445,
      "learning_rate": 0.0007628567068518236,
      "loss": 2.2693,
      "step": 1605
    },
    {
      "epoch": 0.7296683325761018,
      "grad_norm": 3.1088368892669678,
      "learning_rate": 0.0007627041049900809,
      "loss": 0.3982,
      "step": 1606
    },
    {
      "epoch": 0.7301226715129486,
      "grad_norm": 8.742657661437988,
      "learning_rate": 0.0007625515031283382,
      "loss": 1.9332,
      "step": 1607
    },
    {
      "epoch": 0.7305770104497955,
      "grad_norm": 5.368611812591553,
      "learning_rate": 0.0007623989012665955,
      "loss": 0.7944,
      "step": 1608
    },
    {
      "epoch": 0.7310313493866424,
      "grad_norm": 6.625491142272949,
      "learning_rate": 0.0007622462994048528,
      "loss": 1.5114,
      "step": 1609
    },
    {
      "epoch": 0.7314856883234894,
      "grad_norm": 7.580885410308838,
      "learning_rate": 0.0007620936975431101,
      "loss": 1.7313,
      "step": 1610
    },
    {
      "epoch": 0.7319400272603362,
      "grad_norm": 4.708335876464844,
      "learning_rate": 0.0007619410956813674,
      "loss": 0.7979,
      "step": 1611
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 4.57747745513916,
      "learning_rate": 0.0007617884938196245,
      "loss": 1.1964,
      "step": 1612
    },
    {
      "epoch": 0.73284870513403,
      "grad_norm": 6.878145217895508,
      "learning_rate": 0.0007616358919578819,
      "loss": 1.2714,
      "step": 1613
    },
    {
      "epoch": 0.7333030440708769,
      "grad_norm": 10.470074653625488,
      "learning_rate": 0.0007614832900961391,
      "loss": 1.3518,
      "step": 1614
    },
    {
      "epoch": 0.7337573830077238,
      "grad_norm": 3.1122324466705322,
      "learning_rate": 0.0007613306882343964,
      "loss": 0.4551,
      "step": 1615
    },
    {
      "epoch": 0.7342117219445706,
      "grad_norm": 8.36365032196045,
      "learning_rate": 0.0007611780863726538,
      "loss": 1.4812,
      "step": 1616
    },
    {
      "epoch": 0.7346660608814175,
      "grad_norm": 8.632975578308105,
      "learning_rate": 0.000761025484510911,
      "loss": 2.356,
      "step": 1617
    },
    {
      "epoch": 0.7351203998182644,
      "grad_norm": 4.512323379516602,
      "learning_rate": 0.0007608728826491683,
      "loss": 0.6632,
      "step": 1618
    },
    {
      "epoch": 0.7355747387551114,
      "grad_norm": 7.969712734222412,
      "learning_rate": 0.0007607202807874256,
      "loss": 0.8963,
      "step": 1619
    },
    {
      "epoch": 0.7360290776919582,
      "grad_norm": 5.192809581756592,
      "learning_rate": 0.0007605676789256829,
      "loss": 0.8381,
      "step": 1620
    },
    {
      "epoch": 0.7364834166288051,
      "grad_norm": 7.5919880867004395,
      "learning_rate": 0.0007604150770639402,
      "loss": 1.0346,
      "step": 1621
    },
    {
      "epoch": 0.736937755565652,
      "grad_norm": 3.9427573680877686,
      "learning_rate": 0.0007602624752021975,
      "loss": 0.6769,
      "step": 1622
    },
    {
      "epoch": 0.7373920945024989,
      "grad_norm": 7.154022216796875,
      "learning_rate": 0.0007601098733404548,
      "loss": 1.393,
      "step": 1623
    },
    {
      "epoch": 0.7378464334393458,
      "grad_norm": 8.014837265014648,
      "learning_rate": 0.000759957271478712,
      "loss": 1.6902,
      "step": 1624
    },
    {
      "epoch": 0.7383007723761926,
      "grad_norm": 4.697291374206543,
      "learning_rate": 0.0007598046696169694,
      "loss": 0.6971,
      "step": 1625
    },
    {
      "epoch": 0.7387551113130395,
      "grad_norm": 7.6764655113220215,
      "learning_rate": 0.0007596520677552267,
      "loss": 0.6538,
      "step": 1626
    },
    {
      "epoch": 0.7392094502498864,
      "grad_norm": 5.032561302185059,
      "learning_rate": 0.0007594994658934839,
      "loss": 0.9029,
      "step": 1627
    },
    {
      "epoch": 0.7396637891867333,
      "grad_norm": 7.035595893859863,
      "learning_rate": 0.0007593468640317413,
      "loss": 1.4714,
      "step": 1628
    },
    {
      "epoch": 0.7401181281235802,
      "grad_norm": 5.268275737762451,
      "learning_rate": 0.0007591942621699985,
      "loss": 0.8728,
      "step": 1629
    },
    {
      "epoch": 0.7405724670604271,
      "grad_norm": 4.525938510894775,
      "learning_rate": 0.0007590416603082557,
      "loss": 0.9361,
      "step": 1630
    },
    {
      "epoch": 0.741026805997274,
      "grad_norm": 2.658475637435913,
      "learning_rate": 0.000758889058446513,
      "loss": 0.45,
      "step": 1631
    },
    {
      "epoch": 0.7414811449341209,
      "grad_norm": 4.318480014801025,
      "learning_rate": 0.0007587364565847703,
      "loss": 0.7521,
      "step": 1632
    },
    {
      "epoch": 0.7419354838709677,
      "grad_norm": 4.106709957122803,
      "learning_rate": 0.0007585838547230276,
      "loss": 0.8179,
      "step": 1633
    },
    {
      "epoch": 0.7423898228078146,
      "grad_norm": 6.026836395263672,
      "learning_rate": 0.0007584312528612849,
      "loss": 1.13,
      "step": 1634
    },
    {
      "epoch": 0.7428441617446615,
      "grad_norm": 12.278581619262695,
      "learning_rate": 0.0007582786509995422,
      "loss": 1.0593,
      "step": 1635
    },
    {
      "epoch": 0.7432985006815084,
      "grad_norm": 4.075274467468262,
      "learning_rate": 0.0007581260491377994,
      "loss": 0.7264,
      "step": 1636
    },
    {
      "epoch": 0.7437528396183553,
      "grad_norm": 5.087875843048096,
      "learning_rate": 0.0007579734472760568,
      "loss": 0.721,
      "step": 1637
    },
    {
      "epoch": 0.7442071785552021,
      "grad_norm": 7.425424575805664,
      "learning_rate": 0.000757820845414314,
      "loss": 2.0446,
      "step": 1638
    },
    {
      "epoch": 0.7446615174920491,
      "grad_norm": 7.854901313781738,
      "learning_rate": 0.0007576682435525713,
      "loss": 1.6911,
      "step": 1639
    },
    {
      "epoch": 0.745115856428896,
      "grad_norm": 7.711874485015869,
      "learning_rate": 0.0007575156416908287,
      "loss": 1.6452,
      "step": 1640
    },
    {
      "epoch": 0.7455701953657429,
      "grad_norm": 4.428966999053955,
      "learning_rate": 0.0007573630398290859,
      "loss": 0.781,
      "step": 1641
    },
    {
      "epoch": 0.7460245343025897,
      "grad_norm": 8.361799240112305,
      "learning_rate": 0.0007572104379673432,
      "loss": 2.1412,
      "step": 1642
    },
    {
      "epoch": 0.7464788732394366,
      "grad_norm": 14.019743919372559,
      "learning_rate": 0.0007570578361056006,
      "loss": 0.8274,
      "step": 1643
    },
    {
      "epoch": 0.7469332121762835,
      "grad_norm": 5.561796188354492,
      "learning_rate": 0.0007569052342438578,
      "loss": 1.0718,
      "step": 1644
    },
    {
      "epoch": 0.7473875511131304,
      "grad_norm": 6.128159046173096,
      "learning_rate": 0.0007567526323821151,
      "loss": 0.8158,
      "step": 1645
    },
    {
      "epoch": 0.7478418900499773,
      "grad_norm": 6.306799411773682,
      "learning_rate": 0.0007566000305203724,
      "loss": 0.6875,
      "step": 1646
    },
    {
      "epoch": 0.7482962289868241,
      "grad_norm": 5.912095546722412,
      "learning_rate": 0.0007564474286586297,
      "loss": 1.2025,
      "step": 1647
    },
    {
      "epoch": 0.748750567923671,
      "grad_norm": 4.168496131896973,
      "learning_rate": 0.0007562948267968869,
      "loss": 0.7048,
      "step": 1648
    },
    {
      "epoch": 0.749204906860518,
      "grad_norm": 12.14144229888916,
      "learning_rate": 0.0007561422249351442,
      "loss": 0.9869,
      "step": 1649
    },
    {
      "epoch": 0.7496592457973649,
      "grad_norm": 4.290177345275879,
      "learning_rate": 0.0007559896230734015,
      "loss": 0.7869,
      "step": 1650
    },
    {
      "epoch": 0.7501135847342117,
      "grad_norm": 6.370873928070068,
      "learning_rate": 0.0007558370212116587,
      "loss": 1.8465,
      "step": 1651
    },
    {
      "epoch": 0.7505679236710586,
      "grad_norm": 5.69942569732666,
      "learning_rate": 0.0007556844193499161,
      "loss": 0.7578,
      "step": 1652
    },
    {
      "epoch": 0.7510222626079055,
      "grad_norm": 5.903049468994141,
      "learning_rate": 0.0007555318174881733,
      "loss": 1.08,
      "step": 1653
    },
    {
      "epoch": 0.7514766015447524,
      "grad_norm": 4.0709381103515625,
      "learning_rate": 0.0007553792156264306,
      "loss": 0.8388,
      "step": 1654
    },
    {
      "epoch": 0.7519309404815993,
      "grad_norm": 5.611852645874023,
      "learning_rate": 0.000755226613764688,
      "loss": 1.1289,
      "step": 1655
    },
    {
      "epoch": 0.7523852794184461,
      "grad_norm": 6.473081588745117,
      "learning_rate": 0.0007550740119029452,
      "loss": 0.7621,
      "step": 1656
    },
    {
      "epoch": 0.752839618355293,
      "grad_norm": 4.991074562072754,
      "learning_rate": 0.0007549214100412025,
      "loss": 1.3535,
      "step": 1657
    },
    {
      "epoch": 0.7532939572921399,
      "grad_norm": 8.110067367553711,
      "learning_rate": 0.0007547688081794598,
      "loss": 1.4643,
      "step": 1658
    },
    {
      "epoch": 0.7537482962289869,
      "grad_norm": 5.342687129974365,
      "learning_rate": 0.0007546162063177171,
      "loss": 1.1467,
      "step": 1659
    },
    {
      "epoch": 0.7542026351658337,
      "grad_norm": 6.783014297485352,
      "learning_rate": 0.0007544636044559743,
      "loss": 1.3423,
      "step": 1660
    },
    {
      "epoch": 0.7546569741026806,
      "grad_norm": 8.455617904663086,
      "learning_rate": 0.0007543110025942317,
      "loss": 1.6355,
      "step": 1661
    },
    {
      "epoch": 0.7551113130395275,
      "grad_norm": 11.67534065246582,
      "learning_rate": 0.000754158400732489,
      "loss": 1.6126,
      "step": 1662
    },
    {
      "epoch": 0.7555656519763744,
      "grad_norm": 8.222787857055664,
      "learning_rate": 0.0007540057988707462,
      "loss": 1.2738,
      "step": 1663
    },
    {
      "epoch": 0.7560199909132213,
      "grad_norm": 4.123501300811768,
      "learning_rate": 0.0007538531970090036,
      "loss": 0.2718,
      "step": 1664
    },
    {
      "epoch": 0.7564743298500681,
      "grad_norm": 6.324760437011719,
      "learning_rate": 0.0007537005951472608,
      "loss": 0.7034,
      "step": 1665
    },
    {
      "epoch": 0.756928668786915,
      "grad_norm": 8.047626495361328,
      "learning_rate": 0.0007535479932855181,
      "loss": 1.6115,
      "step": 1666
    },
    {
      "epoch": 0.7573830077237619,
      "grad_norm": 6.493831157684326,
      "learning_rate": 0.0007533953914237754,
      "loss": 1.2344,
      "step": 1667
    },
    {
      "epoch": 0.7578373466606089,
      "grad_norm": 6.425709247589111,
      "learning_rate": 0.0007532427895620326,
      "loss": 0.7106,
      "step": 1668
    },
    {
      "epoch": 0.7582916855974557,
      "grad_norm": 4.499257564544678,
      "learning_rate": 0.0007530901877002899,
      "loss": 1.3235,
      "step": 1669
    },
    {
      "epoch": 0.7587460245343026,
      "grad_norm": 4.733073711395264,
      "learning_rate": 0.0007529375858385472,
      "loss": 0.9366,
      "step": 1670
    },
    {
      "epoch": 0.7592003634711495,
      "grad_norm": 4.393289089202881,
      "learning_rate": 0.0007527849839768045,
      "loss": 0.5276,
      "step": 1671
    },
    {
      "epoch": 0.7596547024079964,
      "grad_norm": 8.228900909423828,
      "learning_rate": 0.0007526323821150617,
      "loss": 0.9671,
      "step": 1672
    },
    {
      "epoch": 0.7601090413448433,
      "grad_norm": 5.771513938903809,
      "learning_rate": 0.0007524797802533191,
      "loss": 1.1978,
      "step": 1673
    },
    {
      "epoch": 0.7605633802816901,
      "grad_norm": 7.778597831726074,
      "learning_rate": 0.0007523271783915764,
      "loss": 1.2571,
      "step": 1674
    },
    {
      "epoch": 0.761017719218537,
      "grad_norm": 5.2204742431640625,
      "learning_rate": 0.0007521745765298336,
      "loss": 1.2329,
      "step": 1675
    },
    {
      "epoch": 0.7614720581553839,
      "grad_norm": 6.2904791831970215,
      "learning_rate": 0.000752021974668091,
      "loss": 1.0975,
      "step": 1676
    },
    {
      "epoch": 0.7619263970922308,
      "grad_norm": 8.519696235656738,
      "learning_rate": 0.0007518693728063482,
      "loss": 1.8318,
      "step": 1677
    },
    {
      "epoch": 0.7623807360290777,
      "grad_norm": 5.8324971199035645,
      "learning_rate": 0.0007517167709446055,
      "loss": 0.927,
      "step": 1678
    },
    {
      "epoch": 0.7628350749659246,
      "grad_norm": 9.13117504119873,
      "learning_rate": 0.0007515641690828629,
      "loss": 1.827,
      "step": 1679
    },
    {
      "epoch": 0.7632894139027715,
      "grad_norm": 6.279883861541748,
      "learning_rate": 0.0007514115672211201,
      "loss": 1.2573,
      "step": 1680
    },
    {
      "epoch": 0.7637437528396184,
      "grad_norm": 6.865844249725342,
      "learning_rate": 0.0007512589653593774,
      "loss": 1.1013,
      "step": 1681
    },
    {
      "epoch": 0.7641980917764652,
      "grad_norm": 8.680283546447754,
      "learning_rate": 0.0007511063634976347,
      "loss": 1.2843,
      "step": 1682
    },
    {
      "epoch": 0.7646524307133121,
      "grad_norm": 8.263264656066895,
      "learning_rate": 0.000750953761635892,
      "loss": 2.6908,
      "step": 1683
    },
    {
      "epoch": 0.765106769650159,
      "grad_norm": 6.060764789581299,
      "learning_rate": 0.0007508011597741494,
      "loss": 0.951,
      "step": 1684
    },
    {
      "epoch": 0.7655611085870059,
      "grad_norm": 4.649331092834473,
      "learning_rate": 0.0007506485579124065,
      "loss": 1.1269,
      "step": 1685
    },
    {
      "epoch": 0.7660154475238528,
      "grad_norm": 3.099290370941162,
      "learning_rate": 0.0007504959560506638,
      "loss": 0.3723,
      "step": 1686
    },
    {
      "epoch": 0.7664697864606996,
      "grad_norm": 5.840421676635742,
      "learning_rate": 0.000750343354188921,
      "loss": 1.6247,
      "step": 1687
    },
    {
      "epoch": 0.7669241253975466,
      "grad_norm": 9.141487121582031,
      "learning_rate": 0.0007501907523271784,
      "loss": 1.6078,
      "step": 1688
    },
    {
      "epoch": 0.7673784643343935,
      "grad_norm": 7.4466447830200195,
      "learning_rate": 0.0007500381504654356,
      "loss": 1.647,
      "step": 1689
    },
    {
      "epoch": 0.7678328032712404,
      "grad_norm": 4.828832626342773,
      "learning_rate": 0.0007498855486036929,
      "loss": 1.0999,
      "step": 1690
    },
    {
      "epoch": 0.7682871422080872,
      "grad_norm": 5.943685054779053,
      "learning_rate": 0.0007497329467419503,
      "loss": 1.1121,
      "step": 1691
    },
    {
      "epoch": 0.7687414811449341,
      "grad_norm": 6.2399067878723145,
      "learning_rate": 0.0007495803448802075,
      "loss": 0.8645,
      "step": 1692
    },
    {
      "epoch": 0.769195820081781,
      "grad_norm": 10.21334457397461,
      "learning_rate": 0.0007494277430184649,
      "loss": 1.2468,
      "step": 1693
    },
    {
      "epoch": 0.7696501590186279,
      "grad_norm": 8.743246078491211,
      "learning_rate": 0.0007492751411567221,
      "loss": 1.6583,
      "step": 1694
    },
    {
      "epoch": 0.7701044979554748,
      "grad_norm": 6.2244439125061035,
      "learning_rate": 0.0007491225392949794,
      "loss": 0.9671,
      "step": 1695
    },
    {
      "epoch": 0.7705588368923216,
      "grad_norm": 3.7252533435821533,
      "learning_rate": 0.0007489699374332368,
      "loss": 0.5197,
      "step": 1696
    },
    {
      "epoch": 0.7710131758291685,
      "grad_norm": 7.393462181091309,
      "learning_rate": 0.000748817335571494,
      "loss": 1.8443,
      "step": 1697
    },
    {
      "epoch": 0.7714675147660155,
      "grad_norm": 6.198462009429932,
      "learning_rate": 0.0007486647337097513,
      "loss": 0.8716,
      "step": 1698
    },
    {
      "epoch": 0.7719218537028624,
      "grad_norm": 4.162138938903809,
      "learning_rate": 0.0007485121318480086,
      "loss": 0.4211,
      "step": 1699
    },
    {
      "epoch": 0.7723761926397092,
      "grad_norm": 4.988912105560303,
      "learning_rate": 0.0007483595299862659,
      "loss": 1.4142,
      "step": 1700
    },
    {
      "epoch": 0.7728305315765561,
      "grad_norm": 4.599401473999023,
      "learning_rate": 0.0007482069281245232,
      "loss": 1.0,
      "step": 1701
    },
    {
      "epoch": 0.773284870513403,
      "grad_norm": 4.370307445526123,
      "learning_rate": 0.0007480543262627805,
      "loss": 0.6151,
      "step": 1702
    },
    {
      "epoch": 0.7737392094502499,
      "grad_norm": 6.181445121765137,
      "learning_rate": 0.0007479017244010377,
      "loss": 0.5831,
      "step": 1703
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 4.886289119720459,
      "learning_rate": 0.0007477491225392949,
      "loss": 0.9631,
      "step": 1704
    },
    {
      "epoch": 0.7746478873239436,
      "grad_norm": 6.923476219177246,
      "learning_rate": 0.0007475965206775523,
      "loss": 0.7589,
      "step": 1705
    },
    {
      "epoch": 0.7751022262607905,
      "grad_norm": 6.887092590332031,
      "learning_rate": 0.0007474439188158095,
      "loss": 1.5093,
      "step": 1706
    },
    {
      "epoch": 0.7755565651976375,
      "grad_norm": 5.323845863342285,
      "learning_rate": 0.0007472913169540668,
      "loss": 0.9897,
      "step": 1707
    },
    {
      "epoch": 0.7760109041344844,
      "grad_norm": 5.029244899749756,
      "learning_rate": 0.0007471387150923242,
      "loss": 0.5427,
      "step": 1708
    },
    {
      "epoch": 0.7764652430713312,
      "grad_norm": 5.758382797241211,
      "learning_rate": 0.0007469861132305814,
      "loss": 0.7495,
      "step": 1709
    },
    {
      "epoch": 0.7769195820081781,
      "grad_norm": 5.757443904876709,
      "learning_rate": 0.0007468335113688387,
      "loss": 1.0312,
      "step": 1710
    },
    {
      "epoch": 0.777373920945025,
      "grad_norm": 17.89948272705078,
      "learning_rate": 0.000746680909507096,
      "loss": 2.0773,
      "step": 1711
    },
    {
      "epoch": 0.7778282598818719,
      "grad_norm": 6.717979431152344,
      "learning_rate": 0.0007465283076453533,
      "loss": 1.4686,
      "step": 1712
    },
    {
      "epoch": 0.7782825988187188,
      "grad_norm": 4.78776741027832,
      "learning_rate": 0.0007463757057836106,
      "loss": 0.8591,
      "step": 1713
    },
    {
      "epoch": 0.7787369377555656,
      "grad_norm": 4.390621662139893,
      "learning_rate": 0.0007462231039218679,
      "loss": 0.7471,
      "step": 1714
    },
    {
      "epoch": 0.7791912766924125,
      "grad_norm": 7.507774829864502,
      "learning_rate": 0.0007460705020601252,
      "loss": 1.2579,
      "step": 1715
    },
    {
      "epoch": 0.7796456156292594,
      "grad_norm": 6.770198345184326,
      "learning_rate": 0.0007459179001983824,
      "loss": 0.8653,
      "step": 1716
    },
    {
      "epoch": 0.7800999545661064,
      "grad_norm": 8.512845993041992,
      "learning_rate": 0.0007457652983366398,
      "loss": 0.9269,
      "step": 1717
    },
    {
      "epoch": 0.7805542935029532,
      "grad_norm": 6.145389556884766,
      "learning_rate": 0.0007456126964748971,
      "loss": 1.2347,
      "step": 1718
    },
    {
      "epoch": 0.7810086324398001,
      "grad_norm": 5.091829299926758,
      "learning_rate": 0.0007454600946131543,
      "loss": 0.7127,
      "step": 1719
    },
    {
      "epoch": 0.781462971376647,
      "grad_norm": 5.0236406326293945,
      "learning_rate": 0.0007453074927514117,
      "loss": 0.7501,
      "step": 1720
    },
    {
      "epoch": 0.7819173103134939,
      "grad_norm": 6.669018268585205,
      "learning_rate": 0.0007451548908896689,
      "loss": 1.4914,
      "step": 1721
    },
    {
      "epoch": 0.7823716492503408,
      "grad_norm": 3.4523611068725586,
      "learning_rate": 0.0007450022890279261,
      "loss": 0.3122,
      "step": 1722
    },
    {
      "epoch": 0.7828259881871876,
      "grad_norm": 10.143718719482422,
      "learning_rate": 0.0007448496871661834,
      "loss": 1.0083,
      "step": 1723
    },
    {
      "epoch": 0.7832803271240345,
      "grad_norm": 2.423574447631836,
      "learning_rate": 0.0007446970853044407,
      "loss": 0.3154,
      "step": 1724
    },
    {
      "epoch": 0.7837346660608814,
      "grad_norm": 4.852202415466309,
      "learning_rate": 0.000744544483442698,
      "loss": 0.9623,
      "step": 1725
    },
    {
      "epoch": 0.7841890049977283,
      "grad_norm": 4.977994918823242,
      "learning_rate": 0.0007443918815809553,
      "loss": 1.1576,
      "step": 1726
    },
    {
      "epoch": 0.7846433439345752,
      "grad_norm": 6.673286437988281,
      "learning_rate": 0.0007442392797192126,
      "loss": 0.9625,
      "step": 1727
    },
    {
      "epoch": 0.7850976828714221,
      "grad_norm": 4.40566873550415,
      "learning_rate": 0.0007440866778574698,
      "loss": 0.5228,
      "step": 1728
    },
    {
      "epoch": 0.785552021808269,
      "grad_norm": 8.336620330810547,
      "learning_rate": 0.0007439340759957272,
      "loss": 1.759,
      "step": 1729
    },
    {
      "epoch": 0.7860063607451159,
      "grad_norm": 6.658580303192139,
      "learning_rate": 0.0007437814741339845,
      "loss": 0.919,
      "step": 1730
    },
    {
      "epoch": 0.7864606996819627,
      "grad_norm": 3.516814708709717,
      "learning_rate": 0.0007436288722722417,
      "loss": 0.5198,
      "step": 1731
    },
    {
      "epoch": 0.7869150386188096,
      "grad_norm": 7.029669761657715,
      "learning_rate": 0.0007434762704104991,
      "loss": 0.7626,
      "step": 1732
    },
    {
      "epoch": 0.7873693775556565,
      "grad_norm": 6.825895309448242,
      "learning_rate": 0.0007433236685487563,
      "loss": 0.9518,
      "step": 1733
    },
    {
      "epoch": 0.7878237164925034,
      "grad_norm": 6.938842296600342,
      "learning_rate": 0.0007431710666870136,
      "loss": 1.0041,
      "step": 1734
    },
    {
      "epoch": 0.7882780554293503,
      "grad_norm": 8.446057319641113,
      "learning_rate": 0.000743018464825271,
      "loss": 1.4962,
      "step": 1735
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 3.173511028289795,
      "learning_rate": 0.0007428658629635282,
      "loss": 0.3971,
      "step": 1736
    },
    {
      "epoch": 0.7891867333030441,
      "grad_norm": 7.380854606628418,
      "learning_rate": 0.0007427132611017855,
      "loss": 1.7504,
      "step": 1737
    },
    {
      "epoch": 0.789641072239891,
      "grad_norm": 4.066669464111328,
      "learning_rate": 0.0007425606592400428,
      "loss": 0.4162,
      "step": 1738
    },
    {
      "epoch": 0.7900954111767379,
      "grad_norm": 4.5273756980896,
      "learning_rate": 0.0007424080573783001,
      "loss": 0.6166,
      "step": 1739
    },
    {
      "epoch": 0.7905497501135847,
      "grad_norm": 3.9709744453430176,
      "learning_rate": 0.0007422554555165572,
      "loss": 0.3963,
      "step": 1740
    },
    {
      "epoch": 0.7910040890504316,
      "grad_norm": 4.944493293762207,
      "learning_rate": 0.0007421028536548146,
      "loss": 0.959,
      "step": 1741
    },
    {
      "epoch": 0.7914584279872785,
      "grad_norm": 5.835123538970947,
      "learning_rate": 0.0007419502517930719,
      "loss": 0.9646,
      "step": 1742
    },
    {
      "epoch": 0.7919127669241254,
      "grad_norm": 4.559453010559082,
      "learning_rate": 0.0007417976499313291,
      "loss": 1.0965,
      "step": 1743
    },
    {
      "epoch": 0.7923671058609723,
      "grad_norm": 7.348373889923096,
      "learning_rate": 0.0007416450480695865,
      "loss": 1.2508,
      "step": 1744
    },
    {
      "epoch": 0.7928214447978191,
      "grad_norm": 7.297049045562744,
      "learning_rate": 0.0007414924462078437,
      "loss": 0.7523,
      "step": 1745
    },
    {
      "epoch": 0.793275783734666,
      "grad_norm": 3.630253791809082,
      "learning_rate": 0.000741339844346101,
      "loss": 0.7229,
      "step": 1746
    },
    {
      "epoch": 0.793730122671513,
      "grad_norm": 4.479067325592041,
      "learning_rate": 0.0007411872424843584,
      "loss": 0.5932,
      "step": 1747
    },
    {
      "epoch": 0.7941844616083599,
      "grad_norm": 7.232802867889404,
      "learning_rate": 0.0007410346406226156,
      "loss": 1.5923,
      "step": 1748
    },
    {
      "epoch": 0.7946388005452067,
      "grad_norm": 5.384496212005615,
      "learning_rate": 0.0007408820387608729,
      "loss": 1.1077,
      "step": 1749
    },
    {
      "epoch": 0.7950931394820536,
      "grad_norm": 3.11967134475708,
      "learning_rate": 0.0007407294368991302,
      "loss": 0.3546,
      "step": 1750
    },
    {
      "epoch": 0.7955474784189005,
      "grad_norm": 5.280231475830078,
      "learning_rate": 0.0007405768350373875,
      "loss": 1.1249,
      "step": 1751
    },
    {
      "epoch": 0.7960018173557474,
      "grad_norm": 4.573071479797363,
      "learning_rate": 0.0007404242331756448,
      "loss": 0.6613,
      "step": 1752
    },
    {
      "epoch": 0.7964561562925943,
      "grad_norm": 7.660194396972656,
      "learning_rate": 0.0007402716313139021,
      "loss": 1.4079,
      "step": 1753
    },
    {
      "epoch": 0.7969104952294411,
      "grad_norm": 7.905807018280029,
      "learning_rate": 0.0007401190294521594,
      "loss": 1.2013,
      "step": 1754
    },
    {
      "epoch": 0.797364834166288,
      "grad_norm": 2.716914653778076,
      "learning_rate": 0.0007399664275904166,
      "loss": 0.2603,
      "step": 1755
    },
    {
      "epoch": 0.797819173103135,
      "grad_norm": 5.549619674682617,
      "learning_rate": 0.000739813825728674,
      "loss": 1.0474,
      "step": 1756
    },
    {
      "epoch": 0.7982735120399819,
      "grad_norm": 3.3877310752868652,
      "learning_rate": 0.0007396612238669313,
      "loss": 0.8083,
      "step": 1757
    },
    {
      "epoch": 0.7987278509768287,
      "grad_norm": 4.99025297164917,
      "learning_rate": 0.0007395086220051884,
      "loss": 1.1539,
      "step": 1758
    },
    {
      "epoch": 0.7991821899136756,
      "grad_norm": 6.229437828063965,
      "learning_rate": 0.0007393560201434458,
      "loss": 1.2304,
      "step": 1759
    },
    {
      "epoch": 0.7996365288505225,
      "grad_norm": 4.519434928894043,
      "learning_rate": 0.000739203418281703,
      "loss": 0.5344,
      "step": 1760
    },
    {
      "epoch": 0.8000908677873694,
      "grad_norm": 3.0162479877471924,
      "learning_rate": 0.0007390508164199603,
      "loss": 0.41,
      "step": 1761
    },
    {
      "epoch": 0.8005452067242163,
      "grad_norm": 6.627788543701172,
      "learning_rate": 0.0007388982145582176,
      "loss": 1.7461,
      "step": 1762
    },
    {
      "epoch": 0.8009995456610631,
      "grad_norm": 4.341945171356201,
      "learning_rate": 0.0007387456126964749,
      "loss": 0.7238,
      "step": 1763
    },
    {
      "epoch": 0.80145388459791,
      "grad_norm": 4.2279558181762695,
      "learning_rate": 0.0007385930108347322,
      "loss": 0.4246,
      "step": 1764
    },
    {
      "epoch": 0.8019082235347569,
      "grad_norm": 5.881608009338379,
      "learning_rate": 0.0007384404089729895,
      "loss": 1.291,
      "step": 1765
    },
    {
      "epoch": 0.8023625624716039,
      "grad_norm": 4.099097728729248,
      "learning_rate": 0.0007382878071112468,
      "loss": 0.4485,
      "step": 1766
    },
    {
      "epoch": 0.8028169014084507,
      "grad_norm": 4.7362871170043945,
      "learning_rate": 0.000738135205249504,
      "loss": 0.9016,
      "step": 1767
    },
    {
      "epoch": 0.8032712403452976,
      "grad_norm": 5.212220191955566,
      "learning_rate": 0.0007379826033877614,
      "loss": 0.8551,
      "step": 1768
    },
    {
      "epoch": 0.8037255792821445,
      "grad_norm": 5.721065521240234,
      "learning_rate": 0.0007378300015260187,
      "loss": 0.882,
      "step": 1769
    },
    {
      "epoch": 0.8041799182189914,
      "grad_norm": 3.469090700149536,
      "learning_rate": 0.0007376773996642759,
      "loss": 0.2671,
      "step": 1770
    },
    {
      "epoch": 0.8046342571558383,
      "grad_norm": 10.323538780212402,
      "learning_rate": 0.0007375247978025333,
      "loss": 0.6668,
      "step": 1771
    },
    {
      "epoch": 0.8050885960926851,
      "grad_norm": 3.2319443225860596,
      "learning_rate": 0.0007373721959407905,
      "loss": 0.3787,
      "step": 1772
    },
    {
      "epoch": 0.805542935029532,
      "grad_norm": 6.987828254699707,
      "learning_rate": 0.0007372195940790478,
      "loss": 0.7465,
      "step": 1773
    },
    {
      "epoch": 0.8059972739663789,
      "grad_norm": 7.655574798583984,
      "learning_rate": 0.0007370669922173052,
      "loss": 1.8912,
      "step": 1774
    },
    {
      "epoch": 0.8064516129032258,
      "grad_norm": 10.028095245361328,
      "learning_rate": 0.0007369143903555624,
      "loss": 2.1226,
      "step": 1775
    },
    {
      "epoch": 0.8069059518400727,
      "grad_norm": 5.089249134063721,
      "learning_rate": 0.0007367617884938196,
      "loss": 0.763,
      "step": 1776
    },
    {
      "epoch": 0.8073602907769196,
      "grad_norm": 6.949735641479492,
      "learning_rate": 0.0007366091866320769,
      "loss": 0.9887,
      "step": 1777
    },
    {
      "epoch": 0.8078146297137665,
      "grad_norm": 6.018179893493652,
      "learning_rate": 0.0007364565847703342,
      "loss": 1.4703,
      "step": 1778
    },
    {
      "epoch": 0.8082689686506134,
      "grad_norm": 3.6515347957611084,
      "learning_rate": 0.0007363039829085914,
      "loss": 0.3964,
      "step": 1779
    },
    {
      "epoch": 0.8087233075874602,
      "grad_norm": 6.471455097198486,
      "learning_rate": 0.0007361513810468488,
      "loss": 1.2641,
      "step": 1780
    },
    {
      "epoch": 0.8091776465243071,
      "grad_norm": 4.729971408843994,
      "learning_rate": 0.0007359987791851061,
      "loss": 0.5695,
      "step": 1781
    },
    {
      "epoch": 0.809631985461154,
      "grad_norm": 7.841596603393555,
      "learning_rate": 0.0007358461773233633,
      "loss": 1.1882,
      "step": 1782
    },
    {
      "epoch": 0.8100863243980009,
      "grad_norm": 5.986236095428467,
      "learning_rate": 0.0007356935754616207,
      "loss": 0.6714,
      "step": 1783
    },
    {
      "epoch": 0.8105406633348478,
      "grad_norm": 6.701154708862305,
      "learning_rate": 0.0007355409735998779,
      "loss": 0.6167,
      "step": 1784
    },
    {
      "epoch": 0.8109950022716946,
      "grad_norm": 8.643529891967773,
      "learning_rate": 0.0007353883717381352,
      "loss": 1.2355,
      "step": 1785
    },
    {
      "epoch": 0.8114493412085416,
      "grad_norm": 6.616333961486816,
      "learning_rate": 0.0007352357698763926,
      "loss": 1.3891,
      "step": 1786
    },
    {
      "epoch": 0.8119036801453885,
      "grad_norm": 7.736835956573486,
      "learning_rate": 0.0007350831680146498,
      "loss": 1.2452,
      "step": 1787
    },
    {
      "epoch": 0.8123580190822354,
      "grad_norm": 7.108563423156738,
      "learning_rate": 0.0007349305661529071,
      "loss": 0.6932,
      "step": 1788
    },
    {
      "epoch": 0.8128123580190822,
      "grad_norm": 6.60685396194458,
      "learning_rate": 0.0007347779642911644,
      "loss": 1.3226,
      "step": 1789
    },
    {
      "epoch": 0.8132666969559291,
      "grad_norm": 6.902732849121094,
      "learning_rate": 0.0007346253624294217,
      "loss": 0.8349,
      "step": 1790
    },
    {
      "epoch": 0.813721035892776,
      "grad_norm": 4.7068657875061035,
      "learning_rate": 0.0007344727605676789,
      "loss": 0.8939,
      "step": 1791
    },
    {
      "epoch": 0.8141753748296229,
      "grad_norm": 5.200576305389404,
      "learning_rate": 0.0007343201587059363,
      "loss": 1.0515,
      "step": 1792
    },
    {
      "epoch": 0.8146297137664698,
      "grad_norm": 5.960424900054932,
      "learning_rate": 0.0007341675568441936,
      "loss": 1.0383,
      "step": 1793
    },
    {
      "epoch": 0.8150840527033166,
      "grad_norm": 5.234079360961914,
      "learning_rate": 0.0007340149549824508,
      "loss": 0.9445,
      "step": 1794
    },
    {
      "epoch": 0.8155383916401635,
      "grad_norm": 3.396192789077759,
      "learning_rate": 0.0007338623531207081,
      "loss": 0.6497,
      "step": 1795
    },
    {
      "epoch": 0.8159927305770105,
      "grad_norm": 2.605476140975952,
      "learning_rate": 0.0007337097512589653,
      "loss": 0.2615,
      "step": 1796
    },
    {
      "epoch": 0.8164470695138574,
      "grad_norm": 5.556195259094238,
      "learning_rate": 0.0007335571493972226,
      "loss": 0.8271,
      "step": 1797
    },
    {
      "epoch": 0.8169014084507042,
      "grad_norm": 6.7656025886535645,
      "learning_rate": 0.00073340454753548,
      "loss": 1.3223,
      "step": 1798
    },
    {
      "epoch": 0.8173557473875511,
      "grad_norm": 7.054252624511719,
      "learning_rate": 0.0007332519456737372,
      "loss": 0.7383,
      "step": 1799
    },
    {
      "epoch": 0.817810086324398,
      "grad_norm": 5.50374174118042,
      "learning_rate": 0.0007330993438119945,
      "loss": 1.1818,
      "step": 1800
    },
    {
      "epoch": 0.8182644252612449,
      "grad_norm": 6.3486175537109375,
      "learning_rate": 0.0007329467419502518,
      "loss": 0.8965,
      "step": 1801
    },
    {
      "epoch": 0.8187187641980918,
      "grad_norm": 4.901193141937256,
      "learning_rate": 0.0007327941400885091,
      "loss": 0.8484,
      "step": 1802
    },
    {
      "epoch": 0.8191731031349386,
      "grad_norm": 4.3269853591918945,
      "learning_rate": 0.0007326415382267663,
      "loss": 0.3125,
      "step": 1803
    },
    {
      "epoch": 0.8196274420717855,
      "grad_norm": 7.678954601287842,
      "learning_rate": 0.0007324889363650237,
      "loss": 1.6111,
      "step": 1804
    },
    {
      "epoch": 0.8200817810086325,
      "grad_norm": 5.734117031097412,
      "learning_rate": 0.000732336334503281,
      "loss": 1.022,
      "step": 1805
    },
    {
      "epoch": 0.8205361199454794,
      "grad_norm": 10.40030288696289,
      "learning_rate": 0.0007321837326415382,
      "loss": 0.9908,
      "step": 1806
    },
    {
      "epoch": 0.8209904588823262,
      "grad_norm": 10.432741165161133,
      "learning_rate": 0.0007320311307797956,
      "loss": 2.4123,
      "step": 1807
    },
    {
      "epoch": 0.8214447978191731,
      "grad_norm": 7.810321807861328,
      "learning_rate": 0.0007318785289180528,
      "loss": 2.1518,
      "step": 1808
    },
    {
      "epoch": 0.82189913675602,
      "grad_norm": 4.6946282386779785,
      "learning_rate": 0.0007317259270563101,
      "loss": 0.7509,
      "step": 1809
    },
    {
      "epoch": 0.8223534756928669,
      "grad_norm": 5.418595790863037,
      "learning_rate": 0.0007315733251945675,
      "loss": 0.7142,
      "step": 1810
    },
    {
      "epoch": 0.8228078146297138,
      "grad_norm": 6.26311731338501,
      "learning_rate": 0.0007314207233328247,
      "loss": 0.7122,
      "step": 1811
    },
    {
      "epoch": 0.8232621535665606,
      "grad_norm": 6.581384658813477,
      "learning_rate": 0.000731268121471082,
      "loss": 1.4393,
      "step": 1812
    },
    {
      "epoch": 0.8237164925034075,
      "grad_norm": 2.869912624359131,
      "learning_rate": 0.0007311155196093392,
      "loss": 0.4808,
      "step": 1813
    },
    {
      "epoch": 0.8241708314402544,
      "grad_norm": 3.541003704071045,
      "learning_rate": 0.0007309629177475965,
      "loss": 0.504,
      "step": 1814
    },
    {
      "epoch": 0.8246251703771014,
      "grad_norm": 6.688127040863037,
      "learning_rate": 0.0007308103158858537,
      "loss": 1.4102,
      "step": 1815
    },
    {
      "epoch": 0.8250795093139482,
      "grad_norm": 6.942176342010498,
      "learning_rate": 0.0007306577140241111,
      "loss": 1.4597,
      "step": 1816
    },
    {
      "epoch": 0.8255338482507951,
      "grad_norm": 5.932709217071533,
      "learning_rate": 0.0007305051121623684,
      "loss": 1.0327,
      "step": 1817
    },
    {
      "epoch": 0.825988187187642,
      "grad_norm": 6.758030891418457,
      "learning_rate": 0.0007303525103006256,
      "loss": 0.8473,
      "step": 1818
    },
    {
      "epoch": 0.8264425261244889,
      "grad_norm": 7.3684234619140625,
      "learning_rate": 0.000730199908438883,
      "loss": 1.858,
      "step": 1819
    },
    {
      "epoch": 0.8268968650613358,
      "grad_norm": 8.905669212341309,
      "learning_rate": 0.0007300473065771402,
      "loss": 1.5766,
      "step": 1820
    },
    {
      "epoch": 0.8273512039981826,
      "grad_norm": 6.20892858505249,
      "learning_rate": 0.0007298947047153975,
      "loss": 1.3151,
      "step": 1821
    },
    {
      "epoch": 0.8278055429350295,
      "grad_norm": 8.539573669433594,
      "learning_rate": 0.0007297421028536549,
      "loss": 1.7091,
      "step": 1822
    },
    {
      "epoch": 0.8282598818718764,
      "grad_norm": 4.137585163116455,
      "learning_rate": 0.0007295895009919121,
      "loss": 1.2406,
      "step": 1823
    },
    {
      "epoch": 0.8287142208087233,
      "grad_norm": 3.8927860260009766,
      "learning_rate": 0.0007294368991301694,
      "loss": 0.6067,
      "step": 1824
    },
    {
      "epoch": 0.8291685597455702,
      "grad_norm": 5.993590354919434,
      "learning_rate": 0.0007292842972684267,
      "loss": 0.5672,
      "step": 1825
    },
    {
      "epoch": 0.8296228986824171,
      "grad_norm": 4.685159206390381,
      "learning_rate": 0.000729131695406684,
      "loss": 0.5205,
      "step": 1826
    },
    {
      "epoch": 0.830077237619264,
      "grad_norm": 6.563284873962402,
      "learning_rate": 0.0007289790935449413,
      "loss": 1.1197,
      "step": 1827
    },
    {
      "epoch": 0.8305315765561109,
      "grad_norm": 6.0055832862854,
      "learning_rate": 0.0007288264916831986,
      "loss": 1.0767,
      "step": 1828
    },
    {
      "epoch": 0.8309859154929577,
      "grad_norm": 6.123500823974609,
      "learning_rate": 0.0007286738898214559,
      "loss": 0.9821,
      "step": 1829
    },
    {
      "epoch": 0.8314402544298046,
      "grad_norm": 7.038214206695557,
      "learning_rate": 0.0007285212879597131,
      "loss": 0.4442,
      "step": 1830
    },
    {
      "epoch": 0.8318945933666515,
      "grad_norm": 4.880025386810303,
      "learning_rate": 0.0007283686860979704,
      "loss": 0.5378,
      "step": 1831
    },
    {
      "epoch": 0.8323489323034984,
      "grad_norm": 3.7434487342834473,
      "learning_rate": 0.0007282160842362276,
      "loss": 0.5596,
      "step": 1832
    },
    {
      "epoch": 0.8328032712403453,
      "grad_norm": 8.969287872314453,
      "learning_rate": 0.0007280634823744849,
      "loss": 1.6091,
      "step": 1833
    },
    {
      "epoch": 0.8332576101771921,
      "grad_norm": 6.876681804656982,
      "learning_rate": 0.0007279108805127423,
      "loss": 1.2113,
      "step": 1834
    },
    {
      "epoch": 0.8337119491140391,
      "grad_norm": 5.8361592292785645,
      "learning_rate": 0.0007277582786509995,
      "loss": 1.017,
      "step": 1835
    },
    {
      "epoch": 0.834166288050886,
      "grad_norm": 9.343118667602539,
      "learning_rate": 0.0007276056767892568,
      "loss": 0.9007,
      "step": 1836
    },
    {
      "epoch": 0.8346206269877329,
      "grad_norm": 14.064599990844727,
      "learning_rate": 0.0007274530749275141,
      "loss": 1.701,
      "step": 1837
    },
    {
      "epoch": 0.8350749659245797,
      "grad_norm": 5.453215599060059,
      "learning_rate": 0.0007273004730657714,
      "loss": 0.9605,
      "step": 1838
    },
    {
      "epoch": 0.8355293048614266,
      "grad_norm": 8.192866325378418,
      "learning_rate": 0.0007271478712040287,
      "loss": 2.493,
      "step": 1839
    },
    {
      "epoch": 0.8359836437982735,
      "grad_norm": 6.940364837646484,
      "learning_rate": 0.000726995269342286,
      "loss": 1.8251,
      "step": 1840
    },
    {
      "epoch": 0.8364379827351204,
      "grad_norm": 6.204977512359619,
      "learning_rate": 0.0007268426674805433,
      "loss": 0.9884,
      "step": 1841
    },
    {
      "epoch": 0.8368923216719673,
      "grad_norm": 6.997471332550049,
      "learning_rate": 0.0007266900656188005,
      "loss": 1.0378,
      "step": 1842
    },
    {
      "epoch": 0.8373466606088141,
      "grad_norm": 5.668605804443359,
      "learning_rate": 0.0007265374637570579,
      "loss": 0.6817,
      "step": 1843
    },
    {
      "epoch": 0.8378009995456611,
      "grad_norm": 7.069530487060547,
      "learning_rate": 0.0007263848618953152,
      "loss": 0.8249,
      "step": 1844
    },
    {
      "epoch": 0.838255338482508,
      "grad_norm": 7.501122951507568,
      "learning_rate": 0.0007262322600335724,
      "loss": 1.2926,
      "step": 1845
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 7.877874851226807,
      "learning_rate": 0.0007260796581718298,
      "loss": 1.742,
      "step": 1846
    },
    {
      "epoch": 0.8391640163562017,
      "grad_norm": 6.3793206214904785,
      "learning_rate": 0.000725927056310087,
      "loss": 0.8588,
      "step": 1847
    },
    {
      "epoch": 0.8396183552930486,
      "grad_norm": 8.79432487487793,
      "learning_rate": 0.0007257744544483443,
      "loss": 1.1639,
      "step": 1848
    },
    {
      "epoch": 0.8400726942298955,
      "grad_norm": 4.240730285644531,
      "learning_rate": 0.0007256218525866015,
      "loss": 0.8709,
      "step": 1849
    },
    {
      "epoch": 0.8405270331667424,
      "grad_norm": 4.2784624099731445,
      "learning_rate": 0.0007254692507248588,
      "loss": 0.5233,
      "step": 1850
    },
    {
      "epoch": 0.8409813721035893,
      "grad_norm": 2.969538450241089,
      "learning_rate": 0.0007253166488631161,
      "loss": 0.5829,
      "step": 1851
    },
    {
      "epoch": 0.8414357110404361,
      "grad_norm": 5.4641852378845215,
      "learning_rate": 0.0007251640470013734,
      "loss": 0.7332,
      "step": 1852
    },
    {
      "epoch": 0.841890049977283,
      "grad_norm": 5.894360065460205,
      "learning_rate": 0.0007250114451396307,
      "loss": 0.7876,
      "step": 1853
    },
    {
      "epoch": 0.84234438891413,
      "grad_norm": 5.752452373504639,
      "learning_rate": 0.0007248588432778879,
      "loss": 1.2132,
      "step": 1854
    },
    {
      "epoch": 0.8427987278509769,
      "grad_norm": 3.9444353580474854,
      "learning_rate": 0.0007247062414161453,
      "loss": 0.7134,
      "step": 1855
    },
    {
      "epoch": 0.8432530667878237,
      "grad_norm": 4.319251537322998,
      "learning_rate": 0.0007245536395544026,
      "loss": 0.4658,
      "step": 1856
    },
    {
      "epoch": 0.8437074057246706,
      "grad_norm": 5.848561763763428,
      "learning_rate": 0.0007244010376926598,
      "loss": 0.9482,
      "step": 1857
    },
    {
      "epoch": 0.8441617446615175,
      "grad_norm": 7.805546283721924,
      "learning_rate": 0.0007242484358309172,
      "loss": 0.9119,
      "step": 1858
    },
    {
      "epoch": 0.8446160835983644,
      "grad_norm": 6.325373649597168,
      "learning_rate": 0.0007240958339691744,
      "loss": 0.4798,
      "step": 1859
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 8.164684295654297,
      "learning_rate": 0.0007239432321074317,
      "loss": 1.2866,
      "step": 1860
    },
    {
      "epoch": 0.8455247614720581,
      "grad_norm": 7.494873523712158,
      "learning_rate": 0.0007237906302456891,
      "loss": 1.4496,
      "step": 1861
    },
    {
      "epoch": 0.845979100408905,
      "grad_norm": 4.397274971008301,
      "learning_rate": 0.0007236380283839463,
      "loss": 0.9527,
      "step": 1862
    },
    {
      "epoch": 0.8464334393457519,
      "grad_norm": 4.942344665527344,
      "learning_rate": 0.0007234854265222036,
      "loss": 0.9766,
      "step": 1863
    },
    {
      "epoch": 0.8468877782825989,
      "grad_norm": 5.646752834320068,
      "learning_rate": 0.0007233328246604609,
      "loss": 1.7562,
      "step": 1864
    },
    {
      "epoch": 0.8473421172194457,
      "grad_norm": 8.126665115356445,
      "learning_rate": 0.0007231802227987182,
      "loss": 1.4772,
      "step": 1865
    },
    {
      "epoch": 0.8477964561562926,
      "grad_norm": 6.436826705932617,
      "learning_rate": 0.0007230276209369754,
      "loss": 1.8961,
      "step": 1866
    },
    {
      "epoch": 0.8482507950931395,
      "grad_norm": 8.196511268615723,
      "learning_rate": 0.0007228750190752328,
      "loss": 1.011,
      "step": 1867
    },
    {
      "epoch": 0.8487051340299864,
      "grad_norm": 6.160547733306885,
      "learning_rate": 0.00072272241721349,
      "loss": 0.9505,
      "step": 1868
    },
    {
      "epoch": 0.8491594729668333,
      "grad_norm": 6.139994144439697,
      "learning_rate": 0.0007225698153517472,
      "loss": 0.918,
      "step": 1869
    },
    {
      "epoch": 0.8496138119036801,
      "grad_norm": 8.811053276062012,
      "learning_rate": 0.0007224172134900046,
      "loss": 1.1765,
      "step": 1870
    },
    {
      "epoch": 0.850068150840527,
      "grad_norm": 7.752175331115723,
      "learning_rate": 0.0007222646116282618,
      "loss": 0.946,
      "step": 1871
    },
    {
      "epoch": 0.8505224897773739,
      "grad_norm": 4.393950462341309,
      "learning_rate": 0.0007221120097665191,
      "loss": 0.49,
      "step": 1872
    },
    {
      "epoch": 0.8509768287142208,
      "grad_norm": 4.8644208908081055,
      "learning_rate": 0.0007219594079047765,
      "loss": 0.8355,
      "step": 1873
    },
    {
      "epoch": 0.8514311676510677,
      "grad_norm": 6.503478527069092,
      "learning_rate": 0.0007218068060430337,
      "loss": 1.7693,
      "step": 1874
    },
    {
      "epoch": 0.8518855065879146,
      "grad_norm": 5.191594123840332,
      "learning_rate": 0.000721654204181291,
      "loss": 0.8513,
      "step": 1875
    },
    {
      "epoch": 0.8523398455247615,
      "grad_norm": 6.490695953369141,
      "learning_rate": 0.0007215016023195483,
      "loss": 1.4365,
      "step": 1876
    },
    {
      "epoch": 0.8527941844616084,
      "grad_norm": 5.439698219299316,
      "learning_rate": 0.0007213490004578056,
      "loss": 0.6865,
      "step": 1877
    },
    {
      "epoch": 0.8532485233984552,
      "grad_norm": 6.476342678070068,
      "learning_rate": 0.0007211963985960629,
      "loss": 1.0204,
      "step": 1878
    },
    {
      "epoch": 0.8537028623353021,
      "grad_norm": 3.7910218238830566,
      "learning_rate": 0.0007210437967343202,
      "loss": 0.4433,
      "step": 1879
    },
    {
      "epoch": 0.854157201272149,
      "grad_norm": 6.71093225479126,
      "learning_rate": 0.0007208911948725775,
      "loss": 1.3994,
      "step": 1880
    },
    {
      "epoch": 0.8546115402089959,
      "grad_norm": 5.0969390869140625,
      "learning_rate": 0.0007207385930108347,
      "loss": 1.0751,
      "step": 1881
    },
    {
      "epoch": 0.8550658791458428,
      "grad_norm": 3.127092123031616,
      "learning_rate": 0.0007205859911490921,
      "loss": 0.2548,
      "step": 1882
    },
    {
      "epoch": 0.8555202180826896,
      "grad_norm": 7.107275485992432,
      "learning_rate": 0.0007204333892873494,
      "loss": 1.2255,
      "step": 1883
    },
    {
      "epoch": 0.8559745570195366,
      "grad_norm": 3.266273021697998,
      "learning_rate": 0.0007202807874256066,
      "loss": 0.4288,
      "step": 1884
    },
    {
      "epoch": 0.8564288959563835,
      "grad_norm": 5.210782527923584,
      "learning_rate": 0.000720128185563864,
      "loss": 0.6763,
      "step": 1885
    },
    {
      "epoch": 0.8568832348932304,
      "grad_norm": 7.342110633850098,
      "learning_rate": 0.0007199755837021211,
      "loss": 1.4129,
      "step": 1886
    },
    {
      "epoch": 0.8573375738300772,
      "grad_norm": 9.53869915008545,
      "learning_rate": 0.0007198229818403784,
      "loss": 0.5519,
      "step": 1887
    },
    {
      "epoch": 0.8577919127669241,
      "grad_norm": 6.55891227722168,
      "learning_rate": 0.0007196703799786357,
      "loss": 1.3736,
      "step": 1888
    },
    {
      "epoch": 0.858246251703771,
      "grad_norm": 3.9626853466033936,
      "learning_rate": 0.000719517778116893,
      "loss": 0.7924,
      "step": 1889
    },
    {
      "epoch": 0.8587005906406179,
      "grad_norm": 6.990512847900391,
      "learning_rate": 0.0007193651762551503,
      "loss": 1.1103,
      "step": 1890
    },
    {
      "epoch": 0.8591549295774648,
      "grad_norm": 8.044767379760742,
      "learning_rate": 0.0007192125743934076,
      "loss": 0.8168,
      "step": 1891
    },
    {
      "epoch": 0.8596092685143116,
      "grad_norm": 5.258362770080566,
      "learning_rate": 0.0007190599725316649,
      "loss": 0.5206,
      "step": 1892
    },
    {
      "epoch": 0.8600636074511586,
      "grad_norm": 6.528013229370117,
      "learning_rate": 0.0007189073706699221,
      "loss": 0.7153,
      "step": 1893
    },
    {
      "epoch": 0.8605179463880055,
      "grad_norm": 7.497963905334473,
      "learning_rate": 0.0007187547688081795,
      "loss": 1.1803,
      "step": 1894
    },
    {
      "epoch": 0.8609722853248524,
      "grad_norm": 5.800473690032959,
      "learning_rate": 0.0007186021669464368,
      "loss": 0.7142,
      "step": 1895
    },
    {
      "epoch": 0.8614266242616992,
      "grad_norm": 5.07067346572876,
      "learning_rate": 0.000718449565084694,
      "loss": 0.6818,
      "step": 1896
    },
    {
      "epoch": 0.8618809631985461,
      "grad_norm": 4.473248481750488,
      "learning_rate": 0.0007182969632229514,
      "loss": 0.7373,
      "step": 1897
    },
    {
      "epoch": 0.862335302135393,
      "grad_norm": 11.67205810546875,
      "learning_rate": 0.0007181443613612086,
      "loss": 2.0626,
      "step": 1898
    },
    {
      "epoch": 0.8627896410722399,
      "grad_norm": 6.872565746307373,
      "learning_rate": 0.0007179917594994659,
      "loss": 0.8128,
      "step": 1899
    },
    {
      "epoch": 0.8632439800090868,
      "grad_norm": 4.947861671447754,
      "learning_rate": 0.0007178391576377233,
      "loss": 0.6253,
      "step": 1900
    },
    {
      "epoch": 0.8636983189459336,
      "grad_norm": 5.619885444641113,
      "learning_rate": 0.0007176865557759805,
      "loss": 1.3029,
      "step": 1901
    },
    {
      "epoch": 0.8641526578827805,
      "grad_norm": 5.444053649902344,
      "learning_rate": 0.0007175339539142378,
      "loss": 1.1244,
      "step": 1902
    },
    {
      "epoch": 0.8646069968196275,
      "grad_norm": 5.304084777832031,
      "learning_rate": 0.0007173813520524951,
      "loss": 1.151,
      "step": 1903
    },
    {
      "epoch": 0.8650613357564744,
      "grad_norm": 6.978363037109375,
      "learning_rate": 0.0007172287501907523,
      "loss": 1.0198,
      "step": 1904
    },
    {
      "epoch": 0.8655156746933212,
      "grad_norm": 5.182434558868408,
      "learning_rate": 0.0007170761483290095,
      "loss": 1.2855,
      "step": 1905
    },
    {
      "epoch": 0.8659700136301681,
      "grad_norm": 7.5368876457214355,
      "learning_rate": 0.0007169235464672669,
      "loss": 0.8473,
      "step": 1906
    },
    {
      "epoch": 0.866424352567015,
      "grad_norm": 5.3685302734375,
      "learning_rate": 0.0007167709446055242,
      "loss": 0.825,
      "step": 1907
    },
    {
      "epoch": 0.8668786915038619,
      "grad_norm": 7.244801044464111,
      "learning_rate": 0.0007166183427437814,
      "loss": 1.0562,
      "step": 1908
    },
    {
      "epoch": 0.8673330304407088,
      "grad_norm": 4.107983589172363,
      "learning_rate": 0.0007164657408820388,
      "loss": 0.3071,
      "step": 1909
    },
    {
      "epoch": 0.8677873693775556,
      "grad_norm": 6.746215343475342,
      "learning_rate": 0.000716313139020296,
      "loss": 1.9459,
      "step": 1910
    },
    {
      "epoch": 0.8682417083144025,
      "grad_norm": 5.089417457580566,
      "learning_rate": 0.0007161605371585533,
      "loss": 1.5397,
      "step": 1911
    },
    {
      "epoch": 0.8686960472512494,
      "grad_norm": 4.712769031524658,
      "learning_rate": 0.0007160079352968107,
      "loss": 0.8534,
      "step": 1912
    },
    {
      "epoch": 0.8691503861880964,
      "grad_norm": 4.7029876708984375,
      "learning_rate": 0.0007158553334350679,
      "loss": 0.6969,
      "step": 1913
    },
    {
      "epoch": 0.8696047251249432,
      "grad_norm": 6.375208377838135,
      "learning_rate": 0.0007157027315733252,
      "loss": 1.0629,
      "step": 1914
    },
    {
      "epoch": 0.8700590640617901,
      "grad_norm": 4.202013969421387,
      "learning_rate": 0.0007155501297115825,
      "loss": 0.6608,
      "step": 1915
    },
    {
      "epoch": 0.870513402998637,
      "grad_norm": 4.874462127685547,
      "learning_rate": 0.0007153975278498398,
      "loss": 0.6259,
      "step": 1916
    },
    {
      "epoch": 0.8709677419354839,
      "grad_norm": 2.966601848602295,
      "learning_rate": 0.000715244925988097,
      "loss": 0.385,
      "step": 1917
    },
    {
      "epoch": 0.8714220808723308,
      "grad_norm": 5.664721965789795,
      "learning_rate": 0.0007150923241263544,
      "loss": 0.7865,
      "step": 1918
    },
    {
      "epoch": 0.8718764198091776,
      "grad_norm": 4.8374810218811035,
      "learning_rate": 0.0007149397222646117,
      "loss": 0.6713,
      "step": 1919
    },
    {
      "epoch": 0.8723307587460245,
      "grad_norm": 6.022850036621094,
      "learning_rate": 0.0007147871204028689,
      "loss": 1.3627,
      "step": 1920
    },
    {
      "epoch": 0.8727850976828714,
      "grad_norm": 6.103410720825195,
      "learning_rate": 0.0007146345185411263,
      "loss": 0.5571,
      "step": 1921
    },
    {
      "epoch": 0.8732394366197183,
      "grad_norm": 5.345830917358398,
      "learning_rate": 0.0007144819166793834,
      "loss": 0.2844,
      "step": 1922
    },
    {
      "epoch": 0.8736937755565652,
      "grad_norm": 6.236686706542969,
      "learning_rate": 0.0007143293148176407,
      "loss": 1.3729,
      "step": 1923
    },
    {
      "epoch": 0.8741481144934121,
      "grad_norm": 6.340873718261719,
      "learning_rate": 0.0007141767129558981,
      "loss": 0.45,
      "step": 1924
    },
    {
      "epoch": 0.874602453430259,
      "grad_norm": 5.220257759094238,
      "learning_rate": 0.0007140241110941553,
      "loss": 0.8306,
      "step": 1925
    },
    {
      "epoch": 0.8750567923671059,
      "grad_norm": 4.683383941650391,
      "learning_rate": 0.0007138715092324126,
      "loss": 0.4811,
      "step": 1926
    },
    {
      "epoch": 0.8755111313039527,
      "grad_norm": 4.469421863555908,
      "learning_rate": 0.0007137189073706699,
      "loss": 1.1149,
      "step": 1927
    },
    {
      "epoch": 0.8759654702407996,
      "grad_norm": 5.483222484588623,
      "learning_rate": 0.0007135663055089272,
      "loss": 0.6754,
      "step": 1928
    },
    {
      "epoch": 0.8764198091776465,
      "grad_norm": 5.927076816558838,
      "learning_rate": 0.0007134137036471844,
      "loss": 1.1382,
      "step": 1929
    },
    {
      "epoch": 0.8768741481144934,
      "grad_norm": 6.893660545349121,
      "learning_rate": 0.0007132611017854418,
      "loss": 1.3296,
      "step": 1930
    },
    {
      "epoch": 0.8773284870513403,
      "grad_norm": 5.253678798675537,
      "learning_rate": 0.0007131084999236991,
      "loss": 0.672,
      "step": 1931
    },
    {
      "epoch": 0.8777828259881872,
      "grad_norm": 7.308913230895996,
      "learning_rate": 0.0007129558980619563,
      "loss": 1.149,
      "step": 1932
    },
    {
      "epoch": 0.8782371649250341,
      "grad_norm": 7.520915508270264,
      "learning_rate": 0.0007128032962002137,
      "loss": 1.8067,
      "step": 1933
    },
    {
      "epoch": 0.878691503861881,
      "grad_norm": 3.790001392364502,
      "learning_rate": 0.0007126506943384709,
      "loss": 0.4597,
      "step": 1934
    },
    {
      "epoch": 0.8791458427987279,
      "grad_norm": 5.534965991973877,
      "learning_rate": 0.0007124980924767282,
      "loss": 0.8858,
      "step": 1935
    },
    {
      "epoch": 0.8796001817355747,
      "grad_norm": 5.704861640930176,
      "learning_rate": 0.0007123454906149856,
      "loss": 0.5513,
      "step": 1936
    },
    {
      "epoch": 0.8800545206724216,
      "grad_norm": 4.630179405212402,
      "learning_rate": 0.0007121928887532428,
      "loss": 0.7115,
      "step": 1937
    },
    {
      "epoch": 0.8805088596092685,
      "grad_norm": 7.922321796417236,
      "learning_rate": 0.0007120402868915002,
      "loss": 1.9029,
      "step": 1938
    },
    {
      "epoch": 0.8809631985461154,
      "grad_norm": 6.776327610015869,
      "learning_rate": 0.0007118876850297574,
      "loss": 0.9387,
      "step": 1939
    },
    {
      "epoch": 0.8814175374829623,
      "grad_norm": 3.4562690258026123,
      "learning_rate": 0.0007117350831680147,
      "loss": 0.3624,
      "step": 1940
    },
    {
      "epoch": 0.8818718764198091,
      "grad_norm": 5.7279372215271,
      "learning_rate": 0.0007115824813062718,
      "loss": 0.6521,
      "step": 1941
    },
    {
      "epoch": 0.8823262153566561,
      "grad_norm": 5.111639499664307,
      "learning_rate": 0.0007114298794445292,
      "loss": 0.9817,
      "step": 1942
    },
    {
      "epoch": 0.882780554293503,
      "grad_norm": 3.125272512435913,
      "learning_rate": 0.0007112772775827865,
      "loss": 0.3382,
      "step": 1943
    },
    {
      "epoch": 0.8832348932303499,
      "grad_norm": 5.420891761779785,
      "learning_rate": 0.0007111246757210437,
      "loss": 0.7491,
      "step": 1944
    },
    {
      "epoch": 0.8836892321671967,
      "grad_norm": 5.310026168823242,
      "learning_rate": 0.0007109720738593011,
      "loss": 1.1236,
      "step": 1945
    },
    {
      "epoch": 0.8841435711040436,
      "grad_norm": 5.4610443115234375,
      "learning_rate": 0.0007108194719975583,
      "loss": 0.8051,
      "step": 1946
    },
    {
      "epoch": 0.8845979100408905,
      "grad_norm": 4.037468433380127,
      "learning_rate": 0.0007106668701358157,
      "loss": 0.3783,
      "step": 1947
    },
    {
      "epoch": 0.8850522489777374,
      "grad_norm": 8.212574005126953,
      "learning_rate": 0.000710514268274073,
      "loss": 2.0641,
      "step": 1948
    },
    {
      "epoch": 0.8855065879145843,
      "grad_norm": 6.245050430297852,
      "learning_rate": 0.0007103616664123302,
      "loss": 0.6219,
      "step": 1949
    },
    {
      "epoch": 0.8859609268514311,
      "grad_norm": 6.279829502105713,
      "learning_rate": 0.0007102090645505876,
      "loss": 1.1376,
      "step": 1950
    },
    {
      "epoch": 0.886415265788278,
      "grad_norm": 5.423976898193359,
      "learning_rate": 0.0007100564626888448,
      "loss": 0.9512,
      "step": 1951
    },
    {
      "epoch": 0.886869604725125,
      "grad_norm": 5.565937519073486,
      "learning_rate": 0.0007099038608271021,
      "loss": 0.5866,
      "step": 1952
    },
    {
      "epoch": 0.8873239436619719,
      "grad_norm": 3.2887492179870605,
      "learning_rate": 0.0007097512589653595,
      "loss": 0.3798,
      "step": 1953
    },
    {
      "epoch": 0.8877782825988187,
      "grad_norm": 8.716856956481934,
      "learning_rate": 0.0007095986571036167,
      "loss": 1.2801,
      "step": 1954
    },
    {
      "epoch": 0.8882326215356656,
      "grad_norm": 5.98683500289917,
      "learning_rate": 0.000709446055241874,
      "loss": 0.6739,
      "step": 1955
    },
    {
      "epoch": 0.8886869604725125,
      "grad_norm": 3.947787284851074,
      "learning_rate": 0.0007092934533801313,
      "loss": 0.7065,
      "step": 1956
    },
    {
      "epoch": 0.8891412994093594,
      "grad_norm": 2.9420864582061768,
      "learning_rate": 0.0007091408515183886,
      "loss": 0.2127,
      "step": 1957
    },
    {
      "epoch": 0.8895956383462063,
      "grad_norm": 8.21181583404541,
      "learning_rate": 0.0007089882496566459,
      "loss": 1.4819,
      "step": 1958
    },
    {
      "epoch": 0.8900499772830531,
      "grad_norm": 3.9228971004486084,
      "learning_rate": 0.0007088356477949031,
      "loss": 0.3254,
      "step": 1959
    },
    {
      "epoch": 0.8905043162199,
      "grad_norm": 4.058672904968262,
      "learning_rate": 0.0007086830459331604,
      "loss": 0.4195,
      "step": 1960
    },
    {
      "epoch": 0.8909586551567469,
      "grad_norm": 4.327905178070068,
      "learning_rate": 0.0007085304440714176,
      "loss": 0.4077,
      "step": 1961
    },
    {
      "epoch": 0.8914129940935939,
      "grad_norm": 8.722265243530273,
      "learning_rate": 0.000708377842209675,
      "loss": 1.3232,
      "step": 1962
    },
    {
      "epoch": 0.8918673330304407,
      "grad_norm": 6.429074287414551,
      "learning_rate": 0.0007082252403479322,
      "loss": 0.9172,
      "step": 1963
    },
    {
      "epoch": 0.8923216719672876,
      "grad_norm": 2.5801069736480713,
      "learning_rate": 0.0007080726384861895,
      "loss": 0.1898,
      "step": 1964
    },
    {
      "epoch": 0.8927760109041345,
      "grad_norm": 3.4617929458618164,
      "learning_rate": 0.0007079200366244469,
      "loss": 1.1471,
      "step": 1965
    },
    {
      "epoch": 0.8932303498409814,
      "grad_norm": 7.024360179901123,
      "learning_rate": 0.0007077674347627041,
      "loss": 1.1965,
      "step": 1966
    },
    {
      "epoch": 0.8936846887778283,
      "grad_norm": 6.406704425811768,
      "learning_rate": 0.0007076148329009614,
      "loss": 1.2743,
      "step": 1967
    },
    {
      "epoch": 0.8941390277146751,
      "grad_norm": 7.127075672149658,
      "learning_rate": 0.0007074622310392187,
      "loss": 1.783,
      "step": 1968
    },
    {
      "epoch": 0.894593366651522,
      "grad_norm": 6.516239643096924,
      "learning_rate": 0.000707309629177476,
      "loss": 0.8316,
      "step": 1969
    },
    {
      "epoch": 0.8950477055883689,
      "grad_norm": 4.2204132080078125,
      "learning_rate": 0.0007071570273157333,
      "loss": 0.6816,
      "step": 1970
    },
    {
      "epoch": 0.8955020445252158,
      "grad_norm": 4.999958038330078,
      "learning_rate": 0.0007070044254539906,
      "loss": 0.8212,
      "step": 1971
    },
    {
      "epoch": 0.8959563834620627,
      "grad_norm": 3.891820192337036,
      "learning_rate": 0.0007068518235922479,
      "loss": 0.3277,
      "step": 1972
    },
    {
      "epoch": 0.8964107223989096,
      "grad_norm": 4.922902584075928,
      "learning_rate": 0.0007066992217305051,
      "loss": 0.5095,
      "step": 1973
    },
    {
      "epoch": 0.8968650613357565,
      "grad_norm": 4.895660400390625,
      "learning_rate": 0.0007065466198687625,
      "loss": 1.2226,
      "step": 1974
    },
    {
      "epoch": 0.8973194002726034,
      "grad_norm": 7.869164943695068,
      "learning_rate": 0.0007063940180070198,
      "loss": 1.1854,
      "step": 1975
    },
    {
      "epoch": 0.8977737392094502,
      "grad_norm": 6.767821788787842,
      "learning_rate": 0.000706241416145277,
      "loss": 1.1474,
      "step": 1976
    },
    {
      "epoch": 0.8982280781462971,
      "grad_norm": 6.22881555557251,
      "learning_rate": 0.0007060888142835343,
      "loss": 0.8912,
      "step": 1977
    },
    {
      "epoch": 0.898682417083144,
      "grad_norm": 6.987730503082275,
      "learning_rate": 0.0007059362124217915,
      "loss": 0.9931,
      "step": 1978
    },
    {
      "epoch": 0.8991367560199909,
      "grad_norm": 5.792884349822998,
      "learning_rate": 0.0007057836105600488,
      "loss": 1.2982,
      "step": 1979
    },
    {
      "epoch": 0.8995910949568378,
      "grad_norm": 7.034243106842041,
      "learning_rate": 0.0007056310086983061,
      "loss": 0.8764,
      "step": 1980
    },
    {
      "epoch": 0.9000454338936847,
      "grad_norm": 4.895236968994141,
      "learning_rate": 0.0007054784068365634,
      "loss": 0.8057,
      "step": 1981
    },
    {
      "epoch": 0.9004997728305316,
      "grad_norm": 5.484593868255615,
      "learning_rate": 0.0007053258049748207,
      "loss": 0.5558,
      "step": 1982
    },
    {
      "epoch": 0.9009541117673785,
      "grad_norm": 5.7938232421875,
      "learning_rate": 0.000705173203113078,
      "loss": 0.6774,
      "step": 1983
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 4.93589973449707,
      "learning_rate": 0.0007050206012513353,
      "loss": 0.5062,
      "step": 1984
    },
    {
      "epoch": 0.9018627896410722,
      "grad_norm": 2.7446393966674805,
      "learning_rate": 0.0007048679993895925,
      "loss": 0.7101,
      "step": 1985
    },
    {
      "epoch": 0.9023171285779191,
      "grad_norm": 5.69279670715332,
      "learning_rate": 0.0007047153975278499,
      "loss": 1.3304,
      "step": 1986
    },
    {
      "epoch": 0.902771467514766,
      "grad_norm": 6.147347450256348,
      "learning_rate": 0.0007045627956661072,
      "loss": 0.4917,
      "step": 1987
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 4.808304309844971,
      "learning_rate": 0.0007044101938043644,
      "loss": 0.7245,
      "step": 1988
    },
    {
      "epoch": 0.9036801453884598,
      "grad_norm": 3.0480692386627197,
      "learning_rate": 0.0007042575919426218,
      "loss": 0.5023,
      "step": 1989
    },
    {
      "epoch": 0.9041344843253066,
      "grad_norm": 4.809059143066406,
      "learning_rate": 0.000704104990080879,
      "loss": 0.7064,
      "step": 1990
    },
    {
      "epoch": 0.9045888232621536,
      "grad_norm": 5.870518207550049,
      "learning_rate": 0.0007039523882191363,
      "loss": 1.2094,
      "step": 1991
    },
    {
      "epoch": 0.9050431621990005,
      "grad_norm": 4.002080917358398,
      "learning_rate": 0.0007037997863573937,
      "loss": 0.5494,
      "step": 1992
    },
    {
      "epoch": 0.9054975011358474,
      "grad_norm": 4.997768878936768,
      "learning_rate": 0.0007036471844956509,
      "loss": 0.9387,
      "step": 1993
    },
    {
      "epoch": 0.9059518400726942,
      "grad_norm": 7.832595348358154,
      "learning_rate": 0.0007034945826339082,
      "loss": 0.8402,
      "step": 1994
    },
    {
      "epoch": 0.9064061790095411,
      "grad_norm": 7.167962551116943,
      "learning_rate": 0.0007033419807721654,
      "loss": 0.7674,
      "step": 1995
    },
    {
      "epoch": 0.906860517946388,
      "grad_norm": 4.659988880157471,
      "learning_rate": 0.0007031893789104227,
      "loss": 0.3989,
      "step": 1996
    },
    {
      "epoch": 0.9073148568832349,
      "grad_norm": 5.15620231628418,
      "learning_rate": 0.0007030367770486799,
      "loss": 0.8569,
      "step": 1997
    },
    {
      "epoch": 0.9077691958200818,
      "grad_norm": 5.403616905212402,
      "learning_rate": 0.0007028841751869373,
      "loss": 0.7395,
      "step": 1998
    },
    {
      "epoch": 0.9082235347569286,
      "grad_norm": 6.569815635681152,
      "learning_rate": 0.0007027315733251946,
      "loss": 0.7654,
      "step": 1999
    },
    {
      "epoch": 0.9086778736937755,
      "grad_norm": 6.170792579650879,
      "learning_rate": 0.0007025789714634518,
      "loss": 1.2942,
      "step": 2000
    },
    {
      "epoch": 0.9091322126306225,
      "grad_norm": 7.311605453491211,
      "learning_rate": 0.0007024263696017092,
      "loss": 1.2642,
      "step": 2001
    },
    {
      "epoch": 0.9095865515674694,
      "grad_norm": 5.122086048126221,
      "learning_rate": 0.0007022737677399664,
      "loss": 0.65,
      "step": 2002
    },
    {
      "epoch": 0.9100408905043162,
      "grad_norm": 7.785153388977051,
      "learning_rate": 0.0007021211658782237,
      "loss": 1.4613,
      "step": 2003
    },
    {
      "epoch": 0.9104952294411631,
      "grad_norm": 5.780588626861572,
      "learning_rate": 0.0007019685640164811,
      "loss": 1.1231,
      "step": 2004
    },
    {
      "epoch": 0.91094956837801,
      "grad_norm": 5.931447982788086,
      "learning_rate": 0.0007018159621547383,
      "loss": 0.6552,
      "step": 2005
    },
    {
      "epoch": 0.9114039073148569,
      "grad_norm": 4.565131187438965,
      "learning_rate": 0.0007016633602929956,
      "loss": 0.5371,
      "step": 2006
    },
    {
      "epoch": 0.9118582462517038,
      "grad_norm": 6.071425437927246,
      "learning_rate": 0.0007015107584312529,
      "loss": 1.0,
      "step": 2007
    },
    {
      "epoch": 0.9123125851885506,
      "grad_norm": 5.873400688171387,
      "learning_rate": 0.0007013581565695102,
      "loss": 1.1207,
      "step": 2008
    },
    {
      "epoch": 0.9127669241253975,
      "grad_norm": 4.6697306632995605,
      "learning_rate": 0.0007012055547077675,
      "loss": 0.3824,
      "step": 2009
    },
    {
      "epoch": 0.9132212630622444,
      "grad_norm": 3.4053189754486084,
      "learning_rate": 0.0007010529528460248,
      "loss": 0.8735,
      "step": 2010
    },
    {
      "epoch": 0.9136756019990914,
      "grad_norm": 6.547510623931885,
      "learning_rate": 0.0007009003509842821,
      "loss": 1.4251,
      "step": 2011
    },
    {
      "epoch": 0.9141299409359382,
      "grad_norm": 7.330219268798828,
      "learning_rate": 0.0007007477491225393,
      "loss": 1.3606,
      "step": 2012
    },
    {
      "epoch": 0.9145842798727851,
      "grad_norm": 5.19648551940918,
      "learning_rate": 0.0007005951472607967,
      "loss": 0.7697,
      "step": 2013
    },
    {
      "epoch": 0.915038618809632,
      "grad_norm": 4.557535171508789,
      "learning_rate": 0.0007004425453990538,
      "loss": 0.641,
      "step": 2014
    },
    {
      "epoch": 0.9154929577464789,
      "grad_norm": 5.209771156311035,
      "learning_rate": 0.0007002899435373111,
      "loss": 1.0701,
      "step": 2015
    },
    {
      "epoch": 0.9159472966833258,
      "grad_norm": 6.498602390289307,
      "learning_rate": 0.0007001373416755685,
      "loss": 1.3783,
      "step": 2016
    },
    {
      "epoch": 0.9164016356201726,
      "grad_norm": 3.1994919776916504,
      "learning_rate": 0.0006999847398138257,
      "loss": 0.2153,
      "step": 2017
    },
    {
      "epoch": 0.9168559745570195,
      "grad_norm": 4.121938228607178,
      "learning_rate": 0.000699832137952083,
      "loss": 0.4254,
      "step": 2018
    },
    {
      "epoch": 0.9173103134938664,
      "grad_norm": 5.196599960327148,
      "learning_rate": 0.0006996795360903403,
      "loss": 0.8877,
      "step": 2019
    },
    {
      "epoch": 0.9177646524307133,
      "grad_norm": 4.127929210662842,
      "learning_rate": 0.0006995269342285976,
      "loss": 0.4904,
      "step": 2020
    },
    {
      "epoch": 0.9182189913675602,
      "grad_norm": 7.0154032707214355,
      "learning_rate": 0.0006993743323668549,
      "loss": 1.6387,
      "step": 2021
    },
    {
      "epoch": 0.9186733303044071,
      "grad_norm": 10.962752342224121,
      "learning_rate": 0.0006992217305051122,
      "loss": 1.3821,
      "step": 2022
    },
    {
      "epoch": 0.919127669241254,
      "grad_norm": 5.344366550445557,
      "learning_rate": 0.0006990691286433695,
      "loss": 0.9465,
      "step": 2023
    },
    {
      "epoch": 0.9195820081781009,
      "grad_norm": 4.537943363189697,
      "learning_rate": 0.0006989165267816267,
      "loss": 1.1446,
      "step": 2024
    },
    {
      "epoch": 0.9200363471149478,
      "grad_norm": 6.657045841217041,
      "learning_rate": 0.0006987639249198841,
      "loss": 1.2017,
      "step": 2025
    },
    {
      "epoch": 0.9204906860517946,
      "grad_norm": 5.906113624572754,
      "learning_rate": 0.0006986113230581414,
      "loss": 0.8577,
      "step": 2026
    },
    {
      "epoch": 0.9209450249886415,
      "grad_norm": 7.203835487365723,
      "learning_rate": 0.0006984587211963986,
      "loss": 1.8921,
      "step": 2027
    },
    {
      "epoch": 0.9213993639254884,
      "grad_norm": 6.7763776779174805,
      "learning_rate": 0.000698306119334656,
      "loss": 0.9233,
      "step": 2028
    },
    {
      "epoch": 0.9218537028623353,
      "grad_norm": 5.944580554962158,
      "learning_rate": 0.0006981535174729132,
      "loss": 1.1253,
      "step": 2029
    },
    {
      "epoch": 0.9223080417991822,
      "grad_norm": 5.834329605102539,
      "learning_rate": 0.0006980009156111705,
      "loss": 0.7614,
      "step": 2030
    },
    {
      "epoch": 0.9227623807360291,
      "grad_norm": 6.8939409255981445,
      "learning_rate": 0.0006978483137494279,
      "loss": 1.1708,
      "step": 2031
    },
    {
      "epoch": 0.923216719672876,
      "grad_norm": 6.764132499694824,
      "learning_rate": 0.000697695711887685,
      "loss": 1.8177,
      "step": 2032
    },
    {
      "epoch": 0.9236710586097229,
      "grad_norm": 8.85224723815918,
      "learning_rate": 0.0006975431100259423,
      "loss": 1.6865,
      "step": 2033
    },
    {
      "epoch": 0.9241253975465697,
      "grad_norm": 5.285190582275391,
      "learning_rate": 0.0006973905081641996,
      "loss": 1.0406,
      "step": 2034
    },
    {
      "epoch": 0.9245797364834166,
      "grad_norm": 6.144356727600098,
      "learning_rate": 0.0006972379063024569,
      "loss": 0.9666,
      "step": 2035
    },
    {
      "epoch": 0.9250340754202635,
      "grad_norm": 7.395075798034668,
      "learning_rate": 0.0006970853044407141,
      "loss": 1.1379,
      "step": 2036
    },
    {
      "epoch": 0.9254884143571104,
      "grad_norm": 4.384398460388184,
      "learning_rate": 0.0006969327025789715,
      "loss": 0.7626,
      "step": 2037
    },
    {
      "epoch": 0.9259427532939573,
      "grad_norm": 8.474690437316895,
      "learning_rate": 0.0006967801007172288,
      "loss": 1.5839,
      "step": 2038
    },
    {
      "epoch": 0.9263970922308041,
      "grad_norm": 2.750568151473999,
      "learning_rate": 0.000696627498855486,
      "loss": 0.2265,
      "step": 2039
    },
    {
      "epoch": 0.9268514311676511,
      "grad_norm": 4.0753092765808105,
      "learning_rate": 0.0006964748969937434,
      "loss": 0.763,
      "step": 2040
    },
    {
      "epoch": 0.927305770104498,
      "grad_norm": 5.4336090087890625,
      "learning_rate": 0.0006963222951320006,
      "loss": 0.5569,
      "step": 2041
    },
    {
      "epoch": 0.9277601090413449,
      "grad_norm": 5.577949047088623,
      "learning_rate": 0.0006961696932702579,
      "loss": 0.7325,
      "step": 2042
    },
    {
      "epoch": 0.9282144479781917,
      "grad_norm": 4.524528980255127,
      "learning_rate": 0.0006960170914085153,
      "loss": 0.8214,
      "step": 2043
    },
    {
      "epoch": 0.9286687869150386,
      "grad_norm": 4.559654712677002,
      "learning_rate": 0.0006958644895467725,
      "loss": 1.2702,
      "step": 2044
    },
    {
      "epoch": 0.9291231258518855,
      "grad_norm": 8.559505462646484,
      "learning_rate": 0.0006957118876850298,
      "loss": 1.0067,
      "step": 2045
    },
    {
      "epoch": 0.9295774647887324,
      "grad_norm": 5.511291980743408,
      "learning_rate": 0.0006955592858232871,
      "loss": 0.9499,
      "step": 2046
    },
    {
      "epoch": 0.9300318037255793,
      "grad_norm": 5.140843868255615,
      "learning_rate": 0.0006954066839615444,
      "loss": 0.3305,
      "step": 2047
    },
    {
      "epoch": 0.9304861426624261,
      "grad_norm": 5.420475959777832,
      "learning_rate": 0.0006952540820998016,
      "loss": 0.7938,
      "step": 2048
    },
    {
      "epoch": 0.930940481599273,
      "grad_norm": 4.2873101234436035,
      "learning_rate": 0.000695101480238059,
      "loss": 0.4467,
      "step": 2049
    },
    {
      "epoch": 0.93139482053612,
      "grad_norm": 5.8762407302856445,
      "learning_rate": 0.0006949488783763162,
      "loss": 0.7616,
      "step": 2050
    },
    {
      "epoch": 0.9318491594729669,
      "grad_norm": 4.38421630859375,
      "learning_rate": 0.0006947962765145734,
      "loss": 0.5827,
      "step": 2051
    },
    {
      "epoch": 0.9323034984098137,
      "grad_norm": 3.084681987762451,
      "learning_rate": 0.0006946436746528308,
      "loss": 0.3552,
      "step": 2052
    },
    {
      "epoch": 0.9327578373466606,
      "grad_norm": 6.678351402282715,
      "learning_rate": 0.000694491072791088,
      "loss": 0.8168,
      "step": 2053
    },
    {
      "epoch": 0.9332121762835075,
      "grad_norm": 4.735074043273926,
      "learning_rate": 0.0006943384709293453,
      "loss": 0.6629,
      "step": 2054
    },
    {
      "epoch": 0.9336665152203544,
      "grad_norm": 6.248415470123291,
      "learning_rate": 0.0006941858690676027,
      "loss": 0.9084,
      "step": 2055
    },
    {
      "epoch": 0.9341208541572013,
      "grad_norm": 7.409706115722656,
      "learning_rate": 0.0006940332672058599,
      "loss": 0.9223,
      "step": 2056
    },
    {
      "epoch": 0.9345751930940481,
      "grad_norm": 4.350240707397461,
      "learning_rate": 0.0006938806653441172,
      "loss": 0.2766,
      "step": 2057
    },
    {
      "epoch": 0.935029532030895,
      "grad_norm": 7.194037914276123,
      "learning_rate": 0.0006937280634823745,
      "loss": 1.4067,
      "step": 2058
    },
    {
      "epoch": 0.9354838709677419,
      "grad_norm": 7.984813690185547,
      "learning_rate": 0.0006935754616206318,
      "loss": 1.1086,
      "step": 2059
    },
    {
      "epoch": 0.9359382099045889,
      "grad_norm": 5.023776054382324,
      "learning_rate": 0.000693422859758889,
      "loss": 0.8641,
      "step": 2060
    },
    {
      "epoch": 0.9363925488414357,
      "grad_norm": 6.6793317794799805,
      "learning_rate": 0.0006932702578971464,
      "loss": 0.9157,
      "step": 2061
    },
    {
      "epoch": 0.9368468877782826,
      "grad_norm": 4.723928451538086,
      "learning_rate": 0.0006931176560354037,
      "loss": 0.4177,
      "step": 2062
    },
    {
      "epoch": 0.9373012267151295,
      "grad_norm": 3.9985225200653076,
      "learning_rate": 0.0006929650541736609,
      "loss": 0.1684,
      "step": 2063
    },
    {
      "epoch": 0.9377555656519764,
      "grad_norm": 6.180245399475098,
      "learning_rate": 0.0006928124523119183,
      "loss": 1.0942,
      "step": 2064
    },
    {
      "epoch": 0.9382099045888233,
      "grad_norm": 6.659726142883301,
      "learning_rate": 0.0006926598504501755,
      "loss": 0.9402,
      "step": 2065
    },
    {
      "epoch": 0.9386642435256701,
      "grad_norm": 6.623106002807617,
      "learning_rate": 0.0006925072485884328,
      "loss": 0.7785,
      "step": 2066
    },
    {
      "epoch": 0.939118582462517,
      "grad_norm": 4.454534530639648,
      "learning_rate": 0.0006923546467266902,
      "loss": 0.4756,
      "step": 2067
    },
    {
      "epoch": 0.9395729213993639,
      "grad_norm": 4.622603416442871,
      "learning_rate": 0.0006922020448649474,
      "loss": 0.8102,
      "step": 2068
    },
    {
      "epoch": 0.9400272603362109,
      "grad_norm": 5.439027309417725,
      "learning_rate": 0.0006920494430032046,
      "loss": 0.7365,
      "step": 2069
    },
    {
      "epoch": 0.9404815992730577,
      "grad_norm": 8.632781028747559,
      "learning_rate": 0.0006918968411414619,
      "loss": 1.1332,
      "step": 2070
    },
    {
      "epoch": 0.9409359382099046,
      "grad_norm": 4.63347053527832,
      "learning_rate": 0.0006917442392797192,
      "loss": 0.715,
      "step": 2071
    },
    {
      "epoch": 0.9413902771467515,
      "grad_norm": 4.5464582443237305,
      "learning_rate": 0.0006915916374179764,
      "loss": 0.8656,
      "step": 2072
    },
    {
      "epoch": 0.9418446160835984,
      "grad_norm": 3.617842197418213,
      "learning_rate": 0.0006914390355562338,
      "loss": 0.5519,
      "step": 2073
    },
    {
      "epoch": 0.9422989550204453,
      "grad_norm": 5.072118759155273,
      "learning_rate": 0.0006912864336944911,
      "loss": 0.7861,
      "step": 2074
    },
    {
      "epoch": 0.9427532939572921,
      "grad_norm": 5.842392921447754,
      "learning_rate": 0.0006911338318327483,
      "loss": 1.0794,
      "step": 2075
    },
    {
      "epoch": 0.943207632894139,
      "grad_norm": 6.1749372482299805,
      "learning_rate": 0.0006909812299710057,
      "loss": 0.9633,
      "step": 2076
    },
    {
      "epoch": 0.9436619718309859,
      "grad_norm": 5.732656002044678,
      "learning_rate": 0.0006908286281092629,
      "loss": 0.4645,
      "step": 2077
    },
    {
      "epoch": 0.9441163107678328,
      "grad_norm": 6.043637752532959,
      "learning_rate": 0.0006906760262475202,
      "loss": 1.0011,
      "step": 2078
    },
    {
      "epoch": 0.9445706497046797,
      "grad_norm": 5.471591949462891,
      "learning_rate": 0.0006905234243857776,
      "loss": 0.3433,
      "step": 2079
    },
    {
      "epoch": 0.9450249886415266,
      "grad_norm": 6.425339698791504,
      "learning_rate": 0.0006903708225240348,
      "loss": 1.0389,
      "step": 2080
    },
    {
      "epoch": 0.9454793275783735,
      "grad_norm": 4.384903907775879,
      "learning_rate": 0.0006902182206622921,
      "loss": 0.356,
      "step": 2081
    },
    {
      "epoch": 0.9459336665152204,
      "grad_norm": 3.6266977787017822,
      "learning_rate": 0.0006900656188005494,
      "loss": 0.275,
      "step": 2082
    },
    {
      "epoch": 0.9463880054520672,
      "grad_norm": 5.804725646972656,
      "learning_rate": 0.0006899130169388067,
      "loss": 0.7105,
      "step": 2083
    },
    {
      "epoch": 0.9468423443889141,
      "grad_norm": 5.287444591522217,
      "learning_rate": 0.000689760415077064,
      "loss": 0.8226,
      "step": 2084
    },
    {
      "epoch": 0.947296683325761,
      "grad_norm": 4.906837463378906,
      "learning_rate": 0.0006896078132153213,
      "loss": 0.6542,
      "step": 2085
    },
    {
      "epoch": 0.9477510222626079,
      "grad_norm": 3.956883430480957,
      "learning_rate": 0.0006894552113535786,
      "loss": 0.7667,
      "step": 2086
    },
    {
      "epoch": 0.9482053611994548,
      "grad_norm": 5.840696811676025,
      "learning_rate": 0.0006893026094918357,
      "loss": 1.2249,
      "step": 2087
    },
    {
      "epoch": 0.9486597001363016,
      "grad_norm": 8.414755821228027,
      "learning_rate": 0.0006891500076300931,
      "loss": 0.8647,
      "step": 2088
    },
    {
      "epoch": 0.9491140390731486,
      "grad_norm": 2.8077991008758545,
      "learning_rate": 0.0006889974057683503,
      "loss": 0.4209,
      "step": 2089
    },
    {
      "epoch": 0.9495683780099955,
      "grad_norm": 5.932242393493652,
      "learning_rate": 0.0006888448039066076,
      "loss": 1.2927,
      "step": 2090
    },
    {
      "epoch": 0.9500227169468424,
      "grad_norm": 2.5040533542633057,
      "learning_rate": 0.000688692202044865,
      "loss": 0.1868,
      "step": 2091
    },
    {
      "epoch": 0.9504770558836892,
      "grad_norm": 6.4902448654174805,
      "learning_rate": 0.0006885396001831222,
      "loss": 0.5767,
      "step": 2092
    },
    {
      "epoch": 0.9509313948205361,
      "grad_norm": 9.10944652557373,
      "learning_rate": 0.0006883869983213795,
      "loss": 0.9808,
      "step": 2093
    },
    {
      "epoch": 0.951385733757383,
      "grad_norm": 5.4667887687683105,
      "learning_rate": 0.0006882343964596368,
      "loss": 0.6674,
      "step": 2094
    },
    {
      "epoch": 0.9518400726942299,
      "grad_norm": 7.508358001708984,
      "learning_rate": 0.0006880817945978941,
      "loss": 1.5717,
      "step": 2095
    },
    {
      "epoch": 0.9522944116310768,
      "grad_norm": 5.231922626495361,
      "learning_rate": 0.0006879291927361514,
      "loss": 0.7213,
      "step": 2096
    },
    {
      "epoch": 0.9527487505679236,
      "grad_norm": 5.3546247482299805,
      "learning_rate": 0.0006877765908744087,
      "loss": 0.5994,
      "step": 2097
    },
    {
      "epoch": 0.9532030895047705,
      "grad_norm": 9.389524459838867,
      "learning_rate": 0.000687623989012666,
      "loss": 1.4907,
      "step": 2098
    },
    {
      "epoch": 0.9536574284416175,
      "grad_norm": 10.425429344177246,
      "learning_rate": 0.0006874713871509232,
      "loss": 1.5236,
      "step": 2099
    },
    {
      "epoch": 0.9541117673784644,
      "grad_norm": 7.4511332511901855,
      "learning_rate": 0.0006873187852891806,
      "loss": 1.6015,
      "step": 2100
    },
    {
      "epoch": 0.9545661063153112,
      "grad_norm": 3.929046869277954,
      "learning_rate": 0.0006871661834274379,
      "loss": 0.7645,
      "step": 2101
    },
    {
      "epoch": 0.9550204452521581,
      "grad_norm": 5.187803745269775,
      "learning_rate": 0.0006870135815656951,
      "loss": 0.59,
      "step": 2102
    },
    {
      "epoch": 0.955474784189005,
      "grad_norm": 8.772862434387207,
      "learning_rate": 0.0006868609797039525,
      "loss": 1.9155,
      "step": 2103
    },
    {
      "epoch": 0.9559291231258519,
      "grad_norm": 6.9824371337890625,
      "learning_rate": 0.0006867083778422097,
      "loss": 0.8507,
      "step": 2104
    },
    {
      "epoch": 0.9563834620626988,
      "grad_norm": 7.279548645019531,
      "learning_rate": 0.0006865557759804669,
      "loss": 1.0064,
      "step": 2105
    },
    {
      "epoch": 0.9568378009995456,
      "grad_norm": 4.085732936859131,
      "learning_rate": 0.0006864031741187242,
      "loss": 0.4949,
      "step": 2106
    },
    {
      "epoch": 0.9572921399363925,
      "grad_norm": 4.949189186096191,
      "learning_rate": 0.0006862505722569815,
      "loss": 0.2295,
      "step": 2107
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 4.102118015289307,
      "learning_rate": 0.0006860979703952388,
      "loss": 0.3919,
      "step": 2108
    },
    {
      "epoch": 0.9582008178100864,
      "grad_norm": 5.828589916229248,
      "learning_rate": 0.0006859453685334961,
      "loss": 1.1843,
      "step": 2109
    },
    {
      "epoch": 0.9586551567469332,
      "grad_norm": 5.934272766113281,
      "learning_rate": 0.0006857927666717534,
      "loss": 0.8919,
      "step": 2110
    },
    {
      "epoch": 0.9591094956837801,
      "grad_norm": 7.086771011352539,
      "learning_rate": 0.0006856401648100106,
      "loss": 1.4282,
      "step": 2111
    },
    {
      "epoch": 0.959563834620627,
      "grad_norm": 9.648992538452148,
      "learning_rate": 0.000685487562948268,
      "loss": 1.1776,
      "step": 2112
    },
    {
      "epoch": 0.9600181735574739,
      "grad_norm": 5.124054908752441,
      "learning_rate": 0.0006853349610865253,
      "loss": 0.8797,
      "step": 2113
    },
    {
      "epoch": 0.9604725124943208,
      "grad_norm": 6.952889442443848,
      "learning_rate": 0.0006851823592247825,
      "loss": 0.7117,
      "step": 2114
    },
    {
      "epoch": 0.9609268514311676,
      "grad_norm": 4.106746196746826,
      "learning_rate": 0.0006850297573630399,
      "loss": 0.4156,
      "step": 2115
    },
    {
      "epoch": 0.9613811903680145,
      "grad_norm": 4.806198596954346,
      "learning_rate": 0.0006848771555012971,
      "loss": 0.905,
      "step": 2116
    },
    {
      "epoch": 0.9618355293048614,
      "grad_norm": 6.044643878936768,
      "learning_rate": 0.0006847245536395544,
      "loss": 0.8575,
      "step": 2117
    },
    {
      "epoch": 0.9622898682417084,
      "grad_norm": 5.374110221862793,
      "learning_rate": 0.0006845719517778118,
      "loss": 1.0996,
      "step": 2118
    },
    {
      "epoch": 0.9627442071785552,
      "grad_norm": 9.840723991394043,
      "learning_rate": 0.000684419349916069,
      "loss": 1.3672,
      "step": 2119
    },
    {
      "epoch": 0.9631985461154021,
      "grad_norm": 6.419771671295166,
      "learning_rate": 0.0006842667480543263,
      "loss": 1.977,
      "step": 2120
    },
    {
      "epoch": 0.963652885052249,
      "grad_norm": 4.817221641540527,
      "learning_rate": 0.0006841141461925836,
      "loss": 0.7205,
      "step": 2121
    },
    {
      "epoch": 0.9641072239890959,
      "grad_norm": 5.4574055671691895,
      "learning_rate": 0.0006839615443308409,
      "loss": 0.8091,
      "step": 2122
    },
    {
      "epoch": 0.9645615629259428,
      "grad_norm": 4.344059944152832,
      "learning_rate": 0.000683808942469098,
      "loss": 0.744,
      "step": 2123
    },
    {
      "epoch": 0.9650159018627896,
      "grad_norm": 3.786834478378296,
      "learning_rate": 0.0006836563406073554,
      "loss": 0.5783,
      "step": 2124
    },
    {
      "epoch": 0.9654702407996365,
      "grad_norm": 7.1932854652404785,
      "learning_rate": 0.0006835037387456127,
      "loss": 0.6766,
      "step": 2125
    },
    {
      "epoch": 0.9659245797364834,
      "grad_norm": 10.507695198059082,
      "learning_rate": 0.0006833511368838699,
      "loss": 0.908,
      "step": 2126
    },
    {
      "epoch": 0.9663789186733303,
      "grad_norm": 4.682920932769775,
      "learning_rate": 0.0006831985350221273,
      "loss": 0.4191,
      "step": 2127
    },
    {
      "epoch": 0.9668332576101772,
      "grad_norm": 5.872903823852539,
      "learning_rate": 0.0006830459331603845,
      "loss": 0.4046,
      "step": 2128
    },
    {
      "epoch": 0.9672875965470241,
      "grad_norm": 4.4036712646484375,
      "learning_rate": 0.0006828933312986418,
      "loss": 0.593,
      "step": 2129
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 6.087384223937988,
      "learning_rate": 0.0006827407294368992,
      "loss": 1.383,
      "step": 2130
    },
    {
      "epoch": 0.9681962744207179,
      "grad_norm": 2.363961696624756,
      "learning_rate": 0.0006825881275751564,
      "loss": 0.2128,
      "step": 2131
    },
    {
      "epoch": 0.9686506133575647,
      "grad_norm": 6.008056163787842,
      "learning_rate": 0.0006824355257134137,
      "loss": 0.6314,
      "step": 2132
    },
    {
      "epoch": 0.9691049522944116,
      "grad_norm": 6.250514984130859,
      "learning_rate": 0.000682282923851671,
      "loss": 0.9227,
      "step": 2133
    },
    {
      "epoch": 0.9695592912312585,
      "grad_norm": 9.159768104553223,
      "learning_rate": 0.0006821303219899283,
      "loss": 1.4347,
      "step": 2134
    },
    {
      "epoch": 0.9700136301681054,
      "grad_norm": 3.502830982208252,
      "learning_rate": 0.0006819777201281856,
      "loss": 0.418,
      "step": 2135
    },
    {
      "epoch": 0.9704679691049523,
      "grad_norm": 5.548464775085449,
      "learning_rate": 0.0006818251182664429,
      "loss": 0.5922,
      "step": 2136
    },
    {
      "epoch": 0.9709223080417991,
      "grad_norm": 5.369658946990967,
      "learning_rate": 0.0006816725164047002,
      "loss": 0.8621,
      "step": 2137
    },
    {
      "epoch": 0.9713766469786461,
      "grad_norm": 4.654785633087158,
      "learning_rate": 0.0006815199145429574,
      "loss": 0.7539,
      "step": 2138
    },
    {
      "epoch": 0.971830985915493,
      "grad_norm": 5.200718879699707,
      "learning_rate": 0.0006813673126812148,
      "loss": 0.8193,
      "step": 2139
    },
    {
      "epoch": 0.9722853248523399,
      "grad_norm": 7.549620628356934,
      "learning_rate": 0.000681214710819472,
      "loss": 1.4674,
      "step": 2140
    },
    {
      "epoch": 0.9727396637891867,
      "grad_norm": 4.965696334838867,
      "learning_rate": 0.0006810621089577293,
      "loss": 0.7305,
      "step": 2141
    },
    {
      "epoch": 0.9731940027260336,
      "grad_norm": 2.3229005336761475,
      "learning_rate": 0.0006809095070959866,
      "loss": 0.175,
      "step": 2142
    },
    {
      "epoch": 0.9736483416628805,
      "grad_norm": 5.4697041511535645,
      "learning_rate": 0.0006807569052342438,
      "loss": 1.1774,
      "step": 2143
    },
    {
      "epoch": 0.9741026805997274,
      "grad_norm": 5.711564540863037,
      "learning_rate": 0.0006806043033725011,
      "loss": 1.1203,
      "step": 2144
    },
    {
      "epoch": 0.9745570195365743,
      "grad_norm": 5.822653293609619,
      "learning_rate": 0.0006804517015107584,
      "loss": 0.7087,
      "step": 2145
    },
    {
      "epoch": 0.9750113584734211,
      "grad_norm": 8.34938907623291,
      "learning_rate": 0.0006802990996490157,
      "loss": 1.6725,
      "step": 2146
    },
    {
      "epoch": 0.975465697410268,
      "grad_norm": 4.827529430389404,
      "learning_rate": 0.000680146497787273,
      "loss": 0.4629,
      "step": 2147
    },
    {
      "epoch": 0.975920036347115,
      "grad_norm": 3.9937901496887207,
      "learning_rate": 0.0006799938959255303,
      "loss": 0.5219,
      "step": 2148
    },
    {
      "epoch": 0.9763743752839619,
      "grad_norm": 8.121403694152832,
      "learning_rate": 0.0006798412940637876,
      "loss": 0.7354,
      "step": 2149
    },
    {
      "epoch": 0.9768287142208087,
      "grad_norm": 9.27087688446045,
      "learning_rate": 0.0006796886922020448,
      "loss": 0.6956,
      "step": 2150
    },
    {
      "epoch": 0.9772830531576556,
      "grad_norm": 4.18889045715332,
      "learning_rate": 0.0006795360903403022,
      "loss": 0.8185,
      "step": 2151
    },
    {
      "epoch": 0.9777373920945025,
      "grad_norm": 3.8327982425689697,
      "learning_rate": 0.0006793834884785595,
      "loss": 0.4558,
      "step": 2152
    },
    {
      "epoch": 0.9781917310313494,
      "grad_norm": 6.747805595397949,
      "learning_rate": 0.0006792308866168167,
      "loss": 1.2008,
      "step": 2153
    },
    {
      "epoch": 0.9786460699681963,
      "grad_norm": 5.454352378845215,
      "learning_rate": 0.0006790782847550741,
      "loss": 0.5254,
      "step": 2154
    },
    {
      "epoch": 0.9791004089050431,
      "grad_norm": 6.983964920043945,
      "learning_rate": 0.0006789256828933313,
      "loss": 1.5219,
      "step": 2155
    },
    {
      "epoch": 0.97955474784189,
      "grad_norm": 6.95999813079834,
      "learning_rate": 0.0006787730810315886,
      "loss": 0.6064,
      "step": 2156
    },
    {
      "epoch": 0.980009086778737,
      "grad_norm": 7.03669548034668,
      "learning_rate": 0.000678620479169846,
      "loss": 0.9904,
      "step": 2157
    },
    {
      "epoch": 0.9804634257155839,
      "grad_norm": 8.695530891418457,
      "learning_rate": 0.0006784678773081032,
      "loss": 0.9068,
      "step": 2158
    },
    {
      "epoch": 0.9809177646524307,
      "grad_norm": 6.718227863311768,
      "learning_rate": 0.0006783152754463605,
      "loss": 0.8031,
      "step": 2159
    },
    {
      "epoch": 0.9813721035892776,
      "grad_norm": 6.190205097198486,
      "learning_rate": 0.0006781626735846177,
      "loss": 0.5617,
      "step": 2160
    },
    {
      "epoch": 0.9818264425261245,
      "grad_norm": 6.773263931274414,
      "learning_rate": 0.000678010071722875,
      "loss": 1.1928,
      "step": 2161
    },
    {
      "epoch": 0.9822807814629714,
      "grad_norm": 4.189763069152832,
      "learning_rate": 0.0006778574698611322,
      "loss": 0.589,
      "step": 2162
    },
    {
      "epoch": 0.9827351203998183,
      "grad_norm": 6.7655487060546875,
      "learning_rate": 0.0006777048679993896,
      "loss": 1.6225,
      "step": 2163
    },
    {
      "epoch": 0.9831894593366651,
      "grad_norm": 4.452278137207031,
      "learning_rate": 0.0006775522661376469,
      "loss": 0.5888,
      "step": 2164
    },
    {
      "epoch": 0.983643798273512,
      "grad_norm": 5.007771015167236,
      "learning_rate": 0.0006773996642759041,
      "loss": 0.6354,
      "step": 2165
    },
    {
      "epoch": 0.9840981372103589,
      "grad_norm": 8.073589324951172,
      "learning_rate": 0.0006772470624141615,
      "loss": 1.5732,
      "step": 2166
    },
    {
      "epoch": 0.9845524761472059,
      "grad_norm": 5.60862398147583,
      "learning_rate": 0.0006770944605524187,
      "loss": 0.4264,
      "step": 2167
    },
    {
      "epoch": 0.9850068150840527,
      "grad_norm": 1.9305694103240967,
      "learning_rate": 0.000676941858690676,
      "loss": 0.2353,
      "step": 2168
    },
    {
      "epoch": 0.9854611540208996,
      "grad_norm": 5.131899356842041,
      "learning_rate": 0.0006767892568289334,
      "loss": 0.5592,
      "step": 2169
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 3.165468454360962,
      "learning_rate": 0.0006766366549671906,
      "loss": 0.2763,
      "step": 2170
    },
    {
      "epoch": 0.9863698318945934,
      "grad_norm": 5.303527355194092,
      "learning_rate": 0.0006764840531054479,
      "loss": 0.9226,
      "step": 2171
    },
    {
      "epoch": 0.9868241708314403,
      "grad_norm": 7.085631370544434,
      "learning_rate": 0.0006763314512437052,
      "loss": 0.8222,
      "step": 2172
    },
    {
      "epoch": 0.9872785097682871,
      "grad_norm": 6.572265625,
      "learning_rate": 0.0006761788493819625,
      "loss": 1.2363,
      "step": 2173
    },
    {
      "epoch": 0.987732848705134,
      "grad_norm": 8.568450927734375,
      "learning_rate": 0.0006760262475202197,
      "loss": 1.2153,
      "step": 2174
    },
    {
      "epoch": 0.9881871876419809,
      "grad_norm": 7.019843101501465,
      "learning_rate": 0.0006758736456584771,
      "loss": 1.2834,
      "step": 2175
    },
    {
      "epoch": 0.9886415265788278,
      "grad_norm": 6.772791385650635,
      "learning_rate": 0.0006757210437967344,
      "loss": 1.2074,
      "step": 2176
    },
    {
      "epoch": 0.9890958655156747,
      "grad_norm": 6.941165924072266,
      "learning_rate": 0.0006755684419349916,
      "loss": 1.2768,
      "step": 2177
    },
    {
      "epoch": 0.9895502044525216,
      "grad_norm": 6.022774696350098,
      "learning_rate": 0.0006754158400732489,
      "loss": 1.4643,
      "step": 2178
    },
    {
      "epoch": 0.9900045433893685,
      "grad_norm": 6.705077171325684,
      "learning_rate": 0.0006752632382115061,
      "loss": 0.7066,
      "step": 2179
    },
    {
      "epoch": 0.9904588823262154,
      "grad_norm": 3.74477219581604,
      "learning_rate": 0.0006751106363497634,
      "loss": 0.2838,
      "step": 2180
    },
    {
      "epoch": 0.9909132212630622,
      "grad_norm": 5.143665790557861,
      "learning_rate": 0.0006749580344880208,
      "loss": 0.9862,
      "step": 2181
    },
    {
      "epoch": 0.9913675601999091,
      "grad_norm": 4.339573383331299,
      "learning_rate": 0.000674805432626278,
      "loss": 0.5373,
      "step": 2182
    },
    {
      "epoch": 0.991821899136756,
      "grad_norm": 5.761904716491699,
      "learning_rate": 0.0006746528307645353,
      "loss": 1.2256,
      "step": 2183
    },
    {
      "epoch": 0.9922762380736029,
      "grad_norm": 7.716029644012451,
      "learning_rate": 0.0006745002289027926,
      "loss": 0.5284,
      "step": 2184
    },
    {
      "epoch": 0.9927305770104498,
      "grad_norm": 4.326215744018555,
      "learning_rate": 0.0006743476270410499,
      "loss": 0.7082,
      "step": 2185
    },
    {
      "epoch": 0.9931849159472966,
      "grad_norm": 6.564938068389893,
      "learning_rate": 0.0006741950251793071,
      "loss": 1.4411,
      "step": 2186
    },
    {
      "epoch": 0.9936392548841436,
      "grad_norm": 6.798888206481934,
      "learning_rate": 0.0006740424233175645,
      "loss": 1.3383,
      "step": 2187
    },
    {
      "epoch": 0.9940935938209905,
      "grad_norm": 3.3701670169830322,
      "learning_rate": 0.0006738898214558218,
      "loss": 0.439,
      "step": 2188
    },
    {
      "epoch": 0.9945479327578374,
      "grad_norm": 4.399795055389404,
      "learning_rate": 0.000673737219594079,
      "loss": 1.225,
      "step": 2189
    },
    {
      "epoch": 0.9950022716946842,
      "grad_norm": 4.796753406524658,
      "learning_rate": 0.0006735846177323364,
      "loss": 0.8978,
      "step": 2190
    },
    {
      "epoch": 0.9954566106315311,
      "grad_norm": 7.221077919006348,
      "learning_rate": 0.0006734320158705936,
      "loss": 1.1108,
      "step": 2191
    },
    {
      "epoch": 0.995910949568378,
      "grad_norm": 4.793304443359375,
      "learning_rate": 0.000673279414008851,
      "loss": 1.0736,
      "step": 2192
    },
    {
      "epoch": 0.9963652885052249,
      "grad_norm": 3.594615936279297,
      "learning_rate": 0.0006731268121471083,
      "loss": 0.5576,
      "step": 2193
    },
    {
      "epoch": 0.9968196274420718,
      "grad_norm": 6.406000137329102,
      "learning_rate": 0.0006729742102853655,
      "loss": 0.7734,
      "step": 2194
    },
    {
      "epoch": 0.9972739663789186,
      "grad_norm": 4.660029888153076,
      "learning_rate": 0.0006728216084236229,
      "loss": 0.6914,
      "step": 2195
    },
    {
      "epoch": 0.9977283053157655,
      "grad_norm": 7.439437389373779,
      "learning_rate": 0.00067266900656188,
      "loss": 1.6483,
      "step": 2196
    },
    {
      "epoch": 0.9981826442526125,
      "grad_norm": 9.147397994995117,
      "learning_rate": 0.0006725164047001373,
      "loss": 2.5134,
      "step": 2197
    },
    {
      "epoch": 0.9986369831894594,
      "grad_norm": 6.8392791748046875,
      "learning_rate": 0.0006723638028383945,
      "loss": 1.4791,
      "step": 2198
    },
    {
      "epoch": 0.9990913221263062,
      "grad_norm": 6.224771499633789,
      "learning_rate": 0.0006722112009766519,
      "loss": 0.9093,
      "step": 2199
    },
    {
      "epoch": 0.9995456610631531,
      "grad_norm": 6.2813944816589355,
      "learning_rate": 0.0006720585991149092,
      "loss": 1.3066,
      "step": 2200
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.832905292510986,
      "learning_rate": 0.0006719059972531665,
      "loss": 0.2409,
      "step": 2201
    },
    {
      "epoch": 1.000454338936847,
      "grad_norm": 3.467315196990967,
      "learning_rate": 0.0006717533953914238,
      "loss": 0.2238,
      "step": 2202
    },
    {
      "epoch": 1.0009086778736938,
      "grad_norm": 2.1531057357788086,
      "learning_rate": 0.000671600793529681,
      "loss": 0.2311,
      "step": 2203
    },
    {
      "epoch": 1.0013630168105407,
      "grad_norm": 3.120332717895508,
      "learning_rate": 0.0006714481916679384,
      "loss": 0.4183,
      "step": 2204
    },
    {
      "epoch": 1.0018173557473875,
      "grad_norm": 2.951125383377075,
      "learning_rate": 0.0006712955898061957,
      "loss": 0.2992,
      "step": 2205
    },
    {
      "epoch": 1.0022716946842345,
      "grad_norm": 7.67844820022583,
      "learning_rate": 0.0006711429879444529,
      "loss": 0.7794,
      "step": 2206
    },
    {
      "epoch": 1.0027260336210813,
      "grad_norm": 4.350703239440918,
      "learning_rate": 0.0006709903860827103,
      "loss": 0.4073,
      "step": 2207
    },
    {
      "epoch": 1.0031803725579282,
      "grad_norm": 3.376727819442749,
      "learning_rate": 0.0006708377842209675,
      "loss": 0.3421,
      "step": 2208
    },
    {
      "epoch": 1.003634711494775,
      "grad_norm": 4.580035209655762,
      "learning_rate": 0.0006706851823592248,
      "loss": 0.7442,
      "step": 2209
    },
    {
      "epoch": 1.004089050431622,
      "grad_norm": 3.086271286010742,
      "learning_rate": 0.0006705325804974822,
      "loss": 0.2756,
      "step": 2210
    },
    {
      "epoch": 1.004543389368469,
      "grad_norm": 6.142261505126953,
      "learning_rate": 0.0006703799786357394,
      "loss": 0.5278,
      "step": 2211
    },
    {
      "epoch": 1.0049977283053158,
      "grad_norm": 6.3278679847717285,
      "learning_rate": 0.0006702273767739967,
      "loss": 0.9037,
      "step": 2212
    },
    {
      "epoch": 1.0054520672421627,
      "grad_norm": 6.245236396789551,
      "learning_rate": 0.000670074774912254,
      "loss": 1.0087,
      "step": 2213
    },
    {
      "epoch": 1.0059064061790095,
      "grad_norm": 5.2595391273498535,
      "learning_rate": 0.0006699221730505113,
      "loss": 0.4773,
      "step": 2214
    },
    {
      "epoch": 1.0063607451158565,
      "grad_norm": 9.716976165771484,
      "learning_rate": 0.0006697695711887684,
      "loss": 1.2453,
      "step": 2215
    },
    {
      "epoch": 1.0068150840527033,
      "grad_norm": 3.7863500118255615,
      "learning_rate": 0.0006696169693270258,
      "loss": 0.3468,
      "step": 2216
    },
    {
      "epoch": 1.0072694229895502,
      "grad_norm": 3.18182110786438,
      "learning_rate": 0.0006694643674652831,
      "loss": 0.3461,
      "step": 2217
    },
    {
      "epoch": 1.007723761926397,
      "grad_norm": 7.906353950500488,
      "learning_rate": 0.0006693117656035403,
      "loss": 0.8076,
      "step": 2218
    },
    {
      "epoch": 1.008178100863244,
      "grad_norm": 6.414095878601074,
      "learning_rate": 0.0006691591637417977,
      "loss": 0.6906,
      "step": 2219
    },
    {
      "epoch": 1.0086324398000908,
      "grad_norm": 3.4839725494384766,
      "learning_rate": 0.000669006561880055,
      "loss": 0.529,
      "step": 2220
    },
    {
      "epoch": 1.0090867787369378,
      "grad_norm": 1.9749773740768433,
      "learning_rate": 0.0006688539600183122,
      "loss": 0.1344,
      "step": 2221
    },
    {
      "epoch": 1.0095411176737847,
      "grad_norm": 7.095025062561035,
      "learning_rate": 0.0006687013581565696,
      "loss": 0.6567,
      "step": 2222
    },
    {
      "epoch": 1.0099954566106315,
      "grad_norm": 3.9302945137023926,
      "learning_rate": 0.0006685487562948268,
      "loss": 0.4215,
      "step": 2223
    },
    {
      "epoch": 1.0104497955474785,
      "grad_norm": 7.669315338134766,
      "learning_rate": 0.0006683961544330841,
      "loss": 0.7274,
      "step": 2224
    },
    {
      "epoch": 1.0109041344843253,
      "grad_norm": 8.059486389160156,
      "learning_rate": 0.0006682435525713414,
      "loss": 0.8999,
      "step": 2225
    },
    {
      "epoch": 1.0113584734211722,
      "grad_norm": 5.722674369812012,
      "learning_rate": 0.0006680909507095987,
      "loss": 0.9937,
      "step": 2226
    },
    {
      "epoch": 1.011812812358019,
      "grad_norm": 4.636408805847168,
      "learning_rate": 0.000667938348847856,
      "loss": 0.691,
      "step": 2227
    },
    {
      "epoch": 1.012267151294866,
      "grad_norm": 8.064949989318848,
      "learning_rate": 0.0006677857469861133,
      "loss": 1.1374,
      "step": 2228
    },
    {
      "epoch": 1.0127214902317128,
      "grad_norm": 4.844327926635742,
      "learning_rate": 0.0006676331451243706,
      "loss": 0.497,
      "step": 2229
    },
    {
      "epoch": 1.0131758291685597,
      "grad_norm": 5.083385467529297,
      "learning_rate": 0.0006674805432626278,
      "loss": 0.502,
      "step": 2230
    },
    {
      "epoch": 1.0136301681054067,
      "grad_norm": 4.448597431182861,
      "learning_rate": 0.0006673279414008852,
      "loss": 0.3852,
      "step": 2231
    },
    {
      "epoch": 1.0140845070422535,
      "grad_norm": 4.719607353210449,
      "learning_rate": 0.0006671753395391425,
      "loss": 1.2178,
      "step": 2232
    },
    {
      "epoch": 1.0145388459791005,
      "grad_norm": 6.855868816375732,
      "learning_rate": 0.0006670227376773996,
      "loss": 0.9311,
      "step": 2233
    },
    {
      "epoch": 1.0149931849159473,
      "grad_norm": 5.794977188110352,
      "learning_rate": 0.000666870135815657,
      "loss": 0.7478,
      "step": 2234
    },
    {
      "epoch": 1.0154475238527942,
      "grad_norm": 6.7322998046875,
      "learning_rate": 0.0006667175339539142,
      "loss": 0.7902,
      "step": 2235
    },
    {
      "epoch": 1.015901862789641,
      "grad_norm": 4.9949421882629395,
      "learning_rate": 0.0006665649320921715,
      "loss": 0.8904,
      "step": 2236
    },
    {
      "epoch": 1.016356201726488,
      "grad_norm": 5.432937145233154,
      "learning_rate": 0.0006664123302304288,
      "loss": 0.882,
      "step": 2237
    },
    {
      "epoch": 1.0168105406633348,
      "grad_norm": 4.371043682098389,
      "learning_rate": 0.0006662597283686861,
      "loss": 0.8249,
      "step": 2238
    },
    {
      "epoch": 1.0172648796001817,
      "grad_norm": 6.558482646942139,
      "learning_rate": 0.0006661071265069434,
      "loss": 0.768,
      "step": 2239
    },
    {
      "epoch": 1.0177192185370285,
      "grad_norm": 4.5936808586120605,
      "learning_rate": 0.0006659545246452007,
      "loss": 0.5123,
      "step": 2240
    },
    {
      "epoch": 1.0181735574738755,
      "grad_norm": 6.681919574737549,
      "learning_rate": 0.000665801922783458,
      "loss": 0.3014,
      "step": 2241
    },
    {
      "epoch": 1.0186278964107225,
      "grad_norm": 5.6848859786987305,
      "learning_rate": 0.0006656493209217152,
      "loss": 0.9027,
      "step": 2242
    },
    {
      "epoch": 1.0190822353475693,
      "grad_norm": 3.519460916519165,
      "learning_rate": 0.0006654967190599726,
      "loss": 0.181,
      "step": 2243
    },
    {
      "epoch": 1.0195365742844162,
      "grad_norm": 5.662287712097168,
      "learning_rate": 0.0006653441171982299,
      "loss": 0.511,
      "step": 2244
    },
    {
      "epoch": 1.019990913221263,
      "grad_norm": 6.578303813934326,
      "learning_rate": 0.0006651915153364871,
      "loss": 0.5573,
      "step": 2245
    },
    {
      "epoch": 1.02044525215811,
      "grad_norm": 4.5742926597595215,
      "learning_rate": 0.0006650389134747445,
      "loss": 0.6432,
      "step": 2246
    },
    {
      "epoch": 1.0208995910949568,
      "grad_norm": 4.095380783081055,
      "learning_rate": 0.0006648863116130017,
      "loss": 0.9055,
      "step": 2247
    },
    {
      "epoch": 1.0213539300318037,
      "grad_norm": 6.051626682281494,
      "learning_rate": 0.000664733709751259,
      "loss": 0.8853,
      "step": 2248
    },
    {
      "epoch": 1.0218082689686505,
      "grad_norm": 10.260047912597656,
      "learning_rate": 0.0006645811078895164,
      "loss": 1.051,
      "step": 2249
    },
    {
      "epoch": 1.0222626079054975,
      "grad_norm": 2.9803473949432373,
      "learning_rate": 0.0006644285060277736,
      "loss": 0.2814,
      "step": 2250
    },
    {
      "epoch": 1.0227169468423445,
      "grad_norm": 3.086634874343872,
      "learning_rate": 0.0006642759041660308,
      "loss": 0.2649,
      "step": 2251
    },
    {
      "epoch": 1.0231712857791913,
      "grad_norm": 5.090752601623535,
      "learning_rate": 0.0006641233023042881,
      "loss": 0.6204,
      "step": 2252
    },
    {
      "epoch": 1.0236256247160382,
      "grad_norm": 7.4444990158081055,
      "learning_rate": 0.0006639707004425454,
      "loss": 0.8871,
      "step": 2253
    },
    {
      "epoch": 1.024079963652885,
      "grad_norm": 3.847109079360962,
      "learning_rate": 0.0006638180985808026,
      "loss": 0.2362,
      "step": 2254
    },
    {
      "epoch": 1.024534302589732,
      "grad_norm": 6.764350414276123,
      "learning_rate": 0.00066366549671906,
      "loss": 1.6824,
      "step": 2255
    },
    {
      "epoch": 1.0249886415265788,
      "grad_norm": 5.703283786773682,
      "learning_rate": 0.0006635128948573173,
      "loss": 0.8932,
      "step": 2256
    },
    {
      "epoch": 1.0254429804634257,
      "grad_norm": 5.501344203948975,
      "learning_rate": 0.0006633602929955745,
      "loss": 0.6775,
      "step": 2257
    },
    {
      "epoch": 1.0258973194002725,
      "grad_norm": 4.296960830688477,
      "learning_rate": 0.0006632076911338319,
      "loss": 0.4209,
      "step": 2258
    },
    {
      "epoch": 1.0263516583371195,
      "grad_norm": 5.744040489196777,
      "learning_rate": 0.0006630550892720891,
      "loss": 0.431,
      "step": 2259
    },
    {
      "epoch": 1.0268059972739665,
      "grad_norm": 8.597875595092773,
      "learning_rate": 0.0006629024874103464,
      "loss": 0.8154,
      "step": 2260
    },
    {
      "epoch": 1.0272603362108133,
      "grad_norm": 5.399383544921875,
      "learning_rate": 0.0006627498855486038,
      "loss": 0.7894,
      "step": 2261
    },
    {
      "epoch": 1.0277146751476602,
      "grad_norm": 4.703012466430664,
      "learning_rate": 0.000662597283686861,
      "loss": 0.5646,
      "step": 2262
    },
    {
      "epoch": 1.028169014084507,
      "grad_norm": 4.497751712799072,
      "learning_rate": 0.0006624446818251183,
      "loss": 0.4316,
      "step": 2263
    },
    {
      "epoch": 1.028623353021354,
      "grad_norm": 5.657229423522949,
      "learning_rate": 0.0006622920799633756,
      "loss": 0.5478,
      "step": 2264
    },
    {
      "epoch": 1.0290776919582008,
      "grad_norm": 3.741956949234009,
      "learning_rate": 0.0006621394781016329,
      "loss": 0.5328,
      "step": 2265
    },
    {
      "epoch": 1.0295320308950477,
      "grad_norm": 5.633684158325195,
      "learning_rate": 0.0006619868762398901,
      "loss": 0.7894,
      "step": 2266
    },
    {
      "epoch": 1.0299863698318945,
      "grad_norm": 3.2699403762817383,
      "learning_rate": 0.0006618342743781475,
      "loss": 0.4482,
      "step": 2267
    },
    {
      "epoch": 1.0304407087687415,
      "grad_norm": 3.9448843002319336,
      "learning_rate": 0.0006616816725164048,
      "loss": 0.5404,
      "step": 2268
    },
    {
      "epoch": 1.0308950477055883,
      "grad_norm": 4.984920024871826,
      "learning_rate": 0.0006615290706546619,
      "loss": 0.4455,
      "step": 2269
    },
    {
      "epoch": 1.0313493866424353,
      "grad_norm": 5.5236334800720215,
      "learning_rate": 0.0006613764687929193,
      "loss": 1.1698,
      "step": 2270
    },
    {
      "epoch": 1.0318037255792822,
      "grad_norm": 5.293984889984131,
      "learning_rate": 0.0006612238669311765,
      "loss": 0.3662,
      "step": 2271
    },
    {
      "epoch": 1.032258064516129,
      "grad_norm": 2.2918002605438232,
      "learning_rate": 0.0006610712650694338,
      "loss": 0.3746,
      "step": 2272
    },
    {
      "epoch": 1.032712403452976,
      "grad_norm": 4.964775562286377,
      "learning_rate": 0.0006609186632076912,
      "loss": 0.5687,
      "step": 2273
    },
    {
      "epoch": 1.0331667423898228,
      "grad_norm": 8.231818199157715,
      "learning_rate": 0.0006607660613459484,
      "loss": 1.1435,
      "step": 2274
    },
    {
      "epoch": 1.0336210813266697,
      "grad_norm": 3.8391294479370117,
      "learning_rate": 0.0006606134594842057,
      "loss": 0.6322,
      "step": 2275
    },
    {
      "epoch": 1.0340754202635165,
      "grad_norm": 6.01459264755249,
      "learning_rate": 0.000660460857622463,
      "loss": 0.7934,
      "step": 2276
    },
    {
      "epoch": 1.0345297592003635,
      "grad_norm": 2.6437227725982666,
      "learning_rate": 0.0006603082557607203,
      "loss": 0.1916,
      "step": 2277
    },
    {
      "epoch": 1.0349840981372103,
      "grad_norm": 3.3478479385375977,
      "learning_rate": 0.0006601556538989776,
      "loss": 0.4037,
      "step": 2278
    },
    {
      "epoch": 1.0354384370740572,
      "grad_norm": 6.294277191162109,
      "learning_rate": 0.0006600030520372349,
      "loss": 0.8172,
      "step": 2279
    },
    {
      "epoch": 1.0358927760109042,
      "grad_norm": 4.1975507736206055,
      "learning_rate": 0.0006598504501754922,
      "loss": 0.2385,
      "step": 2280
    },
    {
      "epoch": 1.036347114947751,
      "grad_norm": 5.268637657165527,
      "learning_rate": 0.0006596978483137494,
      "loss": 0.6748,
      "step": 2281
    },
    {
      "epoch": 1.036801453884598,
      "grad_norm": 4.527780055999756,
      "learning_rate": 0.0006595452464520068,
      "loss": 0.4453,
      "step": 2282
    },
    {
      "epoch": 1.0372557928214448,
      "grad_norm": 4.304635524749756,
      "learning_rate": 0.000659392644590264,
      "loss": 0.3995,
      "step": 2283
    },
    {
      "epoch": 1.0377101317582917,
      "grad_norm": 5.359988689422607,
      "learning_rate": 0.0006592400427285213,
      "loss": 0.5353,
      "step": 2284
    },
    {
      "epoch": 1.0381644706951385,
      "grad_norm": 5.196450710296631,
      "learning_rate": 0.0006590874408667787,
      "loss": 0.7747,
      "step": 2285
    },
    {
      "epoch": 1.0386188096319855,
      "grad_norm": 9.682246208190918,
      "learning_rate": 0.0006589348390050359,
      "loss": 1.1719,
      "step": 2286
    },
    {
      "epoch": 1.0390731485688323,
      "grad_norm": 6.233440399169922,
      "learning_rate": 0.0006587822371432932,
      "loss": 1.1186,
      "step": 2287
    },
    {
      "epoch": 1.0395274875056792,
      "grad_norm": 4.871574878692627,
      "learning_rate": 0.0006586296352815504,
      "loss": 0.8373,
      "step": 2288
    },
    {
      "epoch": 1.039981826442526,
      "grad_norm": 6.437852382659912,
      "learning_rate": 0.0006584770334198077,
      "loss": 0.7847,
      "step": 2289
    },
    {
      "epoch": 1.040436165379373,
      "grad_norm": 5.883391857147217,
      "learning_rate": 0.000658324431558065,
      "loss": 0.8018,
      "step": 2290
    },
    {
      "epoch": 1.04089050431622,
      "grad_norm": 3.925856351852417,
      "learning_rate": 0.0006581718296963223,
      "loss": 0.3719,
      "step": 2291
    },
    {
      "epoch": 1.0413448432530668,
      "grad_norm": 4.6736741065979,
      "learning_rate": 0.0006580192278345796,
      "loss": 0.3495,
      "step": 2292
    },
    {
      "epoch": 1.0417991821899137,
      "grad_norm": 6.346406936645508,
      "learning_rate": 0.0006578666259728368,
      "loss": 0.7879,
      "step": 2293
    },
    {
      "epoch": 1.0422535211267605,
      "grad_norm": 4.227712631225586,
      "learning_rate": 0.0006577140241110942,
      "loss": 0.4351,
      "step": 2294
    },
    {
      "epoch": 1.0427078600636075,
      "grad_norm": 3.7103641033172607,
      "learning_rate": 0.0006575614222493515,
      "loss": 0.2187,
      "step": 2295
    },
    {
      "epoch": 1.0431621990004543,
      "grad_norm": 3.2089498043060303,
      "learning_rate": 0.0006574088203876087,
      "loss": 0.1953,
      "step": 2296
    },
    {
      "epoch": 1.0436165379373012,
      "grad_norm": 6.743566989898682,
      "learning_rate": 0.0006572562185258661,
      "loss": 0.9424,
      "step": 2297
    },
    {
      "epoch": 1.044070876874148,
      "grad_norm": 3.974766254425049,
      "learning_rate": 0.0006571036166641233,
      "loss": 0.3684,
      "step": 2298
    },
    {
      "epoch": 1.044525215810995,
      "grad_norm": 6.511399745941162,
      "learning_rate": 0.0006569510148023806,
      "loss": 1.1339,
      "step": 2299
    },
    {
      "epoch": 1.044979554747842,
      "grad_norm": 7.209133625030518,
      "learning_rate": 0.000656798412940638,
      "loss": 0.9137,
      "step": 2300
    },
    {
      "epoch": 1.0454338936846888,
      "grad_norm": 1.4867393970489502,
      "learning_rate": 0.0006566458110788952,
      "loss": 0.0638,
      "step": 2301
    },
    {
      "epoch": 1.0458882326215357,
      "grad_norm": 6.79109001159668,
      "learning_rate": 0.0006564932092171525,
      "loss": 0.5476,
      "step": 2302
    },
    {
      "epoch": 1.0463425715583825,
      "grad_norm": 5.211540699005127,
      "learning_rate": 0.0006563406073554098,
      "loss": 0.4353,
      "step": 2303
    },
    {
      "epoch": 1.0467969104952295,
      "grad_norm": 3.700582265853882,
      "learning_rate": 0.0006561880054936671,
      "loss": 0.3737,
      "step": 2304
    },
    {
      "epoch": 1.0472512494320763,
      "grad_norm": 6.405941963195801,
      "learning_rate": 0.0006560354036319243,
      "loss": 1.0106,
      "step": 2305
    },
    {
      "epoch": 1.0477055883689232,
      "grad_norm": 4.039255619049072,
      "learning_rate": 0.0006558828017701816,
      "loss": 0.4706,
      "step": 2306
    },
    {
      "epoch": 1.04815992730577,
      "grad_norm": 7.1565656661987305,
      "learning_rate": 0.0006557301999084389,
      "loss": 0.9479,
      "step": 2307
    },
    {
      "epoch": 1.048614266242617,
      "grad_norm": 4.619601249694824,
      "learning_rate": 0.0006555775980466961,
      "loss": 0.3232,
      "step": 2308
    },
    {
      "epoch": 1.049068605179464,
      "grad_norm": 3.3168222904205322,
      "learning_rate": 0.0006554249961849535,
      "loss": 0.501,
      "step": 2309
    },
    {
      "epoch": 1.0495229441163108,
      "grad_norm": 11.557112693786621,
      "learning_rate": 0.0006552723943232107,
      "loss": 0.8212,
      "step": 2310
    },
    {
      "epoch": 1.0499772830531577,
      "grad_norm": 5.527540683746338,
      "learning_rate": 0.000655119792461468,
      "loss": 0.6983,
      "step": 2311
    },
    {
      "epoch": 1.0504316219900045,
      "grad_norm": 2.8298449516296387,
      "learning_rate": 0.0006549671905997254,
      "loss": 0.2165,
      "step": 2312
    },
    {
      "epoch": 1.0508859609268515,
      "grad_norm": 3.0590875148773193,
      "learning_rate": 0.0006548145887379826,
      "loss": 0.37,
      "step": 2313
    },
    {
      "epoch": 1.0513402998636983,
      "grad_norm": 2.5691440105438232,
      "learning_rate": 0.0006546619868762399,
      "loss": 0.2982,
      "step": 2314
    },
    {
      "epoch": 1.0517946388005452,
      "grad_norm": 7.288059711456299,
      "learning_rate": 0.0006545093850144972,
      "loss": 0.8101,
      "step": 2315
    },
    {
      "epoch": 1.052248977737392,
      "grad_norm": 4.738519191741943,
      "learning_rate": 0.0006543567831527545,
      "loss": 0.4529,
      "step": 2316
    },
    {
      "epoch": 1.052703316674239,
      "grad_norm": 4.784206867218018,
      "learning_rate": 0.0006542041812910117,
      "loss": 0.7678,
      "step": 2317
    },
    {
      "epoch": 1.0531576556110858,
      "grad_norm": 3.1722466945648193,
      "learning_rate": 0.0006540515794292691,
      "loss": 0.3571,
      "step": 2318
    },
    {
      "epoch": 1.0536119945479328,
      "grad_norm": 6.034582138061523,
      "learning_rate": 0.0006538989775675264,
      "loss": 0.7581,
      "step": 2319
    },
    {
      "epoch": 1.0540663334847797,
      "grad_norm": 5.982300758361816,
      "learning_rate": 0.0006537463757057836,
      "loss": 0.9795,
      "step": 2320
    },
    {
      "epoch": 1.0545206724216265,
      "grad_norm": 7.484699726104736,
      "learning_rate": 0.000653593773844041,
      "loss": 0.852,
      "step": 2321
    },
    {
      "epoch": 1.0549750113584735,
      "grad_norm": 4.677771091461182,
      "learning_rate": 0.0006534411719822982,
      "loss": 0.5801,
      "step": 2322
    },
    {
      "epoch": 1.0554293502953203,
      "grad_norm": 6.887696743011475,
      "learning_rate": 0.0006532885701205555,
      "loss": 0.7108,
      "step": 2323
    },
    {
      "epoch": 1.0558836892321672,
      "grad_norm": 8.025755882263184,
      "learning_rate": 0.0006531359682588128,
      "loss": 0.5892,
      "step": 2324
    },
    {
      "epoch": 1.056338028169014,
      "grad_norm": 3.514902114868164,
      "learning_rate": 0.00065298336639707,
      "loss": 0.5991,
      "step": 2325
    },
    {
      "epoch": 1.056792367105861,
      "grad_norm": 2.6570136547088623,
      "learning_rate": 0.0006528307645353273,
      "loss": 0.2605,
      "step": 2326
    },
    {
      "epoch": 1.0572467060427078,
      "grad_norm": 3.6314315795898438,
      "learning_rate": 0.0006526781626735846,
      "loss": 0.5606,
      "step": 2327
    },
    {
      "epoch": 1.0577010449795547,
      "grad_norm": 4.10750150680542,
      "learning_rate": 0.0006525255608118419,
      "loss": 0.6307,
      "step": 2328
    },
    {
      "epoch": 1.0581553839164017,
      "grad_norm": 6.071979999542236,
      "learning_rate": 0.0006523729589500991,
      "loss": 1.1598,
      "step": 2329
    },
    {
      "epoch": 1.0586097228532485,
      "grad_norm": 6.483487129211426,
      "learning_rate": 0.0006522203570883565,
      "loss": 0.5247,
      "step": 2330
    },
    {
      "epoch": 1.0590640617900955,
      "grad_norm": 5.973603248596191,
      "learning_rate": 0.0006520677552266138,
      "loss": 0.8326,
      "step": 2331
    },
    {
      "epoch": 1.0595184007269423,
      "grad_norm": 1.3106589317321777,
      "learning_rate": 0.000651915153364871,
      "loss": 0.2241,
      "step": 2332
    },
    {
      "epoch": 1.0599727396637892,
      "grad_norm": 3.3051950931549072,
      "learning_rate": 0.0006517625515031284,
      "loss": 0.4585,
      "step": 2333
    },
    {
      "epoch": 1.060427078600636,
      "grad_norm": 4.111680030822754,
      "learning_rate": 0.0006516099496413856,
      "loss": 0.5334,
      "step": 2334
    },
    {
      "epoch": 1.060881417537483,
      "grad_norm": 4.029128551483154,
      "learning_rate": 0.0006514573477796429,
      "loss": 0.434,
      "step": 2335
    },
    {
      "epoch": 1.0613357564743298,
      "grad_norm": 1.8262900114059448,
      "learning_rate": 0.0006513047459179003,
      "loss": 0.0744,
      "step": 2336
    },
    {
      "epoch": 1.0617900954111767,
      "grad_norm": 2.9997446537017822,
      "learning_rate": 0.0006511521440561575,
      "loss": 0.1855,
      "step": 2337
    },
    {
      "epoch": 1.0622444343480235,
      "grad_norm": 5.595127582550049,
      "learning_rate": 0.0006509995421944148,
      "loss": 0.8877,
      "step": 2338
    },
    {
      "epoch": 1.0626987732848705,
      "grad_norm": 5.955653667449951,
      "learning_rate": 0.0006508469403326721,
      "loss": 0.5118,
      "step": 2339
    },
    {
      "epoch": 1.0631531122217175,
      "grad_norm": 3.9157633781433105,
      "learning_rate": 0.0006506943384709294,
      "loss": 0.2392,
      "step": 2340
    },
    {
      "epoch": 1.0636074511585643,
      "grad_norm": 6.425772666931152,
      "learning_rate": 0.0006505417366091867,
      "loss": 0.8666,
      "step": 2341
    },
    {
      "epoch": 1.0640617900954112,
      "grad_norm": 6.448369026184082,
      "learning_rate": 0.0006503891347474439,
      "loss": 0.4715,
      "step": 2342
    },
    {
      "epoch": 1.064516129032258,
      "grad_norm": 5.407177925109863,
      "learning_rate": 0.0006502365328857012,
      "loss": 0.5139,
      "step": 2343
    },
    {
      "epoch": 1.064970467969105,
      "grad_norm": 5.308297634124756,
      "learning_rate": 0.0006500839310239584,
      "loss": 1.0113,
      "step": 2344
    },
    {
      "epoch": 1.0654248069059518,
      "grad_norm": 5.550444602966309,
      "learning_rate": 0.0006499313291622158,
      "loss": 0.7008,
      "step": 2345
    },
    {
      "epoch": 1.0658791458427987,
      "grad_norm": 6.7486748695373535,
      "learning_rate": 0.000649778727300473,
      "loss": 0.5407,
      "step": 2346
    },
    {
      "epoch": 1.0663334847796455,
      "grad_norm": 4.492806911468506,
      "learning_rate": 0.0006496261254387303,
      "loss": 0.3711,
      "step": 2347
    },
    {
      "epoch": 1.0667878237164925,
      "grad_norm": 5.591123580932617,
      "learning_rate": 0.0006494735235769877,
      "loss": 0.3426,
      "step": 2348
    },
    {
      "epoch": 1.0672421626533395,
      "grad_norm": 6.740211486816406,
      "learning_rate": 0.0006493209217152449,
      "loss": 0.5094,
      "step": 2349
    },
    {
      "epoch": 1.0676965015901863,
      "grad_norm": 3.16058087348938,
      "learning_rate": 0.0006491683198535022,
      "loss": 0.3026,
      "step": 2350
    },
    {
      "epoch": 1.0681508405270332,
      "grad_norm": 3.778519630432129,
      "learning_rate": 0.0006490157179917595,
      "loss": 0.1453,
      "step": 2351
    },
    {
      "epoch": 1.06860517946388,
      "grad_norm": 7.254361629486084,
      "learning_rate": 0.0006488631161300168,
      "loss": 0.7567,
      "step": 2352
    },
    {
      "epoch": 1.069059518400727,
      "grad_norm": 4.274618148803711,
      "learning_rate": 0.0006487105142682741,
      "loss": 0.3304,
      "step": 2353
    },
    {
      "epoch": 1.0695138573375738,
      "grad_norm": 7.185515403747559,
      "learning_rate": 0.0006485579124065314,
      "loss": 0.3068,
      "step": 2354
    },
    {
      "epoch": 1.0699681962744207,
      "grad_norm": 4.164913654327393,
      "learning_rate": 0.0006484053105447887,
      "loss": 0.9012,
      "step": 2355
    },
    {
      "epoch": 1.0704225352112675,
      "grad_norm": 6.459129333496094,
      "learning_rate": 0.0006482527086830459,
      "loss": 1.1778,
      "step": 2356
    },
    {
      "epoch": 1.0708768741481145,
      "grad_norm": 5.187686920166016,
      "learning_rate": 0.0006481001068213033,
      "loss": 0.5726,
      "step": 2357
    },
    {
      "epoch": 1.0713312130849615,
      "grad_norm": 7.2819342613220215,
      "learning_rate": 0.0006479475049595606,
      "loss": 0.7216,
      "step": 2358
    },
    {
      "epoch": 1.0717855520218083,
      "grad_norm": 6.013956069946289,
      "learning_rate": 0.0006477949030978178,
      "loss": 0.4212,
      "step": 2359
    },
    {
      "epoch": 1.0722398909586552,
      "grad_norm": 4.686492919921875,
      "learning_rate": 0.0006476423012360752,
      "loss": 0.3855,
      "step": 2360
    },
    {
      "epoch": 1.072694229895502,
      "grad_norm": 1.658806562423706,
      "learning_rate": 0.0006474896993743323,
      "loss": 0.1333,
      "step": 2361
    },
    {
      "epoch": 1.073148568832349,
      "grad_norm": 5.538332939147949,
      "learning_rate": 0.0006473370975125896,
      "loss": 0.579,
      "step": 2362
    },
    {
      "epoch": 1.0736029077691958,
      "grad_norm": 4.560490608215332,
      "learning_rate": 0.000647184495650847,
      "loss": 0.9585,
      "step": 2363
    },
    {
      "epoch": 1.0740572467060427,
      "grad_norm": 4.189254283905029,
      "learning_rate": 0.0006470318937891042,
      "loss": 0.3737,
      "step": 2364
    },
    {
      "epoch": 1.0745115856428895,
      "grad_norm": 3.5739526748657227,
      "learning_rate": 0.0006468792919273615,
      "loss": 0.2685,
      "step": 2365
    },
    {
      "epoch": 1.0749659245797365,
      "grad_norm": 3.4747672080993652,
      "learning_rate": 0.0006467266900656188,
      "loss": 0.3734,
      "step": 2366
    },
    {
      "epoch": 1.0754202635165835,
      "grad_norm": 3.1124472618103027,
      "learning_rate": 0.0006465740882038761,
      "loss": 0.1456,
      "step": 2367
    },
    {
      "epoch": 1.0758746024534303,
      "grad_norm": 7.35132360458374,
      "learning_rate": 0.0006464214863421333,
      "loss": 0.5374,
      "step": 2368
    },
    {
      "epoch": 1.0763289413902772,
      "grad_norm": 6.302086353302002,
      "learning_rate": 0.0006462688844803907,
      "loss": 0.574,
      "step": 2369
    },
    {
      "epoch": 1.076783280327124,
      "grad_norm": 5.873526096343994,
      "learning_rate": 0.000646116282618648,
      "loss": 0.9046,
      "step": 2370
    },
    {
      "epoch": 1.077237619263971,
      "grad_norm": 7.316616535186768,
      "learning_rate": 0.0006459636807569052,
      "loss": 1.297,
      "step": 2371
    },
    {
      "epoch": 1.0776919582008178,
      "grad_norm": 7.429558753967285,
      "learning_rate": 0.0006458110788951626,
      "loss": 0.8722,
      "step": 2372
    },
    {
      "epoch": 1.0781462971376647,
      "grad_norm": 5.7547688484191895,
      "learning_rate": 0.0006456584770334198,
      "loss": 0.7773,
      "step": 2373
    },
    {
      "epoch": 1.0786006360745115,
      "grad_norm": 6.401236534118652,
      "learning_rate": 0.0006455058751716771,
      "loss": 1.12,
      "step": 2374
    },
    {
      "epoch": 1.0790549750113585,
      "grad_norm": 6.376558303833008,
      "learning_rate": 0.0006453532733099345,
      "loss": 0.819,
      "step": 2375
    },
    {
      "epoch": 1.0795093139482053,
      "grad_norm": 5.9498982429504395,
      "learning_rate": 0.0006452006714481917,
      "loss": 0.7045,
      "step": 2376
    },
    {
      "epoch": 1.0799636528850522,
      "grad_norm": 5.2795538902282715,
      "learning_rate": 0.000645048069586449,
      "loss": 0.6836,
      "step": 2377
    },
    {
      "epoch": 1.0804179918218992,
      "grad_norm": 8.635455131530762,
      "learning_rate": 0.0006448954677247063,
      "loss": 0.926,
      "step": 2378
    },
    {
      "epoch": 1.080872330758746,
      "grad_norm": 4.2005615234375,
      "learning_rate": 0.0006447428658629635,
      "loss": 0.7103,
      "step": 2379
    },
    {
      "epoch": 1.081326669695593,
      "grad_norm": 4.688621997833252,
      "learning_rate": 0.0006445902640012207,
      "loss": 0.4675,
      "step": 2380
    },
    {
      "epoch": 1.0817810086324398,
      "grad_norm": 6.766337871551514,
      "learning_rate": 0.0006444376621394781,
      "loss": 0.7158,
      "step": 2381
    },
    {
      "epoch": 1.0822353475692867,
      "grad_norm": 6.158735752105713,
      "learning_rate": 0.0006442850602777354,
      "loss": 0.973,
      "step": 2382
    },
    {
      "epoch": 1.0826896865061335,
      "grad_norm": 3.4013259410858154,
      "learning_rate": 0.0006441324584159926,
      "loss": 0.332,
      "step": 2383
    },
    {
      "epoch": 1.0831440254429805,
      "grad_norm": 2.9837169647216797,
      "learning_rate": 0.00064397985655425,
      "loss": 0.1325,
      "step": 2384
    },
    {
      "epoch": 1.0835983643798273,
      "grad_norm": 7.085361003875732,
      "learning_rate": 0.0006438272546925072,
      "loss": 1.0325,
      "step": 2385
    },
    {
      "epoch": 1.0840527033166742,
      "grad_norm": 6.46080207824707,
      "learning_rate": 0.0006436746528307645,
      "loss": 0.9022,
      "step": 2386
    },
    {
      "epoch": 1.084507042253521,
      "grad_norm": 4.205491542816162,
      "learning_rate": 0.0006435220509690219,
      "loss": 0.599,
      "step": 2387
    },
    {
      "epoch": 1.084961381190368,
      "grad_norm": 5.070319652557373,
      "learning_rate": 0.0006433694491072791,
      "loss": 0.6495,
      "step": 2388
    },
    {
      "epoch": 1.085415720127215,
      "grad_norm": 2.664996385574341,
      "learning_rate": 0.0006432168472455364,
      "loss": 0.0987,
      "step": 2389
    },
    {
      "epoch": 1.0858700590640618,
      "grad_norm": 6.936916828155518,
      "learning_rate": 0.0006430642453837937,
      "loss": 1.1322,
      "step": 2390
    },
    {
      "epoch": 1.0863243980009087,
      "grad_norm": 4.86273717880249,
      "learning_rate": 0.000642911643522051,
      "loss": 0.4232,
      "step": 2391
    },
    {
      "epoch": 1.0867787369377555,
      "grad_norm": 5.499362945556641,
      "learning_rate": 0.0006427590416603082,
      "loss": 0.683,
      "step": 2392
    },
    {
      "epoch": 1.0872330758746025,
      "grad_norm": 3.904902458190918,
      "learning_rate": 0.0006426064397985656,
      "loss": 0.4252,
      "step": 2393
    },
    {
      "epoch": 1.0876874148114493,
      "grad_norm": 10.064096450805664,
      "learning_rate": 0.0006424538379368229,
      "loss": 1.9255,
      "step": 2394
    },
    {
      "epoch": 1.0881417537482962,
      "grad_norm": 2.634481906890869,
      "learning_rate": 0.0006423012360750801,
      "loss": 0.2029,
      "step": 2395
    },
    {
      "epoch": 1.088596092685143,
      "grad_norm": 5.561137676239014,
      "learning_rate": 0.0006421486342133375,
      "loss": 1.048,
      "step": 2396
    },
    {
      "epoch": 1.08905043162199,
      "grad_norm": 7.071663856506348,
      "learning_rate": 0.0006419960323515946,
      "loss": 0.757,
      "step": 2397
    },
    {
      "epoch": 1.089504770558837,
      "grad_norm": 3.580366849899292,
      "learning_rate": 0.0006418434304898519,
      "loss": 0.6586,
      "step": 2398
    },
    {
      "epoch": 1.0899591094956838,
      "grad_norm": 3.636291265487671,
      "learning_rate": 0.0006416908286281093,
      "loss": 0.3025,
      "step": 2399
    },
    {
      "epoch": 1.0904134484325307,
      "grad_norm": 5.132367134094238,
      "learning_rate": 0.0006415382267663665,
      "loss": 0.8992,
      "step": 2400
    },
    {
      "epoch": 1.0908677873693775,
      "grad_norm": 4.810544490814209,
      "learning_rate": 0.0006413856249046238,
      "loss": 0.6668,
      "step": 2401
    },
    {
      "epoch": 1.0913221263062245,
      "grad_norm": 6.774171352386475,
      "learning_rate": 0.0006412330230428811,
      "loss": 0.8185,
      "step": 2402
    },
    {
      "epoch": 1.0917764652430713,
      "grad_norm": 6.311766624450684,
      "learning_rate": 0.0006410804211811384,
      "loss": 0.7361,
      "step": 2403
    },
    {
      "epoch": 1.0922308041799182,
      "grad_norm": 4.125761032104492,
      "learning_rate": 0.0006409278193193957,
      "loss": 0.4594,
      "step": 2404
    },
    {
      "epoch": 1.092685143116765,
      "grad_norm": 4.091350078582764,
      "learning_rate": 0.000640775217457653,
      "loss": 0.3737,
      "step": 2405
    },
    {
      "epoch": 1.093139482053612,
      "grad_norm": 4.919712066650391,
      "learning_rate": 0.0006406226155959103,
      "loss": 0.9048,
      "step": 2406
    },
    {
      "epoch": 1.093593820990459,
      "grad_norm": 5.511566638946533,
      "learning_rate": 0.0006404700137341675,
      "loss": 0.9234,
      "step": 2407
    },
    {
      "epoch": 1.0940481599273058,
      "grad_norm": 7.121581077575684,
      "learning_rate": 0.0006403174118724249,
      "loss": 0.6203,
      "step": 2408
    },
    {
      "epoch": 1.0945024988641527,
      "grad_norm": 6.504981994628906,
      "learning_rate": 0.0006401648100106822,
      "loss": 0.6115,
      "step": 2409
    },
    {
      "epoch": 1.0949568378009995,
      "grad_norm": 6.59949254989624,
      "learning_rate": 0.0006400122081489394,
      "loss": 0.4737,
      "step": 2410
    },
    {
      "epoch": 1.0954111767378465,
      "grad_norm": 6.921025276184082,
      "learning_rate": 0.0006398596062871968,
      "loss": 1.1665,
      "step": 2411
    },
    {
      "epoch": 1.0958655156746933,
      "grad_norm": 3.8173582553863525,
      "learning_rate": 0.000639707004425454,
      "loss": 0.5503,
      "step": 2412
    },
    {
      "epoch": 1.0963198546115402,
      "grad_norm": 5.051908016204834,
      "learning_rate": 0.0006395544025637113,
      "loss": 0.3606,
      "step": 2413
    },
    {
      "epoch": 1.096774193548387,
      "grad_norm": 5.284976005554199,
      "learning_rate": 0.0006394018007019686,
      "loss": 0.8475,
      "step": 2414
    },
    {
      "epoch": 1.097228532485234,
      "grad_norm": 4.381118297576904,
      "learning_rate": 0.0006392491988402258,
      "loss": 0.453,
      "step": 2415
    },
    {
      "epoch": 1.097682871422081,
      "grad_norm": 5.46884298324585,
      "learning_rate": 0.000639096596978483,
      "loss": 0.6535,
      "step": 2416
    },
    {
      "epoch": 1.0981372103589278,
      "grad_norm": 6.5378289222717285,
      "learning_rate": 0.0006389439951167404,
      "loss": 0.7219,
      "step": 2417
    },
    {
      "epoch": 1.0985915492957747,
      "grad_norm": 8.133096694946289,
      "learning_rate": 0.0006387913932549977,
      "loss": 1.357,
      "step": 2418
    },
    {
      "epoch": 1.0990458882326215,
      "grad_norm": 7.63338041305542,
      "learning_rate": 0.0006386387913932549,
      "loss": 1.0319,
      "step": 2419
    },
    {
      "epoch": 1.0995002271694685,
      "grad_norm": 4.661574840545654,
      "learning_rate": 0.0006384861895315123,
      "loss": 0.5886,
      "step": 2420
    },
    {
      "epoch": 1.0999545661063153,
      "grad_norm": 4.22615385055542,
      "learning_rate": 0.0006383335876697696,
      "loss": 0.3062,
      "step": 2421
    },
    {
      "epoch": 1.1004089050431622,
      "grad_norm": 11.717535018920898,
      "learning_rate": 0.0006381809858080268,
      "loss": 1.0694,
      "step": 2422
    },
    {
      "epoch": 1.100863243980009,
      "grad_norm": 6.6558380126953125,
      "learning_rate": 0.0006380283839462842,
      "loss": 0.4507,
      "step": 2423
    },
    {
      "epoch": 1.101317582916856,
      "grad_norm": 8.706762313842773,
      "learning_rate": 0.0006378757820845414,
      "loss": 1.3535,
      "step": 2424
    },
    {
      "epoch": 1.1017719218537028,
      "grad_norm": 5.836156845092773,
      "learning_rate": 0.0006377231802227987,
      "loss": 0.8409,
      "step": 2425
    },
    {
      "epoch": 1.1022262607905498,
      "grad_norm": 6.0104804039001465,
      "learning_rate": 0.000637570578361056,
      "loss": 1.1501,
      "step": 2426
    },
    {
      "epoch": 1.1026805997273967,
      "grad_norm": 2.2676775455474854,
      "learning_rate": 0.0006374179764993133,
      "loss": 0.1296,
      "step": 2427
    },
    {
      "epoch": 1.1031349386642435,
      "grad_norm": 2.1403558254241943,
      "learning_rate": 0.0006372653746375706,
      "loss": 0.2285,
      "step": 2428
    },
    {
      "epoch": 1.1035892776010905,
      "grad_norm": 8.519424438476562,
      "learning_rate": 0.0006371127727758279,
      "loss": 0.583,
      "step": 2429
    },
    {
      "epoch": 1.1040436165379373,
      "grad_norm": 7.606138706207275,
      "learning_rate": 0.0006369601709140852,
      "loss": 1.2715,
      "step": 2430
    },
    {
      "epoch": 1.1044979554747842,
      "grad_norm": 7.063669681549072,
      "learning_rate": 0.0006368075690523424,
      "loss": 0.9981,
      "step": 2431
    },
    {
      "epoch": 1.104952294411631,
      "grad_norm": 4.814049243927002,
      "learning_rate": 0.0006366549671905998,
      "loss": 0.6185,
      "step": 2432
    },
    {
      "epoch": 1.105406633348478,
      "grad_norm": 7.045476913452148,
      "learning_rate": 0.0006365023653288571,
      "loss": 0.9105,
      "step": 2433
    },
    {
      "epoch": 1.1058609722853248,
      "grad_norm": 4.263678073883057,
      "learning_rate": 0.0006363497634671142,
      "loss": 0.2352,
      "step": 2434
    },
    {
      "epoch": 1.1063153112221717,
      "grad_norm": 5.904022693634033,
      "learning_rate": 0.0006361971616053716,
      "loss": 0.4758,
      "step": 2435
    },
    {
      "epoch": 1.1067696501590185,
      "grad_norm": 5.265417575836182,
      "learning_rate": 0.0006360445597436288,
      "loss": 0.3966,
      "step": 2436
    },
    {
      "epoch": 1.1072239890958655,
      "grad_norm": 7.084077835083008,
      "learning_rate": 0.0006358919578818861,
      "loss": 0.828,
      "step": 2437
    },
    {
      "epoch": 1.1076783280327125,
      "grad_norm": 6.523720741271973,
      "learning_rate": 0.0006357393560201435,
      "loss": 0.671,
      "step": 2438
    },
    {
      "epoch": 1.1081326669695593,
      "grad_norm": 6.390928745269775,
      "learning_rate": 0.0006355867541584007,
      "loss": 1.0332,
      "step": 2439
    },
    {
      "epoch": 1.1085870059064062,
      "grad_norm": 8.991576194763184,
      "learning_rate": 0.000635434152296658,
      "loss": 0.7422,
      "step": 2440
    },
    {
      "epoch": 1.109041344843253,
      "grad_norm": 6.535904884338379,
      "learning_rate": 0.0006352815504349153,
      "loss": 1.0726,
      "step": 2441
    },
    {
      "epoch": 1.1094956837801,
      "grad_norm": 8.713395118713379,
      "learning_rate": 0.0006351289485731726,
      "loss": 0.7931,
      "step": 2442
    },
    {
      "epoch": 1.1099500227169468,
      "grad_norm": 4.437860488891602,
      "learning_rate": 0.0006349763467114298,
      "loss": 0.7738,
      "step": 2443
    },
    {
      "epoch": 1.1104043616537937,
      "grad_norm": 6.201180934906006,
      "learning_rate": 0.0006348237448496872,
      "loss": 0.5935,
      "step": 2444
    },
    {
      "epoch": 1.1108587005906405,
      "grad_norm": 5.445373058319092,
      "learning_rate": 0.0006346711429879445,
      "loss": 0.9581,
      "step": 2445
    },
    {
      "epoch": 1.1113130395274875,
      "grad_norm": 3.9942193031311035,
      "learning_rate": 0.0006345185411262018,
      "loss": 0.3517,
      "step": 2446
    },
    {
      "epoch": 1.1117673784643345,
      "grad_norm": 6.216915130615234,
      "learning_rate": 0.0006343659392644591,
      "loss": 0.7261,
      "step": 2447
    },
    {
      "epoch": 1.1122217174011813,
      "grad_norm": 2.2767832279205322,
      "learning_rate": 0.0006342133374027163,
      "loss": 0.1513,
      "step": 2448
    },
    {
      "epoch": 1.1126760563380282,
      "grad_norm": 6.04709529876709,
      "learning_rate": 0.0006340607355409737,
      "loss": 1.0358,
      "step": 2449
    },
    {
      "epoch": 1.113130395274875,
      "grad_norm": 2.661626100540161,
      "learning_rate": 0.000633908133679231,
      "loss": 0.2937,
      "step": 2450
    },
    {
      "epoch": 1.113584734211722,
      "grad_norm": 5.104527950286865,
      "learning_rate": 0.0006337555318174882,
      "loss": 0.558,
      "step": 2451
    },
    {
      "epoch": 1.1140390731485688,
      "grad_norm": 2.1205620765686035,
      "learning_rate": 0.0006336029299557454,
      "loss": 0.3471,
      "step": 2452
    },
    {
      "epoch": 1.1144934120854157,
      "grad_norm": 9.66561508178711,
      "learning_rate": 0.0006334503280940027,
      "loss": 1.8903,
      "step": 2453
    },
    {
      "epoch": 1.1149477510222625,
      "grad_norm": 6.509611129760742,
      "learning_rate": 0.00063329772623226,
      "loss": 0.4798,
      "step": 2454
    },
    {
      "epoch": 1.1154020899591095,
      "grad_norm": 4.599593162536621,
      "learning_rate": 0.0006331451243705174,
      "loss": 0.4985,
      "step": 2455
    },
    {
      "epoch": 1.1158564288959565,
      "grad_norm": 5.890725612640381,
      "learning_rate": 0.0006329925225087746,
      "loss": 0.8067,
      "step": 2456
    },
    {
      "epoch": 1.1163107678328033,
      "grad_norm": 4.694099426269531,
      "learning_rate": 0.0006328399206470319,
      "loss": 0.7901,
      "step": 2457
    },
    {
      "epoch": 1.1167651067696502,
      "grad_norm": 4.450953006744385,
      "learning_rate": 0.0006326873187852892,
      "loss": 0.616,
      "step": 2458
    },
    {
      "epoch": 1.117219445706497,
      "grad_norm": 4.29156494140625,
      "learning_rate": 0.0006325347169235465,
      "loss": 0.4571,
      "step": 2459
    },
    {
      "epoch": 1.117673784643344,
      "grad_norm": 5.4457502365112305,
      "learning_rate": 0.0006323821150618037,
      "loss": 0.417,
      "step": 2460
    },
    {
      "epoch": 1.1181281235801908,
      "grad_norm": 4.198627948760986,
      "learning_rate": 0.0006322295132000611,
      "loss": 0.2158,
      "step": 2461
    },
    {
      "epoch": 1.1185824625170377,
      "grad_norm": 6.531342506408691,
      "learning_rate": 0.0006320769113383184,
      "loss": 0.2807,
      "step": 2462
    },
    {
      "epoch": 1.1190368014538845,
      "grad_norm": 5.324395656585693,
      "learning_rate": 0.0006319243094765756,
      "loss": 0.5585,
      "step": 2463
    },
    {
      "epoch": 1.1194911403907315,
      "grad_norm": 4.656090259552002,
      "learning_rate": 0.000631771707614833,
      "loss": 0.3549,
      "step": 2464
    },
    {
      "epoch": 1.1199454793275785,
      "grad_norm": 5.977701187133789,
      "learning_rate": 0.0006316191057530902,
      "loss": 0.3147,
      "step": 2465
    },
    {
      "epoch": 1.1203998182644253,
      "grad_norm": 5.148125171661377,
      "learning_rate": 0.0006314665038913475,
      "loss": 0.4463,
      "step": 2466
    },
    {
      "epoch": 1.1208541572012722,
      "grad_norm": 6.9299421310424805,
      "learning_rate": 0.0006313139020296049,
      "loss": 0.9058,
      "step": 2467
    },
    {
      "epoch": 1.121308496138119,
      "grad_norm": 8.036308288574219,
      "learning_rate": 0.0006311613001678621,
      "loss": 0.9505,
      "step": 2468
    },
    {
      "epoch": 1.121762835074966,
      "grad_norm": 3.7580037117004395,
      "learning_rate": 0.0006310086983061194,
      "loss": 0.3012,
      "step": 2469
    },
    {
      "epoch": 1.1222171740118128,
      "grad_norm": 8.648531913757324,
      "learning_rate": 0.0006308560964443766,
      "loss": 0.9706,
      "step": 2470
    },
    {
      "epoch": 1.1226715129486597,
      "grad_norm": 5.394298553466797,
      "learning_rate": 0.0006307034945826339,
      "loss": 0.3758,
      "step": 2471
    },
    {
      "epoch": 1.1231258518855065,
      "grad_norm": 3.8575403690338135,
      "learning_rate": 0.0006305508927208911,
      "loss": 0.4181,
      "step": 2472
    },
    {
      "epoch": 1.1235801908223535,
      "grad_norm": 4.168056488037109,
      "learning_rate": 0.0006303982908591485,
      "loss": 0.4248,
      "step": 2473
    },
    {
      "epoch": 1.1240345297592003,
      "grad_norm": 7.953227996826172,
      "learning_rate": 0.0006302456889974058,
      "loss": 1.1979,
      "step": 2474
    },
    {
      "epoch": 1.1244888686960473,
      "grad_norm": 6.35919189453125,
      "learning_rate": 0.000630093087135663,
      "loss": 0.8393,
      "step": 2475
    },
    {
      "epoch": 1.1249432076328942,
      "grad_norm": 6.291561126708984,
      "learning_rate": 0.0006299404852739204,
      "loss": 1.0751,
      "step": 2476
    },
    {
      "epoch": 1.125397546569741,
      "grad_norm": 6.176083087921143,
      "learning_rate": 0.0006297878834121776,
      "loss": 0.3168,
      "step": 2477
    },
    {
      "epoch": 1.125851885506588,
      "grad_norm": 5.461402893066406,
      "learning_rate": 0.0006296352815504349,
      "loss": 0.3992,
      "step": 2478
    },
    {
      "epoch": 1.1263062244434348,
      "grad_norm": 6.067312240600586,
      "learning_rate": 0.0006294826796886923,
      "loss": 0.6677,
      "step": 2479
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 3.1427817344665527,
      "learning_rate": 0.0006293300778269495,
      "loss": 0.3438,
      "step": 2480
    },
    {
      "epoch": 1.1272149023171285,
      "grad_norm": 7.016294479370117,
      "learning_rate": 0.0006291774759652068,
      "loss": 0.9577,
      "step": 2481
    },
    {
      "epoch": 1.1276692412539755,
      "grad_norm": 6.961044788360596,
      "learning_rate": 0.0006290248741034641,
      "loss": 0.5071,
      "step": 2482
    },
    {
      "epoch": 1.1281235801908223,
      "grad_norm": 8.074202537536621,
      "learning_rate": 0.0006288722722417214,
      "loss": 1.142,
      "step": 2483
    },
    {
      "epoch": 1.1285779191276692,
      "grad_norm": 5.53190279006958,
      "learning_rate": 0.0006287196703799787,
      "loss": 0.4231,
      "step": 2484
    },
    {
      "epoch": 1.129032258064516,
      "grad_norm": 3.7976791858673096,
      "learning_rate": 0.000628567068518236,
      "loss": 0.1485,
      "step": 2485
    },
    {
      "epoch": 1.129486597001363,
      "grad_norm": 3.130258321762085,
      "learning_rate": 0.0006284144666564933,
      "loss": 0.3234,
      "step": 2486
    },
    {
      "epoch": 1.12994093593821,
      "grad_norm": 8.03842830657959,
      "learning_rate": 0.0006282618647947505,
      "loss": 1.5097,
      "step": 2487
    },
    {
      "epoch": 1.1303952748750568,
      "grad_norm": 4.271386623382568,
      "learning_rate": 0.0006281092629330078,
      "loss": 0.6233,
      "step": 2488
    },
    {
      "epoch": 1.1308496138119037,
      "grad_norm": 4.053390979766846,
      "learning_rate": 0.000627956661071265,
      "loss": 0.4977,
      "step": 2489
    },
    {
      "epoch": 1.1313039527487505,
      "grad_norm": 5.780361652374268,
      "learning_rate": 0.0006278040592095223,
      "loss": 0.6187,
      "step": 2490
    },
    {
      "epoch": 1.1317582916855975,
      "grad_norm": 5.133591651916504,
      "learning_rate": 0.0006276514573477797,
      "loss": 0.6761,
      "step": 2491
    },
    {
      "epoch": 1.1322126306224443,
      "grad_norm": 6.5390448570251465,
      "learning_rate": 0.0006274988554860369,
      "loss": 0.889,
      "step": 2492
    },
    {
      "epoch": 1.1326669695592912,
      "grad_norm": 4.660546779632568,
      "learning_rate": 0.0006273462536242942,
      "loss": 0.6636,
      "step": 2493
    },
    {
      "epoch": 1.133121308496138,
      "grad_norm": 7.077826023101807,
      "learning_rate": 0.0006271936517625515,
      "loss": 0.7832,
      "step": 2494
    },
    {
      "epoch": 1.133575647432985,
      "grad_norm": 7.14663028717041,
      "learning_rate": 0.0006270410499008088,
      "loss": 1.6401,
      "step": 2495
    },
    {
      "epoch": 1.134029986369832,
      "grad_norm": 4.652126312255859,
      "learning_rate": 0.0006268884480390661,
      "loss": 0.8281,
      "step": 2496
    },
    {
      "epoch": 1.1344843253066788,
      "grad_norm": 9.885639190673828,
      "learning_rate": 0.0006267358461773234,
      "loss": 0.7551,
      "step": 2497
    },
    {
      "epoch": 1.1349386642435257,
      "grad_norm": 5.393986225128174,
      "learning_rate": 0.0006265832443155807,
      "loss": 0.7056,
      "step": 2498
    },
    {
      "epoch": 1.1353930031803725,
      "grad_norm": 8.150755882263184,
      "learning_rate": 0.0006264306424538379,
      "loss": 0.8261,
      "step": 2499
    },
    {
      "epoch": 1.1358473421172195,
      "grad_norm": 5.659180641174316,
      "learning_rate": 0.0006262780405920953,
      "loss": 0.5805,
      "step": 2500
    },
    {
      "epoch": 1.1363016810540663,
      "grad_norm": 7.363814830780029,
      "learning_rate": 0.0006261254387303526,
      "loss": 1.0295,
      "step": 2501
    },
    {
      "epoch": 1.1367560199909132,
      "grad_norm": 6.277945041656494,
      "learning_rate": 0.0006259728368686098,
      "loss": 0.5745,
      "step": 2502
    },
    {
      "epoch": 1.13721035892776,
      "grad_norm": 2.8920235633850098,
      "learning_rate": 0.0006258202350068672,
      "loss": 0.4079,
      "step": 2503
    },
    {
      "epoch": 1.137664697864607,
      "grad_norm": 4.549100875854492,
      "learning_rate": 0.0006256676331451244,
      "loss": 0.4249,
      "step": 2504
    },
    {
      "epoch": 1.138119036801454,
      "grad_norm": 5.373491287231445,
      "learning_rate": 0.0006255150312833817,
      "loss": 1.1036,
      "step": 2505
    },
    {
      "epoch": 1.1385733757383008,
      "grad_norm": 6.103880405426025,
      "learning_rate": 0.0006253624294216391,
      "loss": 0.7081,
      "step": 2506
    },
    {
      "epoch": 1.1390277146751477,
      "grad_norm": 6.57904052734375,
      "learning_rate": 0.0006252098275598962,
      "loss": 0.5657,
      "step": 2507
    },
    {
      "epoch": 1.1394820536119945,
      "grad_norm": 16.31537437438965,
      "learning_rate": 0.0006250572256981535,
      "loss": 0.7153,
      "step": 2508
    },
    {
      "epoch": 1.1399363925488415,
      "grad_norm": 5.172234058380127,
      "learning_rate": 0.0006249046238364108,
      "loss": 0.483,
      "step": 2509
    },
    {
      "epoch": 1.1403907314856883,
      "grad_norm": 6.919475078582764,
      "learning_rate": 0.0006247520219746681,
      "loss": 1.2217,
      "step": 2510
    },
    {
      "epoch": 1.1408450704225352,
      "grad_norm": 4.884195804595947,
      "learning_rate": 0.0006245994201129253,
      "loss": 0.5007,
      "step": 2511
    },
    {
      "epoch": 1.141299409359382,
      "grad_norm": 4.854722499847412,
      "learning_rate": 0.0006244468182511827,
      "loss": 0.5916,
      "step": 2512
    },
    {
      "epoch": 1.141753748296229,
      "grad_norm": 6.498429775238037,
      "learning_rate": 0.00062429421638944,
      "loss": 1.0738,
      "step": 2513
    },
    {
      "epoch": 1.142208087233076,
      "grad_norm": 5.567363739013672,
      "learning_rate": 0.0006241416145276972,
      "loss": 0.8027,
      "step": 2514
    },
    {
      "epoch": 1.1426624261699228,
      "grad_norm": 4.264369964599609,
      "learning_rate": 0.0006239890126659546,
      "loss": 0.3077,
      "step": 2515
    },
    {
      "epoch": 1.1431167651067697,
      "grad_norm": 5.69927453994751,
      "learning_rate": 0.0006238364108042118,
      "loss": 0.5507,
      "step": 2516
    },
    {
      "epoch": 1.1435711040436165,
      "grad_norm": 4.637866497039795,
      "learning_rate": 0.0006236838089424691,
      "loss": 0.6334,
      "step": 2517
    },
    {
      "epoch": 1.1440254429804635,
      "grad_norm": 3.834022045135498,
      "learning_rate": 0.0006235312070807265,
      "loss": 0.3963,
      "step": 2518
    },
    {
      "epoch": 1.1444797819173103,
      "grad_norm": 5.237020492553711,
      "learning_rate": 0.0006233786052189837,
      "loss": 0.4453,
      "step": 2519
    },
    {
      "epoch": 1.1449341208541572,
      "grad_norm": 2.4999210834503174,
      "learning_rate": 0.000623226003357241,
      "loss": 0.3892,
      "step": 2520
    },
    {
      "epoch": 1.145388459791004,
      "grad_norm": 7.206876754760742,
      "learning_rate": 0.0006230734014954983,
      "loss": 1.0524,
      "step": 2521
    },
    {
      "epoch": 1.145842798727851,
      "grad_norm": 7.824370861053467,
      "learning_rate": 0.0006229207996337556,
      "loss": 1.1954,
      "step": 2522
    },
    {
      "epoch": 1.146297137664698,
      "grad_norm": 4.672296047210693,
      "learning_rate": 0.0006227681977720128,
      "loss": 0.567,
      "step": 2523
    },
    {
      "epoch": 1.1467514766015448,
      "grad_norm": 4.555317401885986,
      "learning_rate": 0.0006226155959102702,
      "loss": 0.393,
      "step": 2524
    },
    {
      "epoch": 1.1472058155383915,
      "grad_norm": 4.833882808685303,
      "learning_rate": 0.0006224629940485274,
      "loss": 0.8202,
      "step": 2525
    },
    {
      "epoch": 1.1476601544752385,
      "grad_norm": 3.44449520111084,
      "learning_rate": 0.0006223103921867846,
      "loss": 0.228,
      "step": 2526
    },
    {
      "epoch": 1.1481144934120855,
      "grad_norm": 4.221444129943848,
      "learning_rate": 0.000622157790325042,
      "loss": 0.2228,
      "step": 2527
    },
    {
      "epoch": 1.1485688323489323,
      "grad_norm": 5.331342697143555,
      "learning_rate": 0.0006220051884632992,
      "loss": 0.5329,
      "step": 2528
    },
    {
      "epoch": 1.1490231712857792,
      "grad_norm": 3.6310875415802,
      "learning_rate": 0.0006218525866015565,
      "loss": 0.9321,
      "step": 2529
    },
    {
      "epoch": 1.149477510222626,
      "grad_norm": 3.395500421524048,
      "learning_rate": 0.0006216999847398139,
      "loss": 0.2857,
      "step": 2530
    },
    {
      "epoch": 1.149931849159473,
      "grad_norm": 5.914249420166016,
      "learning_rate": 0.0006215473828780711,
      "loss": 0.8001,
      "step": 2531
    },
    {
      "epoch": 1.1503861880963198,
      "grad_norm": 8.38698673248291,
      "learning_rate": 0.0006213947810163284,
      "loss": 0.6236,
      "step": 2532
    },
    {
      "epoch": 1.1508405270331667,
      "grad_norm": 5.626473426818848,
      "learning_rate": 0.0006212421791545857,
      "loss": 0.5135,
      "step": 2533
    },
    {
      "epoch": 1.1512948659700135,
      "grad_norm": 7.684513092041016,
      "learning_rate": 0.000621089577292843,
      "loss": 1.189,
      "step": 2534
    },
    {
      "epoch": 1.1517492049068605,
      "grad_norm": 5.867911338806152,
      "learning_rate": 0.0006209369754311003,
      "loss": 1.5417,
      "step": 2535
    },
    {
      "epoch": 1.1522035438437075,
      "grad_norm": 6.584537029266357,
      "learning_rate": 0.0006207843735693576,
      "loss": 1.1486,
      "step": 2536
    },
    {
      "epoch": 1.1526578827805543,
      "grad_norm": 2.2324914932250977,
      "learning_rate": 0.0006206317717076149,
      "loss": 0.214,
      "step": 2537
    },
    {
      "epoch": 1.1531122217174012,
      "grad_norm": 7.319744110107422,
      "learning_rate": 0.0006204791698458721,
      "loss": 0.4292,
      "step": 2538
    },
    {
      "epoch": 1.153566560654248,
      "grad_norm": 6.272181034088135,
      "learning_rate": 0.0006203265679841295,
      "loss": 1.7393,
      "step": 2539
    },
    {
      "epoch": 1.154020899591095,
      "grad_norm": 7.487597942352295,
      "learning_rate": 0.0006201739661223867,
      "loss": 1.0933,
      "step": 2540
    },
    {
      "epoch": 1.1544752385279418,
      "grad_norm": 3.4230098724365234,
      "learning_rate": 0.000620021364260644,
      "loss": 0.1714,
      "step": 2541
    },
    {
      "epoch": 1.1549295774647887,
      "grad_norm": 2.423614978790283,
      "learning_rate": 0.0006198687623989014,
      "loss": 0.2245,
      "step": 2542
    },
    {
      "epoch": 1.1553839164016355,
      "grad_norm": 4.590084075927734,
      "learning_rate": 0.0006197161605371585,
      "loss": 0.3493,
      "step": 2543
    },
    {
      "epoch": 1.1558382553384825,
      "grad_norm": 5.232919692993164,
      "learning_rate": 0.0006195635586754158,
      "loss": 0.5824,
      "step": 2544
    },
    {
      "epoch": 1.1562925942753295,
      "grad_norm": 4.559950828552246,
      "learning_rate": 0.0006194109568136731,
      "loss": 0.6462,
      "step": 2545
    },
    {
      "epoch": 1.1567469332121763,
      "grad_norm": 5.551370143890381,
      "learning_rate": 0.0006192583549519304,
      "loss": 0.7912,
      "step": 2546
    },
    {
      "epoch": 1.1572012721490232,
      "grad_norm": 3.8842532634735107,
      "learning_rate": 0.0006191057530901877,
      "loss": 0.3295,
      "step": 2547
    },
    {
      "epoch": 1.15765561108587,
      "grad_norm": 4.613779067993164,
      "learning_rate": 0.000618953151228445,
      "loss": 0.3565,
      "step": 2548
    },
    {
      "epoch": 1.158109950022717,
      "grad_norm": 6.101029872894287,
      "learning_rate": 0.0006188005493667023,
      "loss": 1.2336,
      "step": 2549
    },
    {
      "epoch": 1.1585642889595638,
      "grad_norm": 7.574988842010498,
      "learning_rate": 0.0006186479475049595,
      "loss": 1.0084,
      "step": 2550
    },
    {
      "epoch": 1.1590186278964107,
      "grad_norm": 2.0246169567108154,
      "learning_rate": 0.0006184953456432169,
      "loss": 0.1256,
      "step": 2551
    },
    {
      "epoch": 1.1594729668332575,
      "grad_norm": 5.172162055969238,
      "learning_rate": 0.0006183427437814742,
      "loss": 0.3835,
      "step": 2552
    },
    {
      "epoch": 1.1599273057701045,
      "grad_norm": 6.390658855438232,
      "learning_rate": 0.0006181901419197314,
      "loss": 0.7543,
      "step": 2553
    },
    {
      "epoch": 1.1603816447069515,
      "grad_norm": 5.998085975646973,
      "learning_rate": 0.0006180375400579888,
      "loss": 0.9218,
      "step": 2554
    },
    {
      "epoch": 1.1608359836437983,
      "grad_norm": 4.442567348480225,
      "learning_rate": 0.000617884938196246,
      "loss": 0.5858,
      "step": 2555
    },
    {
      "epoch": 1.1612903225806452,
      "grad_norm": 6.477155685424805,
      "learning_rate": 0.0006177323363345033,
      "loss": 0.5955,
      "step": 2556
    },
    {
      "epoch": 1.161744661517492,
      "grad_norm": 4.970384120941162,
      "learning_rate": 0.0006175797344727607,
      "loss": 0.786,
      "step": 2557
    },
    {
      "epoch": 1.162199000454339,
      "grad_norm": 4.985353946685791,
      "learning_rate": 0.0006174271326110179,
      "loss": 0.4843,
      "step": 2558
    },
    {
      "epoch": 1.1626533393911858,
      "grad_norm": 2.500217914581299,
      "learning_rate": 0.0006172745307492752,
      "loss": 0.2076,
      "step": 2559
    },
    {
      "epoch": 1.1631076783280327,
      "grad_norm": 3.58182692527771,
      "learning_rate": 0.0006171219288875325,
      "loss": 0.3509,
      "step": 2560
    },
    {
      "epoch": 1.1635620172648795,
      "grad_norm": 3.425654649734497,
      "learning_rate": 0.0006169693270257897,
      "loss": 0.2313,
      "step": 2561
    },
    {
      "epoch": 1.1640163562017265,
      "grad_norm": 7.1617841720581055,
      "learning_rate": 0.0006168167251640469,
      "loss": 0.5551,
      "step": 2562
    },
    {
      "epoch": 1.1644706951385735,
      "grad_norm": 6.257028102874756,
      "learning_rate": 0.0006166641233023043,
      "loss": 2.0204,
      "step": 2563
    },
    {
      "epoch": 1.1649250340754203,
      "grad_norm": 3.575928211212158,
      "learning_rate": 0.0006165115214405616,
      "loss": 0.4452,
      "step": 2564
    },
    {
      "epoch": 1.1653793730122672,
      "grad_norm": 4.14722204208374,
      "learning_rate": 0.0006163589195788188,
      "loss": 0.2902,
      "step": 2565
    },
    {
      "epoch": 1.165833711949114,
      "grad_norm": 3.4534428119659424,
      "learning_rate": 0.0006162063177170762,
      "loss": 0.3465,
      "step": 2566
    },
    {
      "epoch": 1.166288050885961,
      "grad_norm": 6.671140670776367,
      "learning_rate": 0.0006160537158553334,
      "loss": 0.7701,
      "step": 2567
    },
    {
      "epoch": 1.1667423898228078,
      "grad_norm": 4.430437088012695,
      "learning_rate": 0.0006159011139935907,
      "loss": 0.2998,
      "step": 2568
    },
    {
      "epoch": 1.1671967287596547,
      "grad_norm": 3.7015528678894043,
      "learning_rate": 0.000615748512131848,
      "loss": 0.3442,
      "step": 2569
    },
    {
      "epoch": 1.1676510676965015,
      "grad_norm": 6.2740702629089355,
      "learning_rate": 0.0006155959102701053,
      "loss": 1.0763,
      "step": 2570
    },
    {
      "epoch": 1.1681054066333485,
      "grad_norm": 6.892889976501465,
      "learning_rate": 0.0006154433084083626,
      "loss": 1.0925,
      "step": 2571
    },
    {
      "epoch": 1.1685597455701955,
      "grad_norm": 7.45421028137207,
      "learning_rate": 0.0006152907065466199,
      "loss": 0.9184,
      "step": 2572
    },
    {
      "epoch": 1.1690140845070423,
      "grad_norm": 6.335560321807861,
      "learning_rate": 0.0006151381046848772,
      "loss": 0.8161,
      "step": 2573
    },
    {
      "epoch": 1.1694684234438892,
      "grad_norm": 6.396345615386963,
      "learning_rate": 0.0006149855028231344,
      "loss": 0.7359,
      "step": 2574
    },
    {
      "epoch": 1.169922762380736,
      "grad_norm": 8.040322303771973,
      "learning_rate": 0.0006148329009613918,
      "loss": 0.6732,
      "step": 2575
    },
    {
      "epoch": 1.170377101317583,
      "grad_norm": 1.5894242525100708,
      "learning_rate": 0.0006146802990996491,
      "loss": 0.0864,
      "step": 2576
    },
    {
      "epoch": 1.1708314402544298,
      "grad_norm": 4.345595359802246,
      "learning_rate": 0.0006145276972379063,
      "loss": 0.4084,
      "step": 2577
    },
    {
      "epoch": 1.1712857791912767,
      "grad_norm": 4.490791320800781,
      "learning_rate": 0.0006143750953761637,
      "loss": 0.8311,
      "step": 2578
    },
    {
      "epoch": 1.1717401181281235,
      "grad_norm": 3.775209426879883,
      "learning_rate": 0.0006142224935144209,
      "loss": 0.2722,
      "step": 2579
    },
    {
      "epoch": 1.1721944570649705,
      "grad_norm": 8.38598346710205,
      "learning_rate": 0.0006140698916526781,
      "loss": 1.5532,
      "step": 2580
    },
    {
      "epoch": 1.1726487960018173,
      "grad_norm": 5.861232757568359,
      "learning_rate": 0.0006139172897909355,
      "loss": 0.805,
      "step": 2581
    },
    {
      "epoch": 1.1731031349386642,
      "grad_norm": 6.1950178146362305,
      "learning_rate": 0.0006137646879291927,
      "loss": 0.9539,
      "step": 2582
    },
    {
      "epoch": 1.173557473875511,
      "grad_norm": 3.304558515548706,
      "learning_rate": 0.00061361208606745,
      "loss": 0.375,
      "step": 2583
    },
    {
      "epoch": 1.174011812812358,
      "grad_norm": 16.158174514770508,
      "learning_rate": 0.0006134594842057073,
      "loss": 0.9324,
      "step": 2584
    },
    {
      "epoch": 1.174466151749205,
      "grad_norm": 3.8099100589752197,
      "learning_rate": 0.0006133068823439646,
      "loss": 0.7324,
      "step": 2585
    },
    {
      "epoch": 1.1749204906860518,
      "grad_norm": 6.475446701049805,
      "learning_rate": 0.0006131542804822218,
      "loss": 0.1764,
      "step": 2586
    },
    {
      "epoch": 1.1753748296228987,
      "grad_norm": 3.265183210372925,
      "learning_rate": 0.0006130016786204792,
      "loss": 0.2646,
      "step": 2587
    },
    {
      "epoch": 1.1758291685597455,
      "grad_norm": 4.9916090965271,
      "learning_rate": 0.0006128490767587365,
      "loss": 1.1392,
      "step": 2588
    },
    {
      "epoch": 1.1762835074965925,
      "grad_norm": 2.9154586791992188,
      "learning_rate": 0.0006126964748969937,
      "loss": 0.2873,
      "step": 2589
    },
    {
      "epoch": 1.1767378464334393,
      "grad_norm": 7.811084747314453,
      "learning_rate": 0.0006125438730352511,
      "loss": 1.3994,
      "step": 2590
    },
    {
      "epoch": 1.1771921853702862,
      "grad_norm": 5.82649564743042,
      "learning_rate": 0.0006123912711735083,
      "loss": 0.8711,
      "step": 2591
    },
    {
      "epoch": 1.177646524307133,
      "grad_norm": 4.601613521575928,
      "learning_rate": 0.0006122386693117656,
      "loss": 0.418,
      "step": 2592
    },
    {
      "epoch": 1.17810086324398,
      "grad_norm": 13.808738708496094,
      "learning_rate": 0.000612086067450023,
      "loss": 1.8103,
      "step": 2593
    },
    {
      "epoch": 1.178555202180827,
      "grad_norm": 6.059254169464111,
      "learning_rate": 0.0006119334655882802,
      "loss": 0.7083,
      "step": 2594
    },
    {
      "epoch": 1.1790095411176738,
      "grad_norm": 7.045945167541504,
      "learning_rate": 0.0006117808637265375,
      "loss": 1.1759,
      "step": 2595
    },
    {
      "epoch": 1.1794638800545207,
      "grad_norm": 4.631771087646484,
      "learning_rate": 0.0006116282618647948,
      "loss": 0.5651,
      "step": 2596
    },
    {
      "epoch": 1.1799182189913675,
      "grad_norm": 6.419643878936768,
      "learning_rate": 0.0006114756600030521,
      "loss": 0.7648,
      "step": 2597
    },
    {
      "epoch": 1.1803725579282145,
      "grad_norm": 9.524314880371094,
      "learning_rate": 0.0006113230581413092,
      "loss": 1.7887,
      "step": 2598
    },
    {
      "epoch": 1.1808268968650613,
      "grad_norm": 5.399214267730713,
      "learning_rate": 0.0006111704562795666,
      "loss": 0.4506,
      "step": 2599
    },
    {
      "epoch": 1.1812812358019082,
      "grad_norm": 6.668942451477051,
      "learning_rate": 0.0006110178544178239,
      "loss": 1.0977,
      "step": 2600
    },
    {
      "epoch": 1.181735574738755,
      "grad_norm": 4.5344624519348145,
      "learning_rate": 0.0006108652525560811,
      "loss": 0.4162,
      "step": 2601
    },
    {
      "epoch": 1.182189913675602,
      "grad_norm": 5.278345584869385,
      "learning_rate": 0.0006107126506943385,
      "loss": 0.9275,
      "step": 2602
    },
    {
      "epoch": 1.182644252612449,
      "grad_norm": 8.60588550567627,
      "learning_rate": 0.0006105600488325957,
      "loss": 0.4661,
      "step": 2603
    },
    {
      "epoch": 1.1830985915492958,
      "grad_norm": 7.9513468742370605,
      "learning_rate": 0.000610407446970853,
      "loss": 0.6025,
      "step": 2604
    },
    {
      "epoch": 1.1835529304861427,
      "grad_norm": 6.461592674255371,
      "learning_rate": 0.0006102548451091104,
      "loss": 0.9093,
      "step": 2605
    },
    {
      "epoch": 1.1840072694229895,
      "grad_norm": 3.3517086505889893,
      "learning_rate": 0.0006101022432473676,
      "loss": 0.4824,
      "step": 2606
    },
    {
      "epoch": 1.1844616083598365,
      "grad_norm": 4.5454792976379395,
      "learning_rate": 0.0006099496413856249,
      "loss": 0.4492,
      "step": 2607
    },
    {
      "epoch": 1.1849159472966833,
      "grad_norm": 7.1621222496032715,
      "learning_rate": 0.0006097970395238822,
      "loss": 0.9459,
      "step": 2608
    },
    {
      "epoch": 1.1853702862335302,
      "grad_norm": 5.492733001708984,
      "learning_rate": 0.0006096444376621395,
      "loss": 0.8806,
      "step": 2609
    },
    {
      "epoch": 1.185824625170377,
      "grad_norm": 4.505495071411133,
      "learning_rate": 0.0006094918358003968,
      "loss": 0.1928,
      "step": 2610
    },
    {
      "epoch": 1.186278964107224,
      "grad_norm": 5.041391372680664,
      "learning_rate": 0.0006093392339386541,
      "loss": 0.4898,
      "step": 2611
    },
    {
      "epoch": 1.186733303044071,
      "grad_norm": 6.843071937561035,
      "learning_rate": 0.0006091866320769114,
      "loss": 0.6804,
      "step": 2612
    },
    {
      "epoch": 1.1871876419809178,
      "grad_norm": 4.471441745758057,
      "learning_rate": 0.0006090340302151686,
      "loss": 0.2647,
      "step": 2613
    },
    {
      "epoch": 1.1876419809177647,
      "grad_norm": 5.272098541259766,
      "learning_rate": 0.000608881428353426,
      "loss": 0.4604,
      "step": 2614
    },
    {
      "epoch": 1.1880963198546115,
      "grad_norm": 4.855858325958252,
      "learning_rate": 0.0006087288264916833,
      "loss": 0.8061,
      "step": 2615
    },
    {
      "epoch": 1.1885506587914585,
      "grad_norm": 5.374824047088623,
      "learning_rate": 0.0006085762246299404,
      "loss": 0.7174,
      "step": 2616
    },
    {
      "epoch": 1.1890049977283053,
      "grad_norm": 18.167221069335938,
      "learning_rate": 0.0006084236227681978,
      "loss": 1.0652,
      "step": 2617
    },
    {
      "epoch": 1.1894593366651522,
      "grad_norm": 5.702600002288818,
      "learning_rate": 0.000608271020906455,
      "loss": 0.5813,
      "step": 2618
    },
    {
      "epoch": 1.189913675601999,
      "grad_norm": 6.230209827423096,
      "learning_rate": 0.0006081184190447123,
      "loss": 1.2114,
      "step": 2619
    },
    {
      "epoch": 1.190368014538846,
      "grad_norm": 5.254981517791748,
      "learning_rate": 0.0006079658171829696,
      "loss": 0.534,
      "step": 2620
    },
    {
      "epoch": 1.190822353475693,
      "grad_norm": 5.047598838806152,
      "learning_rate": 0.0006078132153212269,
      "loss": 0.2653,
      "step": 2621
    },
    {
      "epoch": 1.1912766924125398,
      "grad_norm": 4.864027500152588,
      "learning_rate": 0.0006076606134594842,
      "loss": 1.1998,
      "step": 2622
    },
    {
      "epoch": 1.1917310313493867,
      "grad_norm": 5.505671501159668,
      "learning_rate": 0.0006075080115977415,
      "loss": 0.7486,
      "step": 2623
    },
    {
      "epoch": 1.1921853702862335,
      "grad_norm": 8.04446792602539,
      "learning_rate": 0.0006073554097359988,
      "loss": 0.5377,
      "step": 2624
    },
    {
      "epoch": 1.1926397092230805,
      "grad_norm": 2.694422721862793,
      "learning_rate": 0.000607202807874256,
      "loss": 0.2486,
      "step": 2625
    },
    {
      "epoch": 1.1930940481599273,
      "grad_norm": 3.777846097946167,
      "learning_rate": 0.0006070502060125134,
      "loss": 0.2365,
      "step": 2626
    },
    {
      "epoch": 1.1935483870967742,
      "grad_norm": 4.462184906005859,
      "learning_rate": 0.0006068976041507707,
      "loss": 0.5341,
      "step": 2627
    },
    {
      "epoch": 1.194002726033621,
      "grad_norm": 3.1106033325195312,
      "learning_rate": 0.0006067450022890279,
      "loss": 0.2005,
      "step": 2628
    },
    {
      "epoch": 1.194457064970468,
      "grad_norm": 5.278122425079346,
      "learning_rate": 0.0006065924004272853,
      "loss": 0.5032,
      "step": 2629
    },
    {
      "epoch": 1.1949114039073148,
      "grad_norm": 3.5053346157073975,
      "learning_rate": 0.0006064397985655425,
      "loss": 0.1385,
      "step": 2630
    },
    {
      "epoch": 1.1953657428441617,
      "grad_norm": 8.340495109558105,
      "learning_rate": 0.0006062871967037998,
      "loss": 1.1099,
      "step": 2631
    },
    {
      "epoch": 1.1958200817810085,
      "grad_norm": 4.599844455718994,
      "learning_rate": 0.0006061345948420572,
      "loss": 0.5005,
      "step": 2632
    },
    {
      "epoch": 1.1962744207178555,
      "grad_norm": 3.9324562549591064,
      "learning_rate": 0.0006059819929803144,
      "loss": 0.2301,
      "step": 2633
    },
    {
      "epoch": 1.1967287596547025,
      "grad_norm": 8.268214225769043,
      "learning_rate": 0.0006058293911185716,
      "loss": 0.48,
      "step": 2634
    },
    {
      "epoch": 1.1971830985915493,
      "grad_norm": 3.2483787536621094,
      "learning_rate": 0.0006056767892568289,
      "loss": 0.2804,
      "step": 2635
    },
    {
      "epoch": 1.1976374375283962,
      "grad_norm": 4.924520492553711,
      "learning_rate": 0.0006055241873950862,
      "loss": 0.3642,
      "step": 2636
    },
    {
      "epoch": 1.198091776465243,
      "grad_norm": 3.039118766784668,
      "learning_rate": 0.0006053715855333434,
      "loss": 0.2006,
      "step": 2637
    },
    {
      "epoch": 1.19854611540209,
      "grad_norm": 7.8100690841674805,
      "learning_rate": 0.0006052189836716008,
      "loss": 0.7856,
      "step": 2638
    },
    {
      "epoch": 1.1990004543389368,
      "grad_norm": 12.116826057434082,
      "learning_rate": 0.0006050663818098581,
      "loss": 1.3882,
      "step": 2639
    },
    {
      "epoch": 1.1994547932757837,
      "grad_norm": 5.298192977905273,
      "learning_rate": 0.0006049137799481153,
      "loss": 0.3146,
      "step": 2640
    },
    {
      "epoch": 1.1999091322126305,
      "grad_norm": 5.767218589782715,
      "learning_rate": 0.0006047611780863727,
      "loss": 0.4029,
      "step": 2641
    },
    {
      "epoch": 1.2003634711494775,
      "grad_norm": 3.251476287841797,
      "learning_rate": 0.0006046085762246299,
      "loss": 0.3316,
      "step": 2642
    },
    {
      "epoch": 1.2008178100863245,
      "grad_norm": 4.340591907501221,
      "learning_rate": 0.0006044559743628872,
      "loss": 0.3312,
      "step": 2643
    },
    {
      "epoch": 1.2012721490231713,
      "grad_norm": 3.9482369422912598,
      "learning_rate": 0.0006043033725011446,
      "loss": 0.2804,
      "step": 2644
    },
    {
      "epoch": 1.2017264879600182,
      "grad_norm": 4.891209602355957,
      "learning_rate": 0.0006041507706394018,
      "loss": 0.3107,
      "step": 2645
    },
    {
      "epoch": 1.202180826896865,
      "grad_norm": 7.567348480224609,
      "learning_rate": 0.0006039981687776591,
      "loss": 1.0757,
      "step": 2646
    },
    {
      "epoch": 1.202635165833712,
      "grad_norm": 2.2035841941833496,
      "learning_rate": 0.0006038455669159164,
      "loss": 0.0926,
      "step": 2647
    },
    {
      "epoch": 1.2030895047705588,
      "grad_norm": 3.342705726623535,
      "learning_rate": 0.0006036929650541737,
      "loss": 0.3845,
      "step": 2648
    },
    {
      "epoch": 1.2035438437074057,
      "grad_norm": 2.825449228286743,
      "learning_rate": 0.000603540363192431,
      "loss": 0.164,
      "step": 2649
    },
    {
      "epoch": 1.2039981826442525,
      "grad_norm": 6.184600830078125,
      "learning_rate": 0.0006033877613306883,
      "loss": 0.8134,
      "step": 2650
    },
    {
      "epoch": 1.2044525215810995,
      "grad_norm": 7.17039155960083,
      "learning_rate": 0.0006032351594689456,
      "loss": 0.8787,
      "step": 2651
    },
    {
      "epoch": 1.2049068605179465,
      "grad_norm": 5.332004070281982,
      "learning_rate": 0.0006030825576072028,
      "loss": 0.5638,
      "step": 2652
    },
    {
      "epoch": 1.2053611994547933,
      "grad_norm": 4.883859157562256,
      "learning_rate": 0.0006029299557454601,
      "loss": 0.6533,
      "step": 2653
    },
    {
      "epoch": 1.2058155383916402,
      "grad_norm": 5.853453159332275,
      "learning_rate": 0.0006027773538837173,
      "loss": 0.4985,
      "step": 2654
    },
    {
      "epoch": 1.206269877328487,
      "grad_norm": 6.311800956726074,
      "learning_rate": 0.0006026247520219746,
      "loss": 1.0301,
      "step": 2655
    },
    {
      "epoch": 1.206724216265334,
      "grad_norm": 6.757375717163086,
      "learning_rate": 0.000602472150160232,
      "loss": 1.0078,
      "step": 2656
    },
    {
      "epoch": 1.2071785552021808,
      "grad_norm": 6.527394771575928,
      "learning_rate": 0.0006023195482984892,
      "loss": 0.6452,
      "step": 2657
    },
    {
      "epoch": 1.2076328941390277,
      "grad_norm": 5.5498552322387695,
      "learning_rate": 0.0006021669464367465,
      "loss": 0.264,
      "step": 2658
    },
    {
      "epoch": 1.2080872330758745,
      "grad_norm": 4.982663631439209,
      "learning_rate": 0.0006020143445750038,
      "loss": 0.7235,
      "step": 2659
    },
    {
      "epoch": 1.2085415720127215,
      "grad_norm": 8.175492286682129,
      "learning_rate": 0.0006018617427132611,
      "loss": 0.3593,
      "step": 2660
    },
    {
      "epoch": 1.2089959109495685,
      "grad_norm": 5.868834972381592,
      "learning_rate": 0.0006017091408515184,
      "loss": 0.58,
      "step": 2661
    },
    {
      "epoch": 1.2094502498864153,
      "grad_norm": 2.078213691711426,
      "learning_rate": 0.0006015565389897757,
      "loss": 0.2375,
      "step": 2662
    },
    {
      "epoch": 1.2099045888232622,
      "grad_norm": 6.664051532745361,
      "learning_rate": 0.000601403937128033,
      "loss": 0.7502,
      "step": 2663
    },
    {
      "epoch": 1.210358927760109,
      "grad_norm": 4.579822063446045,
      "learning_rate": 0.0006012513352662902,
      "loss": 0.6866,
      "step": 2664
    },
    {
      "epoch": 1.210813266696956,
      "grad_norm": 5.398420333862305,
      "learning_rate": 0.0006010987334045476,
      "loss": 0.687,
      "step": 2665
    },
    {
      "epoch": 1.2112676056338028,
      "grad_norm": 6.732497692108154,
      "learning_rate": 0.0006009461315428048,
      "loss": 0.534,
      "step": 2666
    },
    {
      "epoch": 1.2117219445706497,
      "grad_norm": 3.9561846256256104,
      "learning_rate": 0.0006007935296810621,
      "loss": 0.6187,
      "step": 2667
    },
    {
      "epoch": 1.2121762835074965,
      "grad_norm": 5.568522930145264,
      "learning_rate": 0.0006006409278193195,
      "loss": 0.9374,
      "step": 2668
    },
    {
      "epoch": 1.2126306224443435,
      "grad_norm": 7.997384071350098,
      "learning_rate": 0.0006004883259575767,
      "loss": 1.2666,
      "step": 2669
    },
    {
      "epoch": 1.2130849613811905,
      "grad_norm": 4.1784210205078125,
      "learning_rate": 0.000600335724095834,
      "loss": 0.4549,
      "step": 2670
    },
    {
      "epoch": 1.2135393003180373,
      "grad_norm": 3.399406909942627,
      "learning_rate": 0.0006001831222340912,
      "loss": 0.4101,
      "step": 2671
    },
    {
      "epoch": 1.2139936392548842,
      "grad_norm": 8.649213790893555,
      "learning_rate": 0.0006000305203723485,
      "loss": 0.8443,
      "step": 2672
    },
    {
      "epoch": 1.214447978191731,
      "grad_norm": 8.667860984802246,
      "learning_rate": 0.0005998779185106058,
      "loss": 0.6162,
      "step": 2673
    },
    {
      "epoch": 1.214902317128578,
      "grad_norm": 4.470831394195557,
      "learning_rate": 0.0005997253166488631,
      "loss": 0.5229,
      "step": 2674
    },
    {
      "epoch": 1.2153566560654248,
      "grad_norm": 4.958192348480225,
      "learning_rate": 0.0005995727147871204,
      "loss": 0.7646,
      "step": 2675
    },
    {
      "epoch": 1.2158109950022717,
      "grad_norm": 5.543149948120117,
      "learning_rate": 0.0005994201129253776,
      "loss": 0.6337,
      "step": 2676
    },
    {
      "epoch": 1.2162653339391185,
      "grad_norm": 6.717907428741455,
      "learning_rate": 0.000599267511063635,
      "loss": 1.0162,
      "step": 2677
    },
    {
      "epoch": 1.2167196728759655,
      "grad_norm": 6.415891170501709,
      "learning_rate": 0.0005991149092018923,
      "loss": 1.1124,
      "step": 2678
    },
    {
      "epoch": 1.2171740118128123,
      "grad_norm": 4.385997772216797,
      "learning_rate": 0.0005989623073401495,
      "loss": 0.3833,
      "step": 2679
    },
    {
      "epoch": 1.2176283507496592,
      "grad_norm": 3.1894195079803467,
      "learning_rate": 0.0005988097054784069,
      "loss": 0.1562,
      "step": 2680
    },
    {
      "epoch": 1.218082689686506,
      "grad_norm": 4.331399917602539,
      "learning_rate": 0.0005986571036166641,
      "loss": 0.5103,
      "step": 2681
    },
    {
      "epoch": 1.218537028623353,
      "grad_norm": 6.62144136428833,
      "learning_rate": 0.0005985045017549214,
      "loss": 0.6996,
      "step": 2682
    },
    {
      "epoch": 1.2189913675602,
      "grad_norm": 7.617419242858887,
      "learning_rate": 0.0005983518998931788,
      "loss": 0.2701,
      "step": 2683
    },
    {
      "epoch": 1.2194457064970468,
      "grad_norm": 7.424868106842041,
      "learning_rate": 0.000598199298031436,
      "loss": 0.58,
      "step": 2684
    },
    {
      "epoch": 1.2199000454338937,
      "grad_norm": 5.65598201751709,
      "learning_rate": 0.0005980466961696933,
      "loss": 0.986,
      "step": 2685
    },
    {
      "epoch": 1.2203543843707405,
      "grad_norm": 6.812723636627197,
      "learning_rate": 0.0005978940943079506,
      "loss": 1.0546,
      "step": 2686
    },
    {
      "epoch": 1.2208087233075875,
      "grad_norm": 6.734095096588135,
      "learning_rate": 0.0005977414924462079,
      "loss": 0.7411,
      "step": 2687
    },
    {
      "epoch": 1.2212630622444343,
      "grad_norm": 4.63640832901001,
      "learning_rate": 0.0005975888905844651,
      "loss": 0.7495,
      "step": 2688
    },
    {
      "epoch": 1.2217174011812812,
      "grad_norm": 8.73779010772705,
      "learning_rate": 0.0005974362887227224,
      "loss": 0.676,
      "step": 2689
    },
    {
      "epoch": 1.222171740118128,
      "grad_norm": 6.273822784423828,
      "learning_rate": 0.0005972836868609797,
      "loss": 1.5194,
      "step": 2690
    },
    {
      "epoch": 1.222626079054975,
      "grad_norm": 2.2708005905151367,
      "learning_rate": 0.0005971310849992369,
      "loss": 0.4021,
      "step": 2691
    },
    {
      "epoch": 1.223080417991822,
      "grad_norm": 6.399991512298584,
      "learning_rate": 0.0005969784831374943,
      "loss": 0.7976,
      "step": 2692
    },
    {
      "epoch": 1.2235347569286688,
      "grad_norm": 4.725553512573242,
      "learning_rate": 0.0005968258812757515,
      "loss": 0.3499,
      "step": 2693
    },
    {
      "epoch": 1.2239890958655157,
      "grad_norm": 5.628228187561035,
      "learning_rate": 0.0005966732794140088,
      "loss": 0.6024,
      "step": 2694
    },
    {
      "epoch": 1.2244434348023625,
      "grad_norm": 4.753073692321777,
      "learning_rate": 0.0005965206775522662,
      "loss": 0.664,
      "step": 2695
    },
    {
      "epoch": 1.2248977737392095,
      "grad_norm": 5.279236316680908,
      "learning_rate": 0.0005963680756905234,
      "loss": 0.817,
      "step": 2696
    },
    {
      "epoch": 1.2253521126760563,
      "grad_norm": 6.2816243171691895,
      "learning_rate": 0.0005962154738287807,
      "loss": 0.7945,
      "step": 2697
    },
    {
      "epoch": 1.2258064516129032,
      "grad_norm": 5.444502353668213,
      "learning_rate": 0.000596062871967038,
      "loss": 0.5707,
      "step": 2698
    },
    {
      "epoch": 1.22626079054975,
      "grad_norm": 6.587789058685303,
      "learning_rate": 0.0005959102701052953,
      "loss": 0.5845,
      "step": 2699
    },
    {
      "epoch": 1.226715129486597,
      "grad_norm": 4.98820686340332,
      "learning_rate": 0.0005957576682435527,
      "loss": 0.9588,
      "step": 2700
    },
    {
      "epoch": 1.227169468423444,
      "grad_norm": 5.178248882293701,
      "learning_rate": 0.0005956050663818099,
      "loss": 0.8997,
      "step": 2701
    },
    {
      "epoch": 1.2276238073602908,
      "grad_norm": 4.154368877410889,
      "learning_rate": 0.0005954524645200672,
      "loss": 0.4352,
      "step": 2702
    },
    {
      "epoch": 1.2280781462971377,
      "grad_norm": 2.538956880569458,
      "learning_rate": 0.0005952998626583245,
      "loss": 0.2391,
      "step": 2703
    },
    {
      "epoch": 1.2285324852339845,
      "grad_norm": 3.9801595211029053,
      "learning_rate": 0.0005951472607965818,
      "loss": 0.4441,
      "step": 2704
    },
    {
      "epoch": 1.2289868241708315,
      "grad_norm": 9.128355026245117,
      "learning_rate": 0.000594994658934839,
      "loss": 1.4634,
      "step": 2705
    },
    {
      "epoch": 1.2294411631076783,
      "grad_norm": 3.89266300201416,
      "learning_rate": 0.0005948420570730964,
      "loss": 0.4228,
      "step": 2706
    },
    {
      "epoch": 1.2298955020445252,
      "grad_norm": 4.770457744598389,
      "learning_rate": 0.0005946894552113536,
      "loss": 0.6242,
      "step": 2707
    },
    {
      "epoch": 1.230349840981372,
      "grad_norm": 2.69939923286438,
      "learning_rate": 0.0005945368533496108,
      "loss": 0.2226,
      "step": 2708
    },
    {
      "epoch": 1.230804179918219,
      "grad_norm": 4.732808589935303,
      "learning_rate": 0.0005943842514878682,
      "loss": 0.8272,
      "step": 2709
    },
    {
      "epoch": 1.231258518855066,
      "grad_norm": 3.4177141189575195,
      "learning_rate": 0.0005942316496261254,
      "loss": 0.3768,
      "step": 2710
    },
    {
      "epoch": 1.2317128577919128,
      "grad_norm": 8.97337818145752,
      "learning_rate": 0.0005940790477643827,
      "loss": 1.2489,
      "step": 2711
    },
    {
      "epoch": 1.2321671967287597,
      "grad_norm": 7.45273494720459,
      "learning_rate": 0.00059392644590264,
      "loss": 1.0821,
      "step": 2712
    },
    {
      "epoch": 1.2326215356656065,
      "grad_norm": 11.23917293548584,
      "learning_rate": 0.0005937738440408973,
      "loss": 0.8015,
      "step": 2713
    },
    {
      "epoch": 1.2330758746024535,
      "grad_norm": 3.9463016986846924,
      "learning_rate": 0.0005936212421791546,
      "loss": 0.3821,
      "step": 2714
    },
    {
      "epoch": 1.2335302135393003,
      "grad_norm": 3.773162364959717,
      "learning_rate": 0.0005934686403174119,
      "loss": 0.3674,
      "step": 2715
    },
    {
      "epoch": 1.2339845524761472,
      "grad_norm": 5.59188175201416,
      "learning_rate": 0.0005933160384556692,
      "loss": 0.5647,
      "step": 2716
    },
    {
      "epoch": 1.234438891412994,
      "grad_norm": 3.080070734024048,
      "learning_rate": 0.0005931634365939264,
      "loss": 0.2027,
      "step": 2717
    },
    {
      "epoch": 1.234893230349841,
      "grad_norm": 5.801822662353516,
      "learning_rate": 0.0005930108347321838,
      "loss": 0.5177,
      "step": 2718
    },
    {
      "epoch": 1.235347569286688,
      "grad_norm": 4.492247581481934,
      "learning_rate": 0.0005928582328704411,
      "loss": 0.6152,
      "step": 2719
    },
    {
      "epoch": 1.2358019082235348,
      "grad_norm": 6.462000846862793,
      "learning_rate": 0.0005927056310086983,
      "loss": 0.4424,
      "step": 2720
    },
    {
      "epoch": 1.2362562471603817,
      "grad_norm": 3.6021528244018555,
      "learning_rate": 0.0005925530291469557,
      "loss": 0.3565,
      "step": 2721
    },
    {
      "epoch": 1.2367105860972285,
      "grad_norm": 6.459052562713623,
      "learning_rate": 0.0005924004272852129,
      "loss": 1.2248,
      "step": 2722
    },
    {
      "epoch": 1.2371649250340755,
      "grad_norm": 6.384239196777344,
      "learning_rate": 0.0005922478254234702,
      "loss": 1.0368,
      "step": 2723
    },
    {
      "epoch": 1.2376192639709223,
      "grad_norm": 4.842936992645264,
      "learning_rate": 0.0005920952235617276,
      "loss": 0.4295,
      "step": 2724
    },
    {
      "epoch": 1.2380736029077692,
      "grad_norm": 3.5142440795898438,
      "learning_rate": 0.0005919426216999848,
      "loss": 0.3098,
      "step": 2725
    },
    {
      "epoch": 1.238527941844616,
      "grad_norm": 5.638852119445801,
      "learning_rate": 0.000591790019838242,
      "loss": 0.7982,
      "step": 2726
    },
    {
      "epoch": 1.238982280781463,
      "grad_norm": 8.59377384185791,
      "learning_rate": 0.0005916374179764993,
      "loss": 0.9405,
      "step": 2727
    },
    {
      "epoch": 1.2394366197183098,
      "grad_norm": 7.591689109802246,
      "learning_rate": 0.0005914848161147566,
      "loss": 0.9795,
      "step": 2728
    },
    {
      "epoch": 1.2398909586551567,
      "grad_norm": 5.110385417938232,
      "learning_rate": 0.0005913322142530138,
      "loss": 0.5094,
      "step": 2729
    },
    {
      "epoch": 1.2403452975920035,
      "grad_norm": 7.458521366119385,
      "learning_rate": 0.0005911796123912712,
      "loss": 0.9491,
      "step": 2730
    },
    {
      "epoch": 1.2407996365288505,
      "grad_norm": 8.38943099975586,
      "learning_rate": 0.0005910270105295285,
      "loss": 0.3077,
      "step": 2731
    },
    {
      "epoch": 1.2412539754656975,
      "grad_norm": 2.6706948280334473,
      "learning_rate": 0.0005908744086677857,
      "loss": 0.3421,
      "step": 2732
    },
    {
      "epoch": 1.2417083144025443,
      "grad_norm": 5.422946929931641,
      "learning_rate": 0.0005907218068060431,
      "loss": 0.6842,
      "step": 2733
    },
    {
      "epoch": 1.2421626533393912,
      "grad_norm": 3.6943087577819824,
      "learning_rate": 0.0005905692049443003,
      "loss": 0.3564,
      "step": 2734
    },
    {
      "epoch": 1.242616992276238,
      "grad_norm": 6.103697776794434,
      "learning_rate": 0.0005904166030825576,
      "loss": 0.7078,
      "step": 2735
    },
    {
      "epoch": 1.243071331213085,
      "grad_norm": 6.07328987121582,
      "learning_rate": 0.000590264001220815,
      "loss": 0.4408,
      "step": 2736
    },
    {
      "epoch": 1.2435256701499318,
      "grad_norm": 6.590995788574219,
      "learning_rate": 0.0005901113993590722,
      "loss": 0.6628,
      "step": 2737
    },
    {
      "epoch": 1.2439800090867787,
      "grad_norm": 7.141434192657471,
      "learning_rate": 0.0005899587974973295,
      "loss": 1.0088,
      "step": 2738
    },
    {
      "epoch": 1.2444343480236255,
      "grad_norm": 7.269532680511475,
      "learning_rate": 0.0005898061956355868,
      "loss": 1.2996,
      "step": 2739
    },
    {
      "epoch": 1.2448886869604725,
      "grad_norm": 5.3065056800842285,
      "learning_rate": 0.0005896535937738441,
      "loss": 0.455,
      "step": 2740
    },
    {
      "epoch": 1.2453430258973195,
      "grad_norm": 5.584517478942871,
      "learning_rate": 0.0005895009919121014,
      "loss": 1.0791,
      "step": 2741
    },
    {
      "epoch": 1.2457973648341663,
      "grad_norm": 7.307117938995361,
      "learning_rate": 0.0005893483900503587,
      "loss": 1.5588,
      "step": 2742
    },
    {
      "epoch": 1.2462517037710132,
      "grad_norm": 6.005057334899902,
      "learning_rate": 0.000589195788188616,
      "loss": 0.7267,
      "step": 2743
    },
    {
      "epoch": 1.24670604270786,
      "grad_norm": 3.4742555618286133,
      "learning_rate": 0.0005890431863268731,
      "loss": 0.3543,
      "step": 2744
    },
    {
      "epoch": 1.247160381644707,
      "grad_norm": 4.0177106857299805,
      "learning_rate": 0.0005888905844651305,
      "loss": 0.5433,
      "step": 2745
    },
    {
      "epoch": 1.2476147205815538,
      "grad_norm": 5.898955345153809,
      "learning_rate": 0.0005887379826033877,
      "loss": 0.4911,
      "step": 2746
    },
    {
      "epoch": 1.2480690595184007,
      "grad_norm": 6.935911178588867,
      "learning_rate": 0.000588585380741645,
      "loss": 1.0818,
      "step": 2747
    },
    {
      "epoch": 1.2485233984552475,
      "grad_norm": 4.289334297180176,
      "learning_rate": 0.0005884327788799024,
      "loss": 0.5205,
      "step": 2748
    },
    {
      "epoch": 1.2489777373920945,
      "grad_norm": 6.523214340209961,
      "learning_rate": 0.0005882801770181596,
      "loss": 0.7697,
      "step": 2749
    },
    {
      "epoch": 1.2494320763289415,
      "grad_norm": 6.565914154052734,
      "learning_rate": 0.0005881275751564169,
      "loss": 0.9812,
      "step": 2750
    },
    {
      "epoch": 1.2498864152657883,
      "grad_norm": 7.224757671356201,
      "learning_rate": 0.0005879749732946742,
      "loss": 1.0846,
      "step": 2751
    },
    {
      "epoch": 1.2503407542026352,
      "grad_norm": 2.6492702960968018,
      "learning_rate": 0.0005878223714329315,
      "loss": 0.1318,
      "step": 2752
    },
    {
      "epoch": 1.250795093139482,
      "grad_norm": 5.698461532592773,
      "learning_rate": 0.0005876697695711888,
      "loss": 0.4957,
      "step": 2753
    },
    {
      "epoch": 1.251249432076329,
      "grad_norm": 4.336414813995361,
      "learning_rate": 0.0005875171677094461,
      "loss": 0.5255,
      "step": 2754
    },
    {
      "epoch": 1.2517037710131758,
      "grad_norm": 5.454244613647461,
      "learning_rate": 0.0005873645658477034,
      "loss": 0.8159,
      "step": 2755
    },
    {
      "epoch": 1.2521581099500227,
      "grad_norm": 2.3493127822875977,
      "learning_rate": 0.0005872119639859606,
      "loss": 0.1622,
      "step": 2756
    },
    {
      "epoch": 1.2526124488868695,
      "grad_norm": 3.861311674118042,
      "learning_rate": 0.000587059362124218,
      "loss": 0.5135,
      "step": 2757
    },
    {
      "epoch": 1.2530667878237165,
      "grad_norm": 4.355525016784668,
      "learning_rate": 0.0005869067602624753,
      "loss": 0.3238,
      "step": 2758
    },
    {
      "epoch": 1.2535211267605635,
      "grad_norm": 3.9205331802368164,
      "learning_rate": 0.0005867541584007325,
      "loss": 0.3333,
      "step": 2759
    },
    {
      "epoch": 1.2539754656974103,
      "grad_norm": 5.299022674560547,
      "learning_rate": 0.0005866015565389899,
      "loss": 0.5327,
      "step": 2760
    },
    {
      "epoch": 1.254429804634257,
      "grad_norm": 4.980801105499268,
      "learning_rate": 0.0005864489546772471,
      "loss": 0.5624,
      "step": 2761
    },
    {
      "epoch": 1.254884143571104,
      "grad_norm": 4.9117536544799805,
      "learning_rate": 0.0005862963528155043,
      "loss": 0.6294,
      "step": 2762
    },
    {
      "epoch": 1.255338482507951,
      "grad_norm": 4.745574951171875,
      "learning_rate": 0.0005861437509537616,
      "loss": 0.343,
      "step": 2763
    },
    {
      "epoch": 1.2557928214447978,
      "grad_norm": 5.310089588165283,
      "learning_rate": 0.0005859911490920189,
      "loss": 0.4216,
      "step": 2764
    },
    {
      "epoch": 1.2562471603816447,
      "grad_norm": 7.62290096282959,
      "learning_rate": 0.0005858385472302762,
      "loss": 0.8847,
      "step": 2765
    },
    {
      "epoch": 1.2567014993184915,
      "grad_norm": 8.488792419433594,
      "learning_rate": 0.0005856859453685335,
      "loss": 1.097,
      "step": 2766
    },
    {
      "epoch": 1.2571558382553385,
      "grad_norm": 4.770362377166748,
      "learning_rate": 0.0005855333435067908,
      "loss": 0.3766,
      "step": 2767
    },
    {
      "epoch": 1.2576101771921855,
      "grad_norm": 7.867516994476318,
      "learning_rate": 0.000585380741645048,
      "loss": 1.0208,
      "step": 2768
    },
    {
      "epoch": 1.2580645161290323,
      "grad_norm": 5.5686750411987305,
      "learning_rate": 0.0005852281397833054,
      "loss": 0.335,
      "step": 2769
    },
    {
      "epoch": 1.258518855065879,
      "grad_norm": 6.925991058349609,
      "learning_rate": 0.0005850755379215627,
      "loss": 0.9367,
      "step": 2770
    },
    {
      "epoch": 1.258973194002726,
      "grad_norm": 3.8118209838867188,
      "learning_rate": 0.0005849229360598199,
      "loss": 0.5985,
      "step": 2771
    },
    {
      "epoch": 1.259427532939573,
      "grad_norm": 5.634553909301758,
      "learning_rate": 0.0005847703341980773,
      "loss": 0.8888,
      "step": 2772
    },
    {
      "epoch": 1.2598818718764198,
      "grad_norm": 5.594226837158203,
      "learning_rate": 0.0005846177323363345,
      "loss": 0.6448,
      "step": 2773
    },
    {
      "epoch": 1.2603362108132667,
      "grad_norm": 7.56029748916626,
      "learning_rate": 0.0005844651304745918,
      "loss": 0.9077,
      "step": 2774
    },
    {
      "epoch": 1.2607905497501135,
      "grad_norm": 7.046568393707275,
      "learning_rate": 0.0005843125286128492,
      "loss": 0.5387,
      "step": 2775
    },
    {
      "epoch": 1.2612448886869605,
      "grad_norm": 3.770557403564453,
      "learning_rate": 0.0005841599267511064,
      "loss": 0.4023,
      "step": 2776
    },
    {
      "epoch": 1.2616992276238075,
      "grad_norm": 6.912549018859863,
      "learning_rate": 0.0005840073248893637,
      "loss": 0.5341,
      "step": 2777
    },
    {
      "epoch": 1.2621535665606542,
      "grad_norm": 3.0918309688568115,
      "learning_rate": 0.000583854723027621,
      "loss": 0.3344,
      "step": 2778
    },
    {
      "epoch": 1.262607905497501,
      "grad_norm": 6.359579563140869,
      "learning_rate": 0.0005837021211658783,
      "loss": 0.9613,
      "step": 2779
    },
    {
      "epoch": 1.263062244434348,
      "grad_norm": 2.2644131183624268,
      "learning_rate": 0.0005835495193041354,
      "loss": 0.2231,
      "step": 2780
    },
    {
      "epoch": 1.263516583371195,
      "grad_norm": 4.845009803771973,
      "learning_rate": 0.0005833969174423928,
      "loss": 0.7833,
      "step": 2781
    },
    {
      "epoch": 1.2639709223080418,
      "grad_norm": 4.169029712677002,
      "learning_rate": 0.0005832443155806501,
      "loss": 0.4289,
      "step": 2782
    },
    {
      "epoch": 1.2644252612448887,
      "grad_norm": 4.532323837280273,
      "learning_rate": 0.0005830917137189073,
      "loss": 0.3358,
      "step": 2783
    },
    {
      "epoch": 1.2648796001817355,
      "grad_norm": 4.275923728942871,
      "learning_rate": 0.0005829391118571647,
      "loss": 0.246,
      "step": 2784
    },
    {
      "epoch": 1.2653339391185825,
      "grad_norm": 6.059632778167725,
      "learning_rate": 0.0005827865099954219,
      "loss": 0.4506,
      "step": 2785
    },
    {
      "epoch": 1.2657882780554295,
      "grad_norm": 4.63668966293335,
      "learning_rate": 0.0005826339081336792,
      "loss": 0.3004,
      "step": 2786
    },
    {
      "epoch": 1.2662426169922762,
      "grad_norm": 3.023078441619873,
      "learning_rate": 0.0005824813062719366,
      "loss": 0.2293,
      "step": 2787
    },
    {
      "epoch": 1.266696955929123,
      "grad_norm": 5.853635311126709,
      "learning_rate": 0.0005823287044101938,
      "loss": 0.3201,
      "step": 2788
    },
    {
      "epoch": 1.26715129486597,
      "grad_norm": 5.55745792388916,
      "learning_rate": 0.0005821761025484511,
      "loss": 0.6036,
      "step": 2789
    },
    {
      "epoch": 1.267605633802817,
      "grad_norm": 6.708773136138916,
      "learning_rate": 0.0005820235006867084,
      "loss": 0.6904,
      "step": 2790
    },
    {
      "epoch": 1.2680599727396638,
      "grad_norm": 5.728597640991211,
      "learning_rate": 0.0005818708988249657,
      "loss": 0.5294,
      "step": 2791
    },
    {
      "epoch": 1.2685143116765107,
      "grad_norm": 4.641844272613525,
      "learning_rate": 0.000581718296963223,
      "loss": 0.5092,
      "step": 2792
    },
    {
      "epoch": 1.2689686506133575,
      "grad_norm": 5.249794006347656,
      "learning_rate": 0.0005815656951014803,
      "loss": 0.6622,
      "step": 2793
    },
    {
      "epoch": 1.2694229895502045,
      "grad_norm": 6.896742343902588,
      "learning_rate": 0.0005814130932397376,
      "loss": 0.6642,
      "step": 2794
    },
    {
      "epoch": 1.2698773284870513,
      "grad_norm": 5.095454216003418,
      "learning_rate": 0.0005812604913779948,
      "loss": 0.8317,
      "step": 2795
    },
    {
      "epoch": 1.2703316674238982,
      "grad_norm": 5.812652111053467,
      "learning_rate": 0.0005811078895162522,
      "loss": 0.8645,
      "step": 2796
    },
    {
      "epoch": 1.270786006360745,
      "grad_norm": 6.347484588623047,
      "learning_rate": 0.0005809552876545094,
      "loss": 0.4153,
      "step": 2797
    },
    {
      "epoch": 1.271240345297592,
      "grad_norm": 5.510236740112305,
      "learning_rate": 0.0005808026857927667,
      "loss": 0.5,
      "step": 2798
    },
    {
      "epoch": 1.271694684234439,
      "grad_norm": 7.261097431182861,
      "learning_rate": 0.000580650083931024,
      "loss": 0.6525,
      "step": 2799
    },
    {
      "epoch": 1.2721490231712858,
      "grad_norm": 5.855044841766357,
      "learning_rate": 0.0005804974820692812,
      "loss": 0.7597,
      "step": 2800
    },
    {
      "epoch": 1.2726033621081327,
      "grad_norm": 4.031728744506836,
      "learning_rate": 0.0005803448802075385,
      "loss": 0.5306,
      "step": 2801
    },
    {
      "epoch": 1.2730577010449795,
      "grad_norm": 3.328508138656616,
      "learning_rate": 0.0005801922783457958,
      "loss": 0.4143,
      "step": 2802
    },
    {
      "epoch": 1.2735120399818265,
      "grad_norm": 3.725046396255493,
      "learning_rate": 0.0005800396764840531,
      "loss": 0.2335,
      "step": 2803
    },
    {
      "epoch": 1.2739663789186733,
      "grad_norm": 4.948781490325928,
      "learning_rate": 0.0005798870746223104,
      "loss": 0.3814,
      "step": 2804
    },
    {
      "epoch": 1.2744207178555202,
      "grad_norm": 4.93378210067749,
      "learning_rate": 0.0005797344727605677,
      "loss": 0.3996,
      "step": 2805
    },
    {
      "epoch": 1.274875056792367,
      "grad_norm": 5.887173652648926,
      "learning_rate": 0.000579581870898825,
      "loss": 0.6139,
      "step": 2806
    },
    {
      "epoch": 1.275329395729214,
      "grad_norm": 3.654553174972534,
      "learning_rate": 0.0005794292690370822,
      "loss": 0.4555,
      "step": 2807
    },
    {
      "epoch": 1.275783734666061,
      "grad_norm": 3.2651779651641846,
      "learning_rate": 0.0005792766671753396,
      "loss": 0.3351,
      "step": 2808
    },
    {
      "epoch": 1.2762380736029078,
      "grad_norm": 7.974386215209961,
      "learning_rate": 0.0005791240653135969,
      "loss": 0.7726,
      "step": 2809
    },
    {
      "epoch": 1.2766924125397547,
      "grad_norm": 6.119267463684082,
      "learning_rate": 0.0005789714634518541,
      "loss": 0.4684,
      "step": 2810
    },
    {
      "epoch": 1.2771467514766015,
      "grad_norm": 4.801933765411377,
      "learning_rate": 0.0005788188615901115,
      "loss": 0.678,
      "step": 2811
    },
    {
      "epoch": 1.2776010904134485,
      "grad_norm": 4.273692607879639,
      "learning_rate": 0.0005786662597283687,
      "loss": 0.3691,
      "step": 2812
    },
    {
      "epoch": 1.2780554293502953,
      "grad_norm": 3.9035325050354004,
      "learning_rate": 0.000578513657866626,
      "loss": 0.3842,
      "step": 2813
    },
    {
      "epoch": 1.2785097682871422,
      "grad_norm": 6.313427448272705,
      "learning_rate": 0.0005783610560048833,
      "loss": 0.4677,
      "step": 2814
    },
    {
      "epoch": 1.278964107223989,
      "grad_norm": 4.141040325164795,
      "learning_rate": 0.0005782084541431406,
      "loss": 0.1569,
      "step": 2815
    },
    {
      "epoch": 1.279418446160836,
      "grad_norm": 2.540337085723877,
      "learning_rate": 0.0005780558522813979,
      "loss": 0.379,
      "step": 2816
    },
    {
      "epoch": 1.279872785097683,
      "grad_norm": 6.02600622177124,
      "learning_rate": 0.0005779032504196551,
      "loss": 0.6076,
      "step": 2817
    },
    {
      "epoch": 1.2803271240345298,
      "grad_norm": 5.551722526550293,
      "learning_rate": 0.0005777506485579124,
      "loss": 0.7238,
      "step": 2818
    },
    {
      "epoch": 1.2807814629713765,
      "grad_norm": 5.837291240692139,
      "learning_rate": 0.0005775980466961696,
      "loss": 0.4849,
      "step": 2819
    },
    {
      "epoch": 1.2812358019082235,
      "grad_norm": 8.382695198059082,
      "learning_rate": 0.000577445444834427,
      "loss": 1.2464,
      "step": 2820
    },
    {
      "epoch": 1.2816901408450705,
      "grad_norm": 2.6099741458892822,
      "learning_rate": 0.0005772928429726843,
      "loss": 0.3566,
      "step": 2821
    },
    {
      "epoch": 1.2821444797819173,
      "grad_norm": 7.137130260467529,
      "learning_rate": 0.0005771402411109415,
      "loss": 0.4962,
      "step": 2822
    },
    {
      "epoch": 1.2825988187187642,
      "grad_norm": 4.375236988067627,
      "learning_rate": 0.0005769876392491989,
      "loss": 0.2602,
      "step": 2823
    },
    {
      "epoch": 1.283053157655611,
      "grad_norm": 4.734473705291748,
      "learning_rate": 0.0005768350373874561,
      "loss": 0.4637,
      "step": 2824
    },
    {
      "epoch": 1.283507496592458,
      "grad_norm": 7.209893703460693,
      "learning_rate": 0.0005766824355257134,
      "loss": 0.6311,
      "step": 2825
    },
    {
      "epoch": 1.283961835529305,
      "grad_norm": 7.0269246101379395,
      "learning_rate": 0.0005765298336639708,
      "loss": 0.4613,
      "step": 2826
    },
    {
      "epoch": 1.2844161744661517,
      "grad_norm": 8.664453506469727,
      "learning_rate": 0.000576377231802228,
      "loss": 1.9639,
      "step": 2827
    },
    {
      "epoch": 1.2848705134029985,
      "grad_norm": 8.371284484863281,
      "learning_rate": 0.0005762246299404853,
      "loss": 0.5677,
      "step": 2828
    },
    {
      "epoch": 1.2853248523398455,
      "grad_norm": 3.9841439723968506,
      "learning_rate": 0.0005760720280787426,
      "loss": 0.367,
      "step": 2829
    },
    {
      "epoch": 1.2857791912766925,
      "grad_norm": 5.189005374908447,
      "learning_rate": 0.0005759194262169999,
      "loss": 0.6322,
      "step": 2830
    },
    {
      "epoch": 1.2862335302135393,
      "grad_norm": 4.770361423492432,
      "learning_rate": 0.0005757668243552571,
      "loss": 0.1748,
      "step": 2831
    },
    {
      "epoch": 1.2866878691503862,
      "grad_norm": 5.464794158935547,
      "learning_rate": 0.0005756142224935145,
      "loss": 0.5913,
      "step": 2832
    },
    {
      "epoch": 1.287142208087233,
      "grad_norm": 6.883209228515625,
      "learning_rate": 0.0005754616206317718,
      "loss": 1.1972,
      "step": 2833
    },
    {
      "epoch": 1.28759654702408,
      "grad_norm": 5.130464553833008,
      "learning_rate": 0.000575309018770029,
      "loss": 0.5237,
      "step": 2834
    },
    {
      "epoch": 1.288050885960927,
      "grad_norm": 5.4532880783081055,
      "learning_rate": 0.0005751564169082863,
      "loss": 0.6445,
      "step": 2835
    },
    {
      "epoch": 1.2885052248977737,
      "grad_norm": 6.061320781707764,
      "learning_rate": 0.0005750038150465435,
      "loss": 0.4197,
      "step": 2836
    },
    {
      "epoch": 1.2889595638346205,
      "grad_norm": 7.3830180168151855,
      "learning_rate": 0.0005748512131848008,
      "loss": 1.4944,
      "step": 2837
    },
    {
      "epoch": 1.2894139027714675,
      "grad_norm": 4.398608684539795,
      "learning_rate": 0.0005746986113230582,
      "loss": 0.4201,
      "step": 2838
    },
    {
      "epoch": 1.2898682417083145,
      "grad_norm": 4.8694257736206055,
      "learning_rate": 0.0005745460094613154,
      "loss": 0.5747,
      "step": 2839
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 2.683943033218384,
      "learning_rate": 0.0005743934075995727,
      "loss": 0.0969,
      "step": 2840
    },
    {
      "epoch": 1.2907769195820082,
      "grad_norm": 5.449563980102539,
      "learning_rate": 0.00057424080573783,
      "loss": 0.3769,
      "step": 2841
    },
    {
      "epoch": 1.291231258518855,
      "grad_norm": 4.049407958984375,
      "learning_rate": 0.0005740882038760873,
      "loss": 0.2497,
      "step": 2842
    },
    {
      "epoch": 1.291685597455702,
      "grad_norm": 4.81001615524292,
      "learning_rate": 0.0005739356020143445,
      "loss": 0.8542,
      "step": 2843
    },
    {
      "epoch": 1.2921399363925488,
      "grad_norm": 1.7005096673965454,
      "learning_rate": 0.0005737830001526019,
      "loss": 0.0932,
      "step": 2844
    },
    {
      "epoch": 1.2925942753293957,
      "grad_norm": 8.529090881347656,
      "learning_rate": 0.0005736303982908592,
      "loss": 1.7858,
      "step": 2845
    },
    {
      "epoch": 1.2930486142662425,
      "grad_norm": 8.062666893005371,
      "learning_rate": 0.0005734777964291164,
      "loss": 0.9443,
      "step": 2846
    },
    {
      "epoch": 1.2935029532030895,
      "grad_norm": 3.9031503200531006,
      "learning_rate": 0.0005733251945673738,
      "loss": 0.664,
      "step": 2847
    },
    {
      "epoch": 1.2939572921399365,
      "grad_norm": 4.445952892303467,
      "learning_rate": 0.000573172592705631,
      "loss": 0.7704,
      "step": 2848
    },
    {
      "epoch": 1.2944116310767833,
      "grad_norm": 4.385923862457275,
      "learning_rate": 0.0005730199908438883,
      "loss": 0.4619,
      "step": 2849
    },
    {
      "epoch": 1.2948659700136302,
      "grad_norm": 3.161771535873413,
      "learning_rate": 0.0005728673889821457,
      "loss": 0.1435,
      "step": 2850
    },
    {
      "epoch": 1.295320308950477,
      "grad_norm": 3.228653907775879,
      "learning_rate": 0.0005727147871204029,
      "loss": 0.4348,
      "step": 2851
    },
    {
      "epoch": 1.295774647887324,
      "grad_norm": 2.0953290462493896,
      "learning_rate": 0.0005725621852586602,
      "loss": 0.1173,
      "step": 2852
    },
    {
      "epoch": 1.2962289868241708,
      "grad_norm": 9.361701011657715,
      "learning_rate": 0.0005724095833969174,
      "loss": 1.0796,
      "step": 2853
    },
    {
      "epoch": 1.2966833257610177,
      "grad_norm": 5.246221542358398,
      "learning_rate": 0.0005722569815351747,
      "loss": 0.3736,
      "step": 2854
    },
    {
      "epoch": 1.2971376646978645,
      "grad_norm": 5.268391132354736,
      "learning_rate": 0.0005721043796734319,
      "loss": 0.7009,
      "step": 2855
    },
    {
      "epoch": 1.2975920036347115,
      "grad_norm": 5.360422134399414,
      "learning_rate": 0.0005719517778116893,
      "loss": 0.5649,
      "step": 2856
    },
    {
      "epoch": 1.2980463425715585,
      "grad_norm": 3.50858473777771,
      "learning_rate": 0.0005717991759499466,
      "loss": 0.6237,
      "step": 2857
    },
    {
      "epoch": 1.2985006815084053,
      "grad_norm": 3.0751914978027344,
      "learning_rate": 0.0005716465740882038,
      "loss": 0.1638,
      "step": 2858
    },
    {
      "epoch": 1.2989550204452522,
      "grad_norm": 7.842036724090576,
      "learning_rate": 0.0005714939722264612,
      "loss": 0.6723,
      "step": 2859
    },
    {
      "epoch": 1.299409359382099,
      "grad_norm": 4.488697052001953,
      "learning_rate": 0.0005713413703647184,
      "loss": 0.7714,
      "step": 2860
    },
    {
      "epoch": 1.299863698318946,
      "grad_norm": 5.628575325012207,
      "learning_rate": 0.0005711887685029757,
      "loss": 0.5377,
      "step": 2861
    },
    {
      "epoch": 1.3003180372557928,
      "grad_norm": 2.2552640438079834,
      "learning_rate": 0.0005710361666412331,
      "loss": 0.223,
      "step": 2862
    },
    {
      "epoch": 1.3007723761926397,
      "grad_norm": 7.1339216232299805,
      "learning_rate": 0.0005708835647794903,
      "loss": 0.9296,
      "step": 2863
    },
    {
      "epoch": 1.3012267151294865,
      "grad_norm": 7.3475341796875,
      "learning_rate": 0.0005707309629177476,
      "loss": 1.343,
      "step": 2864
    },
    {
      "epoch": 1.3016810540663335,
      "grad_norm": 7.5918803215026855,
      "learning_rate": 0.0005705783610560049,
      "loss": 0.6013,
      "step": 2865
    },
    {
      "epoch": 1.3021353930031805,
      "grad_norm": 5.381328582763672,
      "learning_rate": 0.0005704257591942622,
      "loss": 0.5732,
      "step": 2866
    },
    {
      "epoch": 1.3025897319400273,
      "grad_norm": 5.537688255310059,
      "learning_rate": 0.0005702731573325195,
      "loss": 0.5361,
      "step": 2867
    },
    {
      "epoch": 1.303044070876874,
      "grad_norm": 2.453916311264038,
      "learning_rate": 0.0005701205554707768,
      "loss": 0.29,
      "step": 2868
    },
    {
      "epoch": 1.303498409813721,
      "grad_norm": 3.7015130519866943,
      "learning_rate": 0.0005699679536090341,
      "loss": 0.3567,
      "step": 2869
    },
    {
      "epoch": 1.303952748750568,
      "grad_norm": 7.55034065246582,
      "learning_rate": 0.0005698153517472913,
      "loss": 1.1145,
      "step": 2870
    },
    {
      "epoch": 1.3044070876874148,
      "grad_norm": 1.2915854454040527,
      "learning_rate": 0.0005696627498855487,
      "loss": 0.0656,
      "step": 2871
    },
    {
      "epoch": 1.3048614266242617,
      "grad_norm": 4.612459659576416,
      "learning_rate": 0.0005695101480238058,
      "loss": 0.9903,
      "step": 2872
    },
    {
      "epoch": 1.3053157655611085,
      "grad_norm": 5.909906387329102,
      "learning_rate": 0.0005693575461620631,
      "loss": 0.8745,
      "step": 2873
    },
    {
      "epoch": 1.3057701044979555,
      "grad_norm": 1.438362717628479,
      "learning_rate": 0.0005692049443003205,
      "loss": 0.1575,
      "step": 2874
    },
    {
      "epoch": 1.3062244434348025,
      "grad_norm": 6.144589900970459,
      "learning_rate": 0.0005690523424385777,
      "loss": 0.6622,
      "step": 2875
    },
    {
      "epoch": 1.3066787823716493,
      "grad_norm": 6.770294666290283,
      "learning_rate": 0.000568899740576835,
      "loss": 0.6373,
      "step": 2876
    },
    {
      "epoch": 1.307133121308496,
      "grad_norm": 5.36176872253418,
      "learning_rate": 0.0005687471387150923,
      "loss": 0.9555,
      "step": 2877
    },
    {
      "epoch": 1.307587460245343,
      "grad_norm": 6.25904655456543,
      "learning_rate": 0.0005685945368533496,
      "loss": 0.9158,
      "step": 2878
    },
    {
      "epoch": 1.30804179918219,
      "grad_norm": 7.140887260437012,
      "learning_rate": 0.0005684419349916069,
      "loss": 0.8421,
      "step": 2879
    },
    {
      "epoch": 1.3084961381190368,
      "grad_norm": 6.348142623901367,
      "learning_rate": 0.0005682893331298642,
      "loss": 1.056,
      "step": 2880
    },
    {
      "epoch": 1.3089504770558837,
      "grad_norm": 4.407984256744385,
      "learning_rate": 0.0005681367312681215,
      "loss": 0.6011,
      "step": 2881
    },
    {
      "epoch": 1.3094048159927305,
      "grad_norm": 5.51928186416626,
      "learning_rate": 0.0005679841294063787,
      "loss": 0.3845,
      "step": 2882
    },
    {
      "epoch": 1.3098591549295775,
      "grad_norm": 5.043867588043213,
      "learning_rate": 0.0005678315275446361,
      "loss": 0.5271,
      "step": 2883
    },
    {
      "epoch": 1.3103134938664245,
      "grad_norm": 5.626953125,
      "learning_rate": 0.0005676789256828934,
      "loss": 0.5543,
      "step": 2884
    },
    {
      "epoch": 1.3107678328032712,
      "grad_norm": 6.662856578826904,
      "learning_rate": 0.0005675263238211506,
      "loss": 0.7849,
      "step": 2885
    },
    {
      "epoch": 1.311222171740118,
      "grad_norm": 4.037562370300293,
      "learning_rate": 0.000567373721959408,
      "loss": 0.3611,
      "step": 2886
    },
    {
      "epoch": 1.311676510676965,
      "grad_norm": 2.9644463062286377,
      "learning_rate": 0.0005672211200976652,
      "loss": 0.1322,
      "step": 2887
    },
    {
      "epoch": 1.312130849613812,
      "grad_norm": 8.64771556854248,
      "learning_rate": 0.0005670685182359225,
      "loss": 1.1621,
      "step": 2888
    },
    {
      "epoch": 1.3125851885506588,
      "grad_norm": 4.888995170593262,
      "learning_rate": 0.0005669159163741799,
      "loss": 0.4836,
      "step": 2889
    },
    {
      "epoch": 1.3130395274875057,
      "grad_norm": 3.4928534030914307,
      "learning_rate": 0.000566763314512437,
      "loss": 0.3116,
      "step": 2890
    },
    {
      "epoch": 1.3134938664243525,
      "grad_norm": 4.149691581726074,
      "learning_rate": 0.0005666107126506943,
      "loss": 0.4058,
      "step": 2891
    },
    {
      "epoch": 1.3139482053611995,
      "grad_norm": 2.5706348419189453,
      "learning_rate": 0.0005664581107889516,
      "loss": 0.3216,
      "step": 2892
    },
    {
      "epoch": 1.3144025442980463,
      "grad_norm": 9.073897361755371,
      "learning_rate": 0.0005663055089272089,
      "loss": 0.8388,
      "step": 2893
    },
    {
      "epoch": 1.3148568832348932,
      "grad_norm": 3.908205986022949,
      "learning_rate": 0.0005661529070654661,
      "loss": 0.3664,
      "step": 2894
    },
    {
      "epoch": 1.31531122217174,
      "grad_norm": 5.778356075286865,
      "learning_rate": 0.0005660003052037235,
      "loss": 0.6547,
      "step": 2895
    },
    {
      "epoch": 1.315765561108587,
      "grad_norm": 6.9238786697387695,
      "learning_rate": 0.0005658477033419808,
      "loss": 0.8483,
      "step": 2896
    },
    {
      "epoch": 1.316219900045434,
      "grad_norm": 5.166548728942871,
      "learning_rate": 0.000565695101480238,
      "loss": 1.7605,
      "step": 2897
    },
    {
      "epoch": 1.3166742389822808,
      "grad_norm": 2.424635410308838,
      "learning_rate": 0.0005655424996184954,
      "loss": 0.1742,
      "step": 2898
    },
    {
      "epoch": 1.3171285779191277,
      "grad_norm": 6.782174110412598,
      "learning_rate": 0.0005653898977567526,
      "loss": 0.8213,
      "step": 2899
    },
    {
      "epoch": 1.3175829168559745,
      "grad_norm": 5.366535186767578,
      "learning_rate": 0.0005652372958950099,
      "loss": 0.4981,
      "step": 2900
    },
    {
      "epoch": 1.3180372557928215,
      "grad_norm": 5.588714599609375,
      "learning_rate": 0.0005650846940332673,
      "loss": 0.3878,
      "step": 2901
    },
    {
      "epoch": 1.3184915947296683,
      "grad_norm": 8.533279418945312,
      "learning_rate": 0.0005649320921715245,
      "loss": 0.9718,
      "step": 2902
    },
    {
      "epoch": 1.3189459336665152,
      "grad_norm": 3.880091428756714,
      "learning_rate": 0.0005647794903097818,
      "loss": 0.476,
      "step": 2903
    },
    {
      "epoch": 1.319400272603362,
      "grad_norm": 5.124650955200195,
      "learning_rate": 0.0005646268884480391,
      "loss": 0.4068,
      "step": 2904
    },
    {
      "epoch": 1.319854611540209,
      "grad_norm": 8.27536678314209,
      "learning_rate": 0.0005644742865862964,
      "loss": 1.1045,
      "step": 2905
    },
    {
      "epoch": 1.320308950477056,
      "grad_norm": 7.521021366119385,
      "learning_rate": 0.0005643216847245536,
      "loss": 1.0655,
      "step": 2906
    },
    {
      "epoch": 1.3207632894139028,
      "grad_norm": 3.627774238586426,
      "learning_rate": 0.000564169082862811,
      "loss": 0.3959,
      "step": 2907
    },
    {
      "epoch": 1.3212176283507497,
      "grad_norm": 5.907167911529541,
      "learning_rate": 0.0005640164810010682,
      "loss": 0.4222,
      "step": 2908
    },
    {
      "epoch": 1.3216719672875965,
      "grad_norm": 7.341396808624268,
      "learning_rate": 0.0005638638791393254,
      "loss": 0.5815,
      "step": 2909
    },
    {
      "epoch": 1.3221263062244435,
      "grad_norm": 5.004274845123291,
      "learning_rate": 0.0005637112772775828,
      "loss": 0.6005,
      "step": 2910
    },
    {
      "epoch": 1.3225806451612903,
      "grad_norm": 6.257767677307129,
      "learning_rate": 0.00056355867541584,
      "loss": 0.7558,
      "step": 2911
    },
    {
      "epoch": 1.3230349840981372,
      "grad_norm": 4.179393291473389,
      "learning_rate": 0.0005634060735540973,
      "loss": 0.7113,
      "step": 2912
    },
    {
      "epoch": 1.323489323034984,
      "grad_norm": 3.4257094860076904,
      "learning_rate": 0.0005632534716923547,
      "loss": 0.3074,
      "step": 2913
    },
    {
      "epoch": 1.323943661971831,
      "grad_norm": 5.916030406951904,
      "learning_rate": 0.0005631008698306119,
      "loss": 0.5122,
      "step": 2914
    },
    {
      "epoch": 1.324398000908678,
      "grad_norm": 4.211614608764648,
      "learning_rate": 0.0005629482679688692,
      "loss": 0.5843,
      "step": 2915
    },
    {
      "epoch": 1.3248523398455248,
      "grad_norm": 3.8548648357391357,
      "learning_rate": 0.0005627956661071265,
      "loss": 0.5004,
      "step": 2916
    },
    {
      "epoch": 1.3253066787823715,
      "grad_norm": 3.193125009536743,
      "learning_rate": 0.0005626430642453838,
      "loss": 0.2241,
      "step": 2917
    },
    {
      "epoch": 1.3257610177192185,
      "grad_norm": 6.120235443115234,
      "learning_rate": 0.000562490462383641,
      "loss": 0.6553,
      "step": 2918
    },
    {
      "epoch": 1.3262153566560655,
      "grad_norm": 6.758495807647705,
      "learning_rate": 0.0005623378605218984,
      "loss": 0.9733,
      "step": 2919
    },
    {
      "epoch": 1.3266696955929123,
      "grad_norm": 3.6605522632598877,
      "learning_rate": 0.0005621852586601557,
      "loss": 0.2802,
      "step": 2920
    },
    {
      "epoch": 1.3271240345297592,
      "grad_norm": 3.6269452571868896,
      "learning_rate": 0.0005620326567984129,
      "loss": 0.3105,
      "step": 2921
    },
    {
      "epoch": 1.327578373466606,
      "grad_norm": 4.4742817878723145,
      "learning_rate": 0.0005618800549366703,
      "loss": 0.2541,
      "step": 2922
    },
    {
      "epoch": 1.328032712403453,
      "grad_norm": 5.837465286254883,
      "learning_rate": 0.0005617274530749275,
      "loss": 0.6966,
      "step": 2923
    },
    {
      "epoch": 1.3284870513403,
      "grad_norm": 1.7058833837509155,
      "learning_rate": 0.0005615748512131848,
      "loss": 0.0836,
      "step": 2924
    },
    {
      "epoch": 1.3289413902771468,
      "grad_norm": 7.4587082862854,
      "learning_rate": 0.0005614222493514422,
      "loss": 0.9002,
      "step": 2925
    },
    {
      "epoch": 1.3293957292139935,
      "grad_norm": 6.315031051635742,
      "learning_rate": 0.0005612696474896993,
      "loss": 0.7245,
      "step": 2926
    },
    {
      "epoch": 1.3298500681508405,
      "grad_norm": 4.200883865356445,
      "learning_rate": 0.0005611170456279566,
      "loss": 0.5141,
      "step": 2927
    },
    {
      "epoch": 1.3303044070876875,
      "grad_norm": 4.7245073318481445,
      "learning_rate": 0.0005609644437662139,
      "loss": 0.731,
      "step": 2928
    },
    {
      "epoch": 1.3307587460245343,
      "grad_norm": 6.367144584655762,
      "learning_rate": 0.0005608118419044712,
      "loss": 1.0184,
      "step": 2929
    },
    {
      "epoch": 1.3312130849613812,
      "grad_norm": 6.033711910247803,
      "learning_rate": 0.0005606592400427285,
      "loss": 0.9323,
      "step": 2930
    },
    {
      "epoch": 1.331667423898228,
      "grad_norm": 5.034331321716309,
      "learning_rate": 0.0005605066381809858,
      "loss": 0.6065,
      "step": 2931
    },
    {
      "epoch": 1.332121762835075,
      "grad_norm": 5.752861499786377,
      "learning_rate": 0.0005603540363192431,
      "loss": 0.5586,
      "step": 2932
    },
    {
      "epoch": 1.332576101771922,
      "grad_norm": 6.259380340576172,
      "learning_rate": 0.0005602014344575003,
      "loss": 1.0989,
      "step": 2933
    },
    {
      "epoch": 1.3330304407087687,
      "grad_norm": 9.083059310913086,
      "learning_rate": 0.0005600488325957577,
      "loss": 1.1971,
      "step": 2934
    },
    {
      "epoch": 1.3334847796456155,
      "grad_norm": 5.072380542755127,
      "learning_rate": 0.000559896230734015,
      "loss": 0.3714,
      "step": 2935
    },
    {
      "epoch": 1.3339391185824625,
      "grad_norm": 6.212366104125977,
      "learning_rate": 0.0005597436288722722,
      "loss": 0.4795,
      "step": 2936
    },
    {
      "epoch": 1.3343934575193095,
      "grad_norm": 6.888917922973633,
      "learning_rate": 0.0005595910270105296,
      "loss": 1.0312,
      "step": 2937
    },
    {
      "epoch": 1.3348477964561563,
      "grad_norm": 5.962620735168457,
      "learning_rate": 0.0005594384251487868,
      "loss": 0.3368,
      "step": 2938
    },
    {
      "epoch": 1.3353021353930032,
      "grad_norm": 5.834059238433838,
      "learning_rate": 0.0005592858232870441,
      "loss": 0.3527,
      "step": 2939
    },
    {
      "epoch": 1.33575647432985,
      "grad_norm": 5.694520473480225,
      "learning_rate": 0.0005591332214253014,
      "loss": 0.7007,
      "step": 2940
    },
    {
      "epoch": 1.336210813266697,
      "grad_norm": 2.2735824584960938,
      "learning_rate": 0.0005589806195635587,
      "loss": 0.0706,
      "step": 2941
    },
    {
      "epoch": 1.3366651522035438,
      "grad_norm": 7.889559268951416,
      "learning_rate": 0.000558828017701816,
      "loss": 1.2189,
      "step": 2942
    },
    {
      "epoch": 1.3371194911403907,
      "grad_norm": 5.854836940765381,
      "learning_rate": 0.0005586754158400733,
      "loss": 0.8197,
      "step": 2943
    },
    {
      "epoch": 1.3375738300772375,
      "grad_norm": 3.8601953983306885,
      "learning_rate": 0.0005585228139783306,
      "loss": 0.4465,
      "step": 2944
    },
    {
      "epoch": 1.3380281690140845,
      "grad_norm": 5.658490180969238,
      "learning_rate": 0.0005583702121165877,
      "loss": 0.5682,
      "step": 2945
    },
    {
      "epoch": 1.3384825079509315,
      "grad_norm": 4.372593402862549,
      "learning_rate": 0.0005582176102548451,
      "loss": 0.4162,
      "step": 2946
    },
    {
      "epoch": 1.3389368468877783,
      "grad_norm": 3.3880417346954346,
      "learning_rate": 0.0005580650083931024,
      "loss": 0.2208,
      "step": 2947
    },
    {
      "epoch": 1.3393911858246252,
      "grad_norm": 10.07992172241211,
      "learning_rate": 0.0005579124065313596,
      "loss": 1.1551,
      "step": 2948
    },
    {
      "epoch": 1.339845524761472,
      "grad_norm": 4.014451503753662,
      "learning_rate": 0.000557759804669617,
      "loss": 0.3631,
      "step": 2949
    },
    {
      "epoch": 1.340299863698319,
      "grad_norm": 5.001387119293213,
      "learning_rate": 0.0005576072028078742,
      "loss": 0.4487,
      "step": 2950
    },
    {
      "epoch": 1.3407542026351658,
      "grad_norm": 7.437653541564941,
      "learning_rate": 0.0005574546009461315,
      "loss": 1.2571,
      "step": 2951
    },
    {
      "epoch": 1.3412085415720127,
      "grad_norm": 5.653251647949219,
      "learning_rate": 0.0005573019990843889,
      "loss": 0.4829,
      "step": 2952
    },
    {
      "epoch": 1.3416628805088595,
      "grad_norm": 5.206521511077881,
      "learning_rate": 0.0005571493972226461,
      "loss": 0.5096,
      "step": 2953
    },
    {
      "epoch": 1.3421172194457065,
      "grad_norm": 5.280998229980469,
      "learning_rate": 0.0005569967953609035,
      "loss": 0.6216,
      "step": 2954
    },
    {
      "epoch": 1.3425715583825535,
      "grad_norm": 4.14631462097168,
      "learning_rate": 0.0005568441934991607,
      "loss": 0.7118,
      "step": 2955
    },
    {
      "epoch": 1.3430258973194003,
      "grad_norm": 5.568754196166992,
      "learning_rate": 0.000556691591637418,
      "loss": 0.6127,
      "step": 2956
    },
    {
      "epoch": 1.3434802362562472,
      "grad_norm": 4.136496543884277,
      "learning_rate": 0.0005565389897756753,
      "loss": 0.5664,
      "step": 2957
    },
    {
      "epoch": 1.343934575193094,
      "grad_norm": 6.946693420410156,
      "learning_rate": 0.0005563863879139326,
      "loss": 0.5284,
      "step": 2958
    },
    {
      "epoch": 1.344388914129941,
      "grad_norm": 6.442005634307861,
      "learning_rate": 0.0005562337860521899,
      "loss": 0.4867,
      "step": 2959
    },
    {
      "epoch": 1.3448432530667878,
      "grad_norm": 6.008457660675049,
      "learning_rate": 0.0005560811841904472,
      "loss": 1.0178,
      "step": 2960
    },
    {
      "epoch": 1.3452975920036347,
      "grad_norm": 6.867028713226318,
      "learning_rate": 0.0005559285823287045,
      "loss": 0.9572,
      "step": 2961
    },
    {
      "epoch": 1.3457519309404815,
      "grad_norm": 4.173134803771973,
      "learning_rate": 0.0005557759804669617,
      "loss": 0.6004,
      "step": 2962
    },
    {
      "epoch": 1.3462062698773285,
      "grad_norm": 6.200937747955322,
      "learning_rate": 0.000555623378605219,
      "loss": 0.9944,
      "step": 2963
    },
    {
      "epoch": 1.3466606088141755,
      "grad_norm": 6.353360176086426,
      "learning_rate": 0.0005554707767434763,
      "loss": 0.7522,
      "step": 2964
    },
    {
      "epoch": 1.3471149477510223,
      "grad_norm": 2.759493589401245,
      "learning_rate": 0.0005553181748817335,
      "loss": 0.3043,
      "step": 2965
    },
    {
      "epoch": 1.347569286687869,
      "grad_norm": 5.22669792175293,
      "learning_rate": 0.0005551655730199909,
      "loss": 0.9304,
      "step": 2966
    },
    {
      "epoch": 1.348023625624716,
      "grad_norm": 5.570318698883057,
      "learning_rate": 0.0005550129711582481,
      "loss": 0.725,
      "step": 2967
    },
    {
      "epoch": 1.348477964561563,
      "grad_norm": 4.969871997833252,
      "learning_rate": 0.0005548603692965054,
      "loss": 0.4663,
      "step": 2968
    },
    {
      "epoch": 1.3489323034984098,
      "grad_norm": 4.460360050201416,
      "learning_rate": 0.0005547077674347628,
      "loss": 0.4026,
      "step": 2969
    },
    {
      "epoch": 1.3493866424352567,
      "grad_norm": 4.329141139984131,
      "learning_rate": 0.00055455516557302,
      "loss": 0.7294,
      "step": 2970
    },
    {
      "epoch": 1.3498409813721035,
      "grad_norm": 3.1882717609405518,
      "learning_rate": 0.0005544025637112773,
      "loss": 0.579,
      "step": 2971
    },
    {
      "epoch": 1.3502953203089505,
      "grad_norm": 5.38901424407959,
      "learning_rate": 0.0005542499618495346,
      "loss": 0.9181,
      "step": 2972
    },
    {
      "epoch": 1.3507496592457975,
      "grad_norm": 3.4297709465026855,
      "learning_rate": 0.0005540973599877919,
      "loss": 0.3357,
      "step": 2973
    },
    {
      "epoch": 1.3512039981826443,
      "grad_norm": 4.050697326660156,
      "learning_rate": 0.0005539447581260491,
      "loss": 0.381,
      "step": 2974
    },
    {
      "epoch": 1.351658337119491,
      "grad_norm": 7.0203166007995605,
      "learning_rate": 0.0005537921562643065,
      "loss": 0.6159,
      "step": 2975
    },
    {
      "epoch": 1.352112676056338,
      "grad_norm": 4.778031826019287,
      "learning_rate": 0.0005536395544025638,
      "loss": 0.8507,
      "step": 2976
    },
    {
      "epoch": 1.352567014993185,
      "grad_norm": 4.779626846313477,
      "learning_rate": 0.000553486952540821,
      "loss": 0.7817,
      "step": 2977
    },
    {
      "epoch": 1.3530213539300318,
      "grad_norm": 2.99314284324646,
      "learning_rate": 0.0005533343506790784,
      "loss": 0.1787,
      "step": 2978
    },
    {
      "epoch": 1.3534756928668787,
      "grad_norm": 5.607882976531982,
      "learning_rate": 0.0005531817488173356,
      "loss": 0.5848,
      "step": 2979
    },
    {
      "epoch": 1.3539300318037255,
      "grad_norm": 3.09299373626709,
      "learning_rate": 0.0005530291469555929,
      "loss": 0.3501,
      "step": 2980
    },
    {
      "epoch": 1.3543843707405725,
      "grad_norm": 2.9344980716705322,
      "learning_rate": 0.0005528765450938502,
      "loss": 0.2506,
      "step": 2981
    },
    {
      "epoch": 1.3548387096774195,
      "grad_norm": 5.896819591522217,
      "learning_rate": 0.0005527239432321074,
      "loss": 0.7442,
      "step": 2982
    },
    {
      "epoch": 1.3552930486142662,
      "grad_norm": 9.559905052185059,
      "learning_rate": 0.0005525713413703647,
      "loss": 0.9339,
      "step": 2983
    },
    {
      "epoch": 1.355747387551113,
      "grad_norm": 9.728239059448242,
      "learning_rate": 0.000552418739508622,
      "loss": 0.4428,
      "step": 2984
    },
    {
      "epoch": 1.35620172648796,
      "grad_norm": 4.949606895446777,
      "learning_rate": 0.0005522661376468793,
      "loss": 0.4937,
      "step": 2985
    },
    {
      "epoch": 1.356656065424807,
      "grad_norm": 5.714832305908203,
      "learning_rate": 0.0005521135357851365,
      "loss": 0.3709,
      "step": 2986
    },
    {
      "epoch": 1.3571104043616538,
      "grad_norm": 8.441329956054688,
      "learning_rate": 0.0005519609339233939,
      "loss": 1.1968,
      "step": 2987
    },
    {
      "epoch": 1.3575647432985007,
      "grad_norm": 4.501081943511963,
      "learning_rate": 0.0005518083320616512,
      "loss": 0.9707,
      "step": 2988
    },
    {
      "epoch": 1.3580190822353475,
      "grad_norm": 6.045125484466553,
      "learning_rate": 0.0005516557301999084,
      "loss": 0.5506,
      "step": 2989
    },
    {
      "epoch": 1.3584734211721945,
      "grad_norm": 6.351730823516846,
      "learning_rate": 0.0005515031283381658,
      "loss": 0.3749,
      "step": 2990
    },
    {
      "epoch": 1.3589277601090415,
      "grad_norm": 3.2949202060699463,
      "learning_rate": 0.000551350526476423,
      "loss": 0.2506,
      "step": 2991
    },
    {
      "epoch": 1.3593820990458882,
      "grad_norm": 6.915613174438477,
      "learning_rate": 0.0005511979246146803,
      "loss": 0.8966,
      "step": 2992
    },
    {
      "epoch": 1.359836437982735,
      "grad_norm": 4.497519493103027,
      "learning_rate": 0.0005510453227529377,
      "loss": 0.1978,
      "step": 2993
    },
    {
      "epoch": 1.360290776919582,
      "grad_norm": 4.701363563537598,
      "learning_rate": 0.0005508927208911949,
      "loss": 0.7556,
      "step": 2994
    },
    {
      "epoch": 1.360745115856429,
      "grad_norm": 7.3881072998046875,
      "learning_rate": 0.0005507401190294522,
      "loss": 1.2292,
      "step": 2995
    },
    {
      "epoch": 1.3611994547932758,
      "grad_norm": 6.218737602233887,
      "learning_rate": 0.0005505875171677095,
      "loss": 0.7539,
      "step": 2996
    },
    {
      "epoch": 1.3616537937301227,
      "grad_norm": 5.55449104309082,
      "learning_rate": 0.0005504349153059668,
      "loss": 0.4509,
      "step": 2997
    },
    {
      "epoch": 1.3621081326669695,
      "grad_norm": 6.071566104888916,
      "learning_rate": 0.0005502823134442241,
      "loss": 0.7225,
      "step": 2998
    },
    {
      "epoch": 1.3625624716038165,
      "grad_norm": 5.185513973236084,
      "learning_rate": 0.0005501297115824813,
      "loss": 0.4562,
      "step": 2999
    },
    {
      "epoch": 1.3630168105406633,
      "grad_norm": 5.135357856750488,
      "learning_rate": 0.0005499771097207386,
      "loss": 0.7215,
      "step": 3000
    },
    {
      "epoch": 1.3634711494775102,
      "grad_norm": 14.554764747619629,
      "learning_rate": 0.0005498245078589958,
      "loss": 1.2367,
      "step": 3001
    },
    {
      "epoch": 1.363925488414357,
      "grad_norm": 5.176377296447754,
      "learning_rate": 0.0005496719059972532,
      "loss": 0.6261,
      "step": 3002
    },
    {
      "epoch": 1.364379827351204,
      "grad_norm": 6.420321464538574,
      "learning_rate": 0.0005495193041355104,
      "loss": 0.6561,
      "step": 3003
    },
    {
      "epoch": 1.364834166288051,
      "grad_norm": 4.25991678237915,
      "learning_rate": 0.0005493667022737677,
      "loss": 0.1596,
      "step": 3004
    },
    {
      "epoch": 1.3652885052248978,
      "grad_norm": 9.083516120910645,
      "learning_rate": 0.0005492141004120251,
      "loss": 1.1934,
      "step": 3005
    },
    {
      "epoch": 1.3657428441617447,
      "grad_norm": 6.664404392242432,
      "learning_rate": 0.0005490614985502823,
      "loss": 0.7094,
      "step": 3006
    },
    {
      "epoch": 1.3661971830985915,
      "grad_norm": 9.066359519958496,
      "learning_rate": 0.0005489088966885396,
      "loss": 0.6275,
      "step": 3007
    },
    {
      "epoch": 1.3666515220354385,
      "grad_norm": 6.792769908905029,
      "learning_rate": 0.0005487562948267969,
      "loss": 0.6203,
      "step": 3008
    },
    {
      "epoch": 1.3671058609722853,
      "grad_norm": 5.799459934234619,
      "learning_rate": 0.0005486036929650542,
      "loss": 0.6909,
      "step": 3009
    },
    {
      "epoch": 1.3675601999091322,
      "grad_norm": 10.645524978637695,
      "learning_rate": 0.0005484510911033115,
      "loss": 1.5407,
      "step": 3010
    },
    {
      "epoch": 1.368014538845979,
      "grad_norm": 7.074049472808838,
      "learning_rate": 0.0005482984892415688,
      "loss": 1.3703,
      "step": 3011
    },
    {
      "epoch": 1.368468877782826,
      "grad_norm": 5.746401786804199,
      "learning_rate": 0.0005481458873798261,
      "loss": 0.459,
      "step": 3012
    },
    {
      "epoch": 1.368923216719673,
      "grad_norm": 2.7063355445861816,
      "learning_rate": 0.0005479932855180833,
      "loss": 0.2761,
      "step": 3013
    },
    {
      "epoch": 1.3693775556565198,
      "grad_norm": 7.61924409866333,
      "learning_rate": 0.0005478406836563407,
      "loss": 0.7897,
      "step": 3014
    },
    {
      "epoch": 1.3698318945933665,
      "grad_norm": 3.751453161239624,
      "learning_rate": 0.000547688081794598,
      "loss": 0.3036,
      "step": 3015
    },
    {
      "epoch": 1.3702862335302135,
      "grad_norm": 6.263480186462402,
      "learning_rate": 0.0005475354799328552,
      "loss": 0.5581,
      "step": 3016
    },
    {
      "epoch": 1.3707405724670605,
      "grad_norm": 4.910065174102783,
      "learning_rate": 0.0005473828780711126,
      "loss": 0.493,
      "step": 3017
    },
    {
      "epoch": 1.3711949114039073,
      "grad_norm": 3.364300012588501,
      "learning_rate": 0.0005472302762093697,
      "loss": 0.1444,
      "step": 3018
    },
    {
      "epoch": 1.3716492503407542,
      "grad_norm": 1.9697153568267822,
      "learning_rate": 0.000547077674347627,
      "loss": 0.1532,
      "step": 3019
    },
    {
      "epoch": 1.372103589277601,
      "grad_norm": 9.289840698242188,
      "learning_rate": 0.0005469250724858843,
      "loss": 1.0439,
      "step": 3020
    },
    {
      "epoch": 1.372557928214448,
      "grad_norm": 7.925819396972656,
      "learning_rate": 0.0005467724706241416,
      "loss": 0.8999,
      "step": 3021
    },
    {
      "epoch": 1.373012267151295,
      "grad_norm": 2.7773118019104004,
      "learning_rate": 0.0005466198687623989,
      "loss": 0.2243,
      "step": 3022
    },
    {
      "epoch": 1.3734666060881418,
      "grad_norm": 8.217442512512207,
      "learning_rate": 0.0005464672669006562,
      "loss": 0.9339,
      "step": 3023
    },
    {
      "epoch": 1.3739209450249885,
      "grad_norm": 4.709138870239258,
      "learning_rate": 0.0005463146650389135,
      "loss": 0.5634,
      "step": 3024
    },
    {
      "epoch": 1.3743752839618355,
      "grad_norm": 4.532449245452881,
      "learning_rate": 0.0005461620631771707,
      "loss": 0.4253,
      "step": 3025
    },
    {
      "epoch": 1.3748296228986825,
      "grad_norm": 5.726253986358643,
      "learning_rate": 0.0005460094613154281,
      "loss": 0.7424,
      "step": 3026
    },
    {
      "epoch": 1.3752839618355293,
      "grad_norm": 4.821981906890869,
      "learning_rate": 0.0005458568594536854,
      "loss": 0.5982,
      "step": 3027
    },
    {
      "epoch": 1.3757383007723762,
      "grad_norm": 4.9897990226745605,
      "learning_rate": 0.0005457042575919426,
      "loss": 0.4034,
      "step": 3028
    },
    {
      "epoch": 1.376192639709223,
      "grad_norm": 8.103405952453613,
      "learning_rate": 0.0005455516557302,
      "loss": 0.4012,
      "step": 3029
    },
    {
      "epoch": 1.37664697864607,
      "grad_norm": 6.133333683013916,
      "learning_rate": 0.0005453990538684572,
      "loss": 0.6771,
      "step": 3030
    },
    {
      "epoch": 1.377101317582917,
      "grad_norm": 4.621492385864258,
      "learning_rate": 0.0005452464520067145,
      "loss": 0.4421,
      "step": 3031
    },
    {
      "epoch": 1.3775556565197637,
      "grad_norm": 4.224615573883057,
      "learning_rate": 0.0005450938501449719,
      "loss": 0.4561,
      "step": 3032
    },
    {
      "epoch": 1.3780099954566105,
      "grad_norm": 4.5632710456848145,
      "learning_rate": 0.0005449412482832291,
      "loss": 0.4343,
      "step": 3033
    },
    {
      "epoch": 1.3784643343934575,
      "grad_norm": 8.234701156616211,
      "learning_rate": 0.0005447886464214864,
      "loss": 1.2322,
      "step": 3034
    },
    {
      "epoch": 1.3789186733303045,
      "grad_norm": 9.918821334838867,
      "learning_rate": 0.0005446360445597437,
      "loss": 0.7558,
      "step": 3035
    },
    {
      "epoch": 1.3793730122671513,
      "grad_norm": 5.375813961029053,
      "learning_rate": 0.0005444834426980009,
      "loss": 0.4922,
      "step": 3036
    },
    {
      "epoch": 1.3798273512039982,
      "grad_norm": 4.776322364807129,
      "learning_rate": 0.0005443308408362581,
      "loss": 0.6507,
      "step": 3037
    },
    {
      "epoch": 1.380281690140845,
      "grad_norm": 5.378718852996826,
      "learning_rate": 0.0005441782389745155,
      "loss": 0.5714,
      "step": 3038
    },
    {
      "epoch": 1.380736029077692,
      "grad_norm": 6.055474281311035,
      "learning_rate": 0.0005440256371127728,
      "loss": 0.6161,
      "step": 3039
    },
    {
      "epoch": 1.381190368014539,
      "grad_norm": 4.933571815490723,
      "learning_rate": 0.00054387303525103,
      "loss": 0.4306,
      "step": 3040
    },
    {
      "epoch": 1.3816447069513857,
      "grad_norm": 2.4719414710998535,
      "learning_rate": 0.0005437204333892874,
      "loss": 0.1898,
      "step": 3041
    },
    {
      "epoch": 1.3820990458882325,
      "grad_norm": 6.559352874755859,
      "learning_rate": 0.0005435678315275446,
      "loss": 0.992,
      "step": 3042
    },
    {
      "epoch": 1.3825533848250795,
      "grad_norm": 3.2596983909606934,
      "learning_rate": 0.0005434152296658019,
      "loss": 0.4384,
      "step": 3043
    },
    {
      "epoch": 1.3830077237619265,
      "grad_norm": 6.035534381866455,
      "learning_rate": 0.0005432626278040593,
      "loss": 1.1613,
      "step": 3044
    },
    {
      "epoch": 1.3834620626987733,
      "grad_norm": 7.542239665985107,
      "learning_rate": 0.0005431100259423165,
      "loss": 0.8467,
      "step": 3045
    },
    {
      "epoch": 1.3839164016356202,
      "grad_norm": 6.138664245605469,
      "learning_rate": 0.0005429574240805738,
      "loss": 0.7466,
      "step": 3046
    },
    {
      "epoch": 1.384370740572467,
      "grad_norm": 5.547935962677002,
      "learning_rate": 0.0005428048222188311,
      "loss": 0.6764,
      "step": 3047
    },
    {
      "epoch": 1.384825079509314,
      "grad_norm": 6.54602575302124,
      "learning_rate": 0.0005426522203570884,
      "loss": 0.7174,
      "step": 3048
    },
    {
      "epoch": 1.3852794184461608,
      "grad_norm": 4.438045501708984,
      "learning_rate": 0.0005424996184953456,
      "loss": 0.4947,
      "step": 3049
    },
    {
      "epoch": 1.3857337573830077,
      "grad_norm": 3.1007118225097656,
      "learning_rate": 0.000542347016633603,
      "loss": 0.1277,
      "step": 3050
    },
    {
      "epoch": 1.3861880963198545,
      "grad_norm": 5.575108051300049,
      "learning_rate": 0.0005421944147718603,
      "loss": 0.8004,
      "step": 3051
    },
    {
      "epoch": 1.3866424352567015,
      "grad_norm": 5.148551940917969,
      "learning_rate": 0.0005420418129101175,
      "loss": 0.2509,
      "step": 3052
    },
    {
      "epoch": 1.3870967741935485,
      "grad_norm": 8.525960922241211,
      "learning_rate": 0.0005418892110483749,
      "loss": 0.6223,
      "step": 3053
    },
    {
      "epoch": 1.3875511131303953,
      "grad_norm": 3.3460590839385986,
      "learning_rate": 0.000541736609186632,
      "loss": 0.1842,
      "step": 3054
    },
    {
      "epoch": 1.3880054520672422,
      "grad_norm": 5.048137187957764,
      "learning_rate": 0.0005415840073248893,
      "loss": 0.4961,
      "step": 3055
    },
    {
      "epoch": 1.388459791004089,
      "grad_norm": 3.027381181716919,
      "learning_rate": 0.0005414314054631467,
      "loss": 0.2506,
      "step": 3056
    },
    {
      "epoch": 1.388914129940936,
      "grad_norm": 3.6275033950805664,
      "learning_rate": 0.0005412788036014039,
      "loss": 0.3127,
      "step": 3057
    },
    {
      "epoch": 1.3893684688777828,
      "grad_norm": 6.5385661125183105,
      "learning_rate": 0.0005411262017396612,
      "loss": 1.1379,
      "step": 3058
    },
    {
      "epoch": 1.3898228078146297,
      "grad_norm": 5.147462368011475,
      "learning_rate": 0.0005409735998779185,
      "loss": 0.733,
      "step": 3059
    },
    {
      "epoch": 1.3902771467514765,
      "grad_norm": 4.269919395446777,
      "learning_rate": 0.0005408209980161758,
      "loss": 0.5189,
      "step": 3060
    },
    {
      "epoch": 1.3907314856883235,
      "grad_norm": 4.472171783447266,
      "learning_rate": 0.000540668396154433,
      "loss": 0.3326,
      "step": 3061
    },
    {
      "epoch": 1.3911858246251705,
      "grad_norm": 7.044148921966553,
      "learning_rate": 0.0005405157942926904,
      "loss": 0.8534,
      "step": 3062
    },
    {
      "epoch": 1.3916401635620173,
      "grad_norm": 4.662317752838135,
      "learning_rate": 0.0005403631924309477,
      "loss": 0.5904,
      "step": 3063
    },
    {
      "epoch": 1.392094502498864,
      "grad_norm": 6.487403869628906,
      "learning_rate": 0.0005402105905692049,
      "loss": 0.539,
      "step": 3064
    },
    {
      "epoch": 1.392548841435711,
      "grad_norm": 6.8681230545043945,
      "learning_rate": 0.0005400579887074623,
      "loss": 0.875,
      "step": 3065
    },
    {
      "epoch": 1.393003180372558,
      "grad_norm": 2.2483129501342773,
      "learning_rate": 0.0005399053868457195,
      "loss": 0.2368,
      "step": 3066
    },
    {
      "epoch": 1.3934575193094048,
      "grad_norm": 4.9917893409729,
      "learning_rate": 0.0005397527849839768,
      "loss": 0.9866,
      "step": 3067
    },
    {
      "epoch": 1.3939118582462517,
      "grad_norm": 4.182989120483398,
      "learning_rate": 0.0005396001831222342,
      "loss": 0.2868,
      "step": 3068
    },
    {
      "epoch": 1.3943661971830985,
      "grad_norm": 4.85413122177124,
      "learning_rate": 0.0005394475812604914,
      "loss": 0.7177,
      "step": 3069
    },
    {
      "epoch": 1.3948205361199455,
      "grad_norm": 4.806613445281982,
      "learning_rate": 0.0005392949793987487,
      "loss": 0.1653,
      "step": 3070
    },
    {
      "epoch": 1.3952748750567925,
      "grad_norm": 6.851280212402344,
      "learning_rate": 0.000539142377537006,
      "loss": 1.1083,
      "step": 3071
    },
    {
      "epoch": 1.3957292139936393,
      "grad_norm": 4.589348316192627,
      "learning_rate": 0.0005389897756752632,
      "loss": 0.4889,
      "step": 3072
    },
    {
      "epoch": 1.396183552930486,
      "grad_norm": 3.827955722808838,
      "learning_rate": 0.0005388371738135205,
      "loss": 0.2207,
      "step": 3073
    },
    {
      "epoch": 1.396637891867333,
      "grad_norm": 3.9133358001708984,
      "learning_rate": 0.0005386845719517778,
      "loss": 0.6021,
      "step": 3074
    },
    {
      "epoch": 1.39709223080418,
      "grad_norm": 2.626284122467041,
      "learning_rate": 0.0005385319700900351,
      "loss": 0.3134,
      "step": 3075
    },
    {
      "epoch": 1.3975465697410268,
      "grad_norm": 3.214794874191284,
      "learning_rate": 0.0005383793682282923,
      "loss": 0.2563,
      "step": 3076
    },
    {
      "epoch": 1.3980009086778737,
      "grad_norm": 4.7471418380737305,
      "learning_rate": 0.0005382267663665497,
      "loss": 0.2656,
      "step": 3077
    },
    {
      "epoch": 1.3984552476147205,
      "grad_norm": 3.440284252166748,
      "learning_rate": 0.000538074164504807,
      "loss": 0.4828,
      "step": 3078
    },
    {
      "epoch": 1.3989095865515675,
      "grad_norm": 7.207064628601074,
      "learning_rate": 0.0005379215626430642,
      "loss": 0.8414,
      "step": 3079
    },
    {
      "epoch": 1.3993639254884145,
      "grad_norm": 6.59575891494751,
      "learning_rate": 0.0005377689607813216,
      "loss": 0.6478,
      "step": 3080
    },
    {
      "epoch": 1.3998182644252612,
      "grad_norm": 3.26628041267395,
      "learning_rate": 0.0005376163589195788,
      "loss": 0.2143,
      "step": 3081
    },
    {
      "epoch": 1.400272603362108,
      "grad_norm": 4.515564918518066,
      "learning_rate": 0.0005374637570578361,
      "loss": 0.6641,
      "step": 3082
    },
    {
      "epoch": 1.400726942298955,
      "grad_norm": 9.222997665405273,
      "learning_rate": 0.0005373111551960934,
      "loss": 0.8977,
      "step": 3083
    },
    {
      "epoch": 1.401181281235802,
      "grad_norm": 4.338287353515625,
      "learning_rate": 0.0005371585533343507,
      "loss": 0.3067,
      "step": 3084
    },
    {
      "epoch": 1.4016356201726488,
      "grad_norm": 2.4035255908966064,
      "learning_rate": 0.000537005951472608,
      "loss": 0.2308,
      "step": 3085
    },
    {
      "epoch": 1.4020899591094957,
      "grad_norm": 7.919475555419922,
      "learning_rate": 0.0005368533496108653,
      "loss": 0.4639,
      "step": 3086
    },
    {
      "epoch": 1.4025442980463425,
      "grad_norm": 6.301841735839844,
      "learning_rate": 0.0005367007477491226,
      "loss": 0.6063,
      "step": 3087
    },
    {
      "epoch": 1.4029986369831895,
      "grad_norm": 5.933537006378174,
      "learning_rate": 0.0005365481458873798,
      "loss": 0.496,
      "step": 3088
    },
    {
      "epoch": 1.4034529759200365,
      "grad_norm": 7.522385120391846,
      "learning_rate": 0.0005363955440256372,
      "loss": 1.3269,
      "step": 3089
    },
    {
      "epoch": 1.4039073148568832,
      "grad_norm": 8.191275596618652,
      "learning_rate": 0.0005362429421638945,
      "loss": 0.642,
      "step": 3090
    },
    {
      "epoch": 1.40436165379373,
      "grad_norm": 7.693138599395752,
      "learning_rate": 0.0005360903403021516,
      "loss": 1.002,
      "step": 3091
    },
    {
      "epoch": 1.404815992730577,
      "grad_norm": 3.931774616241455,
      "learning_rate": 0.000535937738440409,
      "loss": 0.4222,
      "step": 3092
    },
    {
      "epoch": 1.405270331667424,
      "grad_norm": 6.3845014572143555,
      "learning_rate": 0.0005357851365786662,
      "loss": 1.0499,
      "step": 3093
    },
    {
      "epoch": 1.4057246706042708,
      "grad_norm": 4.168291091918945,
      "learning_rate": 0.0005356325347169235,
      "loss": 0.2357,
      "step": 3094
    },
    {
      "epoch": 1.4061790095411177,
      "grad_norm": 8.411238670349121,
      "learning_rate": 0.0005354799328551809,
      "loss": 0.6298,
      "step": 3095
    },
    {
      "epoch": 1.4066333484779645,
      "grad_norm": 7.092734336853027,
      "learning_rate": 0.0005353273309934381,
      "loss": 0.7189,
      "step": 3096
    },
    {
      "epoch": 1.4070876874148115,
      "grad_norm": 2.088860511779785,
      "learning_rate": 0.0005351747291316954,
      "loss": 0.2575,
      "step": 3097
    },
    {
      "epoch": 1.4075420263516583,
      "grad_norm": 2.75437068939209,
      "learning_rate": 0.0005350221272699527,
      "loss": 0.1042,
      "step": 3098
    },
    {
      "epoch": 1.4079963652885052,
      "grad_norm": 5.46904993057251,
      "learning_rate": 0.00053486952540821,
      "loss": 0.8445,
      "step": 3099
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 6.04897403717041,
      "learning_rate": 0.0005347169235464672,
      "loss": 0.6213,
      "step": 3100
    },
    {
      "epoch": 1.408905043162199,
      "grad_norm": 3.827684164047241,
      "learning_rate": 0.0005345643216847246,
      "loss": 0.5662,
      "step": 3101
    },
    {
      "epoch": 1.409359382099046,
      "grad_norm": 7.617419242858887,
      "learning_rate": 0.0005344117198229819,
      "loss": 1.0364,
      "step": 3102
    },
    {
      "epoch": 1.4098137210358928,
      "grad_norm": 2.37650203704834,
      "learning_rate": 0.0005342591179612391,
      "loss": 0.1343,
      "step": 3103
    },
    {
      "epoch": 1.4102680599727397,
      "grad_norm": 6.682137966156006,
      "learning_rate": 0.0005341065160994965,
      "loss": 0.9121,
      "step": 3104
    },
    {
      "epoch": 1.4107223989095865,
      "grad_norm": 4.849221229553223,
      "learning_rate": 0.0005339539142377537,
      "loss": 0.6548,
      "step": 3105
    },
    {
      "epoch": 1.4111767378464335,
      "grad_norm": 4.439809322357178,
      "learning_rate": 0.000533801312376011,
      "loss": 0.5023,
      "step": 3106
    },
    {
      "epoch": 1.4116310767832803,
      "grad_norm": 4.734665393829346,
      "learning_rate": 0.0005336487105142684,
      "loss": 0.3822,
      "step": 3107
    },
    {
      "epoch": 1.4120854157201272,
      "grad_norm": 7.795331001281738,
      "learning_rate": 0.0005334961086525256,
      "loss": 1.6912,
      "step": 3108
    },
    {
      "epoch": 1.412539754656974,
      "grad_norm": 4.657498359680176,
      "learning_rate": 0.0005333435067907828,
      "loss": 0.6702,
      "step": 3109
    },
    {
      "epoch": 1.412994093593821,
      "grad_norm": 5.1410112380981445,
      "learning_rate": 0.0005331909049290401,
      "loss": 0.7915,
      "step": 3110
    },
    {
      "epoch": 1.413448432530668,
      "grad_norm": 7.28223180770874,
      "learning_rate": 0.0005330383030672974,
      "loss": 1.0102,
      "step": 3111
    },
    {
      "epoch": 1.4139027714675148,
      "grad_norm": 3.2819700241088867,
      "learning_rate": 0.0005328857012055546,
      "loss": 0.3921,
      "step": 3112
    },
    {
      "epoch": 1.4143571104043615,
      "grad_norm": 3.3864223957061768,
      "learning_rate": 0.000532733099343812,
      "loss": 0.9123,
      "step": 3113
    },
    {
      "epoch": 1.4148114493412085,
      "grad_norm": 4.275012969970703,
      "learning_rate": 0.0005325804974820693,
      "loss": 0.472,
      "step": 3114
    },
    {
      "epoch": 1.4152657882780555,
      "grad_norm": 10.235359191894531,
      "learning_rate": 0.0005324278956203265,
      "loss": 1.5484,
      "step": 3115
    },
    {
      "epoch": 1.4157201272149023,
      "grad_norm": 4.008039951324463,
      "learning_rate": 0.0005322752937585839,
      "loss": 0.5972,
      "step": 3116
    },
    {
      "epoch": 1.4161744661517492,
      "grad_norm": 5.330288410186768,
      "learning_rate": 0.0005321226918968411,
      "loss": 0.7006,
      "step": 3117
    },
    {
      "epoch": 1.416628805088596,
      "grad_norm": 7.001466274261475,
      "learning_rate": 0.0005319700900350984,
      "loss": 0.8785,
      "step": 3118
    },
    {
      "epoch": 1.417083144025443,
      "grad_norm": 7.308090686798096,
      "learning_rate": 0.0005318174881733558,
      "loss": 0.9134,
      "step": 3119
    },
    {
      "epoch": 1.41753748296229,
      "grad_norm": 4.775641918182373,
      "learning_rate": 0.000531664886311613,
      "loss": 0.4957,
      "step": 3120
    },
    {
      "epoch": 1.4179918218991368,
      "grad_norm": 6.176577568054199,
      "learning_rate": 0.0005315122844498703,
      "loss": 0.8236,
      "step": 3121
    },
    {
      "epoch": 1.4184461608359835,
      "grad_norm": 7.051044940948486,
      "learning_rate": 0.0005313596825881276,
      "loss": 0.7938,
      "step": 3122
    },
    {
      "epoch": 1.4189004997728305,
      "grad_norm": 3.7359678745269775,
      "learning_rate": 0.0005312070807263849,
      "loss": 0.1633,
      "step": 3123
    },
    {
      "epoch": 1.4193548387096775,
      "grad_norm": 4.043960094451904,
      "learning_rate": 0.0005310544788646422,
      "loss": 0.6824,
      "step": 3124
    },
    {
      "epoch": 1.4198091776465243,
      "grad_norm": 5.640842914581299,
      "learning_rate": 0.0005309018770028995,
      "loss": 0.8393,
      "step": 3125
    },
    {
      "epoch": 1.4202635165833712,
      "grad_norm": 2.7643535137176514,
      "learning_rate": 0.0005307492751411568,
      "loss": 0.1718,
      "step": 3126
    },
    {
      "epoch": 1.420717855520218,
      "grad_norm": 4.10050630569458,
      "learning_rate": 0.0005305966732794139,
      "loss": 0.287,
      "step": 3127
    },
    {
      "epoch": 1.421172194457065,
      "grad_norm": 0.451214075088501,
      "learning_rate": 0.0005304440714176713,
      "loss": 0.0218,
      "step": 3128
    },
    {
      "epoch": 1.421626533393912,
      "grad_norm": 4.87002420425415,
      "learning_rate": 0.0005302914695559285,
      "loss": 0.5502,
      "step": 3129
    },
    {
      "epoch": 1.4220808723307587,
      "grad_norm": 5.323598384857178,
      "learning_rate": 0.0005301388676941858,
      "loss": 0.7942,
      "step": 3130
    },
    {
      "epoch": 1.4225352112676055,
      "grad_norm": 7.9477715492248535,
      "learning_rate": 0.0005299862658324432,
      "loss": 0.539,
      "step": 3131
    },
    {
      "epoch": 1.4229895502044525,
      "grad_norm": 3.5103976726531982,
      "learning_rate": 0.0005298336639707004,
      "loss": 0.2683,
      "step": 3132
    },
    {
      "epoch": 1.4234438891412995,
      "grad_norm": 2.435488224029541,
      "learning_rate": 0.0005296810621089577,
      "loss": 0.4105,
      "step": 3133
    },
    {
      "epoch": 1.4238982280781463,
      "grad_norm": 6.806379318237305,
      "learning_rate": 0.000529528460247215,
      "loss": 0.6185,
      "step": 3134
    },
    {
      "epoch": 1.4243525670149932,
      "grad_norm": 6.349564075469971,
      "learning_rate": 0.0005293758583854723,
      "loss": 0.8724,
      "step": 3135
    },
    {
      "epoch": 1.42480690595184,
      "grad_norm": 6.098018646240234,
      "learning_rate": 0.0005292232565237296,
      "loss": 0.5616,
      "step": 3136
    },
    {
      "epoch": 1.425261244888687,
      "grad_norm": 8.546253204345703,
      "learning_rate": 0.0005290706546619869,
      "loss": 1.4601,
      "step": 3137
    },
    {
      "epoch": 1.425715583825534,
      "grad_norm": 8.512005805969238,
      "learning_rate": 0.0005289180528002442,
      "loss": 1.0162,
      "step": 3138
    },
    {
      "epoch": 1.4261699227623807,
      "grad_norm": 7.483229637145996,
      "learning_rate": 0.0005287654509385014,
      "loss": 0.8696,
      "step": 3139
    },
    {
      "epoch": 1.4266242616992275,
      "grad_norm": 6.542682647705078,
      "learning_rate": 0.0005286128490767588,
      "loss": 0.501,
      "step": 3140
    },
    {
      "epoch": 1.4270786006360745,
      "grad_norm": 4.708446502685547,
      "learning_rate": 0.0005284602472150161,
      "loss": 0.3597,
      "step": 3141
    },
    {
      "epoch": 1.4275329395729215,
      "grad_norm": 8.430071830749512,
      "learning_rate": 0.0005283076453532733,
      "loss": 1.7131,
      "step": 3142
    },
    {
      "epoch": 1.4279872785097683,
      "grad_norm": 4.312011241912842,
      "learning_rate": 0.0005281550434915307,
      "loss": 0.5073,
      "step": 3143
    },
    {
      "epoch": 1.4284416174466152,
      "grad_norm": 6.822579383850098,
      "learning_rate": 0.0005280024416297879,
      "loss": 0.9571,
      "step": 3144
    },
    {
      "epoch": 1.428895956383462,
      "grad_norm": 6.139106273651123,
      "learning_rate": 0.0005278498397680451,
      "loss": 0.9311,
      "step": 3145
    },
    {
      "epoch": 1.429350295320309,
      "grad_norm": 3.1829960346221924,
      "learning_rate": 0.0005276972379063024,
      "loss": 0.4915,
      "step": 3146
    },
    {
      "epoch": 1.4298046342571558,
      "grad_norm": 4.825314044952393,
      "learning_rate": 0.0005275446360445597,
      "loss": 0.6702,
      "step": 3147
    },
    {
      "epoch": 1.4302589731940027,
      "grad_norm": 4.398453712463379,
      "learning_rate": 0.000527392034182817,
      "loss": 0.6476,
      "step": 3148
    },
    {
      "epoch": 1.4307133121308495,
      "grad_norm": 3.343783140182495,
      "learning_rate": 0.0005272394323210743,
      "loss": 0.2922,
      "step": 3149
    },
    {
      "epoch": 1.4311676510676965,
      "grad_norm": 5.1055779457092285,
      "learning_rate": 0.0005270868304593316,
      "loss": 0.6412,
      "step": 3150
    },
    {
      "epoch": 1.4316219900045435,
      "grad_norm": 9.043054580688477,
      "learning_rate": 0.0005269342285975888,
      "loss": 1.2093,
      "step": 3151
    },
    {
      "epoch": 1.4320763289413903,
      "grad_norm": 4.692543983459473,
      "learning_rate": 0.0005267816267358462,
      "loss": 0.5598,
      "step": 3152
    },
    {
      "epoch": 1.4325306678782372,
      "grad_norm": 7.396352767944336,
      "learning_rate": 0.0005266290248741035,
      "loss": 0.8828,
      "step": 3153
    },
    {
      "epoch": 1.432985006815084,
      "grad_norm": 9.012218475341797,
      "learning_rate": 0.0005264764230123607,
      "loss": 1.8075,
      "step": 3154
    },
    {
      "epoch": 1.433439345751931,
      "grad_norm": 5.272018909454346,
      "learning_rate": 0.0005263238211506181,
      "loss": 1.0411,
      "step": 3155
    },
    {
      "epoch": 1.4338936846887778,
      "grad_norm": 3.101018190383911,
      "learning_rate": 0.0005261712192888753,
      "loss": 0.3059,
      "step": 3156
    },
    {
      "epoch": 1.4343480236256247,
      "grad_norm": 5.712530136108398,
      "learning_rate": 0.0005260186174271326,
      "loss": 0.5282,
      "step": 3157
    },
    {
      "epoch": 1.4348023625624715,
      "grad_norm": 4.484728813171387,
      "learning_rate": 0.00052586601556539,
      "loss": 0.6168,
      "step": 3158
    },
    {
      "epoch": 1.4352567014993185,
      "grad_norm": 3.9245753288269043,
      "learning_rate": 0.0005257134137036472,
      "loss": 0.6558,
      "step": 3159
    },
    {
      "epoch": 1.4357110404361655,
      "grad_norm": 3.168334722518921,
      "learning_rate": 0.0005255608118419045,
      "loss": 0.3477,
      "step": 3160
    },
    {
      "epoch": 1.4361653793730123,
      "grad_norm": 1.820505142211914,
      "learning_rate": 0.0005254082099801618,
      "loss": 0.1059,
      "step": 3161
    },
    {
      "epoch": 1.436619718309859,
      "grad_norm": 4.17176628112793,
      "learning_rate": 0.0005252556081184191,
      "loss": 0.5741,
      "step": 3162
    },
    {
      "epoch": 1.437074057246706,
      "grad_norm": 5.495850563049316,
      "learning_rate": 0.0005251030062566763,
      "loss": 0.5424,
      "step": 3163
    },
    {
      "epoch": 1.437528396183553,
      "grad_norm": 4.797206401824951,
      "learning_rate": 0.0005249504043949336,
      "loss": 0.9024,
      "step": 3164
    },
    {
      "epoch": 1.4379827351203998,
      "grad_norm": 4.091606140136719,
      "learning_rate": 0.0005247978025331909,
      "loss": 0.3638,
      "step": 3165
    },
    {
      "epoch": 1.4384370740572467,
      "grad_norm": 5.793308258056641,
      "learning_rate": 0.0005246452006714481,
      "loss": 0.9321,
      "step": 3166
    },
    {
      "epoch": 1.4388914129940935,
      "grad_norm": 4.043207168579102,
      "learning_rate": 0.0005244925988097055,
      "loss": 0.3349,
      "step": 3167
    },
    {
      "epoch": 1.4393457519309405,
      "grad_norm": 3.1721792221069336,
      "learning_rate": 0.0005243399969479627,
      "loss": 0.185,
      "step": 3168
    },
    {
      "epoch": 1.4398000908677875,
      "grad_norm": 3.9987306594848633,
      "learning_rate": 0.00052418739508622,
      "loss": 0.464,
      "step": 3169
    },
    {
      "epoch": 1.4402544298046343,
      "grad_norm": 6.870309352874756,
      "learning_rate": 0.0005240347932244774,
      "loss": 0.7896,
      "step": 3170
    },
    {
      "epoch": 1.440708768741481,
      "grad_norm": 5.175310134887695,
      "learning_rate": 0.0005238821913627346,
      "loss": 0.7036,
      "step": 3171
    },
    {
      "epoch": 1.441163107678328,
      "grad_norm": 12.971610069274902,
      "learning_rate": 0.0005237295895009919,
      "loss": 1.3986,
      "step": 3172
    },
    {
      "epoch": 1.441617446615175,
      "grad_norm": 2.5876896381378174,
      "learning_rate": 0.0005235769876392492,
      "loss": 0.1377,
      "step": 3173
    },
    {
      "epoch": 1.4420717855520218,
      "grad_norm": 4.937745571136475,
      "learning_rate": 0.0005234243857775065,
      "loss": 0.5681,
      "step": 3174
    },
    {
      "epoch": 1.4425261244888687,
      "grad_norm": 6.188632965087891,
      "learning_rate": 0.0005232717839157637,
      "loss": 0.7178,
      "step": 3175
    },
    {
      "epoch": 1.4429804634257155,
      "grad_norm": 5.739080429077148,
      "learning_rate": 0.0005231191820540211,
      "loss": 1.0427,
      "step": 3176
    },
    {
      "epoch": 1.4434348023625625,
      "grad_norm": 6.342195510864258,
      "learning_rate": 0.0005229665801922784,
      "loss": 0.3144,
      "step": 3177
    },
    {
      "epoch": 1.4438891412994095,
      "grad_norm": 4.843430042266846,
      "learning_rate": 0.0005228139783305356,
      "loss": 0.5556,
      "step": 3178
    },
    {
      "epoch": 1.4443434802362562,
      "grad_norm": 7.493574619293213,
      "learning_rate": 0.000522661376468793,
      "loss": 1.0104,
      "step": 3179
    },
    {
      "epoch": 1.444797819173103,
      "grad_norm": 4.544165134429932,
      "learning_rate": 0.0005225087746070502,
      "loss": 0.4794,
      "step": 3180
    },
    {
      "epoch": 1.44525215810995,
      "grad_norm": 6.520674228668213,
      "learning_rate": 0.0005223561727453075,
      "loss": 0.6425,
      "step": 3181
    },
    {
      "epoch": 1.445706497046797,
      "grad_norm": 5.32724142074585,
      "learning_rate": 0.0005222035708835648,
      "loss": 0.3501,
      "step": 3182
    },
    {
      "epoch": 1.4461608359836438,
      "grad_norm": 6.567057132720947,
      "learning_rate": 0.000522050969021822,
      "loss": 0.8838,
      "step": 3183
    },
    {
      "epoch": 1.4466151749204907,
      "grad_norm": 5.361834526062012,
      "learning_rate": 0.0005218983671600793,
      "loss": 0.7005,
      "step": 3184
    },
    {
      "epoch": 1.4470695138573375,
      "grad_norm": 8.966327667236328,
      "learning_rate": 0.0005217457652983366,
      "loss": 0.9179,
      "step": 3185
    },
    {
      "epoch": 1.4475238527941845,
      "grad_norm": 5.699326992034912,
      "learning_rate": 0.0005215931634365939,
      "loss": 0.6976,
      "step": 3186
    },
    {
      "epoch": 1.4479781917310315,
      "grad_norm": 3.3144822120666504,
      "learning_rate": 0.0005214405615748512,
      "loss": 0.233,
      "step": 3187
    },
    {
      "epoch": 1.4484325306678782,
      "grad_norm": 3.2808918952941895,
      "learning_rate": 0.0005212879597131085,
      "loss": 0.1657,
      "step": 3188
    },
    {
      "epoch": 1.448886869604725,
      "grad_norm": 7.457958221435547,
      "learning_rate": 0.0005211353578513658,
      "loss": 0.6829,
      "step": 3189
    },
    {
      "epoch": 1.449341208541572,
      "grad_norm": 7.784405708312988,
      "learning_rate": 0.000520982755989623,
      "loss": 0.7765,
      "step": 3190
    },
    {
      "epoch": 1.449795547478419,
      "grad_norm": 5.27721643447876,
      "learning_rate": 0.0005208301541278804,
      "loss": 0.6591,
      "step": 3191
    },
    {
      "epoch": 1.4502498864152658,
      "grad_norm": 4.327444076538086,
      "learning_rate": 0.0005206775522661376,
      "loss": 0.483,
      "step": 3192
    },
    {
      "epoch": 1.4507042253521127,
      "grad_norm": 10.510385513305664,
      "learning_rate": 0.0005205249504043949,
      "loss": 1.3635,
      "step": 3193
    },
    {
      "epoch": 1.4511585642889595,
      "grad_norm": 9.837637901306152,
      "learning_rate": 0.0005203723485426523,
      "loss": 1.3633,
      "step": 3194
    },
    {
      "epoch": 1.4516129032258065,
      "grad_norm": 4.693342685699463,
      "learning_rate": 0.0005202197466809095,
      "loss": 0.5178,
      "step": 3195
    },
    {
      "epoch": 1.4520672421626533,
      "grad_norm": 1.712269902229309,
      "learning_rate": 0.0005200671448191668,
      "loss": 0.0575,
      "step": 3196
    },
    {
      "epoch": 1.4525215810995002,
      "grad_norm": 7.3113555908203125,
      "learning_rate": 0.0005199145429574241,
      "loss": 0.8316,
      "step": 3197
    },
    {
      "epoch": 1.452975920036347,
      "grad_norm": 7.968295574188232,
      "learning_rate": 0.0005197619410956814,
      "loss": 0.5699,
      "step": 3198
    },
    {
      "epoch": 1.453430258973194,
      "grad_norm": 5.289465427398682,
      "learning_rate": 0.0005196093392339388,
      "loss": 0.8391,
      "step": 3199
    },
    {
      "epoch": 1.453884597910041,
      "grad_norm": 4.804018020629883,
      "learning_rate": 0.0005194567373721959,
      "loss": 0.3663,
      "step": 3200
    },
    {
      "epoch": 1.4543389368468878,
      "grad_norm": 4.192440509796143,
      "learning_rate": 0.0005193041355104532,
      "loss": 0.5154,
      "step": 3201
    },
    {
      "epoch": 1.4547932757837347,
      "grad_norm": 2.6880335807800293,
      "learning_rate": 0.0005191515336487104,
      "loss": 0.2573,
      "step": 3202
    },
    {
      "epoch": 1.4552476147205815,
      "grad_norm": 4.602118015289307,
      "learning_rate": 0.0005189989317869678,
      "loss": 0.4472,
      "step": 3203
    },
    {
      "epoch": 1.4557019536574285,
      "grad_norm": 5.327615737915039,
      "learning_rate": 0.000518846329925225,
      "loss": 0.3408,
      "step": 3204
    },
    {
      "epoch": 1.4561562925942753,
      "grad_norm": 7.954257011413574,
      "learning_rate": 0.0005186937280634823,
      "loss": 1.0339,
      "step": 3205
    },
    {
      "epoch": 1.4566106315311222,
      "grad_norm": 6.495786190032959,
      "learning_rate": 0.0005185411262017397,
      "loss": 0.6589,
      "step": 3206
    },
    {
      "epoch": 1.457064970467969,
      "grad_norm": 7.319449424743652,
      "learning_rate": 0.0005183885243399969,
      "loss": 0.5966,
      "step": 3207
    },
    {
      "epoch": 1.457519309404816,
      "grad_norm": 4.086535930633545,
      "learning_rate": 0.0005182359224782543,
      "loss": 0.2945,
      "step": 3208
    },
    {
      "epoch": 1.457973648341663,
      "grad_norm": 4.949710369110107,
      "learning_rate": 0.0005180833206165116,
      "loss": 0.7383,
      "step": 3209
    },
    {
      "epoch": 1.4584279872785098,
      "grad_norm": 2.486755609512329,
      "learning_rate": 0.0005179307187547688,
      "loss": 0.1843,
      "step": 3210
    },
    {
      "epoch": 1.4588823262153565,
      "grad_norm": 5.771286487579346,
      "learning_rate": 0.0005177781168930262,
      "loss": 0.5191,
      "step": 3211
    },
    {
      "epoch": 1.4593366651522035,
      "grad_norm": 5.1113505363464355,
      "learning_rate": 0.0005176255150312834,
      "loss": 0.3022,
      "step": 3212
    },
    {
      "epoch": 1.4597910040890505,
      "grad_norm": 4.815590858459473,
      "learning_rate": 0.0005174729131695407,
      "loss": 0.2984,
      "step": 3213
    },
    {
      "epoch": 1.4602453430258973,
      "grad_norm": 3.7863166332244873,
      "learning_rate": 0.000517320311307798,
      "loss": 0.2118,
      "step": 3214
    },
    {
      "epoch": 1.4606996819627442,
      "grad_norm": 5.992754936218262,
      "learning_rate": 0.0005171677094460553,
      "loss": 0.3953,
      "step": 3215
    },
    {
      "epoch": 1.461154020899591,
      "grad_norm": 9.18700885772705,
      "learning_rate": 0.0005170151075843126,
      "loss": 0.8223,
      "step": 3216
    },
    {
      "epoch": 1.461608359836438,
      "grad_norm": 6.650477886199951,
      "learning_rate": 0.0005168625057225699,
      "loss": 0.5188,
      "step": 3217
    },
    {
      "epoch": 1.462062698773285,
      "grad_norm": 6.371170997619629,
      "learning_rate": 0.0005167099038608271,
      "loss": 0.713,
      "step": 3218
    },
    {
      "epoch": 1.4625170377101318,
      "grad_norm": 7.05621337890625,
      "learning_rate": 0.0005165573019990843,
      "loss": 0.4049,
      "step": 3219
    },
    {
      "epoch": 1.4629713766469785,
      "grad_norm": 6.816261291503906,
      "learning_rate": 0.0005164047001373417,
      "loss": 0.8245,
      "step": 3220
    },
    {
      "epoch": 1.4634257155838255,
      "grad_norm": 9.414506912231445,
      "learning_rate": 0.000516252098275599,
      "loss": 0.6952,
      "step": 3221
    },
    {
      "epoch": 1.4638800545206725,
      "grad_norm": 3.3046672344207764,
      "learning_rate": 0.0005160994964138562,
      "loss": 0.4233,
      "step": 3222
    },
    {
      "epoch": 1.4643343934575193,
      "grad_norm": 5.432570457458496,
      "learning_rate": 0.0005159468945521136,
      "loss": 0.6087,
      "step": 3223
    },
    {
      "epoch": 1.4647887323943662,
      "grad_norm": 5.944006443023682,
      "learning_rate": 0.0005157942926903708,
      "loss": 0.625,
      "step": 3224
    },
    {
      "epoch": 1.465243071331213,
      "grad_norm": 5.570180892944336,
      "learning_rate": 0.0005156416908286281,
      "loss": 0.6441,
      "step": 3225
    },
    {
      "epoch": 1.46569741026806,
      "grad_norm": 3.136991262435913,
      "learning_rate": 0.0005154890889668855,
      "loss": 0.3729,
      "step": 3226
    },
    {
      "epoch": 1.466151749204907,
      "grad_norm": 17.57920265197754,
      "learning_rate": 0.0005153364871051427,
      "loss": 0.839,
      "step": 3227
    },
    {
      "epoch": 1.4666060881417537,
      "grad_norm": 5.125130653381348,
      "learning_rate": 0.0005151838852434,
      "loss": 0.3326,
      "step": 3228
    },
    {
      "epoch": 1.4670604270786005,
      "grad_norm": 2.5706896781921387,
      "learning_rate": 0.0005150312833816573,
      "loss": 0.1589,
      "step": 3229
    },
    {
      "epoch": 1.4675147660154475,
      "grad_norm": 6.275984287261963,
      "learning_rate": 0.0005148786815199146,
      "loss": 1.1557,
      "step": 3230
    },
    {
      "epoch": 1.4679691049522945,
      "grad_norm": 6.44006872177124,
      "learning_rate": 0.0005147260796581718,
      "loss": 0.7016,
      "step": 3231
    },
    {
      "epoch": 1.4684234438891413,
      "grad_norm": 5.093042373657227,
      "learning_rate": 0.0005145734777964292,
      "loss": 0.374,
      "step": 3232
    },
    {
      "epoch": 1.4688777828259882,
      "grad_norm": 5.208173751831055,
      "learning_rate": 0.0005144208759346865,
      "loss": 0.4552,
      "step": 3233
    },
    {
      "epoch": 1.469332121762835,
      "grad_norm": 13.19494342803955,
      "learning_rate": 0.0005142682740729437,
      "loss": 1.4363,
      "step": 3234
    },
    {
      "epoch": 1.469786460699682,
      "grad_norm": 7.220447540283203,
      "learning_rate": 0.0005141156722112011,
      "loss": 0.8996,
      "step": 3235
    },
    {
      "epoch": 1.470240799636529,
      "grad_norm": 5.634332656860352,
      "learning_rate": 0.0005139630703494583,
      "loss": 0.5434,
      "step": 3236
    },
    {
      "epoch": 1.4706951385733757,
      "grad_norm": 4.867230415344238,
      "learning_rate": 0.0005138104684877155,
      "loss": 0.4574,
      "step": 3237
    },
    {
      "epoch": 1.4711494775102225,
      "grad_norm": 4.918178558349609,
      "learning_rate": 0.0005136578666259729,
      "loss": 0.5133,
      "step": 3238
    },
    {
      "epoch": 1.4716038164470695,
      "grad_norm": 4.614819526672363,
      "learning_rate": 0.0005135052647642301,
      "loss": 0.5663,
      "step": 3239
    },
    {
      "epoch": 1.4720581553839165,
      "grad_norm": 9.99659538269043,
      "learning_rate": 0.0005133526629024874,
      "loss": 0.4725,
      "step": 3240
    },
    {
      "epoch": 1.4725124943207633,
      "grad_norm": 6.514346599578857,
      "learning_rate": 0.0005132000610407447,
      "loss": 0.5748,
      "step": 3241
    },
    {
      "epoch": 1.4729668332576102,
      "grad_norm": 4.569766044616699,
      "learning_rate": 0.000513047459179002,
      "loss": 0.2937,
      "step": 3242
    },
    {
      "epoch": 1.473421172194457,
      "grad_norm": 6.189648628234863,
      "learning_rate": 0.0005128948573172592,
      "loss": 0.8604,
      "step": 3243
    },
    {
      "epoch": 1.473875511131304,
      "grad_norm": 5.74361515045166,
      "learning_rate": 0.0005127422554555166,
      "loss": 0.3718,
      "step": 3244
    },
    {
      "epoch": 1.4743298500681508,
      "grad_norm": 5.035192012786865,
      "learning_rate": 0.0005125896535937739,
      "loss": 0.4635,
      "step": 3245
    },
    {
      "epoch": 1.4747841890049977,
      "grad_norm": 7.2635674476623535,
      "learning_rate": 0.0005124370517320311,
      "loss": 0.9647,
      "step": 3246
    },
    {
      "epoch": 1.4752385279418445,
      "grad_norm": 7.246686935424805,
      "learning_rate": 0.0005122844498702885,
      "loss": 0.9027,
      "step": 3247
    },
    {
      "epoch": 1.4756928668786915,
      "grad_norm": 4.224393844604492,
      "learning_rate": 0.0005121318480085457,
      "loss": 0.3311,
      "step": 3248
    },
    {
      "epoch": 1.4761472058155385,
      "grad_norm": 8.687334060668945,
      "learning_rate": 0.000511979246146803,
      "loss": 1.2355,
      "step": 3249
    },
    {
      "epoch": 1.4766015447523853,
      "grad_norm": 7.173309803009033,
      "learning_rate": 0.0005118266442850604,
      "loss": 0.4918,
      "step": 3250
    },
    {
      "epoch": 1.4770558836892322,
      "grad_norm": 5.576521396636963,
      "learning_rate": 0.0005116740424233176,
      "loss": 0.6084,
      "step": 3251
    },
    {
      "epoch": 1.477510222626079,
      "grad_norm": 2.6906487941741943,
      "learning_rate": 0.0005115214405615749,
      "loss": 0.294,
      "step": 3252
    },
    {
      "epoch": 1.477964561562926,
      "grad_norm": 3.689136028289795,
      "learning_rate": 0.0005113688386998322,
      "loss": 0.4166,
      "step": 3253
    },
    {
      "epoch": 1.4784189004997728,
      "grad_norm": 7.659011363983154,
      "learning_rate": 0.0005112162368380895,
      "loss": 0.8943,
      "step": 3254
    },
    {
      "epoch": 1.4788732394366197,
      "grad_norm": 8.915834426879883,
      "learning_rate": 0.0005110636349763466,
      "loss": 1.114,
      "step": 3255
    },
    {
      "epoch": 1.4793275783734665,
      "grad_norm": 7.058487892150879,
      "learning_rate": 0.000510911033114604,
      "loss": 1.1269,
      "step": 3256
    },
    {
      "epoch": 1.4797819173103135,
      "grad_norm": 3.8022446632385254,
      "learning_rate": 0.0005107584312528613,
      "loss": 0.1713,
      "step": 3257
    },
    {
      "epoch": 1.4802362562471605,
      "grad_norm": 3.7243592739105225,
      "learning_rate": 0.0005106058293911185,
      "loss": 0.6349,
      "step": 3258
    },
    {
      "epoch": 1.4806905951840073,
      "grad_norm": 6.104964733123779,
      "learning_rate": 0.0005104532275293759,
      "loss": 0.7367,
      "step": 3259
    },
    {
      "epoch": 1.4811449341208542,
      "grad_norm": 8.7813081741333,
      "learning_rate": 0.0005103006256676331,
      "loss": 0.6934,
      "step": 3260
    },
    {
      "epoch": 1.481599273057701,
      "grad_norm": 3.47556209564209,
      "learning_rate": 0.0005101480238058904,
      "loss": 0.3536,
      "step": 3261
    },
    {
      "epoch": 1.482053611994548,
      "grad_norm": 5.333038330078125,
      "learning_rate": 0.0005099954219441478,
      "loss": 0.5431,
      "step": 3262
    },
    {
      "epoch": 1.4825079509313948,
      "grad_norm": 4.583590030670166,
      "learning_rate": 0.000509842820082405,
      "loss": 0.2232,
      "step": 3263
    },
    {
      "epoch": 1.4829622898682417,
      "grad_norm": 6.287092685699463,
      "learning_rate": 0.0005096902182206623,
      "loss": 0.2658,
      "step": 3264
    },
    {
      "epoch": 1.4834166288050885,
      "grad_norm": 5.313472270965576,
      "learning_rate": 0.0005095376163589196,
      "loss": 0.5128,
      "step": 3265
    },
    {
      "epoch": 1.4838709677419355,
      "grad_norm": 6.2883195877075195,
      "learning_rate": 0.0005093850144971769,
      "loss": 0.8606,
      "step": 3266
    },
    {
      "epoch": 1.4843253066787825,
      "grad_norm": 6.266199588775635,
      "learning_rate": 0.0005092324126354342,
      "loss": 0.6996,
      "step": 3267
    },
    {
      "epoch": 1.4847796456156293,
      "grad_norm": 4.307782173156738,
      "learning_rate": 0.0005090798107736915,
      "loss": 0.2163,
      "step": 3268
    },
    {
      "epoch": 1.485233984552476,
      "grad_norm": 3.3642146587371826,
      "learning_rate": 0.0005089272089119488,
      "loss": 0.0998,
      "step": 3269
    },
    {
      "epoch": 1.485688323489323,
      "grad_norm": 3.0030853748321533,
      "learning_rate": 0.000508774607050206,
      "loss": 0.175,
      "step": 3270
    },
    {
      "epoch": 1.48614266242617,
      "grad_norm": 7.952065944671631,
      "learning_rate": 0.0005086220051884634,
      "loss": 1.1206,
      "step": 3271
    },
    {
      "epoch": 1.4865970013630168,
      "grad_norm": 3.0539138317108154,
      "learning_rate": 0.0005084694033267207,
      "loss": 0.2171,
      "step": 3272
    },
    {
      "epoch": 1.4870513402998637,
      "grad_norm": 2.9353718757629395,
      "learning_rate": 0.0005083168014649778,
      "loss": 0.2521,
      "step": 3273
    },
    {
      "epoch": 1.4875056792367105,
      "grad_norm": 3.9222218990325928,
      "learning_rate": 0.0005081641996032352,
      "loss": 0.4542,
      "step": 3274
    },
    {
      "epoch": 1.4879600181735575,
      "grad_norm": 5.778743267059326,
      "learning_rate": 0.0005080115977414924,
      "loss": 1.1551,
      "step": 3275
    },
    {
      "epoch": 1.4884143571104045,
      "grad_norm": 3.866328477859497,
      "learning_rate": 0.0005078589958797497,
      "loss": 0.408,
      "step": 3276
    },
    {
      "epoch": 1.4888686960472512,
      "grad_norm": 6.83064079284668,
      "learning_rate": 0.000507706394018007,
      "loss": 0.444,
      "step": 3277
    },
    {
      "epoch": 1.489323034984098,
      "grad_norm": 6.236902236938477,
      "learning_rate": 0.0005075537921562643,
      "loss": 0.7697,
      "step": 3278
    },
    {
      "epoch": 1.489777373920945,
      "grad_norm": 7.412669658660889,
      "learning_rate": 0.0005074011902945216,
      "loss": 1.0698,
      "step": 3279
    },
    {
      "epoch": 1.490231712857792,
      "grad_norm": 7.099177360534668,
      "learning_rate": 0.0005072485884327789,
      "loss": 0.9784,
      "step": 3280
    },
    {
      "epoch": 1.4906860517946388,
      "grad_norm": 6.798006057739258,
      "learning_rate": 0.0005070959865710362,
      "loss": 0.7237,
      "step": 3281
    },
    {
      "epoch": 1.4911403907314857,
      "grad_norm": 4.185273170471191,
      "learning_rate": 0.0005069433847092934,
      "loss": 0.3349,
      "step": 3282
    },
    {
      "epoch": 1.4915947296683325,
      "grad_norm": 4.735050201416016,
      "learning_rate": 0.0005067907828475508,
      "loss": 0.2907,
      "step": 3283
    },
    {
      "epoch": 1.4920490686051795,
      "grad_norm": 4.712800025939941,
      "learning_rate": 0.0005066381809858081,
      "loss": 0.4861,
      "step": 3284
    },
    {
      "epoch": 1.4925034075420265,
      "grad_norm": 7.221519947052002,
      "learning_rate": 0.0005064855791240653,
      "loss": 0.2871,
      "step": 3285
    },
    {
      "epoch": 1.4929577464788732,
      "grad_norm": 4.45894718170166,
      "learning_rate": 0.0005063329772623227,
      "loss": 0.3468,
      "step": 3286
    },
    {
      "epoch": 1.49341208541572,
      "grad_norm": 5.556399345397949,
      "learning_rate": 0.0005061803754005799,
      "loss": 0.3628,
      "step": 3287
    },
    {
      "epoch": 1.493866424352567,
      "grad_norm": 4.214694023132324,
      "learning_rate": 0.0005060277735388372,
      "loss": 0.4086,
      "step": 3288
    },
    {
      "epoch": 1.494320763289414,
      "grad_norm": 6.214810371398926,
      "learning_rate": 0.0005058751716770946,
      "loss": 0.6265,
      "step": 3289
    },
    {
      "epoch": 1.4947751022262608,
      "grad_norm": 7.031861782073975,
      "learning_rate": 0.0005057225698153518,
      "loss": 0.8868,
      "step": 3290
    },
    {
      "epoch": 1.4952294411631077,
      "grad_norm": 5.067119598388672,
      "learning_rate": 0.000505569967953609,
      "loss": 0.3036,
      "step": 3291
    },
    {
      "epoch": 1.4956837800999545,
      "grad_norm": 3.7101776599884033,
      "learning_rate": 0.0005054173660918663,
      "loss": 0.3194,
      "step": 3292
    },
    {
      "epoch": 1.4961381190368015,
      "grad_norm": 0.7749713659286499,
      "learning_rate": 0.0005052647642301236,
      "loss": 0.0289,
      "step": 3293
    },
    {
      "epoch": 1.4965924579736483,
      "grad_norm": 3.7000458240509033,
      "learning_rate": 0.0005051121623683808,
      "loss": 0.2618,
      "step": 3294
    },
    {
      "epoch": 1.4970467969104952,
      "grad_norm": 7.691455841064453,
      "learning_rate": 0.0005049595605066382,
      "loss": 0.6142,
      "step": 3295
    },
    {
      "epoch": 1.497501135847342,
      "grad_norm": 7.222320079803467,
      "learning_rate": 0.0005048069586448955,
      "loss": 1.0577,
      "step": 3296
    },
    {
      "epoch": 1.497955474784189,
      "grad_norm": 6.973972320556641,
      "learning_rate": 0.0005046543567831527,
      "loss": 1.1771,
      "step": 3297
    },
    {
      "epoch": 1.498409813721036,
      "grad_norm": 2.79201078414917,
      "learning_rate": 0.0005045017549214101,
      "loss": 0.1261,
      "step": 3298
    },
    {
      "epoch": 1.4988641526578828,
      "grad_norm": 6.068123817443848,
      "learning_rate": 0.0005043491530596673,
      "loss": 0.7792,
      "step": 3299
    },
    {
      "epoch": 1.4993184915947297,
      "grad_norm": 5.065833568572998,
      "learning_rate": 0.0005041965511979246,
      "loss": 0.4525,
      "step": 3300
    },
    {
      "epoch": 1.4997728305315765,
      "grad_norm": 4.418552398681641,
      "learning_rate": 0.000504043949336182,
      "loss": 0.4608,
      "step": 3301
    },
    {
      "epoch": 1.5002271694684235,
      "grad_norm": 7.663390159606934,
      "learning_rate": 0.0005038913474744392,
      "loss": 0.8305,
      "step": 3302
    },
    {
      "epoch": 1.5006815084052705,
      "grad_norm": 4.69104528427124,
      "learning_rate": 0.0005037387456126965,
      "loss": 0.3829,
      "step": 3303
    },
    {
      "epoch": 1.5011358473421172,
      "grad_norm": 5.179911136627197,
      "learning_rate": 0.0005035861437509538,
      "loss": 0.6188,
      "step": 3304
    },
    {
      "epoch": 1.501590186278964,
      "grad_norm": 5.443544864654541,
      "learning_rate": 0.0005034335418892111,
      "loss": 0.6703,
      "step": 3305
    },
    {
      "epoch": 1.502044525215811,
      "grad_norm": 5.505910873413086,
      "learning_rate": 0.0005032809400274683,
      "loss": 0.4947,
      "step": 3306
    },
    {
      "epoch": 1.502498864152658,
      "grad_norm": 5.163214206695557,
      "learning_rate": 0.0005031283381657257,
      "loss": 0.7936,
      "step": 3307
    },
    {
      "epoch": 1.5029532030895048,
      "grad_norm": 3.37381649017334,
      "learning_rate": 0.000502975736303983,
      "loss": 0.3201,
      "step": 3308
    },
    {
      "epoch": 1.5034075420263515,
      "grad_norm": 9.579708099365234,
      "learning_rate": 0.0005028231344422402,
      "loss": 0.9884,
      "step": 3309
    },
    {
      "epoch": 1.5038618809631985,
      "grad_norm": 5.9730095863342285,
      "learning_rate": 0.0005026705325804975,
      "loss": 0.8073,
      "step": 3310
    },
    {
      "epoch": 1.5043162199000455,
      "grad_norm": 6.339090824127197,
      "learning_rate": 0.0005025179307187547,
      "loss": 1.1299,
      "step": 3311
    },
    {
      "epoch": 1.5047705588368925,
      "grad_norm": 5.2543158531188965,
      "learning_rate": 0.000502365328857012,
      "loss": 0.6282,
      "step": 3312
    },
    {
      "epoch": 1.5052248977737392,
      "grad_norm": 9.946118354797363,
      "learning_rate": 0.0005022127269952694,
      "loss": 1.2746,
      "step": 3313
    },
    {
      "epoch": 1.505679236710586,
      "grad_norm": 3.712127447128296,
      "learning_rate": 0.0005020601251335266,
      "loss": 0.3028,
      "step": 3314
    },
    {
      "epoch": 1.506133575647433,
      "grad_norm": 4.928901672363281,
      "learning_rate": 0.0005019075232717839,
      "loss": 0.7814,
      "step": 3315
    },
    {
      "epoch": 1.50658791458428,
      "grad_norm": 4.705923080444336,
      "learning_rate": 0.0005017549214100412,
      "loss": 0.6858,
      "step": 3316
    },
    {
      "epoch": 1.5070422535211268,
      "grad_norm": 6.407492160797119,
      "learning_rate": 0.0005016023195482985,
      "loss": 0.3835,
      "step": 3317
    },
    {
      "epoch": 1.5074965924579735,
      "grad_norm": 3.394962787628174,
      "learning_rate": 0.0005014497176865557,
      "loss": 0.2888,
      "step": 3318
    },
    {
      "epoch": 1.5079509313948205,
      "grad_norm": 6.712169647216797,
      "learning_rate": 0.0005012971158248131,
      "loss": 0.9109,
      "step": 3319
    },
    {
      "epoch": 1.5084052703316675,
      "grad_norm": 8.913934707641602,
      "learning_rate": 0.0005011445139630704,
      "loss": 0.4589,
      "step": 3320
    },
    {
      "epoch": 1.5088596092685143,
      "grad_norm": 4.678641319274902,
      "learning_rate": 0.0005009919121013276,
      "loss": 0.1309,
      "step": 3321
    },
    {
      "epoch": 1.5093139482053612,
      "grad_norm": 8.848870277404785,
      "learning_rate": 0.000500839310239585,
      "loss": 1.1394,
      "step": 3322
    },
    {
      "epoch": 1.509768287142208,
      "grad_norm": 3.0180678367614746,
      "learning_rate": 0.0005006867083778422,
      "loss": 0.2555,
      "step": 3323
    },
    {
      "epoch": 1.510222626079055,
      "grad_norm": 4.018989562988281,
      "learning_rate": 0.0005005341065160995,
      "loss": 0.4511,
      "step": 3324
    },
    {
      "epoch": 1.510676965015902,
      "grad_norm": 4.552650451660156,
      "learning_rate": 0.0005003815046543569,
      "loss": 0.3171,
      "step": 3325
    },
    {
      "epoch": 1.5111313039527488,
      "grad_norm": 11.028402328491211,
      "learning_rate": 0.0005002289027926141,
      "loss": 1.0756,
      "step": 3326
    },
    {
      "epoch": 1.5115856428895955,
      "grad_norm": 11.080613136291504,
      "learning_rate": 0.0005000763009308714,
      "loss": 0.8975,
      "step": 3327
    },
    {
      "epoch": 1.5120399818264425,
      "grad_norm": 6.466576099395752,
      "learning_rate": 0.0004999236990691286,
      "loss": 0.8265,
      "step": 3328
    },
    {
      "epoch": 1.5124943207632895,
      "grad_norm": 2.4159977436065674,
      "learning_rate": 0.000499771097207386,
      "loss": 0.2784,
      "step": 3329
    },
    {
      "epoch": 1.5129486597001363,
      "grad_norm": 2.322932481765747,
      "learning_rate": 0.0004996184953456433,
      "loss": 0.1619,
      "step": 3330
    },
    {
      "epoch": 1.5134029986369832,
      "grad_norm": 3.7076117992401123,
      "learning_rate": 0.0004994658934839005,
      "loss": 0.305,
      "step": 3331
    },
    {
      "epoch": 1.51385733757383,
      "grad_norm": 4.787801742553711,
      "learning_rate": 0.0004993132916221579,
      "loss": 0.4734,
      "step": 3332
    },
    {
      "epoch": 1.514311676510677,
      "grad_norm": 5.6778764724731445,
      "learning_rate": 0.000499160689760415,
      "loss": 0.8601,
      "step": 3333
    },
    {
      "epoch": 1.514766015447524,
      "grad_norm": 7.184929370880127,
      "learning_rate": 0.0004990080878986724,
      "loss": 0.7736,
      "step": 3334
    },
    {
      "epoch": 1.5152203543843707,
      "grad_norm": 2.717599630355835,
      "learning_rate": 0.0004988554860369297,
      "loss": 0.1459,
      "step": 3335
    },
    {
      "epoch": 1.5156746933212175,
      "grad_norm": 4.496736526489258,
      "learning_rate": 0.0004987028841751869,
      "loss": 0.2269,
      "step": 3336
    },
    {
      "epoch": 1.5161290322580645,
      "grad_norm": 6.796935081481934,
      "learning_rate": 0.0004985502823134443,
      "loss": 0.7693,
      "step": 3337
    },
    {
      "epoch": 1.5165833711949115,
      "grad_norm": 5.916954517364502,
      "learning_rate": 0.0004983976804517015,
      "loss": 0.9418,
      "step": 3338
    },
    {
      "epoch": 1.5170377101317583,
      "grad_norm": 6.480383396148682,
      "learning_rate": 0.0004982450785899588,
      "loss": 0.6158,
      "step": 3339
    },
    {
      "epoch": 1.517492049068605,
      "grad_norm": 5.762387752532959,
      "learning_rate": 0.0004980924767282161,
      "loss": 0.9095,
      "step": 3340
    },
    {
      "epoch": 1.517946388005452,
      "grad_norm": 4.509206295013428,
      "learning_rate": 0.0004979398748664734,
      "loss": 0.2184,
      "step": 3341
    },
    {
      "epoch": 1.518400726942299,
      "grad_norm": 2.792590618133545,
      "learning_rate": 0.0004977872730047307,
      "loss": 0.3177,
      "step": 3342
    },
    {
      "epoch": 1.518855065879146,
      "grad_norm": 4.411055088043213,
      "learning_rate": 0.0004976346711429879,
      "loss": 0.599,
      "step": 3343
    },
    {
      "epoch": 1.5193094048159927,
      "grad_norm": 4.347546100616455,
      "learning_rate": 0.0004974820692812453,
      "loss": 0.4054,
      "step": 3344
    },
    {
      "epoch": 1.5197637437528395,
      "grad_norm": 4.6300048828125,
      "learning_rate": 0.0004973294674195025,
      "loss": 0.3957,
      "step": 3345
    },
    {
      "epoch": 1.5202180826896865,
      "grad_norm": 7.55803108215332,
      "learning_rate": 0.0004971768655577598,
      "loss": 0.3335,
      "step": 3346
    },
    {
      "epoch": 1.5206724216265335,
      "grad_norm": 3.756161689758301,
      "learning_rate": 0.0004970242636960172,
      "loss": 0.3648,
      "step": 3347
    },
    {
      "epoch": 1.5211267605633803,
      "grad_norm": 5.910868167877197,
      "learning_rate": 0.0004968716618342744,
      "loss": 0.5785,
      "step": 3348
    },
    {
      "epoch": 1.521581099500227,
      "grad_norm": 5.326605319976807,
      "learning_rate": 0.0004967190599725317,
      "loss": 1.0894,
      "step": 3349
    },
    {
      "epoch": 1.522035438437074,
      "grad_norm": 4.849514961242676,
      "learning_rate": 0.000496566458110789,
      "loss": 0.657,
      "step": 3350
    },
    {
      "epoch": 1.522489777373921,
      "grad_norm": 5.2554850578308105,
      "learning_rate": 0.0004964138562490462,
      "loss": 0.5887,
      "step": 3351
    },
    {
      "epoch": 1.522944116310768,
      "grad_norm": 7.851941108703613,
      "learning_rate": 0.0004962612543873036,
      "loss": 0.5504,
      "step": 3352
    },
    {
      "epoch": 1.5233984552476147,
      "grad_norm": 3.6626293659210205,
      "learning_rate": 0.0004961086525255608,
      "loss": 0.3232,
      "step": 3353
    },
    {
      "epoch": 1.5238527941844615,
      "grad_norm": 4.6232781410217285,
      "learning_rate": 0.0004959560506638181,
      "loss": 0.5611,
      "step": 3354
    },
    {
      "epoch": 1.5243071331213085,
      "grad_norm": 5.835255146026611,
      "learning_rate": 0.0004958034488020754,
      "loss": 0.6427,
      "step": 3355
    },
    {
      "epoch": 1.5247614720581555,
      "grad_norm": 8.107614517211914,
      "learning_rate": 0.0004956508469403327,
      "loss": 0.8928,
      "step": 3356
    },
    {
      "epoch": 1.5252158109950023,
      "grad_norm": 5.349315166473389,
      "learning_rate": 0.0004954982450785899,
      "loss": 0.7097,
      "step": 3357
    },
    {
      "epoch": 1.525670149931849,
      "grad_norm": 5.345333576202393,
      "learning_rate": 0.0004953456432168473,
      "loss": 0.4558,
      "step": 3358
    },
    {
      "epoch": 1.526124488868696,
      "grad_norm": 3.027365207672119,
      "learning_rate": 0.0004951930413551046,
      "loss": 0.1614,
      "step": 3359
    },
    {
      "epoch": 1.526578827805543,
      "grad_norm": 3.3989291191101074,
      "learning_rate": 0.0004950404394933618,
      "loss": 0.5413,
      "step": 3360
    },
    {
      "epoch": 1.52703316674239,
      "grad_norm": 7.178717613220215,
      "learning_rate": 0.0004948878376316191,
      "loss": 1.4745,
      "step": 3361
    },
    {
      "epoch": 1.5274875056792367,
      "grad_norm": 4.375628471374512,
      "learning_rate": 0.0004947352357698764,
      "loss": 0.5241,
      "step": 3362
    },
    {
      "epoch": 1.5279418446160835,
      "grad_norm": 3.475813627243042,
      "learning_rate": 0.0004945826339081337,
      "loss": 0.1807,
      "step": 3363
    },
    {
      "epoch": 1.5283961835529305,
      "grad_norm": 2.6986725330352783,
      "learning_rate": 0.000494430032046391,
      "loss": 0.2116,
      "step": 3364
    },
    {
      "epoch": 1.5288505224897775,
      "grad_norm": 7.400983810424805,
      "learning_rate": 0.0004942774301846483,
      "loss": 0.7309,
      "step": 3365
    },
    {
      "epoch": 1.5293048614266243,
      "grad_norm": 5.11137580871582,
      "learning_rate": 0.0004941248283229056,
      "loss": 0.752,
      "step": 3366
    },
    {
      "epoch": 1.529759200363471,
      "grad_norm": 4.651496887207031,
      "learning_rate": 0.0004939722264611628,
      "loss": 0.5057,
      "step": 3367
    },
    {
      "epoch": 1.530213539300318,
      "grad_norm": 2.1134791374206543,
      "learning_rate": 0.0004938196245994202,
      "loss": 0.2021,
      "step": 3368
    },
    {
      "epoch": 1.530667878237165,
      "grad_norm": 3.9541149139404297,
      "learning_rate": 0.0004936670227376773,
      "loss": 0.3761,
      "step": 3369
    },
    {
      "epoch": 1.531122217174012,
      "grad_norm": 6.033106327056885,
      "learning_rate": 0.0004935144208759347,
      "loss": 0.791,
      "step": 3370
    },
    {
      "epoch": 1.5315765561108587,
      "grad_norm": 6.818013668060303,
      "learning_rate": 0.000493361819014192,
      "loss": 0.6482,
      "step": 3371
    },
    {
      "epoch": 1.5320308950477055,
      "grad_norm": 5.903110504150391,
      "learning_rate": 0.0004932092171524492,
      "loss": 0.9751,
      "step": 3372
    },
    {
      "epoch": 1.5324852339845525,
      "grad_norm": 3.656038284301758,
      "learning_rate": 0.0004930566152907066,
      "loss": 0.5284,
      "step": 3373
    },
    {
      "epoch": 1.5329395729213995,
      "grad_norm": 6.768863677978516,
      "learning_rate": 0.0004929040134289638,
      "loss": 0.7883,
      "step": 3374
    },
    {
      "epoch": 1.5333939118582463,
      "grad_norm": 4.113038539886475,
      "learning_rate": 0.0004927514115672211,
      "loss": 0.3156,
      "step": 3375
    },
    {
      "epoch": 1.533848250795093,
      "grad_norm": 5.241662979125977,
      "learning_rate": 0.0004925988097054785,
      "loss": 0.5623,
      "step": 3376
    },
    {
      "epoch": 1.53430258973194,
      "grad_norm": 3.478947877883911,
      "learning_rate": 0.0004924462078437357,
      "loss": 0.3501,
      "step": 3377
    },
    {
      "epoch": 1.534756928668787,
      "grad_norm": 4.305234909057617,
      "learning_rate": 0.000492293605981993,
      "loss": 0.2064,
      "step": 3378
    },
    {
      "epoch": 1.5352112676056338,
      "grad_norm": 2.192004919052124,
      "learning_rate": 0.0004921410041202502,
      "loss": 0.1992,
      "step": 3379
    },
    {
      "epoch": 1.5356656065424807,
      "grad_norm": 9.084142684936523,
      "learning_rate": 0.0004919884022585076,
      "loss": 1.3254,
      "step": 3380
    },
    {
      "epoch": 1.5361199454793275,
      "grad_norm": 3.914454460144043,
      "learning_rate": 0.0004918358003967649,
      "loss": 0.3315,
      "step": 3381
    },
    {
      "epoch": 1.5365742844161745,
      "grad_norm": 7.381384372711182,
      "learning_rate": 0.0004916831985350221,
      "loss": 0.7151,
      "step": 3382
    },
    {
      "epoch": 1.5370286233530215,
      "grad_norm": 4.025425910949707,
      "learning_rate": 0.0004915305966732795,
      "loss": 0.2935,
      "step": 3383
    },
    {
      "epoch": 1.5374829622898682,
      "grad_norm": 3.1106317043304443,
      "learning_rate": 0.0004913779948115367,
      "loss": 0.1791,
      "step": 3384
    },
    {
      "epoch": 1.537937301226715,
      "grad_norm": 9.669020652770996,
      "learning_rate": 0.000491225392949794,
      "loss": 1.056,
      "step": 3385
    },
    {
      "epoch": 1.538391640163562,
      "grad_norm": 6.371641635894775,
      "learning_rate": 0.0004910727910880514,
      "loss": 0.7028,
      "step": 3386
    },
    {
      "epoch": 1.538845979100409,
      "grad_norm": 6.440104007720947,
      "learning_rate": 0.0004909201892263085,
      "loss": 0.5677,
      "step": 3387
    },
    {
      "epoch": 1.5393003180372558,
      "grad_norm": 4.545970439910889,
      "learning_rate": 0.0004907675873645659,
      "loss": 0.436,
      "step": 3388
    },
    {
      "epoch": 1.5397546569741025,
      "grad_norm": 3.506370782852173,
      "learning_rate": 0.0004906149855028231,
      "loss": 0.1903,
      "step": 3389
    },
    {
      "epoch": 1.5402089959109495,
      "grad_norm": 4.328026294708252,
      "learning_rate": 0.0004904623836410804,
      "loss": 0.3089,
      "step": 3390
    },
    {
      "epoch": 1.5406633348477965,
      "grad_norm": 4.312992095947266,
      "learning_rate": 0.0004903097817793377,
      "loss": 0.4987,
      "step": 3391
    },
    {
      "epoch": 1.5411176737846435,
      "grad_norm": 4.26358699798584,
      "learning_rate": 0.000490157179917595,
      "loss": 0.3282,
      "step": 3392
    },
    {
      "epoch": 1.5415720127214902,
      "grad_norm": 4.1357526779174805,
      "learning_rate": 0.0004900045780558523,
      "loss": 0.555,
      "step": 3393
    },
    {
      "epoch": 1.542026351658337,
      "grad_norm": 4.4186835289001465,
      "learning_rate": 0.0004898519761941096,
      "loss": 0.3794,
      "step": 3394
    },
    {
      "epoch": 1.542480690595184,
      "grad_norm": 5.268327236175537,
      "learning_rate": 0.0004896993743323669,
      "loss": 0.4446,
      "step": 3395
    },
    {
      "epoch": 1.542935029532031,
      "grad_norm": 3.4921720027923584,
      "learning_rate": 0.0004895467724706241,
      "loss": 0.4684,
      "step": 3396
    },
    {
      "epoch": 1.5433893684688778,
      "grad_norm": 4.156038284301758,
      "learning_rate": 0.0004893941706088814,
      "loss": 0.4801,
      "step": 3397
    },
    {
      "epoch": 1.5438437074057245,
      "grad_norm": 5.4323225021362305,
      "learning_rate": 0.0004892415687471388,
      "loss": 0.8691,
      "step": 3398
    },
    {
      "epoch": 1.5442980463425715,
      "grad_norm": 12.956315994262695,
      "learning_rate": 0.000489088966885396,
      "loss": 0.9378,
      "step": 3399
    },
    {
      "epoch": 1.5447523852794185,
      "grad_norm": 5.532439708709717,
      "learning_rate": 0.0004889363650236533,
      "loss": 0.5641,
      "step": 3400
    },
    {
      "epoch": 1.5452067242162655,
      "grad_norm": 5.125669956207275,
      "learning_rate": 0.0004887837631619106,
      "loss": 0.5438,
      "step": 3401
    },
    {
      "epoch": 1.5456610631531122,
      "grad_norm": 4.672638416290283,
      "learning_rate": 0.0004886311613001679,
      "loss": 0.9459,
      "step": 3402
    },
    {
      "epoch": 1.546115402089959,
      "grad_norm": 4.254983425140381,
      "learning_rate": 0.0004884785594384251,
      "loss": 0.5052,
      "step": 3403
    },
    {
      "epoch": 1.546569741026806,
      "grad_norm": 3.4428536891937256,
      "learning_rate": 0.0004883259575766825,
      "loss": 0.5752,
      "step": 3404
    },
    {
      "epoch": 1.547024079963653,
      "grad_norm": 5.864843845367432,
      "learning_rate": 0.00048817335571493977,
      "loss": 0.5021,
      "step": 3405
    },
    {
      "epoch": 1.5474784189004998,
      "grad_norm": 6.730127334594727,
      "learning_rate": 0.000488020753853197,
      "loss": 0.637,
      "step": 3406
    },
    {
      "epoch": 1.5479327578373465,
      "grad_norm": 7.762166976928711,
      "learning_rate": 0.0004878681519914543,
      "loss": 0.8087,
      "step": 3407
    },
    {
      "epoch": 1.5483870967741935,
      "grad_norm": 5.250002861022949,
      "learning_rate": 0.0004877155501297116,
      "loss": 0.5208,
      "step": 3408
    },
    {
      "epoch": 1.5488414357110405,
      "grad_norm": 3.953195333480835,
      "learning_rate": 0.0004875629482679689,
      "loss": 0.4945,
      "step": 3409
    },
    {
      "epoch": 1.5492957746478875,
      "grad_norm": 4.908101558685303,
      "learning_rate": 0.00048741034640622616,
      "loss": 0.4272,
      "step": 3410
    },
    {
      "epoch": 1.5497501135847342,
      "grad_norm": 2.4788239002227783,
      "learning_rate": 0.00048725774454448347,
      "loss": 0.1919,
      "step": 3411
    },
    {
      "epoch": 1.550204452521581,
      "grad_norm": 1.8073631525039673,
      "learning_rate": 0.0004871051426827408,
      "loss": 0.0834,
      "step": 3412
    },
    {
      "epoch": 1.550658791458428,
      "grad_norm": 4.562803268432617,
      "learning_rate": 0.00048695254082099803,
      "loss": 0.4994,
      "step": 3413
    },
    {
      "epoch": 1.551113130395275,
      "grad_norm": 2.708277463912964,
      "learning_rate": 0.00048679993895925534,
      "loss": 0.1299,
      "step": 3414
    },
    {
      "epoch": 1.5515674693321218,
      "grad_norm": 3.1884374618530273,
      "learning_rate": 0.0004866473370975126,
      "loss": 0.1465,
      "step": 3415
    },
    {
      "epoch": 1.5520218082689685,
      "grad_norm": 5.313383102416992,
      "learning_rate": 0.00048649473523576986,
      "loss": 0.5946,
      "step": 3416
    },
    {
      "epoch": 1.5524761472058155,
      "grad_norm": 6.5920939445495605,
      "learning_rate": 0.00048634213337402717,
      "loss": 0.7943,
      "step": 3417
    },
    {
      "epoch": 1.5529304861426625,
      "grad_norm": 5.562882900238037,
      "learning_rate": 0.0004861895315122845,
      "loss": 0.4594,
      "step": 3418
    },
    {
      "epoch": 1.5533848250795095,
      "grad_norm": 6.224301338195801,
      "learning_rate": 0.00048603692965054173,
      "loss": 0.5151,
      "step": 3419
    },
    {
      "epoch": 1.5538391640163562,
      "grad_norm": 6.296523094177246,
      "learning_rate": 0.00048588432778879905,
      "loss": 0.5387,
      "step": 3420
    },
    {
      "epoch": 1.554293502953203,
      "grad_norm": 3.7558062076568604,
      "learning_rate": 0.00048573172592705636,
      "loss": 0.295,
      "step": 3421
    },
    {
      "epoch": 1.55474784189005,
      "grad_norm": 6.555325984954834,
      "learning_rate": 0.0004855791240653136,
      "loss": 0.7831,
      "step": 3422
    },
    {
      "epoch": 1.555202180826897,
      "grad_norm": 7.19174337387085,
      "learning_rate": 0.0004854265222035709,
      "loss": 0.3538,
      "step": 3423
    },
    {
      "epoch": 1.5556565197637438,
      "grad_norm": 3.9584152698516846,
      "learning_rate": 0.0004852739203418282,
      "loss": 0.2172,
      "step": 3424
    },
    {
      "epoch": 1.5561108587005905,
      "grad_norm": 8.191399574279785,
      "learning_rate": 0.00048512131848008544,
      "loss": 0.703,
      "step": 3425
    },
    {
      "epoch": 1.5565651976374375,
      "grad_norm": 3.8952670097351074,
      "learning_rate": 0.00048496871661834275,
      "loss": 0.4002,
      "step": 3426
    },
    {
      "epoch": 1.5570195365742845,
      "grad_norm": 5.489908695220947,
      "learning_rate": 0.00048481611475660006,
      "loss": 0.4271,
      "step": 3427
    },
    {
      "epoch": 1.5574738755111313,
      "grad_norm": 4.910402297973633,
      "learning_rate": 0.0004846635128948573,
      "loss": 0.7039,
      "step": 3428
    },
    {
      "epoch": 1.5579282144479782,
      "grad_norm": 4.319445610046387,
      "learning_rate": 0.0004845109110331146,
      "loss": 0.5022,
      "step": 3429
    },
    {
      "epoch": 1.558382553384825,
      "grad_norm": 5.77436637878418,
      "learning_rate": 0.00048435830917137193,
      "loss": 0.6475,
      "step": 3430
    },
    {
      "epoch": 1.558836892321672,
      "grad_norm": 3.9846041202545166,
      "learning_rate": 0.0004842057073096292,
      "loss": 0.354,
      "step": 3431
    },
    {
      "epoch": 1.559291231258519,
      "grad_norm": 2.993499755859375,
      "learning_rate": 0.0004840531054478865,
      "loss": 0.3973,
      "step": 3432
    },
    {
      "epoch": 1.5597455701953657,
      "grad_norm": 4.737091541290283,
      "learning_rate": 0.00048390050358614376,
      "loss": 0.6669,
      "step": 3433
    },
    {
      "epoch": 1.5601999091322125,
      "grad_norm": 2.0839109420776367,
      "learning_rate": 0.000483747901724401,
      "loss": 0.1085,
      "step": 3434
    },
    {
      "epoch": 1.5606542480690595,
      "grad_norm": 6.736015319824219,
      "learning_rate": 0.0004835952998626583,
      "loss": 0.7875,
      "step": 3435
    },
    {
      "epoch": 1.5611085870059065,
      "grad_norm": 4.216271877288818,
      "learning_rate": 0.00048344269800091564,
      "loss": 0.5066,
      "step": 3436
    },
    {
      "epoch": 1.5615629259427533,
      "grad_norm": 1.8122689723968506,
      "learning_rate": 0.0004832900961391729,
      "loss": 0.086,
      "step": 3437
    },
    {
      "epoch": 1.5620172648796,
      "grad_norm": 5.051336765289307,
      "learning_rate": 0.0004831374942774302,
      "loss": 0.5745,
      "step": 3438
    },
    {
      "epoch": 1.562471603816447,
      "grad_norm": 5.355201244354248,
      "learning_rate": 0.0004829848924156875,
      "loss": 0.559,
      "step": 3439
    },
    {
      "epoch": 1.562925942753294,
      "grad_norm": 9.422327041625977,
      "learning_rate": 0.00048283229055394477,
      "loss": 0.8806,
      "step": 3440
    },
    {
      "epoch": 1.563380281690141,
      "grad_norm": 2.140846014022827,
      "learning_rate": 0.0004826796886922021,
      "loss": 0.1134,
      "step": 3441
    },
    {
      "epoch": 1.5638346206269877,
      "grad_norm": 5.217075824737549,
      "learning_rate": 0.00048252708683045934,
      "loss": 1.4608,
      "step": 3442
    },
    {
      "epoch": 1.5642889595638345,
      "grad_norm": 0.9796215891838074,
      "learning_rate": 0.0004823744849687166,
      "loss": 0.0593,
      "step": 3443
    },
    {
      "epoch": 1.5647432985006815,
      "grad_norm": 3.2750935554504395,
      "learning_rate": 0.0004822218831069739,
      "loss": 0.303,
      "step": 3444
    },
    {
      "epoch": 1.5651976374375285,
      "grad_norm": 5.5929484367370605,
      "learning_rate": 0.0004820692812452312,
      "loss": 0.5469,
      "step": 3445
    },
    {
      "epoch": 1.5656519763743753,
      "grad_norm": 6.537533283233643,
      "learning_rate": 0.00048191667938348847,
      "loss": 0.549,
      "step": 3446
    },
    {
      "epoch": 1.566106315311222,
      "grad_norm": 1.8111096620559692,
      "learning_rate": 0.0004817640775217458,
      "loss": 0.0551,
      "step": 3447
    },
    {
      "epoch": 1.566560654248069,
      "grad_norm": 5.5654473304748535,
      "learning_rate": 0.0004816114756600031,
      "loss": 0.3979,
      "step": 3448
    },
    {
      "epoch": 1.567014993184916,
      "grad_norm": 4.475262641906738,
      "learning_rate": 0.00048145887379826035,
      "loss": 0.5916,
      "step": 3449
    },
    {
      "epoch": 1.567469332121763,
      "grad_norm": 4.706027507781982,
      "learning_rate": 0.00048130627193651766,
      "loss": 0.548,
      "step": 3450
    },
    {
      "epoch": 1.5679236710586097,
      "grad_norm": 7.019276142120361,
      "learning_rate": 0.0004811536700747749,
      "loss": 0.5064,
      "step": 3451
    },
    {
      "epoch": 1.5683780099954565,
      "grad_norm": 6.196934223175049,
      "learning_rate": 0.00048100106821303217,
      "loss": 1.1907,
      "step": 3452
    },
    {
      "epoch": 1.5688323489323035,
      "grad_norm": 3.866450071334839,
      "learning_rate": 0.0004808484663512895,
      "loss": 0.439,
      "step": 3453
    },
    {
      "epoch": 1.5692866878691505,
      "grad_norm": 1.3502298593521118,
      "learning_rate": 0.0004806958644895468,
      "loss": 0.0867,
      "step": 3454
    },
    {
      "epoch": 1.5697410268059973,
      "grad_norm": 5.364276885986328,
      "learning_rate": 0.00048054326262780405,
      "loss": 0.5676,
      "step": 3455
    },
    {
      "epoch": 1.570195365742844,
      "grad_norm": 3.9990394115448,
      "learning_rate": 0.00048039066076606136,
      "loss": 0.5364,
      "step": 3456
    },
    {
      "epoch": 1.570649704679691,
      "grad_norm": 5.570387363433838,
      "learning_rate": 0.00048023805890431867,
      "loss": 0.7505,
      "step": 3457
    },
    {
      "epoch": 1.571104043616538,
      "grad_norm": 8.768960952758789,
      "learning_rate": 0.0004800854570425759,
      "loss": 1.3074,
      "step": 3458
    },
    {
      "epoch": 1.571558382553385,
      "grad_norm": 2.41827654838562,
      "learning_rate": 0.00047993285518083324,
      "loss": 0.1365,
      "step": 3459
    },
    {
      "epoch": 1.5720127214902317,
      "grad_norm": 8.308850288391113,
      "learning_rate": 0.0004797802533190905,
      "loss": 0.4873,
      "step": 3460
    },
    {
      "epoch": 1.5724670604270785,
      "grad_norm": 4.619317054748535,
      "learning_rate": 0.00047962765145734775,
      "loss": 0.3542,
      "step": 3461
    },
    {
      "epoch": 1.5729213993639255,
      "grad_norm": 3.801159381866455,
      "learning_rate": 0.00047947504959560506,
      "loss": 0.1402,
      "step": 3462
    },
    {
      "epoch": 1.5733757383007725,
      "grad_norm": 7.11729621887207,
      "learning_rate": 0.00047932244773386237,
      "loss": 0.9711,
      "step": 3463
    },
    {
      "epoch": 1.5738300772376193,
      "grad_norm": 7.543684959411621,
      "learning_rate": 0.00047916984587211963,
      "loss": 0.5832,
      "step": 3464
    },
    {
      "epoch": 1.574284416174466,
      "grad_norm": 6.7651848793029785,
      "learning_rate": 0.00047901724401037694,
      "loss": 0.7235,
      "step": 3465
    },
    {
      "epoch": 1.574738755111313,
      "grad_norm": 4.558567047119141,
      "learning_rate": 0.00047886464214863425,
      "loss": 0.4254,
      "step": 3466
    },
    {
      "epoch": 1.57519309404816,
      "grad_norm": 4.624655246734619,
      "learning_rate": 0.0004787120402868915,
      "loss": 0.3434,
      "step": 3467
    },
    {
      "epoch": 1.575647432985007,
      "grad_norm": 3.0680782794952393,
      "learning_rate": 0.0004785594384251488,
      "loss": 0.369,
      "step": 3468
    },
    {
      "epoch": 1.5761017719218537,
      "grad_norm": 6.302219390869141,
      "learning_rate": 0.00047840683656340607,
      "loss": 1.4518,
      "step": 3469
    },
    {
      "epoch": 1.5765561108587005,
      "grad_norm": 5.053959369659424,
      "learning_rate": 0.00047825423470166333,
      "loss": 0.5401,
      "step": 3470
    },
    {
      "epoch": 1.5770104497955475,
      "grad_norm": 6.169377326965332,
      "learning_rate": 0.00047810163283992064,
      "loss": 0.402,
      "step": 3471
    },
    {
      "epoch": 1.5774647887323945,
      "grad_norm": 3.6102001667022705,
      "learning_rate": 0.00047794903097817795,
      "loss": 0.2263,
      "step": 3472
    },
    {
      "epoch": 1.5779191276692413,
      "grad_norm": 3.3782098293304443,
      "learning_rate": 0.0004777964291164352,
      "loss": 0.5284,
      "step": 3473
    },
    {
      "epoch": 1.578373466606088,
      "grad_norm": 2.6542396545410156,
      "learning_rate": 0.0004776438272546925,
      "loss": 0.1552,
      "step": 3474
    },
    {
      "epoch": 1.578827805542935,
      "grad_norm": 5.2007222175598145,
      "learning_rate": 0.00047749122539294983,
      "loss": 0.7984,
      "step": 3475
    },
    {
      "epoch": 1.579282144479782,
      "grad_norm": 5.37970495223999,
      "learning_rate": 0.0004773386235312071,
      "loss": 0.7959,
      "step": 3476
    },
    {
      "epoch": 1.5797364834166288,
      "grad_norm": 5.265091896057129,
      "learning_rate": 0.0004771860216694644,
      "loss": 0.9376,
      "step": 3477
    },
    {
      "epoch": 1.5801908223534757,
      "grad_norm": 6.382909297943115,
      "learning_rate": 0.0004770334198077217,
      "loss": 0.5506,
      "step": 3478
    },
    {
      "epoch": 1.5806451612903225,
      "grad_norm": 8.32009220123291,
      "learning_rate": 0.0004768808179459789,
      "loss": 0.9187,
      "step": 3479
    },
    {
      "epoch": 1.5810995002271695,
      "grad_norm": 4.648778438568115,
      "learning_rate": 0.0004767282160842362,
      "loss": 0.1734,
      "step": 3480
    },
    {
      "epoch": 1.5815538391640165,
      "grad_norm": 5.430562496185303,
      "learning_rate": 0.00047657561422249353,
      "loss": 0.365,
      "step": 3481
    },
    {
      "epoch": 1.5820081781008632,
      "grad_norm": 5.269317626953125,
      "learning_rate": 0.0004764230123607508,
      "loss": 0.1831,
      "step": 3482
    },
    {
      "epoch": 1.58246251703771,
      "grad_norm": 4.0555419921875,
      "learning_rate": 0.0004762704104990081,
      "loss": 0.2354,
      "step": 3483
    },
    {
      "epoch": 1.582916855974557,
      "grad_norm": 5.76217794418335,
      "learning_rate": 0.0004761178086372654,
      "loss": 0.432,
      "step": 3484
    },
    {
      "epoch": 1.583371194911404,
      "grad_norm": 6.6799092292785645,
      "learning_rate": 0.00047596520677552266,
      "loss": 1.1625,
      "step": 3485
    },
    {
      "epoch": 1.5838255338482508,
      "grad_norm": 2.4965462684631348,
      "learning_rate": 0.00047581260491378,
      "loss": 0.1383,
      "step": 3486
    },
    {
      "epoch": 1.5842798727850975,
      "grad_norm": 2.5674126148223877,
      "learning_rate": 0.0004756600030520373,
      "loss": 0.2035,
      "step": 3487
    },
    {
      "epoch": 1.5847342117219445,
      "grad_norm": 7.304149150848389,
      "learning_rate": 0.0004755074011902945,
      "loss": 0.7499,
      "step": 3488
    },
    {
      "epoch": 1.5851885506587915,
      "grad_norm": 4.759154319763184,
      "learning_rate": 0.0004753547993285518,
      "loss": 0.2473,
      "step": 3489
    },
    {
      "epoch": 1.5856428895956385,
      "grad_norm": 8.26710033416748,
      "learning_rate": 0.0004752021974668091,
      "loss": 1.5803,
      "step": 3490
    },
    {
      "epoch": 1.5860972285324852,
      "grad_norm": 4.996990203857422,
      "learning_rate": 0.00047504959560506636,
      "loss": 0.5879,
      "step": 3491
    },
    {
      "epoch": 1.586551567469332,
      "grad_norm": 2.45878529548645,
      "learning_rate": 0.0004748969937433237,
      "loss": 0.2136,
      "step": 3492
    },
    {
      "epoch": 1.587005906406179,
      "grad_norm": 3.9532883167266846,
      "learning_rate": 0.000474744391881581,
      "loss": 0.1775,
      "step": 3493
    },
    {
      "epoch": 1.587460245343026,
      "grad_norm": 4.148066520690918,
      "learning_rate": 0.00047459179001983824,
      "loss": 0.3493,
      "step": 3494
    },
    {
      "epoch": 1.5879145842798728,
      "grad_norm": 8.56745433807373,
      "learning_rate": 0.00047443918815809555,
      "loss": 0.6931,
      "step": 3495
    },
    {
      "epoch": 1.5883689232167195,
      "grad_norm": 8.146475791931152,
      "learning_rate": 0.00047428658629635286,
      "loss": 1.4395,
      "step": 3496
    },
    {
      "epoch": 1.5888232621535665,
      "grad_norm": 7.631603240966797,
      "learning_rate": 0.00047413398443461006,
      "loss": 0.6221,
      "step": 3497
    },
    {
      "epoch": 1.5892776010904135,
      "grad_norm": 4.213558673858643,
      "learning_rate": 0.0004739813825728674,
      "loss": 0.3722,
      "step": 3498
    },
    {
      "epoch": 1.5897319400272605,
      "grad_norm": 3.8850550651550293,
      "learning_rate": 0.0004738287807111247,
      "loss": 0.1734,
      "step": 3499
    },
    {
      "epoch": 1.5901862789641072,
      "grad_norm": 8.37063980102539,
      "learning_rate": 0.00047367617884938194,
      "loss": 0.9677,
      "step": 3500
    },
    {
      "epoch": 1.590640617900954,
      "grad_norm": 5.112694263458252,
      "learning_rate": 0.00047352357698763925,
      "loss": 0.3368,
      "step": 3501
    },
    {
      "epoch": 1.591094956837801,
      "grad_norm": 6.954619407653809,
      "learning_rate": 0.00047337097512589656,
      "loss": 0.6928,
      "step": 3502
    },
    {
      "epoch": 1.591549295774648,
      "grad_norm": 5.410045146942139,
      "learning_rate": 0.0004732183732641538,
      "loss": 0.941,
      "step": 3503
    },
    {
      "epoch": 1.5920036347114948,
      "grad_norm": 6.523885250091553,
      "learning_rate": 0.00047306577140241113,
      "loss": 0.9789,
      "step": 3504
    },
    {
      "epoch": 1.5924579736483415,
      "grad_norm": 3.1014673709869385,
      "learning_rate": 0.00047291316954066844,
      "loss": 0.3143,
      "step": 3505
    },
    {
      "epoch": 1.5929123125851885,
      "grad_norm": 7.505801200866699,
      "learning_rate": 0.00047276056767892564,
      "loss": 0.7331,
      "step": 3506
    },
    {
      "epoch": 1.5933666515220355,
      "grad_norm": 7.704234600067139,
      "learning_rate": 0.00047260796581718295,
      "loss": 0.7235,
      "step": 3507
    },
    {
      "epoch": 1.5938209904588825,
      "grad_norm": 5.696896553039551,
      "learning_rate": 0.00047245536395544026,
      "loss": 0.6264,
      "step": 3508
    },
    {
      "epoch": 1.5942753293957292,
      "grad_norm": 4.1624956130981445,
      "learning_rate": 0.0004723027620936975,
      "loss": 0.3589,
      "step": 3509
    },
    {
      "epoch": 1.594729668332576,
      "grad_norm": 7.000208854675293,
      "learning_rate": 0.00047215016023195483,
      "loss": 0.7971,
      "step": 3510
    },
    {
      "epoch": 1.595184007269423,
      "grad_norm": 6.443619251251221,
      "learning_rate": 0.00047199755837021214,
      "loss": 0.7578,
      "step": 3511
    },
    {
      "epoch": 1.59563834620627,
      "grad_norm": 5.0594682693481445,
      "learning_rate": 0.0004718449565084694,
      "loss": 0.7622,
      "step": 3512
    },
    {
      "epoch": 1.5960926851431168,
      "grad_norm": 5.071300029754639,
      "learning_rate": 0.0004716923546467267,
      "loss": 0.4069,
      "step": 3513
    },
    {
      "epoch": 1.5965470240799635,
      "grad_norm": 5.839334964752197,
      "learning_rate": 0.000471539752784984,
      "loss": 0.7055,
      "step": 3514
    },
    {
      "epoch": 1.5970013630168105,
      "grad_norm": 6.995180606842041,
      "learning_rate": 0.0004713871509232412,
      "loss": 0.5836,
      "step": 3515
    },
    {
      "epoch": 1.5974557019536575,
      "grad_norm": 5.181813716888428,
      "learning_rate": 0.00047123454906149853,
      "loss": 0.466,
      "step": 3516
    },
    {
      "epoch": 1.5979100408905045,
      "grad_norm": 4.2119832038879395,
      "learning_rate": 0.00047108194719975584,
      "loss": 0.3988,
      "step": 3517
    },
    {
      "epoch": 1.5983643798273512,
      "grad_norm": 3.9186108112335205,
      "learning_rate": 0.00047092934533801315,
      "loss": 0.3839,
      "step": 3518
    },
    {
      "epoch": 1.598818718764198,
      "grad_norm": 4.80769157409668,
      "learning_rate": 0.0004707767434762704,
      "loss": 0.4131,
      "step": 3519
    },
    {
      "epoch": 1.599273057701045,
      "grad_norm": 7.625176906585693,
      "learning_rate": 0.0004706241416145277,
      "loss": 0.5133,
      "step": 3520
    },
    {
      "epoch": 1.599727396637892,
      "grad_norm": 5.7933349609375,
      "learning_rate": 0.00047047153975278503,
      "loss": 0.6326,
      "step": 3521
    },
    {
      "epoch": 1.6001817355747388,
      "grad_norm": 1.3755056858062744,
      "learning_rate": 0.0004703189378910423,
      "loss": 0.0546,
      "step": 3522
    },
    {
      "epoch": 1.6006360745115855,
      "grad_norm": 4.4568986892700195,
      "learning_rate": 0.0004701663360292996,
      "loss": 0.4058,
      "step": 3523
    },
    {
      "epoch": 1.6010904134484325,
      "grad_norm": 5.890182018280029,
      "learning_rate": 0.00047001373416755685,
      "loss": 0.3485,
      "step": 3524
    },
    {
      "epoch": 1.6015447523852795,
      "grad_norm": 7.462176322937012,
      "learning_rate": 0.0004698611323058141,
      "loss": 0.7704,
      "step": 3525
    },
    {
      "epoch": 1.6019990913221263,
      "grad_norm": 3.8706185817718506,
      "learning_rate": 0.0004697085304440714,
      "loss": 0.2795,
      "step": 3526
    },
    {
      "epoch": 1.6024534302589732,
      "grad_norm": 9.436980247497559,
      "learning_rate": 0.00046955592858232873,
      "loss": 1.5469,
      "step": 3527
    },
    {
      "epoch": 1.60290776919582,
      "grad_norm": 4.125522613525391,
      "learning_rate": 0.000469403326720586,
      "loss": 0.5873,
      "step": 3528
    },
    {
      "epoch": 1.603362108132667,
      "grad_norm": 8.062796592712402,
      "learning_rate": 0.0004692507248588433,
      "loss": 1.0611,
      "step": 3529
    },
    {
      "epoch": 1.603816447069514,
      "grad_norm": 2.253589391708374,
      "learning_rate": 0.0004690981229971006,
      "loss": 0.3341,
      "step": 3530
    },
    {
      "epoch": 1.6042707860063607,
      "grad_norm": 5.757919788360596,
      "learning_rate": 0.00046894552113535787,
      "loss": 0.5633,
      "step": 3531
    },
    {
      "epoch": 1.6047251249432075,
      "grad_norm": 4.268861770629883,
      "learning_rate": 0.0004687929192736152,
      "loss": 0.5061,
      "step": 3532
    },
    {
      "epoch": 1.6051794638800545,
      "grad_norm": 8.153942108154297,
      "learning_rate": 0.00046864031741187243,
      "loss": 1.049,
      "step": 3533
    },
    {
      "epoch": 1.6056338028169015,
      "grad_norm": 5.687243461608887,
      "learning_rate": 0.0004684877155501297,
      "loss": 0.8554,
      "step": 3534
    },
    {
      "epoch": 1.6060881417537483,
      "grad_norm": 7.884224891662598,
      "learning_rate": 0.000468335113688387,
      "loss": 1.0225,
      "step": 3535
    },
    {
      "epoch": 1.606542480690595,
      "grad_norm": 2.9578700065612793,
      "learning_rate": 0.0004681825118266443,
      "loss": 0.4388,
      "step": 3536
    },
    {
      "epoch": 1.606996819627442,
      "grad_norm": 5.942111015319824,
      "learning_rate": 0.00046802990996490157,
      "loss": 0.6641,
      "step": 3537
    },
    {
      "epoch": 1.607451158564289,
      "grad_norm": 3.278136968612671,
      "learning_rate": 0.0004678773081031589,
      "loss": 0.2344,
      "step": 3538
    },
    {
      "epoch": 1.607905497501136,
      "grad_norm": 1.4251765012741089,
      "learning_rate": 0.0004677247062414162,
      "loss": 0.0465,
      "step": 3539
    },
    {
      "epoch": 1.6083598364379827,
      "grad_norm": 8.383456230163574,
      "learning_rate": 0.00046757210437967344,
      "loss": 0.6647,
      "step": 3540
    },
    {
      "epoch": 1.6088141753748295,
      "grad_norm": 4.049224376678467,
      "learning_rate": 0.00046741950251793076,
      "loss": 0.5863,
      "step": 3541
    },
    {
      "epoch": 1.6092685143116765,
      "grad_norm": 5.076940059661865,
      "learning_rate": 0.000467266900656188,
      "loss": 0.414,
      "step": 3542
    },
    {
      "epoch": 1.6097228532485235,
      "grad_norm": 4.204217910766602,
      "learning_rate": 0.00046711429879444527,
      "loss": 0.3032,
      "step": 3543
    },
    {
      "epoch": 1.6101771921853703,
      "grad_norm": 4.795693397521973,
      "learning_rate": 0.0004669616969327026,
      "loss": 0.1864,
      "step": 3544
    },
    {
      "epoch": 1.610631531122217,
      "grad_norm": 7.128261566162109,
      "learning_rate": 0.0004668090950709599,
      "loss": 1.1924,
      "step": 3545
    },
    {
      "epoch": 1.611085870059064,
      "grad_norm": 2.7444570064544678,
      "learning_rate": 0.00046665649320921715,
      "loss": 0.2111,
      "step": 3546
    },
    {
      "epoch": 1.611540208995911,
      "grad_norm": 2.3309483528137207,
      "learning_rate": 0.00046650389134747446,
      "loss": 0.1242,
      "step": 3547
    },
    {
      "epoch": 1.611994547932758,
      "grad_norm": 5.207153797149658,
      "learning_rate": 0.00046635128948573177,
      "loss": 1.0569,
      "step": 3548
    },
    {
      "epoch": 1.6124488868696047,
      "grad_norm": 11.83588981628418,
      "learning_rate": 0.000466198687623989,
      "loss": 0.5947,
      "step": 3549
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 5.392658710479736,
      "learning_rate": 0.00046604608576224633,
      "loss": 0.4294,
      "step": 3550
    },
    {
      "epoch": 1.6133575647432985,
      "grad_norm": 2.0779075622558594,
      "learning_rate": 0.00046589348390050364,
      "loss": 0.1411,
      "step": 3551
    },
    {
      "epoch": 1.6138119036801455,
      "grad_norm": 3.7424159049987793,
      "learning_rate": 0.00046574088203876085,
      "loss": 0.3305,
      "step": 3552
    },
    {
      "epoch": 1.6142662426169923,
      "grad_norm": 6.80912446975708,
      "learning_rate": 0.00046558828017701816,
      "loss": 1.451,
      "step": 3553
    },
    {
      "epoch": 1.614720581553839,
      "grad_norm": 11.975423812866211,
      "learning_rate": 0.00046543567831527547,
      "loss": 0.9443,
      "step": 3554
    },
    {
      "epoch": 1.615174920490686,
      "grad_norm": 8.581304550170898,
      "learning_rate": 0.0004652830764535327,
      "loss": 0.8459,
      "step": 3555
    },
    {
      "epoch": 1.615629259427533,
      "grad_norm": 4.990270137786865,
      "learning_rate": 0.00046513047459179003,
      "loss": 0.3404,
      "step": 3556
    },
    {
      "epoch": 1.61608359836438,
      "grad_norm": 6.861821174621582,
      "learning_rate": 0.00046497787273004735,
      "loss": 0.3636,
      "step": 3557
    },
    {
      "epoch": 1.6165379373012267,
      "grad_norm": 7.289459228515625,
      "learning_rate": 0.0004648252708683046,
      "loss": 1.1509,
      "step": 3558
    },
    {
      "epoch": 1.6169922762380735,
      "grad_norm": 4.714035511016846,
      "learning_rate": 0.0004646726690065619,
      "loss": 0.5637,
      "step": 3559
    },
    {
      "epoch": 1.6174466151749205,
      "grad_norm": 4.984046936035156,
      "learning_rate": 0.0004645200671448192,
      "loss": 0.2867,
      "step": 3560
    },
    {
      "epoch": 1.6179009541117675,
      "grad_norm": 5.227405071258545,
      "learning_rate": 0.0004643674652830764,
      "loss": 0.2767,
      "step": 3561
    },
    {
      "epoch": 1.6183552930486143,
      "grad_norm": 6.54646635055542,
      "learning_rate": 0.00046421486342133374,
      "loss": 1.1122,
      "step": 3562
    },
    {
      "epoch": 1.618809631985461,
      "grad_norm": 5.989549160003662,
      "learning_rate": 0.00046406226155959105,
      "loss": 0.4101,
      "step": 3563
    },
    {
      "epoch": 1.619263970922308,
      "grad_norm": 2.5033016204833984,
      "learning_rate": 0.0004639096596978483,
      "loss": 0.1905,
      "step": 3564
    },
    {
      "epoch": 1.619718309859155,
      "grad_norm": 5.0605692863464355,
      "learning_rate": 0.0004637570578361056,
      "loss": 0.2262,
      "step": 3565
    },
    {
      "epoch": 1.620172648796002,
      "grad_norm": 2.3508870601654053,
      "learning_rate": 0.0004636044559743629,
      "loss": 0.1691,
      "step": 3566
    },
    {
      "epoch": 1.6206269877328487,
      "grad_norm": 5.1557817459106445,
      "learning_rate": 0.0004634518541126202,
      "loss": 0.3718,
      "step": 3567
    },
    {
      "epoch": 1.6210813266696955,
      "grad_norm": 4.696253776550293,
      "learning_rate": 0.0004632992522508775,
      "loss": 0.4172,
      "step": 3568
    },
    {
      "epoch": 1.6215356656065425,
      "grad_norm": 5.2198004722595215,
      "learning_rate": 0.0004631466503891348,
      "loss": 0.2687,
      "step": 3569
    },
    {
      "epoch": 1.6219900045433895,
      "grad_norm": 5.743132591247559,
      "learning_rate": 0.000462994048527392,
      "loss": 0.4598,
      "step": 3570
    },
    {
      "epoch": 1.6224443434802363,
      "grad_norm": 2.580968141555786,
      "learning_rate": 0.0004628414466656493,
      "loss": 0.0947,
      "step": 3571
    },
    {
      "epoch": 1.622898682417083,
      "grad_norm": 5.186933517456055,
      "learning_rate": 0.0004626888448039066,
      "loss": 0.9881,
      "step": 3572
    },
    {
      "epoch": 1.62335302135393,
      "grad_norm": 3.394113540649414,
      "learning_rate": 0.0004625362429421639,
      "loss": 0.3824,
      "step": 3573
    },
    {
      "epoch": 1.623807360290777,
      "grad_norm": 5.151710033416748,
      "learning_rate": 0.0004623836410804212,
      "loss": 0.39,
      "step": 3574
    },
    {
      "epoch": 1.6242616992276238,
      "grad_norm": 3.3427231311798096,
      "learning_rate": 0.0004622310392186785,
      "loss": 0.1733,
      "step": 3575
    },
    {
      "epoch": 1.6247160381644707,
      "grad_norm": 3.6086771488189697,
      "learning_rate": 0.00046207843735693576,
      "loss": 0.1644,
      "step": 3576
    },
    {
      "epoch": 1.6251703771013175,
      "grad_norm": 4.210333347320557,
      "learning_rate": 0.00046192583549519307,
      "loss": 0.4808,
      "step": 3577
    },
    {
      "epoch": 1.6256247160381645,
      "grad_norm": 6.722044467926025,
      "learning_rate": 0.0004617732336334504,
      "loss": 0.3533,
      "step": 3578
    },
    {
      "epoch": 1.6260790549750115,
      "grad_norm": 2.3197524547576904,
      "learning_rate": 0.0004616206317717076,
      "loss": 0.1834,
      "step": 3579
    },
    {
      "epoch": 1.6265333939118582,
      "grad_norm": 5.570100784301758,
      "learning_rate": 0.0004614680299099649,
      "loss": 0.4498,
      "step": 3580
    },
    {
      "epoch": 1.626987732848705,
      "grad_norm": 9.594637870788574,
      "learning_rate": 0.0004613154280482222,
      "loss": 0.9486,
      "step": 3581
    },
    {
      "epoch": 1.627442071785552,
      "grad_norm": 5.844142913818359,
      "learning_rate": 0.00046116282618647946,
      "loss": 0.4734,
      "step": 3582
    },
    {
      "epoch": 1.627896410722399,
      "grad_norm": 5.74091100692749,
      "learning_rate": 0.00046101022432473677,
      "loss": 0.5898,
      "step": 3583
    },
    {
      "epoch": 1.6283507496592458,
      "grad_norm": 6.606034278869629,
      "learning_rate": 0.0004608576224629941,
      "loss": 0.7707,
      "step": 3584
    },
    {
      "epoch": 1.6288050885960925,
      "grad_norm": 4.619293212890625,
      "learning_rate": 0.00046070502060125134,
      "loss": 0.1577,
      "step": 3585
    },
    {
      "epoch": 1.6292594275329395,
      "grad_norm": 7.91903018951416,
      "learning_rate": 0.00046055241873950865,
      "loss": 0.5878,
      "step": 3586
    },
    {
      "epoch": 1.6297137664697865,
      "grad_norm": 3.1232211589813232,
      "learning_rate": 0.00046039981687776596,
      "loss": 0.1914,
      "step": 3587
    },
    {
      "epoch": 1.6301681054066335,
      "grad_norm": 2.762540102005005,
      "learning_rate": 0.00046024721501602316,
      "loss": 0.27,
      "step": 3588
    },
    {
      "epoch": 1.6306224443434802,
      "grad_norm": 3.55507755279541,
      "learning_rate": 0.00046009461315428047,
      "loss": 0.1853,
      "step": 3589
    },
    {
      "epoch": 1.631076783280327,
      "grad_norm": 5.597663879394531,
      "learning_rate": 0.0004599420112925378,
      "loss": 0.6628,
      "step": 3590
    },
    {
      "epoch": 1.631531122217174,
      "grad_norm": 7.744428634643555,
      "learning_rate": 0.00045978940943079504,
      "loss": 0.9208,
      "step": 3591
    },
    {
      "epoch": 1.631985461154021,
      "grad_norm": 5.4083170890808105,
      "learning_rate": 0.00045963680756905235,
      "loss": 0.7784,
      "step": 3592
    },
    {
      "epoch": 1.6324398000908678,
      "grad_norm": 8.5587739944458,
      "learning_rate": 0.00045948420570730966,
      "loss": 1.2185,
      "step": 3593
    },
    {
      "epoch": 1.6328941390277145,
      "grad_norm": 11.450891494750977,
      "learning_rate": 0.0004593316038455669,
      "loss": 0.2199,
      "step": 3594
    },
    {
      "epoch": 1.6333484779645615,
      "grad_norm": 6.423676490783691,
      "learning_rate": 0.0004591790019838242,
      "loss": 0.6439,
      "step": 3595
    },
    {
      "epoch": 1.6338028169014085,
      "grad_norm": 5.396106243133545,
      "learning_rate": 0.00045902640012208154,
      "loss": 0.4172,
      "step": 3596
    },
    {
      "epoch": 1.6342571558382555,
      "grad_norm": 6.026501178741455,
      "learning_rate": 0.00045887379826033874,
      "loss": 0.3003,
      "step": 3597
    },
    {
      "epoch": 1.6347114947751022,
      "grad_norm": 3.7356691360473633,
      "learning_rate": 0.00045872119639859605,
      "loss": 0.2435,
      "step": 3598
    },
    {
      "epoch": 1.635165833711949,
      "grad_norm": 4.466681957244873,
      "learning_rate": 0.00045856859453685336,
      "loss": 0.5224,
      "step": 3599
    },
    {
      "epoch": 1.635620172648796,
      "grad_norm": 2.5624334812164307,
      "learning_rate": 0.0004584159926751106,
      "loss": 0.3344,
      "step": 3600
    },
    {
      "epoch": 1.636074511585643,
      "grad_norm": 4.522095680236816,
      "learning_rate": 0.00045826339081336793,
      "loss": 0.5326,
      "step": 3601
    },
    {
      "epoch": 1.6365288505224898,
      "grad_norm": 7.396797180175781,
      "learning_rate": 0.00045811078895162524,
      "loss": 0.3867,
      "step": 3602
    },
    {
      "epoch": 1.6369831894593365,
      "grad_norm": 3.6955454349517822,
      "learning_rate": 0.0004579581870898825,
      "loss": 0.1861,
      "step": 3603
    },
    {
      "epoch": 1.6374375283961835,
      "grad_norm": 3.648094654083252,
      "learning_rate": 0.0004578055852281398,
      "loss": 0.4785,
      "step": 3604
    },
    {
      "epoch": 1.6378918673330305,
      "grad_norm": 4.776278018951416,
      "learning_rate": 0.0004576529833663971,
      "loss": 0.6401,
      "step": 3605
    },
    {
      "epoch": 1.6383462062698775,
      "grad_norm": 7.283934593200684,
      "learning_rate": 0.0004575003815046543,
      "loss": 0.4787,
      "step": 3606
    },
    {
      "epoch": 1.6388005452067242,
      "grad_norm": 4.756636619567871,
      "learning_rate": 0.00045734777964291163,
      "loss": 0.5056,
      "step": 3607
    },
    {
      "epoch": 1.639254884143571,
      "grad_norm": 4.924736976623535,
      "learning_rate": 0.00045719517778116894,
      "loss": 0.3134,
      "step": 3608
    },
    {
      "epoch": 1.639709223080418,
      "grad_norm": 2.693654775619507,
      "learning_rate": 0.0004570425759194262,
      "loss": 0.1379,
      "step": 3609
    },
    {
      "epoch": 1.640163562017265,
      "grad_norm": 3.518146276473999,
      "learning_rate": 0.0004568899740576835,
      "loss": 0.3585,
      "step": 3610
    },
    {
      "epoch": 1.6406179009541118,
      "grad_norm": 3.267899513244629,
      "learning_rate": 0.0004567373721959408,
      "loss": 0.2863,
      "step": 3611
    },
    {
      "epoch": 1.6410722398909585,
      "grad_norm": 4.889042854309082,
      "learning_rate": 0.0004565847703341981,
      "loss": 0.3186,
      "step": 3612
    },
    {
      "epoch": 1.6415265788278055,
      "grad_norm": 7.155429363250732,
      "learning_rate": 0.0004564321684724554,
      "loss": 0.8999,
      "step": 3613
    },
    {
      "epoch": 1.6419809177646525,
      "grad_norm": 4.313704967498779,
      "learning_rate": 0.0004562795666107127,
      "loss": 0.5316,
      "step": 3614
    },
    {
      "epoch": 1.6424352567014995,
      "grad_norm": 4.716797351837158,
      "learning_rate": 0.0004561269647489699,
      "loss": 0.2712,
      "step": 3615
    },
    {
      "epoch": 1.6428895956383462,
      "grad_norm": 4.910736560821533,
      "learning_rate": 0.0004559743628872272,
      "loss": 0.4259,
      "step": 3616
    },
    {
      "epoch": 1.643343934575193,
      "grad_norm": 9.503267288208008,
      "learning_rate": 0.0004558217610254845,
      "loss": 0.5978,
      "step": 3617
    },
    {
      "epoch": 1.64379827351204,
      "grad_norm": 5.182280540466309,
      "learning_rate": 0.0004556691591637418,
      "loss": 0.3284,
      "step": 3618
    },
    {
      "epoch": 1.644252612448887,
      "grad_norm": 4.3339362144470215,
      "learning_rate": 0.0004555165573019991,
      "loss": 0.6587,
      "step": 3619
    },
    {
      "epoch": 1.6447069513857338,
      "grad_norm": 5.564652442932129,
      "learning_rate": 0.0004553639554402564,
      "loss": 0.5669,
      "step": 3620
    },
    {
      "epoch": 1.6451612903225805,
      "grad_norm": 3.301961660385132,
      "learning_rate": 0.00045521135357851365,
      "loss": 0.2129,
      "step": 3621
    },
    {
      "epoch": 1.6456156292594275,
      "grad_norm": 4.994252681732178,
      "learning_rate": 0.00045505875171677096,
      "loss": 0.4133,
      "step": 3622
    },
    {
      "epoch": 1.6460699681962745,
      "grad_norm": 5.885488033294678,
      "learning_rate": 0.0004549061498550283,
      "loss": 0.9142,
      "step": 3623
    },
    {
      "epoch": 1.6465243071331213,
      "grad_norm": 4.247874736785889,
      "learning_rate": 0.00045475354799328553,
      "loss": 0.3162,
      "step": 3624
    },
    {
      "epoch": 1.6469786460699682,
      "grad_norm": 5.881559371948242,
      "learning_rate": 0.0004546009461315428,
      "loss": 0.7508,
      "step": 3625
    },
    {
      "epoch": 1.647432985006815,
      "grad_norm": 4.853395938873291,
      "learning_rate": 0.0004544483442698001,
      "loss": 0.2827,
      "step": 3626
    },
    {
      "epoch": 1.647887323943662,
      "grad_norm": 6.367884635925293,
      "learning_rate": 0.00045429574240805735,
      "loss": 0.5146,
      "step": 3627
    },
    {
      "epoch": 1.648341662880509,
      "grad_norm": 5.353633403778076,
      "learning_rate": 0.00045414314054631466,
      "loss": 0.8437,
      "step": 3628
    },
    {
      "epoch": 1.6487960018173557,
      "grad_norm": 5.269170761108398,
      "learning_rate": 0.000453990538684572,
      "loss": 0.4352,
      "step": 3629
    },
    {
      "epoch": 1.6492503407542025,
      "grad_norm": 7.109840393066406,
      "learning_rate": 0.00045383793682282923,
      "loss": 0.8214,
      "step": 3630
    },
    {
      "epoch": 1.6497046796910495,
      "grad_norm": 6.201462745666504,
      "learning_rate": 0.00045368533496108654,
      "loss": 0.5164,
      "step": 3631
    },
    {
      "epoch": 1.6501590186278965,
      "grad_norm": 5.6896796226501465,
      "learning_rate": 0.00045353273309934385,
      "loss": 1.3056,
      "step": 3632
    },
    {
      "epoch": 1.6506133575647433,
      "grad_norm": 5.585601329803467,
      "learning_rate": 0.0004533801312376011,
      "loss": 0.5029,
      "step": 3633
    },
    {
      "epoch": 1.65106769650159,
      "grad_norm": 0.8385468125343323,
      "learning_rate": 0.00045322752937585836,
      "loss": 0.0449,
      "step": 3634
    },
    {
      "epoch": 1.651522035438437,
      "grad_norm": 3.7812795639038086,
      "learning_rate": 0.0004530749275141157,
      "loss": 0.3912,
      "step": 3635
    },
    {
      "epoch": 1.651976374375284,
      "grad_norm": 5.079878807067871,
      "learning_rate": 0.00045292232565237293,
      "loss": 0.3393,
      "step": 3636
    },
    {
      "epoch": 1.652430713312131,
      "grad_norm": 3.736659288406372,
      "learning_rate": 0.00045276972379063024,
      "loss": 0.232,
      "step": 3637
    },
    {
      "epoch": 1.6528850522489777,
      "grad_norm": 4.416874408721924,
      "learning_rate": 0.00045261712192888755,
      "loss": 0.638,
      "step": 3638
    },
    {
      "epoch": 1.6533393911858245,
      "grad_norm": 5.784603118896484,
      "learning_rate": 0.0004524645200671448,
      "loss": 1.047,
      "step": 3639
    },
    {
      "epoch": 1.6537937301226715,
      "grad_norm": 5.805508613586426,
      "learning_rate": 0.0004523119182054021,
      "loss": 0.5057,
      "step": 3640
    },
    {
      "epoch": 1.6542480690595185,
      "grad_norm": 7.2954583168029785,
      "learning_rate": 0.00045215931634365943,
      "loss": 1.3392,
      "step": 3641
    },
    {
      "epoch": 1.6547024079963653,
      "grad_norm": 3.8703765869140625,
      "learning_rate": 0.00045200671448191674,
      "loss": 0.3609,
      "step": 3642
    },
    {
      "epoch": 1.655156746933212,
      "grad_norm": 5.874200820922852,
      "learning_rate": 0.00045185411262017394,
      "loss": 0.9271,
      "step": 3643
    },
    {
      "epoch": 1.655611085870059,
      "grad_norm": 8.426810264587402,
      "learning_rate": 0.00045170151075843125,
      "loss": 1.0424,
      "step": 3644
    },
    {
      "epoch": 1.656065424806906,
      "grad_norm": 2.781956434249878,
      "learning_rate": 0.00045154890889668856,
      "loss": 0.0955,
      "step": 3645
    },
    {
      "epoch": 1.656519763743753,
      "grad_norm": 5.123064041137695,
      "learning_rate": 0.0004513963070349458,
      "loss": 0.7778,
      "step": 3646
    },
    {
      "epoch": 1.6569741026805997,
      "grad_norm": 9.14319896697998,
      "learning_rate": 0.00045124370517320313,
      "loss": 0.99,
      "step": 3647
    },
    {
      "epoch": 1.6574284416174465,
      "grad_norm": 6.318092346191406,
      "learning_rate": 0.00045109110331146044,
      "loss": 1.101,
      "step": 3648
    },
    {
      "epoch": 1.6578827805542935,
      "grad_norm": 6.13385534286499,
      "learning_rate": 0.0004509385014497177,
      "loss": 0.7588,
      "step": 3649
    },
    {
      "epoch": 1.6583371194911405,
      "grad_norm": 1.6796118021011353,
      "learning_rate": 0.000450785899587975,
      "loss": 0.1701,
      "step": 3650
    },
    {
      "epoch": 1.6587914584279873,
      "grad_norm": 4.440943717956543,
      "learning_rate": 0.0004506332977262323,
      "loss": 0.4358,
      "step": 3651
    },
    {
      "epoch": 1.659245797364834,
      "grad_norm": 7.3138957023620605,
      "learning_rate": 0.0004504806958644895,
      "loss": 0.852,
      "step": 3652
    },
    {
      "epoch": 1.659700136301681,
      "grad_norm": 4.620472431182861,
      "learning_rate": 0.00045032809400274683,
      "loss": 0.4686,
      "step": 3653
    },
    {
      "epoch": 1.660154475238528,
      "grad_norm": 6.281775951385498,
      "learning_rate": 0.00045017549214100414,
      "loss": 0.5213,
      "step": 3654
    },
    {
      "epoch": 1.660608814175375,
      "grad_norm": 2.4656145572662354,
      "learning_rate": 0.0004500228902792614,
      "loss": 0.3427,
      "step": 3655
    },
    {
      "epoch": 1.6610631531122217,
      "grad_norm": 2.2997536659240723,
      "learning_rate": 0.0004498702884175187,
      "loss": 0.1426,
      "step": 3656
    },
    {
      "epoch": 1.6615174920490685,
      "grad_norm": 3.686108350753784,
      "learning_rate": 0.000449717686555776,
      "loss": 0.2836,
      "step": 3657
    },
    {
      "epoch": 1.6619718309859155,
      "grad_norm": 5.4090399742126465,
      "learning_rate": 0.0004495650846940333,
      "loss": 0.6397,
      "step": 3658
    },
    {
      "epoch": 1.6624261699227625,
      "grad_norm": 5.70383882522583,
      "learning_rate": 0.0004494124828322906,
      "loss": 0.3787,
      "step": 3659
    },
    {
      "epoch": 1.6628805088596093,
      "grad_norm": 9.434255599975586,
      "learning_rate": 0.0004492598809705479,
      "loss": 1.8139,
      "step": 3660
    },
    {
      "epoch": 1.663334847796456,
      "grad_norm": 4.26970100402832,
      "learning_rate": 0.0004491072791088051,
      "loss": 0.1879,
      "step": 3661
    },
    {
      "epoch": 1.663789186733303,
      "grad_norm": 3.2988619804382324,
      "learning_rate": 0.0004489546772470624,
      "loss": 0.3681,
      "step": 3662
    },
    {
      "epoch": 1.66424352567015,
      "grad_norm": 4.45618200302124,
      "learning_rate": 0.0004488020753853197,
      "loss": 0.4171,
      "step": 3663
    },
    {
      "epoch": 1.664697864606997,
      "grad_norm": 5.360098361968994,
      "learning_rate": 0.000448649473523577,
      "loss": 0.6162,
      "step": 3664
    },
    {
      "epoch": 1.6651522035438437,
      "grad_norm": 4.1906023025512695,
      "learning_rate": 0.0004484968716618343,
      "loss": 0.2108,
      "step": 3665
    },
    {
      "epoch": 1.6656065424806905,
      "grad_norm": 3.699761390686035,
      "learning_rate": 0.0004483442698000916,
      "loss": 0.3792,
      "step": 3666
    },
    {
      "epoch": 1.6660608814175375,
      "grad_norm": 3.8715498447418213,
      "learning_rate": 0.00044819166793834886,
      "loss": 0.5868,
      "step": 3667
    },
    {
      "epoch": 1.6665152203543845,
      "grad_norm": 3.2181508541107178,
      "learning_rate": 0.00044803906607660617,
      "loss": 0.4513,
      "step": 3668
    },
    {
      "epoch": 1.6669695592912313,
      "grad_norm": 6.226287364959717,
      "learning_rate": 0.0004478864642148635,
      "loss": 0.8799,
      "step": 3669
    },
    {
      "epoch": 1.667423898228078,
      "grad_norm": 2.823543071746826,
      "learning_rate": 0.0004477338623531207,
      "loss": 0.3497,
      "step": 3670
    },
    {
      "epoch": 1.667878237164925,
      "grad_norm": 4.0095648765563965,
      "learning_rate": 0.000447581260491378,
      "loss": 0.2256,
      "step": 3671
    },
    {
      "epoch": 1.668332576101772,
      "grad_norm": 4.925972938537598,
      "learning_rate": 0.0004474286586296353,
      "loss": 0.667,
      "step": 3672
    },
    {
      "epoch": 1.6687869150386188,
      "grad_norm": 6.612345218658447,
      "learning_rate": 0.00044727605676789256,
      "loss": 0.8214,
      "step": 3673
    },
    {
      "epoch": 1.6692412539754657,
      "grad_norm": 9.321198463439941,
      "learning_rate": 0.00044712345490614987,
      "loss": 0.7724,
      "step": 3674
    },
    {
      "epoch": 1.6696955929123125,
      "grad_norm": 2.362813949584961,
      "learning_rate": 0.0004469708530444072,
      "loss": 0.1021,
      "step": 3675
    },
    {
      "epoch": 1.6701499318491595,
      "grad_norm": 2.4969916343688965,
      "learning_rate": 0.00044681825118266443,
      "loss": 0.1712,
      "step": 3676
    },
    {
      "epoch": 1.6706042707860065,
      "grad_norm": 2.8831684589385986,
      "learning_rate": 0.00044666564932092174,
      "loss": 0.2791,
      "step": 3677
    },
    {
      "epoch": 1.6710586097228532,
      "grad_norm": 2.9700188636779785,
      "learning_rate": 0.00044651304745917906,
      "loss": 0.2172,
      "step": 3678
    },
    {
      "epoch": 1.6715129486597,
      "grad_norm": 3.7778098583221436,
      "learning_rate": 0.00044636044559743626,
      "loss": 0.2422,
      "step": 3679
    },
    {
      "epoch": 1.671967287596547,
      "grad_norm": 6.959415435791016,
      "learning_rate": 0.00044620784373569357,
      "loss": 0.648,
      "step": 3680
    },
    {
      "epoch": 1.672421626533394,
      "grad_norm": 3.1559393405914307,
      "learning_rate": 0.0004460552418739509,
      "loss": 0.3146,
      "step": 3681
    },
    {
      "epoch": 1.6728759654702408,
      "grad_norm": 4.920531749725342,
      "learning_rate": 0.00044590264001220813,
      "loss": 0.42,
      "step": 3682
    },
    {
      "epoch": 1.6733303044070875,
      "grad_norm": 4.311234951019287,
      "learning_rate": 0.00044575003815046545,
      "loss": 0.51,
      "step": 3683
    },
    {
      "epoch": 1.6737846433439345,
      "grad_norm": 5.095859050750732,
      "learning_rate": 0.00044559743628872276,
      "loss": 0.2899,
      "step": 3684
    },
    {
      "epoch": 1.6742389822807815,
      "grad_norm": 4.512391567230225,
      "learning_rate": 0.00044544483442698,
      "loss": 0.3875,
      "step": 3685
    },
    {
      "epoch": 1.6746933212176285,
      "grad_norm": 6.589087009429932,
      "learning_rate": 0.0004452922325652373,
      "loss": 0.6694,
      "step": 3686
    },
    {
      "epoch": 1.6751476601544752,
      "grad_norm": 4.397953510284424,
      "learning_rate": 0.00044513963070349463,
      "loss": 0.4276,
      "step": 3687
    },
    {
      "epoch": 1.675601999091322,
      "grad_norm": 5.383142471313477,
      "learning_rate": 0.0004449870288417519,
      "loss": 0.4771,
      "step": 3688
    },
    {
      "epoch": 1.676056338028169,
      "grad_norm": 5.744069576263428,
      "learning_rate": 0.00044483442698000915,
      "loss": 0.3137,
      "step": 3689
    },
    {
      "epoch": 1.676510676965016,
      "grad_norm": 8.404120445251465,
      "learning_rate": 0.00044468182511826646,
      "loss": 1.4446,
      "step": 3690
    },
    {
      "epoch": 1.6769650159018628,
      "grad_norm": 2.1909890174865723,
      "learning_rate": 0.0004445292232565237,
      "loss": 0.1586,
      "step": 3691
    },
    {
      "epoch": 1.6774193548387095,
      "grad_norm": 6.008817672729492,
      "learning_rate": 0.000444376621394781,
      "loss": 0.7492,
      "step": 3692
    },
    {
      "epoch": 1.6778736937755565,
      "grad_norm": 4.850879669189453,
      "learning_rate": 0.00044422401953303833,
      "loss": 0.6536,
      "step": 3693
    },
    {
      "epoch": 1.6783280327124035,
      "grad_norm": 12.340755462646484,
      "learning_rate": 0.0004440714176712956,
      "loss": 0.6701,
      "step": 3694
    },
    {
      "epoch": 1.6787823716492505,
      "grad_norm": 2.7206485271453857,
      "learning_rate": 0.0004439188158095529,
      "loss": 0.1242,
      "step": 3695
    },
    {
      "epoch": 1.6792367105860972,
      "grad_norm": 6.021320819854736,
      "learning_rate": 0.0004437662139478102,
      "loss": 0.7647,
      "step": 3696
    },
    {
      "epoch": 1.679691049522944,
      "grad_norm": 2.5549800395965576,
      "learning_rate": 0.00044361361208606747,
      "loss": 0.1076,
      "step": 3697
    },
    {
      "epoch": 1.680145388459791,
      "grad_norm": 5.910832405090332,
      "learning_rate": 0.0004434610102243247,
      "loss": 0.4898,
      "step": 3698
    },
    {
      "epoch": 1.680599727396638,
      "grad_norm": 4.516782760620117,
      "learning_rate": 0.00044330840836258204,
      "loss": 0.4277,
      "step": 3699
    },
    {
      "epoch": 1.6810540663334848,
      "grad_norm": 4.512203216552734,
      "learning_rate": 0.0004431558065008393,
      "loss": 0.23,
      "step": 3700
    },
    {
      "epoch": 1.6815084052703315,
      "grad_norm": 4.030309677124023,
      "learning_rate": 0.0004430032046390966,
      "loss": 0.2357,
      "step": 3701
    },
    {
      "epoch": 1.6819627442071785,
      "grad_norm": 4.992688179016113,
      "learning_rate": 0.0004428506027773539,
      "loss": 0.4105,
      "step": 3702
    },
    {
      "epoch": 1.6824170831440255,
      "grad_norm": 8.044405937194824,
      "learning_rate": 0.00044269800091561117,
      "loss": 0.9529,
      "step": 3703
    },
    {
      "epoch": 1.6828714220808725,
      "grad_norm": 5.719335079193115,
      "learning_rate": 0.0004425453990538685,
      "loss": 0.6947,
      "step": 3704
    },
    {
      "epoch": 1.6833257610177192,
      "grad_norm": 4.7722249031066895,
      "learning_rate": 0.0004423927971921258,
      "loss": 0.3315,
      "step": 3705
    },
    {
      "epoch": 1.683780099954566,
      "grad_norm": 1.4404977560043335,
      "learning_rate": 0.00044224019533038305,
      "loss": 0.1482,
      "step": 3706
    },
    {
      "epoch": 1.684234438891413,
      "grad_norm": 6.558968544006348,
      "learning_rate": 0.0004420875934686403,
      "loss": 0.7375,
      "step": 3707
    },
    {
      "epoch": 1.68468877782826,
      "grad_norm": 4.674490451812744,
      "learning_rate": 0.0004419349916068976,
      "loss": 0.779,
      "step": 3708
    },
    {
      "epoch": 1.6851431167651068,
      "grad_norm": 4.959964275360107,
      "learning_rate": 0.00044178238974515487,
      "loss": 0.2809,
      "step": 3709
    },
    {
      "epoch": 1.6855974557019535,
      "grad_norm": 7.067073345184326,
      "learning_rate": 0.0004416297878834122,
      "loss": 0.5473,
      "step": 3710
    },
    {
      "epoch": 1.6860517946388005,
      "grad_norm": 7.169820785522461,
      "learning_rate": 0.0004414771860216695,
      "loss": 1.1184,
      "step": 3711
    },
    {
      "epoch": 1.6865061335756475,
      "grad_norm": 6.080965995788574,
      "learning_rate": 0.00044132458415992675,
      "loss": 0.7715,
      "step": 3712
    },
    {
      "epoch": 1.6869604725124945,
      "grad_norm": 2.9868533611297607,
      "learning_rate": 0.00044117198229818406,
      "loss": 0.3168,
      "step": 3713
    },
    {
      "epoch": 1.6874148114493412,
      "grad_norm": 2.23679780960083,
      "learning_rate": 0.00044101938043644137,
      "loss": 0.161,
      "step": 3714
    },
    {
      "epoch": 1.687869150386188,
      "grad_norm": 4.9787445068359375,
      "learning_rate": 0.0004408667785746986,
      "loss": 0.4431,
      "step": 3715
    },
    {
      "epoch": 1.688323489323035,
      "grad_norm": 6.895503520965576,
      "learning_rate": 0.0004407141767129559,
      "loss": 0.5717,
      "step": 3716
    },
    {
      "epoch": 1.688777828259882,
      "grad_norm": 4.808368682861328,
      "learning_rate": 0.0004405615748512132,
      "loss": 0.2623,
      "step": 3717
    },
    {
      "epoch": 1.6892321671967288,
      "grad_norm": 6.253732204437256,
      "learning_rate": 0.00044040897298947045,
      "loss": 0.5164,
      "step": 3718
    },
    {
      "epoch": 1.6896865061335755,
      "grad_norm": 8.97414493560791,
      "learning_rate": 0.00044025637112772776,
      "loss": 1.1564,
      "step": 3719
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 5.505107402801514,
      "learning_rate": 0.00044010376926598507,
      "loss": 0.5182,
      "step": 3720
    },
    {
      "epoch": 1.6905951840072695,
      "grad_norm": 3.685059070587158,
      "learning_rate": 0.0004399511674042423,
      "loss": 0.1426,
      "step": 3721
    },
    {
      "epoch": 1.6910495229441163,
      "grad_norm": 8.774518013000488,
      "learning_rate": 0.00043979856554249964,
      "loss": 1.1022,
      "step": 3722
    },
    {
      "epoch": 1.6915038618809632,
      "grad_norm": 4.900638103485107,
      "learning_rate": 0.00043964596368075695,
      "loss": 0.3359,
      "step": 3723
    },
    {
      "epoch": 1.69195820081781,
      "grad_norm": 2.959555149078369,
      "learning_rate": 0.0004394933618190142,
      "loss": 0.1143,
      "step": 3724
    },
    {
      "epoch": 1.692412539754657,
      "grad_norm": 2.5057168006896973,
      "learning_rate": 0.00043934075995727146,
      "loss": 0.2057,
      "step": 3725
    },
    {
      "epoch": 1.692866878691504,
      "grad_norm": 3.926518201828003,
      "learning_rate": 0.00043918815809552877,
      "loss": 0.3188,
      "step": 3726
    },
    {
      "epoch": 1.6933212176283507,
      "grad_norm": 6.920066833496094,
      "learning_rate": 0.00043903555623378603,
      "loss": 1.0715,
      "step": 3727
    },
    {
      "epoch": 1.6937755565651975,
      "grad_norm": 3.2779417037963867,
      "learning_rate": 0.00043888295437204334,
      "loss": 0.2523,
      "step": 3728
    },
    {
      "epoch": 1.6942298955020445,
      "grad_norm": 5.739150524139404,
      "learning_rate": 0.00043873035251030065,
      "loss": 0.6657,
      "step": 3729
    },
    {
      "epoch": 1.6946842344388915,
      "grad_norm": 5.769140243530273,
      "learning_rate": 0.0004385777506485579,
      "loss": 0.8103,
      "step": 3730
    },
    {
      "epoch": 1.6951385733757383,
      "grad_norm": 3.8125243186950684,
      "learning_rate": 0.0004384251487868152,
      "loss": 0.1397,
      "step": 3731
    },
    {
      "epoch": 1.6955929123125852,
      "grad_norm": 6.692543983459473,
      "learning_rate": 0.0004382725469250725,
      "loss": 0.831,
      "step": 3732
    },
    {
      "epoch": 1.696047251249432,
      "grad_norm": 8.034684181213379,
      "learning_rate": 0.0004381199450633298,
      "loss": 0.8638,
      "step": 3733
    },
    {
      "epoch": 1.696501590186279,
      "grad_norm": 10.376799583435059,
      "learning_rate": 0.00043796734320158704,
      "loss": 0.6047,
      "step": 3734
    },
    {
      "epoch": 1.696955929123126,
      "grad_norm": 5.179072856903076,
      "learning_rate": 0.00043781474133984435,
      "loss": 0.422,
      "step": 3735
    },
    {
      "epoch": 1.6974102680599727,
      "grad_norm": 8.36843204498291,
      "learning_rate": 0.0004376621394781016,
      "loss": 0.7245,
      "step": 3736
    },
    {
      "epoch": 1.6978646069968195,
      "grad_norm": 4.199206352233887,
      "learning_rate": 0.0004375095376163589,
      "loss": 0.3634,
      "step": 3737
    },
    {
      "epoch": 1.6983189459336665,
      "grad_norm": 3.575866937637329,
      "learning_rate": 0.00043735693575461623,
      "loss": 0.2748,
      "step": 3738
    },
    {
      "epoch": 1.6987732848705135,
      "grad_norm": 3.971367597579956,
      "learning_rate": 0.0004372043338928735,
      "loss": 0.564,
      "step": 3739
    },
    {
      "epoch": 1.6992276238073603,
      "grad_norm": 1.1337528228759766,
      "learning_rate": 0.0004370517320311308,
      "loss": 0.0442,
      "step": 3740
    },
    {
      "epoch": 1.699681962744207,
      "grad_norm": 5.102257251739502,
      "learning_rate": 0.0004368991301693881,
      "loss": 0.2642,
      "step": 3741
    },
    {
      "epoch": 1.700136301681054,
      "grad_norm": 2.990367889404297,
      "learning_rate": 0.00043674652830764536,
      "loss": 0.193,
      "step": 3742
    },
    {
      "epoch": 1.700590640617901,
      "grad_norm": 7.915017127990723,
      "learning_rate": 0.0004365939264459026,
      "loss": 0.6337,
      "step": 3743
    },
    {
      "epoch": 1.701044979554748,
      "grad_norm": 4.57161808013916,
      "learning_rate": 0.00043644132458415993,
      "loss": 0.1712,
      "step": 3744
    },
    {
      "epoch": 1.7014993184915947,
      "grad_norm": 6.042410850524902,
      "learning_rate": 0.0004362887227224172,
      "loss": 0.6927,
      "step": 3745
    },
    {
      "epoch": 1.7019536574284415,
      "grad_norm": 13.385578155517578,
      "learning_rate": 0.0004361361208606745,
      "loss": 0.9097,
      "step": 3746
    },
    {
      "epoch": 1.7024079963652885,
      "grad_norm": 5.5472893714904785,
      "learning_rate": 0.0004359835189989318,
      "loss": 0.5133,
      "step": 3747
    },
    {
      "epoch": 1.7028623353021355,
      "grad_norm": 4.730813503265381,
      "learning_rate": 0.00043583091713718906,
      "loss": 0.5375,
      "step": 3748
    },
    {
      "epoch": 1.7033166742389823,
      "grad_norm": 5.692793369293213,
      "learning_rate": 0.0004356783152754464,
      "loss": 1.4125,
      "step": 3749
    },
    {
      "epoch": 1.703771013175829,
      "grad_norm": 3.6744391918182373,
      "learning_rate": 0.0004355257134137037,
      "loss": 0.4296,
      "step": 3750
    },
    {
      "epoch": 1.704225352112676,
      "grad_norm": 2.225203037261963,
      "learning_rate": 0.00043537311155196094,
      "loss": 0.13,
      "step": 3751
    },
    {
      "epoch": 1.704679691049523,
      "grad_norm": 3.5485520362854004,
      "learning_rate": 0.0004352205096902182,
      "loss": 0.1709,
      "step": 3752
    },
    {
      "epoch": 1.70513402998637,
      "grad_norm": 9.361161231994629,
      "learning_rate": 0.0004350679078284755,
      "loss": 1.1748,
      "step": 3753
    },
    {
      "epoch": 1.7055883689232167,
      "grad_norm": 5.72972297668457,
      "learning_rate": 0.00043491530596673276,
      "loss": 0.7277,
      "step": 3754
    },
    {
      "epoch": 1.7060427078600635,
      "grad_norm": 1.8491822481155396,
      "learning_rate": 0.0004347627041049901,
      "loss": 0.0503,
      "step": 3755
    },
    {
      "epoch": 1.7064970467969105,
      "grad_norm": 5.59050178527832,
      "learning_rate": 0.0004346101022432474,
      "loss": 0.6042,
      "step": 3756
    },
    {
      "epoch": 1.7069513857337575,
      "grad_norm": 4.637641906738281,
      "learning_rate": 0.00043445750038150464,
      "loss": 0.3037,
      "step": 3757
    },
    {
      "epoch": 1.7074057246706043,
      "grad_norm": 3.5705535411834717,
      "learning_rate": 0.00043430489851976195,
      "loss": 0.3124,
      "step": 3758
    },
    {
      "epoch": 1.707860063607451,
      "grad_norm": 5.0630364418029785,
      "learning_rate": 0.00043415229665801926,
      "loss": 0.9389,
      "step": 3759
    },
    {
      "epoch": 1.708314402544298,
      "grad_norm": 2.7103331089019775,
      "learning_rate": 0.0004339996947962765,
      "loss": 0.1806,
      "step": 3760
    },
    {
      "epoch": 1.708768741481145,
      "grad_norm": 6.801144599914551,
      "learning_rate": 0.00043384709293453383,
      "loss": 0.5731,
      "step": 3761
    },
    {
      "epoch": 1.709223080417992,
      "grad_norm": 5.581742763519287,
      "learning_rate": 0.0004336944910727911,
      "loss": 0.3109,
      "step": 3762
    },
    {
      "epoch": 1.7096774193548387,
      "grad_norm": 6.967057704925537,
      "learning_rate": 0.00043354188921104834,
      "loss": 0.8734,
      "step": 3763
    },
    {
      "epoch": 1.7101317582916855,
      "grad_norm": 3.8984498977661133,
      "learning_rate": 0.00043338928734930565,
      "loss": 0.6902,
      "step": 3764
    },
    {
      "epoch": 1.7105860972285325,
      "grad_norm": 8.320926666259766,
      "learning_rate": 0.00043323668548756296,
      "loss": 0.725,
      "step": 3765
    },
    {
      "epoch": 1.7110404361653795,
      "grad_norm": 2.574112892150879,
      "learning_rate": 0.0004330840836258202,
      "loss": 0.2988,
      "step": 3766
    },
    {
      "epoch": 1.7114947751022263,
      "grad_norm": 3.22892165184021,
      "learning_rate": 0.00043293148176407753,
      "loss": 0.2161,
      "step": 3767
    },
    {
      "epoch": 1.711949114039073,
      "grad_norm": 3.881164073944092,
      "learning_rate": 0.00043277887990233484,
      "loss": 0.1377,
      "step": 3768
    },
    {
      "epoch": 1.71240345297592,
      "grad_norm": 3.7238543033599854,
      "learning_rate": 0.00043262627804059215,
      "loss": 0.4084,
      "step": 3769
    },
    {
      "epoch": 1.712857791912767,
      "grad_norm": 2.4530482292175293,
      "learning_rate": 0.0004324736761788494,
      "loss": 0.1793,
      "step": 3770
    },
    {
      "epoch": 1.7133121308496138,
      "grad_norm": 6.75563907623291,
      "learning_rate": 0.00043232107431710666,
      "loss": 1.0033,
      "step": 3771
    },
    {
      "epoch": 1.7137664697864607,
      "grad_norm": 3.122735023498535,
      "learning_rate": 0.000432168472455364,
      "loss": 0.3995,
      "step": 3772
    },
    {
      "epoch": 1.7142208087233075,
      "grad_norm": 2.4394752979278564,
      "learning_rate": 0.00043201587059362123,
      "loss": 0.1895,
      "step": 3773
    },
    {
      "epoch": 1.7146751476601545,
      "grad_norm": 6.522896766662598,
      "learning_rate": 0.00043186326873187854,
      "loss": 0.4238,
      "step": 3774
    },
    {
      "epoch": 1.7151294865970015,
      "grad_norm": 5.455441474914551,
      "learning_rate": 0.00043171066687013585,
      "loss": 0.3907,
      "step": 3775
    },
    {
      "epoch": 1.7155838255338483,
      "grad_norm": 4.4265456199646,
      "learning_rate": 0.0004315580650083931,
      "loss": 0.452,
      "step": 3776
    },
    {
      "epoch": 1.716038164470695,
      "grad_norm": 11.032106399536133,
      "learning_rate": 0.0004314054631466504,
      "loss": 1.0451,
      "step": 3777
    },
    {
      "epoch": 1.716492503407542,
      "grad_norm": 4.386377811431885,
      "learning_rate": 0.00043125286128490773,
      "loss": 0.5277,
      "step": 3778
    },
    {
      "epoch": 1.716946842344389,
      "grad_norm": 8.361282348632812,
      "learning_rate": 0.000431100259423165,
      "loss": 0.6219,
      "step": 3779
    },
    {
      "epoch": 1.7174011812812358,
      "grad_norm": 4.227060317993164,
      "learning_rate": 0.00043094765756142224,
      "loss": 0.2487,
      "step": 3780
    },
    {
      "epoch": 1.7178555202180827,
      "grad_norm": 3.7053232192993164,
      "learning_rate": 0.00043079505569967955,
      "loss": 0.2938,
      "step": 3781
    },
    {
      "epoch": 1.7183098591549295,
      "grad_norm": 5.569791793823242,
      "learning_rate": 0.0004306424538379368,
      "loss": 0.975,
      "step": 3782
    },
    {
      "epoch": 1.7187641980917765,
      "grad_norm": 3.0123801231384277,
      "learning_rate": 0.0004304898519761941,
      "loss": 0.0942,
      "step": 3783
    },
    {
      "epoch": 1.7192185370286235,
      "grad_norm": 3.722104549407959,
      "learning_rate": 0.00043033725011445143,
      "loss": 0.2595,
      "step": 3784
    },
    {
      "epoch": 1.7196728759654702,
      "grad_norm": 5.417186260223389,
      "learning_rate": 0.0004301846482527087,
      "loss": 0.3406,
      "step": 3785
    },
    {
      "epoch": 1.720127214902317,
      "grad_norm": 1.0060988664627075,
      "learning_rate": 0.000430032046390966,
      "loss": 0.045,
      "step": 3786
    },
    {
      "epoch": 1.720581553839164,
      "grad_norm": 4.194462776184082,
      "learning_rate": 0.0004298794445292233,
      "loss": 0.2623,
      "step": 3787
    },
    {
      "epoch": 1.721035892776011,
      "grad_norm": 7.811380386352539,
      "learning_rate": 0.00042972684266748057,
      "loss": 0.7024,
      "step": 3788
    },
    {
      "epoch": 1.7214902317128578,
      "grad_norm": 2.447507381439209,
      "learning_rate": 0.0004295742408057378,
      "loss": 0.1099,
      "step": 3789
    },
    {
      "epoch": 1.7219445706497045,
      "grad_norm": 3.9242374897003174,
      "learning_rate": 0.00042942163894399513,
      "loss": 0.5665,
      "step": 3790
    },
    {
      "epoch": 1.7223989095865515,
      "grad_norm": 3.5553650856018066,
      "learning_rate": 0.0004292690370822524,
      "loss": 0.3815,
      "step": 3791
    },
    {
      "epoch": 1.7228532485233985,
      "grad_norm": 5.000940799713135,
      "learning_rate": 0.0004291164352205097,
      "loss": 0.5468,
      "step": 3792
    },
    {
      "epoch": 1.7233075874602455,
      "grad_norm": 2.1085469722747803,
      "learning_rate": 0.000428963833358767,
      "loss": 0.1386,
      "step": 3793
    },
    {
      "epoch": 1.7237619263970922,
      "grad_norm": 8.423434257507324,
      "learning_rate": 0.00042881123149702427,
      "loss": 0.9649,
      "step": 3794
    },
    {
      "epoch": 1.724216265333939,
      "grad_norm": 2.6941654682159424,
      "learning_rate": 0.0004286586296352816,
      "loss": 0.2305,
      "step": 3795
    },
    {
      "epoch": 1.724670604270786,
      "grad_norm": 4.673162937164307,
      "learning_rate": 0.0004285060277735389,
      "loss": 0.4744,
      "step": 3796
    },
    {
      "epoch": 1.725124943207633,
      "grad_norm": 4.649356842041016,
      "learning_rate": 0.00042835342591179614,
      "loss": 0.4198,
      "step": 3797
    },
    {
      "epoch": 1.7255792821444798,
      "grad_norm": 3.38871431350708,
      "learning_rate": 0.0004282008240500534,
      "loss": 0.2592,
      "step": 3798
    },
    {
      "epoch": 1.7260336210813265,
      "grad_norm": 4.6453752517700195,
      "learning_rate": 0.0004280482221883107,
      "loss": 0.3291,
      "step": 3799
    },
    {
      "epoch": 1.7264879600181735,
      "grad_norm": 5.207028388977051,
      "learning_rate": 0.00042789562032656797,
      "loss": 1.0467,
      "step": 3800
    },
    {
      "epoch": 1.7269422989550205,
      "grad_norm": 4.40202522277832,
      "learning_rate": 0.0004277430184648253,
      "loss": 0.2387,
      "step": 3801
    },
    {
      "epoch": 1.7273966378918675,
      "grad_norm": 3.4704113006591797,
      "learning_rate": 0.0004275904166030826,
      "loss": 0.2621,
      "step": 3802
    },
    {
      "epoch": 1.7278509768287142,
      "grad_norm": 5.322722911834717,
      "learning_rate": 0.00042743781474133984,
      "loss": 0.3868,
      "step": 3803
    },
    {
      "epoch": 1.728305315765561,
      "grad_norm": 7.561765670776367,
      "learning_rate": 0.00042728521287959716,
      "loss": 0.9708,
      "step": 3804
    },
    {
      "epoch": 1.728759654702408,
      "grad_norm": 6.947030544281006,
      "learning_rate": 0.00042713261101785447,
      "loss": 1.0672,
      "step": 3805
    },
    {
      "epoch": 1.729213993639255,
      "grad_norm": 5.3737592697143555,
      "learning_rate": 0.0004269800091561117,
      "loss": 0.4249,
      "step": 3806
    },
    {
      "epoch": 1.7296683325761018,
      "grad_norm": 3.9786291122436523,
      "learning_rate": 0.000426827407294369,
      "loss": 0.2416,
      "step": 3807
    },
    {
      "epoch": 1.7301226715129485,
      "grad_norm": 1.9017897844314575,
      "learning_rate": 0.0004266748054326263,
      "loss": 0.1303,
      "step": 3808
    },
    {
      "epoch": 1.7305770104497955,
      "grad_norm": 5.131058692932129,
      "learning_rate": 0.00042652220357088355,
      "loss": 0.2534,
      "step": 3809
    },
    {
      "epoch": 1.7310313493866425,
      "grad_norm": 4.692467212677002,
      "learning_rate": 0.00042636960170914086,
      "loss": 0.4664,
      "step": 3810
    },
    {
      "epoch": 1.7314856883234895,
      "grad_norm": 2.167208433151245,
      "learning_rate": 0.00042621699984739817,
      "loss": 0.201,
      "step": 3811
    },
    {
      "epoch": 1.7319400272603362,
      "grad_norm": 4.575955867767334,
      "learning_rate": 0.0004260643979856554,
      "loss": 0.3787,
      "step": 3812
    },
    {
      "epoch": 1.732394366197183,
      "grad_norm": 4.123232364654541,
      "learning_rate": 0.00042591179612391273,
      "loss": 0.3937,
      "step": 3813
    },
    {
      "epoch": 1.73284870513403,
      "grad_norm": 7.168089389801025,
      "learning_rate": 0.00042575919426217004,
      "loss": 0.5837,
      "step": 3814
    },
    {
      "epoch": 1.733303044070877,
      "grad_norm": 6.152133464813232,
      "learning_rate": 0.0004256065924004273,
      "loss": 0.3821,
      "step": 3815
    },
    {
      "epoch": 1.7337573830077238,
      "grad_norm": 2.7484352588653564,
      "learning_rate": 0.00042545399053868456,
      "loss": 0.1979,
      "step": 3816
    },
    {
      "epoch": 1.7342117219445705,
      "grad_norm": 4.19992208480835,
      "learning_rate": 0.00042530138867694187,
      "loss": 0.4838,
      "step": 3817
    },
    {
      "epoch": 1.7346660608814175,
      "grad_norm": 4.806252956390381,
      "learning_rate": 0.0004251487868151991,
      "loss": 0.517,
      "step": 3818
    },
    {
      "epoch": 1.7351203998182645,
      "grad_norm": 4.904183864593506,
      "learning_rate": 0.00042499618495345643,
      "loss": 0.857,
      "step": 3819
    },
    {
      "epoch": 1.7355747387551115,
      "grad_norm": 2.5113961696624756,
      "learning_rate": 0.00042484358309171375,
      "loss": 0.113,
      "step": 3820
    },
    {
      "epoch": 1.7360290776919582,
      "grad_norm": 3.7750847339630127,
      "learning_rate": 0.000424690981229971,
      "loss": 0.2834,
      "step": 3821
    },
    {
      "epoch": 1.736483416628805,
      "grad_norm": 7.635632038116455,
      "learning_rate": 0.0004245383793682283,
      "loss": 0.4451,
      "step": 3822
    },
    {
      "epoch": 1.736937755565652,
      "grad_norm": 4.795980453491211,
      "learning_rate": 0.0004243857775064856,
      "loss": 0.8666,
      "step": 3823
    },
    {
      "epoch": 1.737392094502499,
      "grad_norm": 1.5245980024337769,
      "learning_rate": 0.0004242331756447429,
      "loss": 0.0715,
      "step": 3824
    },
    {
      "epoch": 1.7378464334393458,
      "grad_norm": 8.308791160583496,
      "learning_rate": 0.00042408057378300014,
      "loss": 0.7358,
      "step": 3825
    },
    {
      "epoch": 1.7383007723761925,
      "grad_norm": 4.005039691925049,
      "learning_rate": 0.00042392797192125745,
      "loss": 0.5093,
      "step": 3826
    },
    {
      "epoch": 1.7387551113130395,
      "grad_norm": 5.807004451751709,
      "learning_rate": 0.0004237753700595147,
      "loss": 0.5823,
      "step": 3827
    },
    {
      "epoch": 1.7392094502498865,
      "grad_norm": 5.731955528259277,
      "learning_rate": 0.000423622768197772,
      "loss": 0.5877,
      "step": 3828
    },
    {
      "epoch": 1.7396637891867333,
      "grad_norm": 4.958755970001221,
      "learning_rate": 0.0004234701663360293,
      "loss": 0.7084,
      "step": 3829
    },
    {
      "epoch": 1.7401181281235802,
      "grad_norm": 2.360851526260376,
      "learning_rate": 0.0004233175644742866,
      "loss": 0.3602,
      "step": 3830
    },
    {
      "epoch": 1.740572467060427,
      "grad_norm": 5.645475387573242,
      "learning_rate": 0.0004231649626125439,
      "loss": 0.4326,
      "step": 3831
    },
    {
      "epoch": 1.741026805997274,
      "grad_norm": 2.332157850265503,
      "learning_rate": 0.0004230123607508012,
      "loss": 0.3886,
      "step": 3832
    },
    {
      "epoch": 1.741481144934121,
      "grad_norm": 6.092954158782959,
      "learning_rate": 0.00042285975888905846,
      "loss": 1.2281,
      "step": 3833
    },
    {
      "epoch": 1.7419354838709677,
      "grad_norm": 6.874691486358643,
      "learning_rate": 0.00042270715702731577,
      "loss": 0.7003,
      "step": 3834
    },
    {
      "epoch": 1.7423898228078145,
      "grad_norm": 3.8984427452087402,
      "learning_rate": 0.000422554555165573,
      "loss": 0.4312,
      "step": 3835
    },
    {
      "epoch": 1.7428441617446615,
      "grad_norm": 6.024796485900879,
      "learning_rate": 0.0004224019533038303,
      "loss": 0.5403,
      "step": 3836
    },
    {
      "epoch": 1.7432985006815085,
      "grad_norm": 5.5214738845825195,
      "learning_rate": 0.0004222493514420876,
      "loss": 0.3112,
      "step": 3837
    },
    {
      "epoch": 1.7437528396183553,
      "grad_norm": 4.492739200592041,
      "learning_rate": 0.0004220967495803449,
      "loss": 0.3898,
      "step": 3838
    },
    {
      "epoch": 1.744207178555202,
      "grad_norm": 1.5400813817977905,
      "learning_rate": 0.00042194414771860216,
      "loss": 0.0654,
      "step": 3839
    },
    {
      "epoch": 1.744661517492049,
      "grad_norm": 1.4539929628372192,
      "learning_rate": 0.00042179154585685947,
      "loss": 0.0661,
      "step": 3840
    },
    {
      "epoch": 1.745115856428896,
      "grad_norm": 1.2955063581466675,
      "learning_rate": 0.0004216389439951168,
      "loss": 0.0582,
      "step": 3841
    },
    {
      "epoch": 1.745570195365743,
      "grad_norm": 5.36268424987793,
      "learning_rate": 0.00042148634213337404,
      "loss": 0.5657,
      "step": 3842
    },
    {
      "epoch": 1.7460245343025897,
      "grad_norm": 6.3279900550842285,
      "learning_rate": 0.00042133374027163135,
      "loss": 0.8665,
      "step": 3843
    },
    {
      "epoch": 1.7464788732394365,
      "grad_norm": 4.5006422996521,
      "learning_rate": 0.0004211811384098886,
      "loss": 0.5741,
      "step": 3844
    },
    {
      "epoch": 1.7469332121762835,
      "grad_norm": 8.511054992675781,
      "learning_rate": 0.00042102853654814586,
      "loss": 1.1638,
      "step": 3845
    },
    {
      "epoch": 1.7473875511131305,
      "grad_norm": 8.883260726928711,
      "learning_rate": 0.00042087593468640317,
      "loss": 0.6732,
      "step": 3846
    },
    {
      "epoch": 1.7478418900499773,
      "grad_norm": 5.265704154968262,
      "learning_rate": 0.0004207233328246605,
      "loss": 0.8528,
      "step": 3847
    },
    {
      "epoch": 1.748296228986824,
      "grad_norm": 3.4793636798858643,
      "learning_rate": 0.00042057073096291774,
      "loss": 0.2826,
      "step": 3848
    },
    {
      "epoch": 1.748750567923671,
      "grad_norm": 7.810937881469727,
      "learning_rate": 0.00042041812910117505,
      "loss": 0.7092,
      "step": 3849
    },
    {
      "epoch": 1.749204906860518,
      "grad_norm": 4.794243335723877,
      "learning_rate": 0.00042026552723943236,
      "loss": 0.2244,
      "step": 3850
    },
    {
      "epoch": 1.749659245797365,
      "grad_norm": 5.061434745788574,
      "learning_rate": 0.0004201129253776896,
      "loss": 0.4977,
      "step": 3851
    },
    {
      "epoch": 1.7501135847342117,
      "grad_norm": 6.086993217468262,
      "learning_rate": 0.0004199603235159469,
      "loss": 0.8089,
      "step": 3852
    },
    {
      "epoch": 1.7505679236710585,
      "grad_norm": 5.148555755615234,
      "learning_rate": 0.0004198077216542042,
      "loss": 0.7113,
      "step": 3853
    },
    {
      "epoch": 1.7510222626079055,
      "grad_norm": 6.3727312088012695,
      "learning_rate": 0.00041965511979246144,
      "loss": 0.9178,
      "step": 3854
    },
    {
      "epoch": 1.7514766015447525,
      "grad_norm": 7.221481800079346,
      "learning_rate": 0.00041950251793071875,
      "loss": 0.9625,
      "step": 3855
    },
    {
      "epoch": 1.7519309404815993,
      "grad_norm": 5.386351108551025,
      "learning_rate": 0.00041934991606897606,
      "loss": 0.4723,
      "step": 3856
    },
    {
      "epoch": 1.752385279418446,
      "grad_norm": 6.528626441955566,
      "learning_rate": 0.0004191973142072333,
      "loss": 0.8991,
      "step": 3857
    },
    {
      "epoch": 1.752839618355293,
      "grad_norm": 5.286660194396973,
      "learning_rate": 0.0004190447123454906,
      "loss": 0.5412,
      "step": 3858
    },
    {
      "epoch": 1.75329395729214,
      "grad_norm": 6.9527668952941895,
      "learning_rate": 0.00041889211048374794,
      "loss": 1.1145,
      "step": 3859
    },
    {
      "epoch": 1.753748296228987,
      "grad_norm": 5.41701602935791,
      "learning_rate": 0.0004187395086220052,
      "loss": 0.5681,
      "step": 3860
    },
    {
      "epoch": 1.7542026351658337,
      "grad_norm": 5.209002494812012,
      "learning_rate": 0.0004185869067602625,
      "loss": 0.3441,
      "step": 3861
    },
    {
      "epoch": 1.7546569741026805,
      "grad_norm": 5.491458892822266,
      "learning_rate": 0.00041843430489851976,
      "loss": 0.6266,
      "step": 3862
    },
    {
      "epoch": 1.7551113130395275,
      "grad_norm": 5.299191951751709,
      "learning_rate": 0.000418281703036777,
      "loss": 0.4649,
      "step": 3863
    },
    {
      "epoch": 1.7555656519763745,
      "grad_norm": 6.344557762145996,
      "learning_rate": 0.00041812910117503433,
      "loss": 0.3489,
      "step": 3864
    },
    {
      "epoch": 1.7560199909132213,
      "grad_norm": 8.998178482055664,
      "learning_rate": 0.00041797649931329164,
      "loss": 0.7039,
      "step": 3865
    },
    {
      "epoch": 1.756474329850068,
      "grad_norm": 4.536648273468018,
      "learning_rate": 0.0004178238974515489,
      "loss": 0.5644,
      "step": 3866
    },
    {
      "epoch": 1.756928668786915,
      "grad_norm": 3.79327654838562,
      "learning_rate": 0.0004176712955898062,
      "loss": 0.1452,
      "step": 3867
    },
    {
      "epoch": 1.757383007723762,
      "grad_norm": 6.2968339920043945,
      "learning_rate": 0.0004175186937280635,
      "loss": 1.1859,
      "step": 3868
    },
    {
      "epoch": 1.757837346660609,
      "grad_norm": 4.43122673034668,
      "learning_rate": 0.00041736609186632077,
      "loss": 0.4332,
      "step": 3869
    },
    {
      "epoch": 1.7582916855974557,
      "grad_norm": 6.467646598815918,
      "learning_rate": 0.0004172134900045781,
      "loss": 0.3974,
      "step": 3870
    },
    {
      "epoch": 1.7587460245343025,
      "grad_norm": 3.101191282272339,
      "learning_rate": 0.00041706088814283534,
      "loss": 0.13,
      "step": 3871
    },
    {
      "epoch": 1.7592003634711495,
      "grad_norm": 4.4369635581970215,
      "learning_rate": 0.0004169082862810926,
      "loss": 0.3607,
      "step": 3872
    },
    {
      "epoch": 1.7596547024079965,
      "grad_norm": 5.687168121337891,
      "learning_rate": 0.0004167556844193499,
      "loss": 0.502,
      "step": 3873
    },
    {
      "epoch": 1.7601090413448433,
      "grad_norm": 5.571499824523926,
      "learning_rate": 0.0004166030825576072,
      "loss": 0.5124,
      "step": 3874
    },
    {
      "epoch": 1.76056338028169,
      "grad_norm": 8.717911720275879,
      "learning_rate": 0.0004164504806958645,
      "loss": 0.2341,
      "step": 3875
    },
    {
      "epoch": 1.761017719218537,
      "grad_norm": 6.449706554412842,
      "learning_rate": 0.0004162978788341218,
      "loss": 0.5043,
      "step": 3876
    },
    {
      "epoch": 1.761472058155384,
      "grad_norm": 1.7767701148986816,
      "learning_rate": 0.0004161452769723791,
      "loss": 0.2024,
      "step": 3877
    },
    {
      "epoch": 1.7619263970922308,
      "grad_norm": 3.9281044006347656,
      "learning_rate": 0.00041599267511063635,
      "loss": 0.5191,
      "step": 3878
    },
    {
      "epoch": 1.7623807360290777,
      "grad_norm": 6.931017875671387,
      "learning_rate": 0.00041584007324889366,
      "loss": 0.6614,
      "step": 3879
    },
    {
      "epoch": 1.7628350749659245,
      "grad_norm": 4.46162223815918,
      "learning_rate": 0.0004156874713871509,
      "loss": 0.4575,
      "step": 3880
    },
    {
      "epoch": 1.7632894139027715,
      "grad_norm": 2.6743509769439697,
      "learning_rate": 0.0004155348695254082,
      "loss": 0.3042,
      "step": 3881
    },
    {
      "epoch": 1.7637437528396185,
      "grad_norm": 3.5559091567993164,
      "learning_rate": 0.0004153822676636655,
      "loss": 0.5281,
      "step": 3882
    },
    {
      "epoch": 1.7641980917764652,
      "grad_norm": 6.533328056335449,
      "learning_rate": 0.0004152296658019228,
      "loss": 0.6867,
      "step": 3883
    },
    {
      "epoch": 1.764652430713312,
      "grad_norm": 2.0554141998291016,
      "learning_rate": 0.00041507706394018005,
      "loss": 0.1259,
      "step": 3884
    },
    {
      "epoch": 1.765106769650159,
      "grad_norm": 5.123558521270752,
      "learning_rate": 0.00041492446207843736,
      "loss": 0.2302,
      "step": 3885
    },
    {
      "epoch": 1.765561108587006,
      "grad_norm": 3.01277232170105,
      "learning_rate": 0.0004147718602166947,
      "loss": 0.1585,
      "step": 3886
    },
    {
      "epoch": 1.7660154475238528,
      "grad_norm": 6.395590305328369,
      "learning_rate": 0.00041461925835495193,
      "loss": 0.1634,
      "step": 3887
    },
    {
      "epoch": 1.7664697864606995,
      "grad_norm": 3.9857494831085205,
      "learning_rate": 0.00041446665649320924,
      "loss": 0.2972,
      "step": 3888
    },
    {
      "epoch": 1.7669241253975465,
      "grad_norm": 9.645965576171875,
      "learning_rate": 0.0004143140546314665,
      "loss": 1.1787,
      "step": 3889
    },
    {
      "epoch": 1.7673784643343935,
      "grad_norm": 2.9296514987945557,
      "learning_rate": 0.00041416145276972375,
      "loss": 0.149,
      "step": 3890
    },
    {
      "epoch": 1.7678328032712405,
      "grad_norm": 4.450349807739258,
      "learning_rate": 0.00041400885090798106,
      "loss": 0.2391,
      "step": 3891
    },
    {
      "epoch": 1.7682871422080872,
      "grad_norm": 2.158038377761841,
      "learning_rate": 0.0004138562490462384,
      "loss": 0.1908,
      "step": 3892
    },
    {
      "epoch": 1.768741481144934,
      "grad_norm": 3.9024176597595215,
      "learning_rate": 0.00041370364718449563,
      "loss": 0.1778,
      "step": 3893
    },
    {
      "epoch": 1.769195820081781,
      "grad_norm": 5.671374797821045,
      "learning_rate": 0.00041355104532275294,
      "loss": 0.2696,
      "step": 3894
    },
    {
      "epoch": 1.769650159018628,
      "grad_norm": 7.220964431762695,
      "learning_rate": 0.00041339844346101025,
      "loss": 0.5504,
      "step": 3895
    },
    {
      "epoch": 1.7701044979554748,
      "grad_norm": 6.495514392852783,
      "learning_rate": 0.00041324584159926756,
      "loss": 0.5887,
      "step": 3896
    },
    {
      "epoch": 1.7705588368923215,
      "grad_norm": 6.106009006500244,
      "learning_rate": 0.0004130932397375248,
      "loss": 1.0969,
      "step": 3897
    },
    {
      "epoch": 1.7710131758291685,
      "grad_norm": 5.735771656036377,
      "learning_rate": 0.0004129406378757821,
      "loss": 0.665,
      "step": 3898
    },
    {
      "epoch": 1.7714675147660155,
      "grad_norm": 5.640335559844971,
      "learning_rate": 0.0004127880360140394,
      "loss": 0.3441,
      "step": 3899
    },
    {
      "epoch": 1.7719218537028625,
      "grad_norm": 3.8097589015960693,
      "learning_rate": 0.00041263543415229664,
      "loss": 0.3261,
      "step": 3900
    },
    {
      "epoch": 1.7723761926397092,
      "grad_norm": 6.103779315948486,
      "learning_rate": 0.00041248283229055395,
      "loss": 0.3533,
      "step": 3901
    },
    {
      "epoch": 1.772830531576556,
      "grad_norm": 5.83372688293457,
      "learning_rate": 0.00041233023042881126,
      "loss": 0.6318,
      "step": 3902
    },
    {
      "epoch": 1.773284870513403,
      "grad_norm": 6.047704696655273,
      "learning_rate": 0.0004121776285670685,
      "loss": 0.4747,
      "step": 3903
    },
    {
      "epoch": 1.77373920945025,
      "grad_norm": 3.52744460105896,
      "learning_rate": 0.00041202502670532583,
      "loss": 0.2583,
      "step": 3904
    },
    {
      "epoch": 1.7741935483870968,
      "grad_norm": 3.5237386226654053,
      "learning_rate": 0.00041187242484358314,
      "loss": 0.2718,
      "step": 3905
    },
    {
      "epoch": 1.7746478873239435,
      "grad_norm": 3.244262218475342,
      "learning_rate": 0.0004117198229818404,
      "loss": 0.3471,
      "step": 3906
    },
    {
      "epoch": 1.7751022262607905,
      "grad_norm": 6.297366142272949,
      "learning_rate": 0.0004115672211200977,
      "loss": 0.616,
      "step": 3907
    },
    {
      "epoch": 1.7755565651976375,
      "grad_norm": 3.0665204524993896,
      "learning_rate": 0.00041141461925835496,
      "loss": 0.2984,
      "step": 3908
    },
    {
      "epoch": 1.7760109041344845,
      "grad_norm": 4.310510158538818,
      "learning_rate": 0.0004112620173966122,
      "loss": 0.2953,
      "step": 3909
    },
    {
      "epoch": 1.7764652430713312,
      "grad_norm": 4.080158233642578,
      "learning_rate": 0.00041110941553486953,
      "loss": 0.2896,
      "step": 3910
    },
    {
      "epoch": 1.776919582008178,
      "grad_norm": 7.096498489379883,
      "learning_rate": 0.00041095681367312684,
      "loss": 0.4851,
      "step": 3911
    },
    {
      "epoch": 1.777373920945025,
      "grad_norm": 6.796811103820801,
      "learning_rate": 0.0004108042118113841,
      "loss": 0.5423,
      "step": 3912
    },
    {
      "epoch": 1.777828259881872,
      "grad_norm": 5.365721702575684,
      "learning_rate": 0.0004106516099496414,
      "loss": 0.6638,
      "step": 3913
    },
    {
      "epoch": 1.7782825988187188,
      "grad_norm": 5.925910949707031,
      "learning_rate": 0.0004104990080878987,
      "loss": 0.6544,
      "step": 3914
    },
    {
      "epoch": 1.7787369377555655,
      "grad_norm": 4.530509948730469,
      "learning_rate": 0.000410346406226156,
      "loss": 0.4323,
      "step": 3915
    },
    {
      "epoch": 1.7791912766924125,
      "grad_norm": 4.668478965759277,
      "learning_rate": 0.0004101938043644133,
      "loss": 0.4514,
      "step": 3916
    },
    {
      "epoch": 1.7796456156292595,
      "grad_norm": 5.328839302062988,
      "learning_rate": 0.00041004120250267054,
      "loss": 0.9193,
      "step": 3917
    },
    {
      "epoch": 1.7800999545661065,
      "grad_norm": 4.641455173492432,
      "learning_rate": 0.0004098886006409278,
      "loss": 0.3554,
      "step": 3918
    },
    {
      "epoch": 1.7805542935029532,
      "grad_norm": 4.062233924865723,
      "learning_rate": 0.0004097359987791851,
      "loss": 0.5124,
      "step": 3919
    },
    {
      "epoch": 1.7810086324398,
      "grad_norm": 3.090846061706543,
      "learning_rate": 0.0004095833969174424,
      "loss": 0.1282,
      "step": 3920
    },
    {
      "epoch": 1.781462971376647,
      "grad_norm": 9.373043060302734,
      "learning_rate": 0.0004094307950556997,
      "loss": 1.1351,
      "step": 3921
    },
    {
      "epoch": 1.781917310313494,
      "grad_norm": 6.033864498138428,
      "learning_rate": 0.000409278193193957,
      "loss": 0.7021,
      "step": 3922
    },
    {
      "epoch": 1.7823716492503408,
      "grad_norm": 4.170299530029297,
      "learning_rate": 0.0004091255913322143,
      "loss": 0.2401,
      "step": 3923
    },
    {
      "epoch": 1.7828259881871875,
      "grad_norm": 5.102468013763428,
      "learning_rate": 0.00040897298947047155,
      "loss": 0.4895,
      "step": 3924
    },
    {
      "epoch": 1.7832803271240345,
      "grad_norm": 6.012553691864014,
      "learning_rate": 0.00040882038760872886,
      "loss": 1.4786,
      "step": 3925
    },
    {
      "epoch": 1.7837346660608815,
      "grad_norm": 8.200630187988281,
      "learning_rate": 0.0004086677857469861,
      "loss": 1.0253,
      "step": 3926
    },
    {
      "epoch": 1.7841890049977283,
      "grad_norm": 3.251175880432129,
      "learning_rate": 0.0004085151838852434,
      "loss": 0.2822,
      "step": 3927
    },
    {
      "epoch": 1.7846433439345752,
      "grad_norm": 3.438382625579834,
      "learning_rate": 0.0004083625820235007,
      "loss": 0.17,
      "step": 3928
    },
    {
      "epoch": 1.785097682871422,
      "grad_norm": 3.502042531967163,
      "learning_rate": 0.000408209980161758,
      "loss": 0.1171,
      "step": 3929
    },
    {
      "epoch": 1.785552021808269,
      "grad_norm": 7.070520877838135,
      "learning_rate": 0.00040805737830001526,
      "loss": 0.6142,
      "step": 3930
    },
    {
      "epoch": 1.786006360745116,
      "grad_norm": 4.088088035583496,
      "learning_rate": 0.00040790477643827257,
      "loss": 0.3128,
      "step": 3931
    },
    {
      "epoch": 1.7864606996819627,
      "grad_norm": 6.455574035644531,
      "learning_rate": 0.0004077521745765299,
      "loss": 0.4429,
      "step": 3932
    },
    {
      "epoch": 1.7869150386188095,
      "grad_norm": 13.280593872070312,
      "learning_rate": 0.00040759957271478713,
      "loss": 0.7237,
      "step": 3933
    },
    {
      "epoch": 1.7873693775556565,
      "grad_norm": 2.8267035484313965,
      "learning_rate": 0.00040744697085304444,
      "loss": 0.1813,
      "step": 3934
    },
    {
      "epoch": 1.7878237164925035,
      "grad_norm": 9.08203411102295,
      "learning_rate": 0.0004072943689913017,
      "loss": 1.1194,
      "step": 3935
    },
    {
      "epoch": 1.7882780554293503,
      "grad_norm": 6.112618923187256,
      "learning_rate": 0.00040714176712955896,
      "loss": 0.4925,
      "step": 3936
    },
    {
      "epoch": 1.788732394366197,
      "grad_norm": 5.3809814453125,
      "learning_rate": 0.00040698916526781627,
      "loss": 0.5665,
      "step": 3937
    },
    {
      "epoch": 1.789186733303044,
      "grad_norm": 5.790274620056152,
      "learning_rate": 0.0004068365634060736,
      "loss": 0.5345,
      "step": 3938
    },
    {
      "epoch": 1.789641072239891,
      "grad_norm": 5.102418422698975,
      "learning_rate": 0.00040668396154433083,
      "loss": 0.4807,
      "step": 3939
    },
    {
      "epoch": 1.790095411176738,
      "grad_norm": 4.498145580291748,
      "learning_rate": 0.00040653135968258814,
      "loss": 0.2336,
      "step": 3940
    },
    {
      "epoch": 1.7905497501135847,
      "grad_norm": 5.964015960693359,
      "learning_rate": 0.00040637875782084546,
      "loss": 0.6618,
      "step": 3941
    },
    {
      "epoch": 1.7910040890504315,
      "grad_norm": 7.2915425300598145,
      "learning_rate": 0.0004062261559591027,
      "loss": 0.7135,
      "step": 3942
    },
    {
      "epoch": 1.7914584279872785,
      "grad_norm": 9.596999168395996,
      "learning_rate": 0.00040607355409736,
      "loss": 0.5336,
      "step": 3943
    },
    {
      "epoch": 1.7919127669241255,
      "grad_norm": 4.901956081390381,
      "learning_rate": 0.0004059209522356173,
      "loss": 0.2013,
      "step": 3944
    },
    {
      "epoch": 1.7923671058609723,
      "grad_norm": 3.989833116531372,
      "learning_rate": 0.00040576835037387453,
      "loss": 0.4594,
      "step": 3945
    },
    {
      "epoch": 1.792821444797819,
      "grad_norm": 2.0396556854248047,
      "learning_rate": 0.00040561574851213185,
      "loss": 0.0993,
      "step": 3946
    },
    {
      "epoch": 1.793275783734666,
      "grad_norm": 5.139155387878418,
      "learning_rate": 0.00040546314665038916,
      "loss": 0.306,
      "step": 3947
    },
    {
      "epoch": 1.793730122671513,
      "grad_norm": 3.968973159790039,
      "learning_rate": 0.0004053105447886464,
      "loss": 0.1574,
      "step": 3948
    },
    {
      "epoch": 1.79418446160836,
      "grad_norm": 4.872425079345703,
      "learning_rate": 0.0004051579429269037,
      "loss": 0.4709,
      "step": 3949
    },
    {
      "epoch": 1.7946388005452067,
      "grad_norm": 9.639015197753906,
      "learning_rate": 0.00040500534106516103,
      "loss": 0.7554,
      "step": 3950
    },
    {
      "epoch": 1.7950931394820535,
      "grad_norm": 5.403845310211182,
      "learning_rate": 0.0004048527392034183,
      "loss": 0.7345,
      "step": 3951
    },
    {
      "epoch": 1.7955474784189005,
      "grad_norm": 7.496459007263184,
      "learning_rate": 0.0004047001373416756,
      "loss": 0.6304,
      "step": 3952
    },
    {
      "epoch": 1.7960018173557475,
      "grad_norm": 5.497817516326904,
      "learning_rate": 0.00040454753547993286,
      "loss": 0.4924,
      "step": 3953
    },
    {
      "epoch": 1.7964561562925943,
      "grad_norm": 11.423755645751953,
      "learning_rate": 0.0004043949336181901,
      "loss": 0.6486,
      "step": 3954
    },
    {
      "epoch": 1.796910495229441,
      "grad_norm": 7.529184341430664,
      "learning_rate": 0.0004042423317564474,
      "loss": 1.3022,
      "step": 3955
    },
    {
      "epoch": 1.797364834166288,
      "grad_norm": 3.5049893856048584,
      "learning_rate": 0.00040408972989470473,
      "loss": 0.2614,
      "step": 3956
    },
    {
      "epoch": 1.797819173103135,
      "grad_norm": 4.689541816711426,
      "learning_rate": 0.000403937128032962,
      "loss": 0.5576,
      "step": 3957
    },
    {
      "epoch": 1.798273512039982,
      "grad_norm": 1.7427771091461182,
      "learning_rate": 0.0004037845261712193,
      "loss": 0.086,
      "step": 3958
    },
    {
      "epoch": 1.7987278509768287,
      "grad_norm": 7.686041831970215,
      "learning_rate": 0.0004036319243094766,
      "loss": 0.5248,
      "step": 3959
    },
    {
      "epoch": 1.7991821899136755,
      "grad_norm": 6.001057147979736,
      "learning_rate": 0.00040347932244773387,
      "loss": 0.5448,
      "step": 3960
    },
    {
      "epoch": 1.7996365288505225,
      "grad_norm": 5.230869293212891,
      "learning_rate": 0.0004033267205859912,
      "loss": 0.5198,
      "step": 3961
    },
    {
      "epoch": 1.8000908677873695,
      "grad_norm": 4.729994297027588,
      "learning_rate": 0.00040317411872424844,
      "loss": 0.6727,
      "step": 3962
    },
    {
      "epoch": 1.8005452067242163,
      "grad_norm": 7.942793846130371,
      "learning_rate": 0.0004030215168625057,
      "loss": 0.5967,
      "step": 3963
    },
    {
      "epoch": 1.800999545661063,
      "grad_norm": 2.5704634189605713,
      "learning_rate": 0.000402868915000763,
      "loss": 0.1242,
      "step": 3964
    },
    {
      "epoch": 1.80145388459791,
      "grad_norm": 9.396111488342285,
      "learning_rate": 0.0004027163131390203,
      "loss": 0.5496,
      "step": 3965
    },
    {
      "epoch": 1.801908223534757,
      "grad_norm": 4.40660285949707,
      "learning_rate": 0.00040256371127727757,
      "loss": 0.7118,
      "step": 3966
    },
    {
      "epoch": 1.802362562471604,
      "grad_norm": 5.50151252746582,
      "learning_rate": 0.0004024111094155349,
      "loss": 0.4564,
      "step": 3967
    },
    {
      "epoch": 1.8028169014084507,
      "grad_norm": 5.9354400634765625,
      "learning_rate": 0.0004022585075537922,
      "loss": 1.0099,
      "step": 3968
    },
    {
      "epoch": 1.8032712403452975,
      "grad_norm": 3.6464731693267822,
      "learning_rate": 0.00040210590569204945,
      "loss": 0.2382,
      "step": 3969
    },
    {
      "epoch": 1.8037255792821445,
      "grad_norm": 5.441718578338623,
      "learning_rate": 0.00040195330383030676,
      "loss": 0.7096,
      "step": 3970
    },
    {
      "epoch": 1.8041799182189915,
      "grad_norm": 5.398183822631836,
      "learning_rate": 0.000401800701968564,
      "loss": 0.4323,
      "step": 3971
    },
    {
      "epoch": 1.8046342571558383,
      "grad_norm": 2.9723079204559326,
      "learning_rate": 0.00040164810010682127,
      "loss": 0.483,
      "step": 3972
    },
    {
      "epoch": 1.805088596092685,
      "grad_norm": 5.520763397216797,
      "learning_rate": 0.0004014954982450786,
      "loss": 0.4105,
      "step": 3973
    },
    {
      "epoch": 1.805542935029532,
      "grad_norm": 4.8689374923706055,
      "learning_rate": 0.0004013428963833359,
      "loss": 0.5881,
      "step": 3974
    },
    {
      "epoch": 1.805997273966379,
      "grad_norm": 2.530142068862915,
      "learning_rate": 0.00040119029452159315,
      "loss": 0.1352,
      "step": 3975
    },
    {
      "epoch": 1.8064516129032258,
      "grad_norm": 2.8365888595581055,
      "learning_rate": 0.00040103769265985046,
      "loss": 0.3049,
      "step": 3976
    },
    {
      "epoch": 1.8069059518400727,
      "grad_norm": 5.031599998474121,
      "learning_rate": 0.00040088509079810777,
      "loss": 0.2279,
      "step": 3977
    },
    {
      "epoch": 1.8073602907769195,
      "grad_norm": 4.933864116668701,
      "learning_rate": 0.000400732488936365,
      "loss": 0.4083,
      "step": 3978
    },
    {
      "epoch": 1.8078146297137665,
      "grad_norm": 4.539340019226074,
      "learning_rate": 0.00040057988707462234,
      "loss": 0.2585,
      "step": 3979
    },
    {
      "epoch": 1.8082689686506135,
      "grad_norm": 4.7606000900268555,
      "learning_rate": 0.00040042728521287965,
      "loss": 0.238,
      "step": 3980
    },
    {
      "epoch": 1.8087233075874602,
      "grad_norm": 3.146054267883301,
      "learning_rate": 0.00040027468335113685,
      "loss": 0.2327,
      "step": 3981
    },
    {
      "epoch": 1.809177646524307,
      "grad_norm": 3.6579620838165283,
      "learning_rate": 0.00040012208148939416,
      "loss": 0.3112,
      "step": 3982
    },
    {
      "epoch": 1.809631985461154,
      "grad_norm": 10.581751823425293,
      "learning_rate": 0.00039996947962765147,
      "loss": 1.3458,
      "step": 3983
    },
    {
      "epoch": 1.810086324398001,
      "grad_norm": 2.679784059524536,
      "learning_rate": 0.0003998168777659087,
      "loss": 0.2083,
      "step": 3984
    },
    {
      "epoch": 1.8105406633348478,
      "grad_norm": 5.553125381469727,
      "learning_rate": 0.00039966427590416604,
      "loss": 0.4804,
      "step": 3985
    },
    {
      "epoch": 1.8109950022716945,
      "grad_norm": 4.3040266036987305,
      "learning_rate": 0.00039951167404242335,
      "loss": 0.419,
      "step": 3986
    },
    {
      "epoch": 1.8114493412085415,
      "grad_norm": 4.118916988372803,
      "learning_rate": 0.0003993590721806806,
      "loss": 0.2353,
      "step": 3987
    },
    {
      "epoch": 1.8119036801453885,
      "grad_norm": 5.680283069610596,
      "learning_rate": 0.0003992064703189379,
      "loss": 0.454,
      "step": 3988
    },
    {
      "epoch": 1.8123580190822355,
      "grad_norm": 4.064266681671143,
      "learning_rate": 0.0003990538684571952,
      "loss": 0.1837,
      "step": 3989
    },
    {
      "epoch": 1.8128123580190822,
      "grad_norm": 4.909744739532471,
      "learning_rate": 0.00039890126659545243,
      "loss": 0.4273,
      "step": 3990
    },
    {
      "epoch": 1.813266696955929,
      "grad_norm": 3.535304069519043,
      "learning_rate": 0.00039874866473370974,
      "loss": 0.1494,
      "step": 3991
    },
    {
      "epoch": 1.813721035892776,
      "grad_norm": 5.520397663116455,
      "learning_rate": 0.00039859606287196705,
      "loss": 0.4138,
      "step": 3992
    },
    {
      "epoch": 1.814175374829623,
      "grad_norm": 7.797607898712158,
      "learning_rate": 0.0003984434610102243,
      "loss": 0.267,
      "step": 3993
    },
    {
      "epoch": 1.8146297137664698,
      "grad_norm": 5.058541297912598,
      "learning_rate": 0.0003982908591484816,
      "loss": 0.3388,
      "step": 3994
    },
    {
      "epoch": 1.8150840527033165,
      "grad_norm": 5.7709550857543945,
      "learning_rate": 0.0003981382572867389,
      "loss": 0.7019,
      "step": 3995
    },
    {
      "epoch": 1.8155383916401635,
      "grad_norm": 3.254816770553589,
      "learning_rate": 0.0003979856554249962,
      "loss": 0.1603,
      "step": 3996
    },
    {
      "epoch": 1.8159927305770105,
      "grad_norm": 6.945318222045898,
      "learning_rate": 0.0003978330535632535,
      "loss": 0.8939,
      "step": 3997
    },
    {
      "epoch": 1.8164470695138575,
      "grad_norm": 4.691747665405273,
      "learning_rate": 0.0003976804517015108,
      "loss": 0.2336,
      "step": 3998
    },
    {
      "epoch": 1.8169014084507042,
      "grad_norm": 6.743320465087891,
      "learning_rate": 0.000397527849839768,
      "loss": 0.5372,
      "step": 3999
    },
    {
      "epoch": 1.817355747387551,
      "grad_norm": 6.023064136505127,
      "learning_rate": 0.0003973752479780253,
      "loss": 0.487,
      "step": 4000
    },
    {
      "epoch": 1.817810086324398,
      "grad_norm": 3.7820355892181396,
      "learning_rate": 0.00039722264611628263,
      "loss": 0.1454,
      "step": 4001
    },
    {
      "epoch": 1.818264425261245,
      "grad_norm": 6.008108139038086,
      "learning_rate": 0.0003970700442545399,
      "loss": 0.4166,
      "step": 4002
    },
    {
      "epoch": 1.8187187641980918,
      "grad_norm": 7.976841926574707,
      "learning_rate": 0.0003969174423927972,
      "loss": 1.539,
      "step": 4003
    },
    {
      "epoch": 1.8191731031349385,
      "grad_norm": 6.197981357574463,
      "learning_rate": 0.0003967648405310545,
      "loss": 0.8455,
      "step": 4004
    },
    {
      "epoch": 1.8196274420717855,
      "grad_norm": 5.143143653869629,
      "learning_rate": 0.00039661223866931176,
      "loss": 0.5408,
      "step": 4005
    },
    {
      "epoch": 1.8200817810086325,
      "grad_norm": 2.3777647018432617,
      "learning_rate": 0.00039645963680756907,
      "loss": 0.1664,
      "step": 4006
    },
    {
      "epoch": 1.8205361199454795,
      "grad_norm": 4.757565021514893,
      "learning_rate": 0.0003963070349458264,
      "loss": 0.2605,
      "step": 4007
    },
    {
      "epoch": 1.8209904588823262,
      "grad_norm": 3.5120625495910645,
      "learning_rate": 0.0003961544330840836,
      "loss": 0.3348,
      "step": 4008
    },
    {
      "epoch": 1.821444797819173,
      "grad_norm": 4.600243091583252,
      "learning_rate": 0.0003960018312223409,
      "loss": 0.4981,
      "step": 4009
    },
    {
      "epoch": 1.82189913675602,
      "grad_norm": 5.703783988952637,
      "learning_rate": 0.0003958492293605982,
      "loss": 0.5938,
      "step": 4010
    },
    {
      "epoch": 1.822353475692867,
      "grad_norm": 4.324120998382568,
      "learning_rate": 0.00039569662749885546,
      "loss": 0.1715,
      "step": 4011
    },
    {
      "epoch": 1.8228078146297138,
      "grad_norm": 4.570751190185547,
      "learning_rate": 0.0003955440256371128,
      "loss": 0.2633,
      "step": 4012
    },
    {
      "epoch": 1.8232621535665605,
      "grad_norm": 3.536343574523926,
      "learning_rate": 0.0003953914237753701,
      "loss": 0.1553,
      "step": 4013
    },
    {
      "epoch": 1.8237164925034075,
      "grad_norm": 3.8205130100250244,
      "learning_rate": 0.00039523882191362734,
      "loss": 0.1312,
      "step": 4014
    },
    {
      "epoch": 1.8241708314402545,
      "grad_norm": 7.480072975158691,
      "learning_rate": 0.00039508622005188465,
      "loss": 1.0783,
      "step": 4015
    },
    {
      "epoch": 1.8246251703771015,
      "grad_norm": 4.317116737365723,
      "learning_rate": 0.00039493361819014196,
      "loss": 0.287,
      "step": 4016
    },
    {
      "epoch": 1.8250795093139482,
      "grad_norm": 4.267902851104736,
      "learning_rate": 0.00039478101632839916,
      "loss": 0.338,
      "step": 4017
    },
    {
      "epoch": 1.825533848250795,
      "grad_norm": 3.5641534328460693,
      "learning_rate": 0.0003946284144666565,
      "loss": 0.27,
      "step": 4018
    },
    {
      "epoch": 1.825988187187642,
      "grad_norm": 5.7917327880859375,
      "learning_rate": 0.0003944758126049138,
      "loss": 0.5985,
      "step": 4019
    },
    {
      "epoch": 1.826442526124489,
      "grad_norm": 3.390655040740967,
      "learning_rate": 0.00039432321074317104,
      "loss": 0.342,
      "step": 4020
    },
    {
      "epoch": 1.8268968650613358,
      "grad_norm": 5.405951499938965,
      "learning_rate": 0.00039417060888142835,
      "loss": 0.6017,
      "step": 4021
    },
    {
      "epoch": 1.8273512039981825,
      "grad_norm": 5.344211101531982,
      "learning_rate": 0.00039401800701968566,
      "loss": 0.2396,
      "step": 4022
    },
    {
      "epoch": 1.8278055429350295,
      "grad_norm": 4.2136921882629395,
      "learning_rate": 0.000393865405157943,
      "loss": 0.134,
      "step": 4023
    },
    {
      "epoch": 1.8282598818718765,
      "grad_norm": 6.095253944396973,
      "learning_rate": 0.00039371280329620023,
      "loss": 0.7175,
      "step": 4024
    },
    {
      "epoch": 1.8287142208087233,
      "grad_norm": 2.4477808475494385,
      "learning_rate": 0.00039356020143445754,
      "loss": 0.1976,
      "step": 4025
    },
    {
      "epoch": 1.8291685597455702,
      "grad_norm": 4.248155117034912,
      "learning_rate": 0.0003934075995727148,
      "loss": 0.1726,
      "step": 4026
    },
    {
      "epoch": 1.829622898682417,
      "grad_norm": 7.15653133392334,
      "learning_rate": 0.00039325499771097205,
      "loss": 0.8187,
      "step": 4027
    },
    {
      "epoch": 1.830077237619264,
      "grad_norm": 4.465921401977539,
      "learning_rate": 0.00039310239584922936,
      "loss": 0.3819,
      "step": 4028
    },
    {
      "epoch": 1.830531576556111,
      "grad_norm": 2.9665865898132324,
      "learning_rate": 0.0003929497939874867,
      "loss": 0.154,
      "step": 4029
    },
    {
      "epoch": 1.8309859154929577,
      "grad_norm": 5.389472484588623,
      "learning_rate": 0.00039279719212574393,
      "loss": 0.4805,
      "step": 4030
    },
    {
      "epoch": 1.8314402544298045,
      "grad_norm": 4.011136531829834,
      "learning_rate": 0.00039264459026400124,
      "loss": 0.2752,
      "step": 4031
    },
    {
      "epoch": 1.8318945933666515,
      "grad_norm": 5.24465274810791,
      "learning_rate": 0.00039249198840225855,
      "loss": 0.4345,
      "step": 4032
    },
    {
      "epoch": 1.8323489323034985,
      "grad_norm": 2.520993232727051,
      "learning_rate": 0.0003923393865405158,
      "loss": 0.094,
      "step": 4033
    },
    {
      "epoch": 1.8328032712403453,
      "grad_norm": 5.467137813568115,
      "learning_rate": 0.0003921867846787731,
      "loss": 0.7425,
      "step": 4034
    },
    {
      "epoch": 1.833257610177192,
      "grad_norm": 6.734776020050049,
      "learning_rate": 0.0003920341828170304,
      "loss": 0.5457,
      "step": 4035
    },
    {
      "epoch": 1.833711949114039,
      "grad_norm": 1.7936145067214966,
      "learning_rate": 0.00039188158095528763,
      "loss": 0.1196,
      "step": 4036
    },
    {
      "epoch": 1.834166288050886,
      "grad_norm": 4.07810115814209,
      "learning_rate": 0.00039172897909354494,
      "loss": 0.2986,
      "step": 4037
    },
    {
      "epoch": 1.834620626987733,
      "grad_norm": 5.520940780639648,
      "learning_rate": 0.00039157637723180225,
      "loss": 0.7621,
      "step": 4038
    },
    {
      "epoch": 1.8350749659245797,
      "grad_norm": 4.720187664031982,
      "learning_rate": 0.0003914237753700595,
      "loss": 0.2756,
      "step": 4039
    },
    {
      "epoch": 1.8355293048614265,
      "grad_norm": 5.914726257324219,
      "learning_rate": 0.0003912711735083168,
      "loss": 0.3621,
      "step": 4040
    },
    {
      "epoch": 1.8359836437982735,
      "grad_norm": 3.3278470039367676,
      "learning_rate": 0.00039111857164657413,
      "loss": 0.1823,
      "step": 4041
    },
    {
      "epoch": 1.8364379827351205,
      "grad_norm": 4.3441009521484375,
      "learning_rate": 0.0003909659697848314,
      "loss": 0.3158,
      "step": 4042
    },
    {
      "epoch": 1.8368923216719673,
      "grad_norm": 7.75765323638916,
      "learning_rate": 0.0003908133679230887,
      "loss": 0.6627,
      "step": 4043
    },
    {
      "epoch": 1.837346660608814,
      "grad_norm": 1.7387409210205078,
      "learning_rate": 0.00039066076606134595,
      "loss": 0.0794,
      "step": 4044
    },
    {
      "epoch": 1.837800999545661,
      "grad_norm": 4.4998779296875,
      "learning_rate": 0.0003905081641996032,
      "loss": 0.2784,
      "step": 4045
    },
    {
      "epoch": 1.838255338482508,
      "grad_norm": 3.0881478786468506,
      "learning_rate": 0.0003903555623378605,
      "loss": 0.1644,
      "step": 4046
    },
    {
      "epoch": 1.838709677419355,
      "grad_norm": 5.608922004699707,
      "learning_rate": 0.00039020296047611783,
      "loss": 0.416,
      "step": 4047
    },
    {
      "epoch": 1.8391640163562017,
      "grad_norm": 8.733267784118652,
      "learning_rate": 0.0003900503586143751,
      "loss": 1.4178,
      "step": 4048
    },
    {
      "epoch": 1.8396183552930485,
      "grad_norm": 7.466798782348633,
      "learning_rate": 0.0003898977567526324,
      "loss": 0.5922,
      "step": 4049
    },
    {
      "epoch": 1.8400726942298955,
      "grad_norm": 5.882389545440674,
      "learning_rate": 0.0003897451548908897,
      "loss": 0.3528,
      "step": 4050
    },
    {
      "epoch": 1.8405270331667425,
      "grad_norm": 4.228835105895996,
      "learning_rate": 0.00038959255302914697,
      "loss": 0.2539,
      "step": 4051
    },
    {
      "epoch": 1.8409813721035893,
      "grad_norm": 4.340486526489258,
      "learning_rate": 0.0003894399511674043,
      "loss": 0.2249,
      "step": 4052
    },
    {
      "epoch": 1.841435711040436,
      "grad_norm": 5.980523109436035,
      "learning_rate": 0.0003892873493056616,
      "loss": 0.269,
      "step": 4053
    },
    {
      "epoch": 1.841890049977283,
      "grad_norm": 4.845150470733643,
      "learning_rate": 0.0003891347474439188,
      "loss": 0.4927,
      "step": 4054
    },
    {
      "epoch": 1.84234438891413,
      "grad_norm": 4.478765487670898,
      "learning_rate": 0.0003889821455821761,
      "loss": 0.4861,
      "step": 4055
    },
    {
      "epoch": 1.842798727850977,
      "grad_norm": 7.330571174621582,
      "learning_rate": 0.0003888295437204334,
      "loss": 0.8186,
      "step": 4056
    },
    {
      "epoch": 1.8432530667878237,
      "grad_norm": 4.312718391418457,
      "learning_rate": 0.00038867694185869067,
      "loss": 0.2291,
      "step": 4057
    },
    {
      "epoch": 1.8437074057246705,
      "grad_norm": 4.914857864379883,
      "learning_rate": 0.000388524339996948,
      "loss": 0.4162,
      "step": 4058
    },
    {
      "epoch": 1.8441617446615175,
      "grad_norm": 3.189182758331299,
      "learning_rate": 0.0003883717381352053,
      "loss": 0.349,
      "step": 4059
    },
    {
      "epoch": 1.8446160835983645,
      "grad_norm": 5.750268459320068,
      "learning_rate": 0.00038821913627346254,
      "loss": 0.7128,
      "step": 4060
    },
    {
      "epoch": 1.8450704225352113,
      "grad_norm": 6.18161153793335,
      "learning_rate": 0.00038806653441171985,
      "loss": 0.5733,
      "step": 4061
    },
    {
      "epoch": 1.845524761472058,
      "grad_norm": 4.365655899047852,
      "learning_rate": 0.00038791393254997716,
      "loss": 0.7572,
      "step": 4062
    },
    {
      "epoch": 1.845979100408905,
      "grad_norm": 4.34958028793335,
      "learning_rate": 0.00038776133068823437,
      "loss": 0.3533,
      "step": 4063
    },
    {
      "epoch": 1.846433439345752,
      "grad_norm": 5.5699462890625,
      "learning_rate": 0.0003876087288264917,
      "loss": 0.595,
      "step": 4064
    },
    {
      "epoch": 1.846887778282599,
      "grad_norm": 6.201547622680664,
      "learning_rate": 0.000387456126964749,
      "loss": 0.7482,
      "step": 4065
    },
    {
      "epoch": 1.8473421172194457,
      "grad_norm": 5.698769569396973,
      "learning_rate": 0.00038730352510300624,
      "loss": 0.6373,
      "step": 4066
    },
    {
      "epoch": 1.8477964561562925,
      "grad_norm": 3.2162463665008545,
      "learning_rate": 0.00038715092324126356,
      "loss": 0.2647,
      "step": 4067
    },
    {
      "epoch": 1.8482507950931395,
      "grad_norm": 10.425241470336914,
      "learning_rate": 0.00038699832137952087,
      "loss": 1.3661,
      "step": 4068
    },
    {
      "epoch": 1.8487051340299865,
      "grad_norm": 2.767822027206421,
      "learning_rate": 0.0003868457195177781,
      "loss": 0.2071,
      "step": 4069
    },
    {
      "epoch": 1.8491594729668333,
      "grad_norm": 4.255415439605713,
      "learning_rate": 0.00038669311765603543,
      "loss": 0.565,
      "step": 4070
    },
    {
      "epoch": 1.84961381190368,
      "grad_norm": 4.097095489501953,
      "learning_rate": 0.00038654051579429274,
      "loss": 0.2364,
      "step": 4071
    },
    {
      "epoch": 1.850068150840527,
      "grad_norm": 5.330312728881836,
      "learning_rate": 0.00038638791393254995,
      "loss": 0.4303,
      "step": 4072
    },
    {
      "epoch": 1.850522489777374,
      "grad_norm": 4.683379650115967,
      "learning_rate": 0.00038623531207080726,
      "loss": 0.4384,
      "step": 4073
    },
    {
      "epoch": 1.8509768287142208,
      "grad_norm": 7.295563697814941,
      "learning_rate": 0.00038608271020906457,
      "loss": 0.853,
      "step": 4074
    },
    {
      "epoch": 1.8514311676510677,
      "grad_norm": 5.00408411026001,
      "learning_rate": 0.0003859301083473218,
      "loss": 0.3976,
      "step": 4075
    },
    {
      "epoch": 1.8518855065879145,
      "grad_norm": 3.3161752223968506,
      "learning_rate": 0.00038577750648557913,
      "loss": 0.1278,
      "step": 4076
    },
    {
      "epoch": 1.8523398455247615,
      "grad_norm": 3.8455421924591064,
      "learning_rate": 0.00038562490462383644,
      "loss": 0.1421,
      "step": 4077
    },
    {
      "epoch": 1.8527941844616085,
      "grad_norm": 3.851193904876709,
      "learning_rate": 0.0003854723027620937,
      "loss": 0.3745,
      "step": 4078
    },
    {
      "epoch": 1.8532485233984552,
      "grad_norm": 4.224381446838379,
      "learning_rate": 0.000385319700900351,
      "loss": 0.4348,
      "step": 4079
    },
    {
      "epoch": 1.853702862335302,
      "grad_norm": 1.5723906755447388,
      "learning_rate": 0.0003851670990386083,
      "loss": 0.0565,
      "step": 4080
    },
    {
      "epoch": 1.854157201272149,
      "grad_norm": 3.0584607124328613,
      "learning_rate": 0.0003850144971768655,
      "loss": 0.3044,
      "step": 4081
    },
    {
      "epoch": 1.854611540208996,
      "grad_norm": 2.771533966064453,
      "learning_rate": 0.00038486189531512283,
      "loss": 0.1083,
      "step": 4082
    },
    {
      "epoch": 1.8550658791458428,
      "grad_norm": 6.5934858322143555,
      "learning_rate": 0.00038470929345338015,
      "loss": 0.3876,
      "step": 4083
    },
    {
      "epoch": 1.8555202180826895,
      "grad_norm": 4.210713863372803,
      "learning_rate": 0.0003845566915916374,
      "loss": 0.3097,
      "step": 4084
    },
    {
      "epoch": 1.8559745570195365,
      "grad_norm": 6.615338325500488,
      "learning_rate": 0.0003844040897298947,
      "loss": 1.5871,
      "step": 4085
    },
    {
      "epoch": 1.8564288959563835,
      "grad_norm": 6.92189359664917,
      "learning_rate": 0.000384251487868152,
      "loss": 0.447,
      "step": 4086
    },
    {
      "epoch": 1.8568832348932305,
      "grad_norm": 9.602346420288086,
      "learning_rate": 0.0003840988860064093,
      "loss": 0.6244,
      "step": 4087
    },
    {
      "epoch": 1.8573375738300772,
      "grad_norm": 3.43774676322937,
      "learning_rate": 0.0003839462841446666,
      "loss": 0.2681,
      "step": 4088
    },
    {
      "epoch": 1.857791912766924,
      "grad_norm": 4.917862415313721,
      "learning_rate": 0.0003837936822829239,
      "loss": 0.4767,
      "step": 4089
    },
    {
      "epoch": 1.858246251703771,
      "grad_norm": 3.520540952682495,
      "learning_rate": 0.0003836410804211811,
      "loss": 0.2022,
      "step": 4090
    },
    {
      "epoch": 1.858700590640618,
      "grad_norm": 3.990248918533325,
      "learning_rate": 0.0003834884785594384,
      "loss": 0.1755,
      "step": 4091
    },
    {
      "epoch": 1.8591549295774648,
      "grad_norm": 5.7819085121154785,
      "learning_rate": 0.0003833358766976957,
      "loss": 0.5232,
      "step": 4092
    },
    {
      "epoch": 1.8596092685143115,
      "grad_norm": 3.38619327545166,
      "learning_rate": 0.000383183274835953,
      "loss": 0.3104,
      "step": 4093
    },
    {
      "epoch": 1.8600636074511585,
      "grad_norm": 2.8974320888519287,
      "learning_rate": 0.0003830306729742103,
      "loss": 0.159,
      "step": 4094
    },
    {
      "epoch": 1.8605179463880055,
      "grad_norm": 4.339885711669922,
      "learning_rate": 0.0003828780711124676,
      "loss": 0.4742,
      "step": 4095
    },
    {
      "epoch": 1.8609722853248525,
      "grad_norm": 5.252216815948486,
      "learning_rate": 0.00038272546925072486,
      "loss": 0.7784,
      "step": 4096
    },
    {
      "epoch": 1.8614266242616992,
      "grad_norm": 8.681215286254883,
      "learning_rate": 0.00038257286738898217,
      "loss": 0.3693,
      "step": 4097
    },
    {
      "epoch": 1.861880963198546,
      "grad_norm": 3.404444456100464,
      "learning_rate": 0.0003824202655272395,
      "loss": 0.3754,
      "step": 4098
    },
    {
      "epoch": 1.862335302135393,
      "grad_norm": 10.426180839538574,
      "learning_rate": 0.0003822676636654967,
      "loss": 0.4687,
      "step": 4099
    },
    {
      "epoch": 1.86278964107224,
      "grad_norm": 4.245123863220215,
      "learning_rate": 0.000382115061803754,
      "loss": 0.2137,
      "step": 4100
    },
    {
      "epoch": 1.8632439800090868,
      "grad_norm": 5.3106536865234375,
      "learning_rate": 0.0003819624599420113,
      "loss": 0.3302,
      "step": 4101
    },
    {
      "epoch": 1.8636983189459335,
      "grad_norm": 6.179414749145508,
      "learning_rate": 0.00038180985808026856,
      "loss": 0.634,
      "step": 4102
    },
    {
      "epoch": 1.8641526578827805,
      "grad_norm": 3.7201173305511475,
      "learning_rate": 0.00038165725621852587,
      "loss": 0.3519,
      "step": 4103
    },
    {
      "epoch": 1.8646069968196275,
      "grad_norm": 3.8755335807800293,
      "learning_rate": 0.0003815046543567832,
      "loss": 0.1798,
      "step": 4104
    },
    {
      "epoch": 1.8650613357564745,
      "grad_norm": 6.46118688583374,
      "learning_rate": 0.00038135205249504044,
      "loss": 0.5814,
      "step": 4105
    },
    {
      "epoch": 1.8655156746933212,
      "grad_norm": 10.356796264648438,
      "learning_rate": 0.00038119945063329775,
      "loss": 1.0397,
      "step": 4106
    },
    {
      "epoch": 1.865970013630168,
      "grad_norm": 8.623490333557129,
      "learning_rate": 0.00038104684877155506,
      "loss": 0.9909,
      "step": 4107
    },
    {
      "epoch": 1.866424352567015,
      "grad_norm": 7.5969390869140625,
      "learning_rate": 0.00038089424690981226,
      "loss": 0.4874,
      "step": 4108
    },
    {
      "epoch": 1.866878691503862,
      "grad_norm": 3.8792355060577393,
      "learning_rate": 0.00038074164504806957,
      "loss": 0.3998,
      "step": 4109
    },
    {
      "epoch": 1.8673330304407088,
      "grad_norm": 5.5172295570373535,
      "learning_rate": 0.0003805890431863269,
      "loss": 0.6439,
      "step": 4110
    },
    {
      "epoch": 1.8677873693775555,
      "grad_norm": 3.10237455368042,
      "learning_rate": 0.00038043644132458414,
      "loss": 0.1876,
      "step": 4111
    },
    {
      "epoch": 1.8682417083144025,
      "grad_norm": 3.034334182739258,
      "learning_rate": 0.00038028383946284145,
      "loss": 0.2863,
      "step": 4112
    },
    {
      "epoch": 1.8686960472512495,
      "grad_norm": 8.264420509338379,
      "learning_rate": 0.00038013123760109876,
      "loss": 0.5561,
      "step": 4113
    },
    {
      "epoch": 1.8691503861880965,
      "grad_norm": 6.60746955871582,
      "learning_rate": 0.000379978635739356,
      "loss": 0.4828,
      "step": 4114
    },
    {
      "epoch": 1.8696047251249432,
      "grad_norm": 7.203073978424072,
      "learning_rate": 0.0003798260338776133,
      "loss": 0.3282,
      "step": 4115
    },
    {
      "epoch": 1.87005906406179,
      "grad_norm": 4.628923416137695,
      "learning_rate": 0.00037967343201587064,
      "loss": 0.3939,
      "step": 4116
    },
    {
      "epoch": 1.870513402998637,
      "grad_norm": 5.404961585998535,
      "learning_rate": 0.00037952083015412784,
      "loss": 0.4864,
      "step": 4117
    },
    {
      "epoch": 1.870967741935484,
      "grad_norm": 3.2039635181427,
      "learning_rate": 0.00037936822829238515,
      "loss": 0.2001,
      "step": 4118
    },
    {
      "epoch": 1.8714220808723308,
      "grad_norm": 8.139945030212402,
      "learning_rate": 0.00037921562643064246,
      "loss": 0.902,
      "step": 4119
    },
    {
      "epoch": 1.8718764198091775,
      "grad_norm": 8.05260181427002,
      "learning_rate": 0.0003790630245688997,
      "loss": 0.4045,
      "step": 4120
    },
    {
      "epoch": 1.8723307587460245,
      "grad_norm": 7.569334506988525,
      "learning_rate": 0.000378910422707157,
      "loss": 1.11,
      "step": 4121
    },
    {
      "epoch": 1.8727850976828715,
      "grad_norm": 5.526501178741455,
      "learning_rate": 0.00037875782084541434,
      "loss": 0.9867,
      "step": 4122
    },
    {
      "epoch": 1.8732394366197183,
      "grad_norm": 2.696521759033203,
      "learning_rate": 0.0003786052189836716,
      "loss": 0.206,
      "step": 4123
    },
    {
      "epoch": 1.8736937755565652,
      "grad_norm": 3.648425340652466,
      "learning_rate": 0.0003784526171219289,
      "loss": 0.379,
      "step": 4124
    },
    {
      "epoch": 1.874148114493412,
      "grad_norm": 3.0877580642700195,
      "learning_rate": 0.0003783000152601862,
      "loss": 0.1818,
      "step": 4125
    },
    {
      "epoch": 1.874602453430259,
      "grad_norm": 6.799016952514648,
      "learning_rate": 0.00037814741339844347,
      "loss": 0.4163,
      "step": 4126
    },
    {
      "epoch": 1.875056792367106,
      "grad_norm": 2.984005928039551,
      "learning_rate": 0.00037799481153670073,
      "loss": 0.2467,
      "step": 4127
    },
    {
      "epoch": 1.8755111313039527,
      "grad_norm": 3.5591304302215576,
      "learning_rate": 0.00037784220967495804,
      "loss": 0.1763,
      "step": 4128
    },
    {
      "epoch": 1.8759654702407995,
      "grad_norm": 5.559193134307861,
      "learning_rate": 0.0003776896078132153,
      "loss": 0.7604,
      "step": 4129
    },
    {
      "epoch": 1.8764198091776465,
      "grad_norm": 3.381671190261841,
      "learning_rate": 0.0003775370059514726,
      "loss": 0.2654,
      "step": 4130
    },
    {
      "epoch": 1.8768741481144935,
      "grad_norm": 7.670391082763672,
      "learning_rate": 0.0003773844040897299,
      "loss": 0.6872,
      "step": 4131
    },
    {
      "epoch": 1.8773284870513403,
      "grad_norm": 4.926797389984131,
      "learning_rate": 0.00037723180222798717,
      "loss": 0.4787,
      "step": 4132
    },
    {
      "epoch": 1.8777828259881872,
      "grad_norm": 2.0356028079986572,
      "learning_rate": 0.0003770792003662445,
      "loss": 0.0801,
      "step": 4133
    },
    {
      "epoch": 1.878237164925034,
      "grad_norm": 4.92603874206543,
      "learning_rate": 0.0003769265985045018,
      "loss": 0.3522,
      "step": 4134
    },
    {
      "epoch": 1.878691503861881,
      "grad_norm": 3.076573610305786,
      "learning_rate": 0.00037677399664275905,
      "loss": 0.1282,
      "step": 4135
    },
    {
      "epoch": 1.879145842798728,
      "grad_norm": 3.4788694381713867,
      "learning_rate": 0.0003766213947810163,
      "loss": 0.2712,
      "step": 4136
    },
    {
      "epoch": 1.8796001817355747,
      "grad_norm": 3.796492576599121,
      "learning_rate": 0.0003764687929192736,
      "loss": 0.2762,
      "step": 4137
    },
    {
      "epoch": 1.8800545206724215,
      "grad_norm": 8.94456672668457,
      "learning_rate": 0.0003763161910575309,
      "loss": 0.691,
      "step": 4138
    },
    {
      "epoch": 1.8805088596092685,
      "grad_norm": 3.2825655937194824,
      "learning_rate": 0.0003761635891957882,
      "loss": 0.1088,
      "step": 4139
    },
    {
      "epoch": 1.8809631985461155,
      "grad_norm": 3.5073447227478027,
      "learning_rate": 0.0003760109873340455,
      "loss": 0.3881,
      "step": 4140
    },
    {
      "epoch": 1.8814175374829623,
      "grad_norm": 4.459857940673828,
      "learning_rate": 0.00037585838547230275,
      "loss": 0.2335,
      "step": 4141
    },
    {
      "epoch": 1.881871876419809,
      "grad_norm": 3.4725210666656494,
      "learning_rate": 0.00037570578361056006,
      "loss": 0.2629,
      "step": 4142
    },
    {
      "epoch": 1.882326215356656,
      "grad_norm": 6.1910271644592285,
      "learning_rate": 0.00037555318174881737,
      "loss": 0.5316,
      "step": 4143
    },
    {
      "epoch": 1.882780554293503,
      "grad_norm": 6.619325160980225,
      "learning_rate": 0.0003754005798870747,
      "loss": 0.6731,
      "step": 4144
    },
    {
      "epoch": 1.88323489323035,
      "grad_norm": 7.169477462768555,
      "learning_rate": 0.0003752479780253319,
      "loss": 0.353,
      "step": 4145
    },
    {
      "epoch": 1.8836892321671967,
      "grad_norm": 6.782623291015625,
      "learning_rate": 0.0003750953761635892,
      "loss": 0.4662,
      "step": 4146
    },
    {
      "epoch": 1.8841435711040435,
      "grad_norm": 6.8052897453308105,
      "learning_rate": 0.00037494277430184645,
      "loss": 0.782,
      "step": 4147
    },
    {
      "epoch": 1.8845979100408905,
      "grad_norm": 3.6080336570739746,
      "learning_rate": 0.00037479017244010376,
      "loss": 0.2331,
      "step": 4148
    },
    {
      "epoch": 1.8850522489777375,
      "grad_norm": 4.051154613494873,
      "learning_rate": 0.0003746375705783611,
      "loss": 0.3144,
      "step": 4149
    },
    {
      "epoch": 1.8855065879145843,
      "grad_norm": 8.64153003692627,
      "learning_rate": 0.0003744849687166184,
      "loss": 0.9942,
      "step": 4150
    },
    {
      "epoch": 1.885960926851431,
      "grad_norm": 4.740560531616211,
      "learning_rate": 0.00037433236685487564,
      "loss": 0.391,
      "step": 4151
    },
    {
      "epoch": 1.886415265788278,
      "grad_norm": 10.142102241516113,
      "learning_rate": 0.00037417976499313295,
      "loss": 0.6609,
      "step": 4152
    },
    {
      "epoch": 1.886869604725125,
      "grad_norm": 3.665334939956665,
      "learning_rate": 0.00037402716313139026,
      "loss": 0.1641,
      "step": 4153
    },
    {
      "epoch": 1.887323943661972,
      "grad_norm": 5.111855983734131,
      "learning_rate": 0.00037387456126964746,
      "loss": 0.4742,
      "step": 4154
    },
    {
      "epoch": 1.8877782825988187,
      "grad_norm": 6.739625930786133,
      "learning_rate": 0.0003737219594079048,
      "loss": 0.6434,
      "step": 4155
    },
    {
      "epoch": 1.8882326215356655,
      "grad_norm": 4.544004917144775,
      "learning_rate": 0.0003735693575461621,
      "loss": 0.3661,
      "step": 4156
    },
    {
      "epoch": 1.8886869604725125,
      "grad_norm": 5.296289920806885,
      "learning_rate": 0.00037341675568441934,
      "loss": 0.588,
      "step": 4157
    },
    {
      "epoch": 1.8891412994093595,
      "grad_norm": 4.500444412231445,
      "learning_rate": 0.00037326415382267665,
      "loss": 0.3842,
      "step": 4158
    },
    {
      "epoch": 1.8895956383462063,
      "grad_norm": 5.399082183837891,
      "learning_rate": 0.00037311155196093396,
      "loss": 0.4135,
      "step": 4159
    },
    {
      "epoch": 1.890049977283053,
      "grad_norm": 5.508675575256348,
      "learning_rate": 0.0003729589500991912,
      "loss": 0.6475,
      "step": 4160
    },
    {
      "epoch": 1.8905043162199,
      "grad_norm": 9.381823539733887,
      "learning_rate": 0.00037280634823744853,
      "loss": 0.9513,
      "step": 4161
    },
    {
      "epoch": 1.890958655156747,
      "grad_norm": 4.19394588470459,
      "learning_rate": 0.00037265374637570584,
      "loss": 0.2936,
      "step": 4162
    },
    {
      "epoch": 1.891412994093594,
      "grad_norm": 4.966208457946777,
      "learning_rate": 0.00037250114451396304,
      "loss": 0.397,
      "step": 4163
    },
    {
      "epoch": 1.8918673330304407,
      "grad_norm": 4.3693037033081055,
      "learning_rate": 0.00037234854265222035,
      "loss": 0.4869,
      "step": 4164
    },
    {
      "epoch": 1.8923216719672875,
      "grad_norm": 3.5159008502960205,
      "learning_rate": 0.00037219594079047766,
      "loss": 0.3713,
      "step": 4165
    },
    {
      "epoch": 1.8927760109041345,
      "grad_norm": 4.901679515838623,
      "learning_rate": 0.0003720433389287349,
      "loss": 0.2974,
      "step": 4166
    },
    {
      "epoch": 1.8932303498409815,
      "grad_norm": 6.076570510864258,
      "learning_rate": 0.00037189073706699223,
      "loss": 0.958,
      "step": 4167
    },
    {
      "epoch": 1.8936846887778283,
      "grad_norm": 7.305987358093262,
      "learning_rate": 0.00037173813520524954,
      "loss": 0.4572,
      "step": 4168
    },
    {
      "epoch": 1.894139027714675,
      "grad_norm": 4.44926643371582,
      "learning_rate": 0.0003715855333435068,
      "loss": 0.2513,
      "step": 4169
    },
    {
      "epoch": 1.894593366651522,
      "grad_norm": 3.7112877368927,
      "learning_rate": 0.0003714329314817641,
      "loss": 0.1685,
      "step": 4170
    },
    {
      "epoch": 1.895047705588369,
      "grad_norm": 5.001777172088623,
      "learning_rate": 0.0003712803296200214,
      "loss": 0.281,
      "step": 4171
    },
    {
      "epoch": 1.8955020445252158,
      "grad_norm": 1.3740217685699463,
      "learning_rate": 0.0003711277277582786,
      "loss": 0.0695,
      "step": 4172
    },
    {
      "epoch": 1.8959563834620627,
      "grad_norm": 3.852717876434326,
      "learning_rate": 0.00037097512589653593,
      "loss": 0.2926,
      "step": 4173
    },
    {
      "epoch": 1.8964107223989095,
      "grad_norm": 4.104159355163574,
      "learning_rate": 0.00037082252403479324,
      "loss": 0.602,
      "step": 4174
    },
    {
      "epoch": 1.8968650613357565,
      "grad_norm": 4.55597448348999,
      "learning_rate": 0.0003706699221730505,
      "loss": 0.3183,
      "step": 4175
    },
    {
      "epoch": 1.8973194002726035,
      "grad_norm": 4.512195587158203,
      "learning_rate": 0.0003705173203113078,
      "loss": 0.2655,
      "step": 4176
    },
    {
      "epoch": 1.8977737392094502,
      "grad_norm": 6.07098913192749,
      "learning_rate": 0.0003703647184495651,
      "loss": 1.1764,
      "step": 4177
    },
    {
      "epoch": 1.898228078146297,
      "grad_norm": 5.451591968536377,
      "learning_rate": 0.0003702121165878224,
      "loss": 0.22,
      "step": 4178
    },
    {
      "epoch": 1.898682417083144,
      "grad_norm": 4.318721294403076,
      "learning_rate": 0.0003700595147260797,
      "loss": 0.2591,
      "step": 4179
    },
    {
      "epoch": 1.899136756019991,
      "grad_norm": 7.6418609619140625,
      "learning_rate": 0.000369906912864337,
      "loss": 0.5425,
      "step": 4180
    },
    {
      "epoch": 1.8995910949568378,
      "grad_norm": 3.7793776988983154,
      "learning_rate": 0.0003697543110025942,
      "loss": 0.3243,
      "step": 4181
    },
    {
      "epoch": 1.9000454338936847,
      "grad_norm": 2.4763522148132324,
      "learning_rate": 0.0003696017091408515,
      "loss": 0.1585,
      "step": 4182
    },
    {
      "epoch": 1.9004997728305315,
      "grad_norm": 6.3092522621154785,
      "learning_rate": 0.0003694491072791088,
      "loss": 0.8542,
      "step": 4183
    },
    {
      "epoch": 1.9009541117673785,
      "grad_norm": 5.696770191192627,
      "learning_rate": 0.0003692965054173661,
      "loss": 0.6091,
      "step": 4184
    },
    {
      "epoch": 1.9014084507042255,
      "grad_norm": 4.298449993133545,
      "learning_rate": 0.0003691439035556234,
      "loss": 0.3276,
      "step": 4185
    },
    {
      "epoch": 1.9018627896410722,
      "grad_norm": 7.009204387664795,
      "learning_rate": 0.0003689913016938807,
      "loss": 0.7502,
      "step": 4186
    },
    {
      "epoch": 1.902317128577919,
      "grad_norm": 5.638808727264404,
      "learning_rate": 0.00036883869983213795,
      "loss": 0.9004,
      "step": 4187
    },
    {
      "epoch": 1.902771467514766,
      "grad_norm": 7.024165630340576,
      "learning_rate": 0.00036868609797039526,
      "loss": 0.8338,
      "step": 4188
    },
    {
      "epoch": 1.903225806451613,
      "grad_norm": 4.8945631980896,
      "learning_rate": 0.0003685334961086526,
      "loss": 0.3658,
      "step": 4189
    },
    {
      "epoch": 1.9036801453884598,
      "grad_norm": 6.013129711151123,
      "learning_rate": 0.0003683808942469098,
      "loss": 0.9844,
      "step": 4190
    },
    {
      "epoch": 1.9041344843253065,
      "grad_norm": 3.226829767227173,
      "learning_rate": 0.0003682282923851671,
      "loss": 0.2101,
      "step": 4191
    },
    {
      "epoch": 1.9045888232621535,
      "grad_norm": 10.306405067443848,
      "learning_rate": 0.0003680756905234244,
      "loss": 1.9054,
      "step": 4192
    },
    {
      "epoch": 1.9050431621990005,
      "grad_norm": 7.145691871643066,
      "learning_rate": 0.00036792308866168166,
      "loss": 0.7518,
      "step": 4193
    },
    {
      "epoch": 1.9054975011358475,
      "grad_norm": 2.9393327236175537,
      "learning_rate": 0.00036777048679993897,
      "loss": 0.2056,
      "step": 4194
    },
    {
      "epoch": 1.9059518400726942,
      "grad_norm": 4.736395359039307,
      "learning_rate": 0.0003676178849381963,
      "loss": 0.3844,
      "step": 4195
    },
    {
      "epoch": 1.906406179009541,
      "grad_norm": 6.15548849105835,
      "learning_rate": 0.00036746528307645353,
      "loss": 0.3681,
      "step": 4196
    },
    {
      "epoch": 1.906860517946388,
      "grad_norm": 4.2660017013549805,
      "learning_rate": 0.00036731268121471084,
      "loss": 0.2135,
      "step": 4197
    },
    {
      "epoch": 1.907314856883235,
      "grad_norm": 4.078342437744141,
      "learning_rate": 0.00036716007935296815,
      "loss": 0.5186,
      "step": 4198
    },
    {
      "epoch": 1.9077691958200818,
      "grad_norm": 2.441058874130249,
      "learning_rate": 0.0003670074774912254,
      "loss": 0.087,
      "step": 4199
    },
    {
      "epoch": 1.9082235347569285,
      "grad_norm": 7.825473785400391,
      "learning_rate": 0.00036685487562948267,
      "loss": 0.4755,
      "step": 4200
    },
    {
      "epoch": 1.9086778736937755,
      "grad_norm": 3.9590439796447754,
      "learning_rate": 0.00036670227376774,
      "loss": 0.2751,
      "step": 4201
    },
    {
      "epoch": 1.9091322126306225,
      "grad_norm": 0.9261434674263,
      "learning_rate": 0.00036654967190599723,
      "loss": 0.0503,
      "step": 4202
    },
    {
      "epoch": 1.9095865515674695,
      "grad_norm": 3.666656255722046,
      "learning_rate": 0.00036639707004425454,
      "loss": 0.2003,
      "step": 4203
    },
    {
      "epoch": 1.9100408905043162,
      "grad_norm": 5.187392234802246,
      "learning_rate": 0.00036624446818251186,
      "loss": 0.1974,
      "step": 4204
    },
    {
      "epoch": 1.910495229441163,
      "grad_norm": 4.046458721160889,
      "learning_rate": 0.0003660918663207691,
      "loss": 0.4735,
      "step": 4205
    },
    {
      "epoch": 1.91094956837801,
      "grad_norm": 6.462405681610107,
      "learning_rate": 0.0003659392644590264,
      "loss": 0.4707,
      "step": 4206
    },
    {
      "epoch": 1.911403907314857,
      "grad_norm": 11.062278747558594,
      "learning_rate": 0.00036578666259728373,
      "loss": 0.6521,
      "step": 4207
    },
    {
      "epoch": 1.9118582462517038,
      "grad_norm": 4.897199630737305,
      "learning_rate": 0.000365634060735541,
      "loss": 0.6356,
      "step": 4208
    },
    {
      "epoch": 1.9123125851885505,
      "grad_norm": 3.634993314743042,
      "learning_rate": 0.00036548145887379825,
      "loss": 0.4031,
      "step": 4209
    },
    {
      "epoch": 1.9127669241253975,
      "grad_norm": 5.90142822265625,
      "learning_rate": 0.00036532885701205556,
      "loss": 0.462,
      "step": 4210
    },
    {
      "epoch": 1.9132212630622445,
      "grad_norm": 2.4161264896392822,
      "learning_rate": 0.0003651762551503128,
      "loss": 0.1121,
      "step": 4211
    },
    {
      "epoch": 1.9136756019990915,
      "grad_norm": 4.879628658294678,
      "learning_rate": 0.0003650236532885701,
      "loss": 0.6152,
      "step": 4212
    },
    {
      "epoch": 1.9141299409359382,
      "grad_norm": 8.671652793884277,
      "learning_rate": 0.00036487105142682743,
      "loss": 1.1086,
      "step": 4213
    },
    {
      "epoch": 1.914584279872785,
      "grad_norm": 3.1741349697113037,
      "learning_rate": 0.0003647184495650847,
      "loss": 0.2788,
      "step": 4214
    },
    {
      "epoch": 1.915038618809632,
      "grad_norm": 4.728796482086182,
      "learning_rate": 0.000364565847703342,
      "loss": 0.2151,
      "step": 4215
    },
    {
      "epoch": 1.915492957746479,
      "grad_norm": 5.455236434936523,
      "learning_rate": 0.0003644132458415993,
      "loss": 0.443,
      "step": 4216
    },
    {
      "epoch": 1.9159472966833258,
      "grad_norm": 3.7303433418273926,
      "learning_rate": 0.00036426064397985657,
      "loss": 0.3902,
      "step": 4217
    },
    {
      "epoch": 1.9164016356201725,
      "grad_norm": 7.2760186195373535,
      "learning_rate": 0.0003641080421181138,
      "loss": 0.5678,
      "step": 4218
    },
    {
      "epoch": 1.9168559745570195,
      "grad_norm": 2.396240711212158,
      "learning_rate": 0.00036395544025637113,
      "loss": 0.1988,
      "step": 4219
    },
    {
      "epoch": 1.9173103134938665,
      "grad_norm": 4.862365245819092,
      "learning_rate": 0.0003638028383946284,
      "loss": 0.3061,
      "step": 4220
    },
    {
      "epoch": 1.9177646524307133,
      "grad_norm": 6.083429336547852,
      "learning_rate": 0.0003636502365328857,
      "loss": 0.551,
      "step": 4221
    },
    {
      "epoch": 1.9182189913675602,
      "grad_norm": 9.116913795471191,
      "learning_rate": 0.000363497634671143,
      "loss": 0.3463,
      "step": 4222
    },
    {
      "epoch": 1.918673330304407,
      "grad_norm": 5.876121997833252,
      "learning_rate": 0.00036334503280940027,
      "loss": 0.4183,
      "step": 4223
    },
    {
      "epoch": 1.919127669241254,
      "grad_norm": 3.9024839401245117,
      "learning_rate": 0.0003631924309476576,
      "loss": 0.2457,
      "step": 4224
    },
    {
      "epoch": 1.919582008178101,
      "grad_norm": 4.497193813323975,
      "learning_rate": 0.0003630398290859149,
      "loss": 0.2561,
      "step": 4225
    },
    {
      "epoch": 1.9200363471149478,
      "grad_norm": 7.652925491333008,
      "learning_rate": 0.00036288722722417215,
      "loss": 0.3683,
      "step": 4226
    },
    {
      "epoch": 1.9204906860517945,
      "grad_norm": 7.153951168060303,
      "learning_rate": 0.0003627346253624294,
      "loss": 0.5076,
      "step": 4227
    },
    {
      "epoch": 1.9209450249886415,
      "grad_norm": 2.2370989322662354,
      "learning_rate": 0.0003625820235006867,
      "loss": 0.126,
      "step": 4228
    },
    {
      "epoch": 1.9213993639254885,
      "grad_norm": 9.982087135314941,
      "learning_rate": 0.00036242942163894397,
      "loss": 1.0112,
      "step": 4229
    },
    {
      "epoch": 1.9218537028623353,
      "grad_norm": 3.212604284286499,
      "learning_rate": 0.0003622768197772013,
      "loss": 0.252,
      "step": 4230
    },
    {
      "epoch": 1.9223080417991822,
      "grad_norm": 10.880035400390625,
      "learning_rate": 0.0003621242179154586,
      "loss": 1.6797,
      "step": 4231
    },
    {
      "epoch": 1.922762380736029,
      "grad_norm": 4.523013591766357,
      "learning_rate": 0.00036197161605371585,
      "loss": 0.3914,
      "step": 4232
    },
    {
      "epoch": 1.923216719672876,
      "grad_norm": 2.0948574542999268,
      "learning_rate": 0.00036181901419197316,
      "loss": 0.1394,
      "step": 4233
    },
    {
      "epoch": 1.923671058609723,
      "grad_norm": 8.262796401977539,
      "learning_rate": 0.00036166641233023047,
      "loss": 0.6897,
      "step": 4234
    },
    {
      "epoch": 1.9241253975465697,
      "grad_norm": 5.267937660217285,
      "learning_rate": 0.0003615138104684877,
      "loss": 0.3741,
      "step": 4235
    },
    {
      "epoch": 1.9245797364834165,
      "grad_norm": 3.771510601043701,
      "learning_rate": 0.000361361208606745,
      "loss": 0.3543,
      "step": 4236
    },
    {
      "epoch": 1.9250340754202635,
      "grad_norm": 3.2272605895996094,
      "learning_rate": 0.0003612086067450023,
      "loss": 0.2696,
      "step": 4237
    },
    {
      "epoch": 1.9254884143571105,
      "grad_norm": 8.9231595993042,
      "learning_rate": 0.00036105600488325955,
      "loss": 1.3505,
      "step": 4238
    },
    {
      "epoch": 1.9259427532939573,
      "grad_norm": 3.3201661109924316,
      "learning_rate": 0.00036090340302151686,
      "loss": 0.158,
      "step": 4239
    },
    {
      "epoch": 1.926397092230804,
      "grad_norm": 5.026463031768799,
      "learning_rate": 0.00036075080115977417,
      "loss": 0.2252,
      "step": 4240
    },
    {
      "epoch": 1.926851431167651,
      "grad_norm": 5.327439308166504,
      "learning_rate": 0.0003605981992980314,
      "loss": 0.3917,
      "step": 4241
    },
    {
      "epoch": 1.927305770104498,
      "grad_norm": 4.856227874755859,
      "learning_rate": 0.00036044559743628874,
      "loss": 0.1599,
      "step": 4242
    },
    {
      "epoch": 1.927760109041345,
      "grad_norm": 16.22356605529785,
      "learning_rate": 0.00036029299557454605,
      "loss": 0.9528,
      "step": 4243
    },
    {
      "epoch": 1.9282144479781917,
      "grad_norm": 4.561751842498779,
      "learning_rate": 0.0003601403937128033,
      "loss": 0.3242,
      "step": 4244
    },
    {
      "epoch": 1.9286687869150385,
      "grad_norm": 7.179653644561768,
      "learning_rate": 0.00035998779185106056,
      "loss": 0.2835,
      "step": 4245
    },
    {
      "epoch": 1.9291231258518855,
      "grad_norm": 3.966215133666992,
      "learning_rate": 0.00035983518998931787,
      "loss": 0.3172,
      "step": 4246
    },
    {
      "epoch": 1.9295774647887325,
      "grad_norm": 3.3793320655822754,
      "learning_rate": 0.0003596825881275751,
      "loss": 0.3155,
      "step": 4247
    },
    {
      "epoch": 1.9300318037255793,
      "grad_norm": 4.493769645690918,
      "learning_rate": 0.00035952998626583244,
      "loss": 0.2651,
      "step": 4248
    },
    {
      "epoch": 1.930486142662426,
      "grad_norm": 4.8708882331848145,
      "learning_rate": 0.00035937738440408975,
      "loss": 0.3358,
      "step": 4249
    },
    {
      "epoch": 1.930940481599273,
      "grad_norm": 3.4621217250823975,
      "learning_rate": 0.000359224782542347,
      "loss": 0.228,
      "step": 4250
    },
    {
      "epoch": 1.93139482053612,
      "grad_norm": 6.671980857849121,
      "learning_rate": 0.0003590721806806043,
      "loss": 0.7282,
      "step": 4251
    },
    {
      "epoch": 1.931849159472967,
      "grad_norm": 4.644718170166016,
      "learning_rate": 0.0003589195788188616,
      "loss": 0.2822,
      "step": 4252
    },
    {
      "epoch": 1.9323034984098137,
      "grad_norm": 6.471307754516602,
      "learning_rate": 0.0003587669769571189,
      "loss": 0.4316,
      "step": 4253
    },
    {
      "epoch": 1.9327578373466605,
      "grad_norm": 7.066520690917969,
      "learning_rate": 0.00035861437509537614,
      "loss": 0.331,
      "step": 4254
    },
    {
      "epoch": 1.9332121762835075,
      "grad_norm": 6.651778697967529,
      "learning_rate": 0.00035846177323363345,
      "loss": 0.5365,
      "step": 4255
    },
    {
      "epoch": 1.9336665152203545,
      "grad_norm": 5.856362342834473,
      "learning_rate": 0.0003583091713718907,
      "loss": 1.0402,
      "step": 4256
    },
    {
      "epoch": 1.9341208541572013,
      "grad_norm": 4.139021396636963,
      "learning_rate": 0.000358156569510148,
      "loss": 0.2114,
      "step": 4257
    },
    {
      "epoch": 1.934575193094048,
      "grad_norm": 5.84949254989624,
      "learning_rate": 0.0003580039676484053,
      "loss": 0.4513,
      "step": 4258
    },
    {
      "epoch": 1.935029532030895,
      "grad_norm": 4.404299259185791,
      "learning_rate": 0.0003578513657866626,
      "loss": 0.5645,
      "step": 4259
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 5.882611274719238,
      "learning_rate": 0.0003576987639249199,
      "loss": 0.6267,
      "step": 4260
    },
    {
      "epoch": 1.935938209904589,
      "grad_norm": 3.0525546073913574,
      "learning_rate": 0.0003575461620631772,
      "loss": 0.1335,
      "step": 4261
    },
    {
      "epoch": 1.9363925488414357,
      "grad_norm": 7.542099952697754,
      "learning_rate": 0.00035739356020143446,
      "loss": 1.3985,
      "step": 4262
    },
    {
      "epoch": 1.9368468877782825,
      "grad_norm": 4.042819976806641,
      "learning_rate": 0.0003572409583396917,
      "loss": 0.6035,
      "step": 4263
    },
    {
      "epoch": 1.9373012267151295,
      "grad_norm": 2.5467960834503174,
      "learning_rate": 0.00035708835647794903,
      "loss": 0.1308,
      "step": 4264
    },
    {
      "epoch": 1.9377555656519765,
      "grad_norm": 8.012664794921875,
      "learning_rate": 0.0003569357546162063,
      "loss": 0.6007,
      "step": 4265
    },
    {
      "epoch": 1.9382099045888233,
      "grad_norm": 7.099404335021973,
      "learning_rate": 0.0003567831527544636,
      "loss": 0.556,
      "step": 4266
    },
    {
      "epoch": 1.93866424352567,
      "grad_norm": 4.345366477966309,
      "learning_rate": 0.0003566305508927209,
      "loss": 0.4076,
      "step": 4267
    },
    {
      "epoch": 1.939118582462517,
      "grad_norm": 2.3702609539031982,
      "learning_rate": 0.00035647794903097816,
      "loss": 0.0719,
      "step": 4268
    },
    {
      "epoch": 1.939572921399364,
      "grad_norm": 2.6138291358947754,
      "learning_rate": 0.00035632534716923547,
      "loss": 0.0678,
      "step": 4269
    },
    {
      "epoch": 1.940027260336211,
      "grad_norm": 2.164841890335083,
      "learning_rate": 0.0003561727453074928,
      "loss": 0.1549,
      "step": 4270
    },
    {
      "epoch": 1.9404815992730577,
      "grad_norm": 3.9905734062194824,
      "learning_rate": 0.0003560201434457501,
      "loss": 0.2433,
      "step": 4271
    },
    {
      "epoch": 1.9409359382099045,
      "grad_norm": 8.535024642944336,
      "learning_rate": 0.00035586754158400735,
      "loss": 1.1023,
      "step": 4272
    },
    {
      "epoch": 1.9413902771467515,
      "grad_norm": 4.759146213531494,
      "learning_rate": 0.0003557149397222646,
      "loss": 0.4259,
      "step": 4273
    },
    {
      "epoch": 1.9418446160835985,
      "grad_norm": 4.177338123321533,
      "learning_rate": 0.00035556233786052186,
      "loss": 0.4748,
      "step": 4274
    },
    {
      "epoch": 1.9422989550204453,
      "grad_norm": 5.0812249183654785,
      "learning_rate": 0.0003554097359987792,
      "loss": 0.1802,
      "step": 4275
    },
    {
      "epoch": 1.942753293957292,
      "grad_norm": 2.6944985389709473,
      "learning_rate": 0.0003552571341370365,
      "loss": 0.2116,
      "step": 4276
    },
    {
      "epoch": 1.943207632894139,
      "grad_norm": 6.655102729797363,
      "learning_rate": 0.0003551045322752938,
      "loss": 0.8495,
      "step": 4277
    },
    {
      "epoch": 1.943661971830986,
      "grad_norm": 1.9117997884750366,
      "learning_rate": 0.00035495193041355105,
      "loss": 0.155,
      "step": 4278
    },
    {
      "epoch": 1.9441163107678328,
      "grad_norm": 4.4777750968933105,
      "learning_rate": 0.00035479932855180836,
      "loss": 0.4323,
      "step": 4279
    },
    {
      "epoch": 1.9445706497046797,
      "grad_norm": 2.712331533432007,
      "learning_rate": 0.00035464672669006567,
      "loss": 0.1337,
      "step": 4280
    },
    {
      "epoch": 1.9450249886415265,
      "grad_norm": 3.9792048931121826,
      "learning_rate": 0.00035449412482832293,
      "loss": 0.1053,
      "step": 4281
    },
    {
      "epoch": 1.9454793275783735,
      "grad_norm": 4.008458137512207,
      "learning_rate": 0.0003543415229665802,
      "loss": 0.1794,
      "step": 4282
    },
    {
      "epoch": 1.9459336665152205,
      "grad_norm": 3.6667582988739014,
      "learning_rate": 0.0003541889211048375,
      "loss": 0.4401,
      "step": 4283
    },
    {
      "epoch": 1.9463880054520672,
      "grad_norm": 5.385976791381836,
      "learning_rate": 0.00035403631924309475,
      "loss": 0.549,
      "step": 4284
    },
    {
      "epoch": 1.946842344388914,
      "grad_norm": 4.697057723999023,
      "learning_rate": 0.00035388371738135206,
      "loss": 0.3985,
      "step": 4285
    },
    {
      "epoch": 1.947296683325761,
      "grad_norm": 4.376003265380859,
      "learning_rate": 0.0003537311155196094,
      "loss": 0.4645,
      "step": 4286
    },
    {
      "epoch": 1.947751022262608,
      "grad_norm": 3.3848342895507812,
      "learning_rate": 0.00035357851365786663,
      "loss": 0.1118,
      "step": 4287
    },
    {
      "epoch": 1.9482053611994548,
      "grad_norm": 6.628559112548828,
      "learning_rate": 0.00035342591179612394,
      "loss": 0.4042,
      "step": 4288
    },
    {
      "epoch": 1.9486597001363015,
      "grad_norm": 5.160183429718018,
      "learning_rate": 0.00035327330993438125,
      "loss": 0.4122,
      "step": 4289
    },
    {
      "epoch": 1.9491140390731485,
      "grad_norm": 2.460038900375366,
      "learning_rate": 0.0003531207080726385,
      "loss": 0.2379,
      "step": 4290
    },
    {
      "epoch": 1.9495683780099955,
      "grad_norm": 4.020959377288818,
      "learning_rate": 0.00035296810621089576,
      "loss": 0.4996,
      "step": 4291
    },
    {
      "epoch": 1.9500227169468425,
      "grad_norm": 5.877308368682861,
      "learning_rate": 0.0003528155043491531,
      "loss": 0.2174,
      "step": 4292
    },
    {
      "epoch": 1.9504770558836892,
      "grad_norm": 8.680621147155762,
      "learning_rate": 0.00035266290248741033,
      "loss": 0.4821,
      "step": 4293
    },
    {
      "epoch": 1.950931394820536,
      "grad_norm": 7.930365085601807,
      "learning_rate": 0.00035251030062566764,
      "loss": 0.6581,
      "step": 4294
    },
    {
      "epoch": 1.951385733757383,
      "grad_norm": 1.9488462209701538,
      "learning_rate": 0.00035235769876392495,
      "loss": 0.1187,
      "step": 4295
    },
    {
      "epoch": 1.95184007269423,
      "grad_norm": 3.6552741527557373,
      "learning_rate": 0.0003522050969021822,
      "loss": 0.1206,
      "step": 4296
    },
    {
      "epoch": 1.9522944116310768,
      "grad_norm": 9.07710075378418,
      "learning_rate": 0.0003520524950404395,
      "loss": 0.5402,
      "step": 4297
    },
    {
      "epoch": 1.9527487505679235,
      "grad_norm": 7.762645244598389,
      "learning_rate": 0.00035189989317869683,
      "loss": 0.4602,
      "step": 4298
    },
    {
      "epoch": 1.9532030895047705,
      "grad_norm": 4.465040683746338,
      "learning_rate": 0.0003517472913169541,
      "loss": 0.3013,
      "step": 4299
    },
    {
      "epoch": 1.9536574284416175,
      "grad_norm": 3.9284000396728516,
      "learning_rate": 0.00035159468945521134,
      "loss": 0.0497,
      "step": 4300
    },
    {
      "epoch": 1.9541117673784645,
      "grad_norm": 5.083382606506348,
      "learning_rate": 0.00035144208759346865,
      "loss": 0.3896,
      "step": 4301
    },
    {
      "epoch": 1.9545661063153112,
      "grad_norm": 8.609477996826172,
      "learning_rate": 0.0003512894857317259,
      "loss": 0.7931,
      "step": 4302
    },
    {
      "epoch": 1.955020445252158,
      "grad_norm": 5.1362624168396,
      "learning_rate": 0.0003511368838699832,
      "loss": 0.522,
      "step": 4303
    },
    {
      "epoch": 1.955474784189005,
      "grad_norm": 2.311687469482422,
      "learning_rate": 0.00035098428200824053,
      "loss": 0.3396,
      "step": 4304
    },
    {
      "epoch": 1.955929123125852,
      "grad_norm": 2.7243194580078125,
      "learning_rate": 0.0003508316801464978,
      "loss": 0.1929,
      "step": 4305
    },
    {
      "epoch": 1.9563834620626988,
      "grad_norm": 5.244897365570068,
      "learning_rate": 0.0003506790782847551,
      "loss": 0.581,
      "step": 4306
    },
    {
      "epoch": 1.9568378009995455,
      "grad_norm": 8.652981758117676,
      "learning_rate": 0.0003505264764230124,
      "loss": 0.5454,
      "step": 4307
    },
    {
      "epoch": 1.9572921399363925,
      "grad_norm": 3.7973015308380127,
      "learning_rate": 0.00035037387456126966,
      "loss": 0.3893,
      "step": 4308
    },
    {
      "epoch": 1.9577464788732395,
      "grad_norm": 4.135213375091553,
      "learning_rate": 0.0003502212726995269,
      "loss": 0.2759,
      "step": 4309
    },
    {
      "epoch": 1.9582008178100865,
      "grad_norm": 5.457112789154053,
      "learning_rate": 0.00035006867083778423,
      "loss": 0.3261,
      "step": 4310
    },
    {
      "epoch": 1.9586551567469332,
      "grad_norm": 3.3384761810302734,
      "learning_rate": 0.0003499160689760415,
      "loss": 0.1405,
      "step": 4311
    },
    {
      "epoch": 1.95910949568378,
      "grad_norm": 3.0458803176879883,
      "learning_rate": 0.0003497634671142988,
      "loss": 0.1337,
      "step": 4312
    },
    {
      "epoch": 1.959563834620627,
      "grad_norm": 5.588751792907715,
      "learning_rate": 0.0003496108652525561,
      "loss": 0.4365,
      "step": 4313
    },
    {
      "epoch": 1.960018173557474,
      "grad_norm": 7.84128475189209,
      "learning_rate": 0.00034945826339081337,
      "loss": 0.5673,
      "step": 4314
    },
    {
      "epoch": 1.9604725124943208,
      "grad_norm": 3.1886672973632812,
      "learning_rate": 0.0003493056615290707,
      "loss": 0.1228,
      "step": 4315
    },
    {
      "epoch": 1.9609268514311675,
      "grad_norm": 4.919764995574951,
      "learning_rate": 0.000349153059667328,
      "loss": 0.3897,
      "step": 4316
    },
    {
      "epoch": 1.9613811903680145,
      "grad_norm": 5.552041053771973,
      "learning_rate": 0.00034900045780558524,
      "loss": 0.1898,
      "step": 4317
    },
    {
      "epoch": 1.9618355293048615,
      "grad_norm": 5.126004219055176,
      "learning_rate": 0.0003488478559438425,
      "loss": 0.3825,
      "step": 4318
    },
    {
      "epoch": 1.9622898682417085,
      "grad_norm": 3.8768138885498047,
      "learning_rate": 0.0003486952540820998,
      "loss": 0.4707,
      "step": 4319
    },
    {
      "epoch": 1.9627442071785552,
      "grad_norm": 3.1156094074249268,
      "learning_rate": 0.00034854265222035707,
      "loss": 0.0917,
      "step": 4320
    },
    {
      "epoch": 1.963198546115402,
      "grad_norm": 4.119638919830322,
      "learning_rate": 0.0003483900503586144,
      "loss": 0.7184,
      "step": 4321
    },
    {
      "epoch": 1.963652885052249,
      "grad_norm": 4.037336349487305,
      "learning_rate": 0.0003482374484968717,
      "loss": 0.3205,
      "step": 4322
    },
    {
      "epoch": 1.964107223989096,
      "grad_norm": 7.855966091156006,
      "learning_rate": 0.00034808484663512894,
      "loss": 0.5854,
      "step": 4323
    },
    {
      "epoch": 1.9645615629259428,
      "grad_norm": 5.780165195465088,
      "learning_rate": 0.00034793224477338625,
      "loss": 0.3858,
      "step": 4324
    },
    {
      "epoch": 1.9650159018627895,
      "grad_norm": 4.854145526885986,
      "learning_rate": 0.00034777964291164356,
      "loss": 0.3353,
      "step": 4325
    },
    {
      "epoch": 1.9654702407996365,
      "grad_norm": 3.5685994625091553,
      "learning_rate": 0.0003476270410499008,
      "loss": 0.6221,
      "step": 4326
    },
    {
      "epoch": 1.9659245797364835,
      "grad_norm": 5.457813262939453,
      "learning_rate": 0.0003474744391881581,
      "loss": 0.1843,
      "step": 4327
    },
    {
      "epoch": 1.9663789186733303,
      "grad_norm": 4.562656402587891,
      "learning_rate": 0.0003473218373264154,
      "loss": 0.3775,
      "step": 4328
    },
    {
      "epoch": 1.9668332576101772,
      "grad_norm": 3.6220436096191406,
      "learning_rate": 0.00034716923546467264,
      "loss": 0.3511,
      "step": 4329
    },
    {
      "epoch": 1.967287596547024,
      "grad_norm": 3.978546142578125,
      "learning_rate": 0.00034701663360292996,
      "loss": 0.3068,
      "step": 4330
    },
    {
      "epoch": 1.967741935483871,
      "grad_norm": 4.77462911605835,
      "learning_rate": 0.00034686403174118727,
      "loss": 0.2443,
      "step": 4331
    },
    {
      "epoch": 1.968196274420718,
      "grad_norm": 5.934150695800781,
      "learning_rate": 0.0003467114298794445,
      "loss": 0.6776,
      "step": 4332
    },
    {
      "epoch": 1.9686506133575647,
      "grad_norm": 3.4582152366638184,
      "learning_rate": 0.00034655882801770183,
      "loss": 0.3112,
      "step": 4333
    },
    {
      "epoch": 1.9691049522944115,
      "grad_norm": 4.762792587280273,
      "learning_rate": 0.00034640622615595914,
      "loss": 0.4281,
      "step": 4334
    },
    {
      "epoch": 1.9695592912312585,
      "grad_norm": 6.88860559463501,
      "learning_rate": 0.0003462536242942164,
      "loss": 0.4949,
      "step": 4335
    },
    {
      "epoch": 1.9700136301681055,
      "grad_norm": 7.156238079071045,
      "learning_rate": 0.0003461010224324737,
      "loss": 0.4008,
      "step": 4336
    },
    {
      "epoch": 1.9704679691049523,
      "grad_norm": 3.994053363800049,
      "learning_rate": 0.00034594842057073097,
      "loss": 0.4636,
      "step": 4337
    },
    {
      "epoch": 1.970922308041799,
      "grad_norm": 5.697543621063232,
      "learning_rate": 0.0003457958187089882,
      "loss": 0.5017,
      "step": 4338
    },
    {
      "epoch": 1.971376646978646,
      "grad_norm": 4.529122352600098,
      "learning_rate": 0.00034564321684724553,
      "loss": 0.4237,
      "step": 4339
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 1.0059764385223389,
      "learning_rate": 0.00034549061498550284,
      "loss": 0.0234,
      "step": 4340
    },
    {
      "epoch": 1.97228532485234,
      "grad_norm": 2.468740940093994,
      "learning_rate": 0.0003453380131237601,
      "loss": 0.1847,
      "step": 4341
    },
    {
      "epoch": 1.9727396637891867,
      "grad_norm": 0.7695100903511047,
      "learning_rate": 0.0003451854112620174,
      "loss": 0.0358,
      "step": 4342
    },
    {
      "epoch": 1.9731940027260335,
      "grad_norm": 2.573155403137207,
      "learning_rate": 0.0003450328094002747,
      "loss": 0.1258,
      "step": 4343
    },
    {
      "epoch": 1.9736483416628805,
      "grad_norm": 3.875619649887085,
      "learning_rate": 0.000344880207538532,
      "loss": 0.1585,
      "step": 4344
    },
    {
      "epoch": 1.9741026805997275,
      "grad_norm": 4.515929698944092,
      "learning_rate": 0.0003447276056767893,
      "loss": 0.5151,
      "step": 4345
    },
    {
      "epoch": 1.9745570195365743,
      "grad_norm": 4.004413604736328,
      "learning_rate": 0.00034457500381504655,
      "loss": 0.2485,
      "step": 4346
    },
    {
      "epoch": 1.975011358473421,
      "grad_norm": 5.660634517669678,
      "learning_rate": 0.0003444224019533038,
      "loss": 0.6478,
      "step": 4347
    },
    {
      "epoch": 1.975465697410268,
      "grad_norm": 7.113178253173828,
      "learning_rate": 0.0003442698000915611,
      "loss": 0.5945,
      "step": 4348
    },
    {
      "epoch": 1.975920036347115,
      "grad_norm": 3.183366298675537,
      "learning_rate": 0.0003441171982298184,
      "loss": 0.2095,
      "step": 4349
    },
    {
      "epoch": 1.976374375283962,
      "grad_norm": 2.9333856105804443,
      "learning_rate": 0.0003439645963680757,
      "loss": 0.2437,
      "step": 4350
    },
    {
      "epoch": 1.9768287142208087,
      "grad_norm": 3.146101236343384,
      "learning_rate": 0.000343811994506333,
      "loss": 0.3078,
      "step": 4351
    },
    {
      "epoch": 1.9772830531576555,
      "grad_norm": 6.374154567718506,
      "learning_rate": 0.0003436593926445903,
      "loss": 0.7659,
      "step": 4352
    },
    {
      "epoch": 1.9777373920945025,
      "grad_norm": 3.8769726753234863,
      "learning_rate": 0.00034350679078284756,
      "loss": 0.1907,
      "step": 4353
    },
    {
      "epoch": 1.9781917310313495,
      "grad_norm": 8.657648086547852,
      "learning_rate": 0.00034335418892110487,
      "loss": 1.1092,
      "step": 4354
    },
    {
      "epoch": 1.9786460699681963,
      "grad_norm": 5.431729316711426,
      "learning_rate": 0.0003432015870593621,
      "loss": 0.4677,
      "step": 4355
    },
    {
      "epoch": 1.979100408905043,
      "grad_norm": 3.7931301593780518,
      "learning_rate": 0.0003430489851976194,
      "loss": 0.434,
      "step": 4356
    },
    {
      "epoch": 1.97955474784189,
      "grad_norm": 5.814610958099365,
      "learning_rate": 0.0003428963833358767,
      "loss": 0.5903,
      "step": 4357
    },
    {
      "epoch": 1.980009086778737,
      "grad_norm": 7.8944292068481445,
      "learning_rate": 0.000342743781474134,
      "loss": 0.2934,
      "step": 4358
    },
    {
      "epoch": 1.980463425715584,
      "grad_norm": 7.006594657897949,
      "learning_rate": 0.00034259117961239126,
      "loss": 0.9012,
      "step": 4359
    },
    {
      "epoch": 1.9809177646524307,
      "grad_norm": 1.896960735321045,
      "learning_rate": 0.00034243857775064857,
      "loss": 0.0904,
      "step": 4360
    },
    {
      "epoch": 1.9813721035892775,
      "grad_norm": 3.2279839515686035,
      "learning_rate": 0.0003422859758889059,
      "loss": 0.2095,
      "step": 4361
    },
    {
      "epoch": 1.9818264425261245,
      "grad_norm": 5.203068733215332,
      "learning_rate": 0.00034213337402716314,
      "loss": 0.2637,
      "step": 4362
    },
    {
      "epoch": 1.9822807814629715,
      "grad_norm": 3.5551884174346924,
      "learning_rate": 0.00034198077216542045,
      "loss": 0.1783,
      "step": 4363
    },
    {
      "epoch": 1.9827351203998183,
      "grad_norm": 4.581389904022217,
      "learning_rate": 0.0003418281703036777,
      "loss": 0.2174,
      "step": 4364
    },
    {
      "epoch": 1.983189459336665,
      "grad_norm": 6.435160160064697,
      "learning_rate": 0.00034167556844193496,
      "loss": 0.2573,
      "step": 4365
    },
    {
      "epoch": 1.983643798273512,
      "grad_norm": 7.918447017669678,
      "learning_rate": 0.00034152296658019227,
      "loss": 1.0461,
      "step": 4366
    },
    {
      "epoch": 1.984098137210359,
      "grad_norm": 5.831407070159912,
      "learning_rate": 0.0003413703647184496,
      "loss": 0.6394,
      "step": 4367
    },
    {
      "epoch": 1.984552476147206,
      "grad_norm": 3.2254092693328857,
      "learning_rate": 0.00034121776285670684,
      "loss": 0.1538,
      "step": 4368
    },
    {
      "epoch": 1.9850068150840527,
      "grad_norm": 6.648301124572754,
      "learning_rate": 0.00034106516099496415,
      "loss": 0.6309,
      "step": 4369
    },
    {
      "epoch": 1.9854611540208995,
      "grad_norm": 2.733196258544922,
      "learning_rate": 0.00034091255913322146,
      "loss": 0.0888,
      "step": 4370
    },
    {
      "epoch": 1.9859154929577465,
      "grad_norm": 8.659801483154297,
      "learning_rate": 0.0003407599572714787,
      "loss": 0.8705,
      "step": 4371
    },
    {
      "epoch": 1.9863698318945935,
      "grad_norm": 3.4426004886627197,
      "learning_rate": 0.000340607355409736,
      "loss": 0.432,
      "step": 4372
    },
    {
      "epoch": 1.9868241708314403,
      "grad_norm": 5.223176002502441,
      "learning_rate": 0.0003404547535479933,
      "loss": 0.3783,
      "step": 4373
    },
    {
      "epoch": 1.987278509768287,
      "grad_norm": 3.801701307296753,
      "learning_rate": 0.00034030215168625054,
      "loss": 0.3647,
      "step": 4374
    },
    {
      "epoch": 1.987732848705134,
      "grad_norm": 3.302407741546631,
      "learning_rate": 0.00034014954982450785,
      "loss": 0.1664,
      "step": 4375
    },
    {
      "epoch": 1.988187187641981,
      "grad_norm": 5.066399097442627,
      "learning_rate": 0.00033999694796276516,
      "loss": 0.1967,
      "step": 4376
    },
    {
      "epoch": 1.9886415265788278,
      "grad_norm": 1.7293699979782104,
      "learning_rate": 0.0003398443461010224,
      "loss": 0.1586,
      "step": 4377
    },
    {
      "epoch": 1.9890958655156747,
      "grad_norm": 9.370141983032227,
      "learning_rate": 0.0003396917442392797,
      "loss": 0.7429,
      "step": 4378
    },
    {
      "epoch": 1.9895502044525215,
      "grad_norm": 6.898975849151611,
      "learning_rate": 0.00033953914237753704,
      "loss": 0.656,
      "step": 4379
    },
    {
      "epoch": 1.9900045433893685,
      "grad_norm": 2.8993523120880127,
      "learning_rate": 0.0003393865405157943,
      "loss": 0.3453,
      "step": 4380
    },
    {
      "epoch": 1.9904588823262155,
      "grad_norm": 3.473750114440918,
      "learning_rate": 0.0003392339386540516,
      "loss": 0.0952,
      "step": 4381
    },
    {
      "epoch": 1.9909132212630622,
      "grad_norm": 3.66723895072937,
      "learning_rate": 0.00033908133679230886,
      "loss": 0.4125,
      "step": 4382
    },
    {
      "epoch": 1.991367560199909,
      "grad_norm": 4.075571060180664,
      "learning_rate": 0.0003389287349305661,
      "loss": 0.3583,
      "step": 4383
    },
    {
      "epoch": 1.991821899136756,
      "grad_norm": 3.1159582138061523,
      "learning_rate": 0.0003387761330688234,
      "loss": 0.2366,
      "step": 4384
    },
    {
      "epoch": 1.992276238073603,
      "grad_norm": 4.320130825042725,
      "learning_rate": 0.00033862353120708074,
      "loss": 0.4886,
      "step": 4385
    },
    {
      "epoch": 1.9927305770104498,
      "grad_norm": 3.1069703102111816,
      "learning_rate": 0.000338470929345338,
      "loss": 0.178,
      "step": 4386
    },
    {
      "epoch": 1.9931849159472965,
      "grad_norm": 8.154047966003418,
      "learning_rate": 0.0003383183274835953,
      "loss": 1.5525,
      "step": 4387
    },
    {
      "epoch": 1.9936392548841435,
      "grad_norm": 5.158902645111084,
      "learning_rate": 0.0003381657256218526,
      "loss": 0.2411,
      "step": 4388
    },
    {
      "epoch": 1.9940935938209905,
      "grad_norm": 5.715418815612793,
      "learning_rate": 0.00033801312376010987,
      "loss": 0.6601,
      "step": 4389
    },
    {
      "epoch": 1.9945479327578375,
      "grad_norm": 0.8925508856773376,
      "learning_rate": 0.0003378605218983672,
      "loss": 0.0329,
      "step": 4390
    },
    {
      "epoch": 1.9950022716946842,
      "grad_norm": 6.11361837387085,
      "learning_rate": 0.00033770792003662444,
      "loss": 0.541,
      "step": 4391
    },
    {
      "epoch": 1.995456610631531,
      "grad_norm": 2.023592472076416,
      "learning_rate": 0.0003375553181748817,
      "loss": 0.1113,
      "step": 4392
    },
    {
      "epoch": 1.995910949568378,
      "grad_norm": 5.16713285446167,
      "learning_rate": 0.000337402716313139,
      "loss": 0.6208,
      "step": 4393
    },
    {
      "epoch": 1.996365288505225,
      "grad_norm": 8.942765235900879,
      "learning_rate": 0.0003372501144513963,
      "loss": 0.6903,
      "step": 4394
    },
    {
      "epoch": 1.9968196274420718,
      "grad_norm": 5.754170894622803,
      "learning_rate": 0.00033709751258965357,
      "loss": 0.6895,
      "step": 4395
    },
    {
      "epoch": 1.9972739663789185,
      "grad_norm": 9.467004776000977,
      "learning_rate": 0.0003369449107279109,
      "loss": 0.2399,
      "step": 4396
    },
    {
      "epoch": 1.9977283053157655,
      "grad_norm": 5.766613006591797,
      "learning_rate": 0.0003367923088661682,
      "loss": 0.7578,
      "step": 4397
    },
    {
      "epoch": 1.9981826442526125,
      "grad_norm": 6.850203514099121,
      "learning_rate": 0.0003366397070044255,
      "loss": 0.5321,
      "step": 4398
    },
    {
      "epoch": 1.9986369831894595,
      "grad_norm": 3.719609260559082,
      "learning_rate": 0.00033648710514268276,
      "loss": 0.2183,
      "step": 4399
    },
    {
      "epoch": 1.9990913221263062,
      "grad_norm": 1.5007572174072266,
      "learning_rate": 0.00033633450328094,
      "loss": 0.0597,
      "step": 4400
    },
    {
      "epoch": 1.999545661063153,
      "grad_norm": 4.677270412445068,
      "learning_rate": 0.0003361819014191973,
      "loss": 0.338,
      "step": 4401
    },
    {
      "epoch": 2.0,
      "grad_norm": 12.20007038116455,
      "learning_rate": 0.0003360292995574546,
      "loss": 0.4016,
      "step": 4402
    }
  ],
  "logging_steps": 1,
  "max_steps": 6603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.12511307776e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
